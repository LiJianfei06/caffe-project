WARNING: Logging before InitGoogleLogging() is written to STDERR
I1012 11:55:10.114845 31118 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I1012 11:55:10.114984 31118 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I1012 11:55:10.114992 31118 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I1012 11:55:10.114996 31118 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I1012 11:55:10.115000 31118 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I1012 11:55:10.115003 31118 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I1012 11:55:10.115067 31118 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I1012 11:55:10.115257 31118 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I1012 11:55:10.117086 31118 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I1012 11:55:10.117127 31118 caffe.cpp:269] Using GPUs 0
I1012 11:55:10.123564 31118 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I1012 11:55:10.818434 31118 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I1012 11:55:10.818485 31118 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I1012 11:55:10.918501 31118 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_SE-ResNet_20.prototxt"
test_net: "./test_SE-ResNet_20.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_SE-ResNet_20"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 2
type: "Nesterov"
I1012 11:55:10.918798 31118 solver.cpp:167] Creating training net from train_net file: ./train_SE-ResNet_20.prototxt
I1012 11:55:10.920122 31118 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_SE-ResNet_20.prototxt
I1012 11:55:10.920148 31118 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I1012 11:55:10.921447 31118 net.cpp:82] Initializing net from parameters: 
name: "ResNet-20"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_Pooling"
  type: "Pooling"
  bottom: "conv2_1_1"
  top: "conv2_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_1_2"
  type: "Convolution"
  bottom: "conv2_1_Pooling"
  top: "conv2_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_ReLU2"
  type: "ReLU"
  bottom: "conv2_1_2"
  top: "conv2_1_2"
}
layer {
  name: "conv2_1_3"
  type: "Convolution"
  bottom: "conv2_1_2"
  top: "conv2_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_Prob3"
  type: "Sigmoid"
  bottom: "conv2_1_3"
  top: "conv2_1_3"
}
layer {
  name: "conv2_Axpy_1"
  type: "Axpy"
  bottom: "conv2_1_3"
  bottom: "conv2_1_1"
  bottom: "conv1"
  top: "conv2_Axpy_1"
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_1"
  top: "conv2_Axpy_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Axpy_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_Pooling"
  type: "Pooling"
  bottom: "conv2_2_1"
  top: "conv2_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_2_2"
  type: "Convolution"
  bottom: "conv2_2_Pooling"
  top: "conv2_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_ReLU2"
  type: "ReLU"
  bottom: "conv2_2_2"
  top: "conv2_2_2"
}
layer {
  name: "conv2_2_3"
  type: "Convolution"
  bottom: "conv2_2_2"
  top: "conv2_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_Prob3"
  type: "Sigmoid"
  bottom: "conv2_2_3"
  top: "conv2_2_3"
}
layer {
  name: "conv2_Axpy_2"
  type: "Axpy"
  bottom: "conv2_2_3"
  bottom: "conv2_2_1"
  bottom: "conv2_Axpy_1"
  top: "conv2_Axpy_2"
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_2"
  top: "conv2_Axpy_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Axpy_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_Pooling"
  type: "Pooling"
  bottom: "conv2_3_1"
  top: "conv2_3_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_3_2"
  type: "Convolution"
  bottom: "conv2_3_Pooling"
  top: "conv2_3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_ReLU2"
  type: "ReLU"
  bottom: "conv2_3_2"
  top: "conv2_3_2"
}
layer {
  name: "conv2_3_3"
  type: "Convolution"
  bottom: "conv2_3_2"
  top: "conv2_3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_Prob3"
  type: "Sigmoid"
  bottom: "conv2_3_3"
  top: "conv2_3_3"
}
layer {
  name: "conv2_Axpy_3"
  type: "Axpy"
  bottom: "conv2_3_3"
  bottom: "conv2_3_1"
  bottom: "conv2_Axpy_2"
  top: "conv2_Axpy_3"
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_3"
  top: "conv2_Axpy_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Axpy_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_Pooling"
  type: "Pooling"
  bottom: "conv3_1_1"
  top: "conv3_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_1_2"
  type: "Convolution"
  bottom: "conv3_1_Pooling"
  top: "conv3_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_ReLU2"
  type: "ReLU"
  bottom: "conv3_1_2"
  top: "conv3_1_2"
}
layer {
  name: "conv3_1_3"
  type: "Convolution"
  bottom: "conv3_1_2"
  top: "conv3_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_Prob3"
  type: "Sigmoid"
  bottom: "conv3_1_3"
  top: "conv3_1_3"
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Axpy_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Axpy_1"
  type: "Axpy"
  bottom: "conv3_1_3"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Axpy_1"
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_1"
  top: "conv3_Axpy_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Axpy_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_Pooling"
  type: "Pooling"
  bottom: "conv3_2_1"
  top: "conv3_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_2_2"
  type: "Convolution"
  bottom: "conv3_2_Pooling"
  top: "conv3_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_ReLU2"
  type: "ReLU"
  bottom: "conv3_2_2"
  top: "conv3_2_2"
}
layer {
  name: "conv3_2_3"
  type: "Convolution"
  bottom: "conv3_2_2"
  top: "conv3_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_Prob3"
  type: "Sigmoid"
  bottom: "conv3_2_3"
  top: "conv3_2_3"
}
layer {
  name: "conv3_Axpy_2"
  type: "Axpy"
  bottom: "conv3_2_3"
  bottom: "conv3_2_1"
  bottom: "conv3_Axpy_1"
  top: "conv3_Axpy_2"
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_2"
  top: "conv3_Axpy_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Axpy_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_Pooling"
  type: "Pooling"
  bottom: "conv3_3_1"
  top: "conv3_3_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_3_2"
  type: "Convolution"
  bottom: "conv3_3_Pooling"
  top: "conv3_3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_ReLU2"
  type: "ReLU"
  bottom: "conv3_3_2"
  top: "conv3_3_2"
}
layer {
  name: "conv3_3_3"
  type: "Convolution"
  bottom: "conv3_3_2"
  top: "conv3_3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_Prob3"
  type: "Sigmoid"
  bottom: "conv3_3_3"
  top: "conv3_3_3"
}
layer {
  name: "conv3_Axpy_3"
  type: "Axpy"
  bottom: "conv3_3_3"
  bottom: "conv3_3_1"
  bottom: "conv3_Axpy_2"
  top: "conv3_Axpy_3"
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_3"
  top: "conv3_Axpy_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Axpy_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_Pooling"
  type: "Pooling"
  bottom: "conv4_1_1"
  top: "conv4_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv4_1_2"
  type: "Convolution"
  bottom: "conv4_1_Pooling"
  top: "conv4_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_ReLU2"
  type: "ReLU"
  bottom: "conv4_1_2"
  top: "conv4_1_2"
}
layer {
  name: "conv4_1_3"
  type: "Convolution"
  bottom: "conv4_1_2"
  top: "conv4_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_Prob3"
  type: "Sigmoid"
  bottom: "conv4_1_3"
  top: "conv4_1_3"
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Axpy_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Axpy_1"
  type: "Axpy"
  bottom: "conv4_1_3"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Axpy_1"
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Axpy_1"
  top: "conv4_Axpy_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Axpy_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_Pooling"
  type: "Pooling"
  bottom: "conv4_2_1"
  top: "conv4_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv4_2_2"
  type: "Convolution"
  bottom: "conv4_2_Pooling"
  top: "conv4_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_ReLU2"
  type: "ReLU"
  bottom: "conv4_2_2"
  top: "conv4_2_2"
}
layer {
  name: "conv4_2_3"
  type: "Convolution"
  bottom: "conv4_2_2"
  top: "conv4_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_Prob3"
  type: "Sigmoid"
  bottom: "conv4_2_3"
  top: "conv4_2_3"
}
layer {
  name: "conv4_Axpy_2"
  type: "Axpy"
  bottom: "conv4_2_3"
  bottom: "conv4_2_1"
  bottom: "conv4_Axpy_1"
  top: "conv4_Axpy_2"
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Axpy_2"
  top: "conv4_Axpy_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Axpy_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  nam
I1012 11:55:10.922159 31118 layer_factory.hpp:77] Creating layer Data1
I1012 11:55:10.922376 31118 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I1012 11:55:10.922420 31118 net.cpp:128] Creating Layer Data1
I1012 11:55:10.922430 31118 net.cpp:522] Data1 -> data
I1012 11:55:10.922461 31118 net.cpp:522] Data1 -> label
I1012 11:55:10.924270 31118 data_layer.cpp:45] output data size: 64,3,32,32
I1012 11:55:10.935792 31118 net.cpp:172] Setting up Data1
I1012 11:55:10.935871 31118 net.cpp:186] Top shape: 64 3 32 32 (196608)
I1012 11:55:10.935890 31118 net.cpp:186] Top shape: 64 (64)
I1012 11:55:10.935904 31118 net.cpp:194] Memory required for data: 786688
I1012 11:55:10.935940 31118 layer_factory.hpp:77] Creating layer conv1
I1012 11:55:10.935989 31118 net.cpp:128] Creating Layer conv1
I1012 11:55:10.936009 31118 net.cpp:558] conv1 <- data
I1012 11:55:10.936045 31118 net.cpp:522] conv1 -> conv1
I1012 11:55:11.971597 31118 net.cpp:172] Setting up conv1
I1012 11:55:11.971653 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.971659 31118 net.cpp:194] Memory required for data: 4980992
I1012 11:55:11.971702 31118 layer_factory.hpp:77] Creating layer conv1/bn
I1012 11:55:11.971722 31118 net.cpp:128] Creating Layer conv1/bn
I1012 11:55:11.971729 31118 net.cpp:558] conv1/bn <- conv1
I1012 11:55:11.971737 31118 net.cpp:509] conv1/bn -> conv1 (in-place)
I1012 11:55:11.972059 31118 net.cpp:172] Setting up conv1/bn
I1012 11:55:11.972086 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.972100 31118 net.cpp:194] Memory required for data: 9175296
I1012 11:55:11.972124 31118 layer_factory.hpp:77] Creating layer conv1/scale
I1012 11:55:11.972144 31118 net.cpp:128] Creating Layer conv1/scale
I1012 11:55:11.972159 31118 net.cpp:558] conv1/scale <- conv1
I1012 11:55:11.972178 31118 net.cpp:509] conv1/scale -> conv1 (in-place)
I1012 11:55:11.972244 31118 layer_factory.hpp:77] Creating layer conv1/scale
I1012 11:55:11.972393 31118 net.cpp:172] Setting up conv1/scale
I1012 11:55:11.972414 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.972426 31118 net.cpp:194] Memory required for data: 13369600
I1012 11:55:11.972445 31118 layer_factory.hpp:77] Creating layer conv1/ReLU
I1012 11:55:11.972463 31118 net.cpp:128] Creating Layer conv1/ReLU
I1012 11:55:11.972476 31118 net.cpp:558] conv1/ReLU <- conv1
I1012 11:55:11.972496 31118 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I1012 11:55:11.973227 31118 net.cpp:172] Setting up conv1/ReLU
I1012 11:55:11.973242 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.973249 31118 net.cpp:194] Memory required for data: 17563904
I1012 11:55:11.973254 31118 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I1012 11:55:11.973263 31118 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I1012 11:55:11.973268 31118 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I1012 11:55:11.973278 31118 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I1012 11:55:11.973289 31118 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I1012 11:55:11.973338 31118 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I1012 11:55:11.973376 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.973393 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.973407 31118 net.cpp:194] Memory required for data: 25952512
I1012 11:55:11.973420 31118 layer_factory.hpp:77] Creating layer conv2_1_0
I1012 11:55:11.973448 31118 net.cpp:128] Creating Layer conv2_1_0
I1012 11:55:11.973462 31118 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I1012 11:55:11.973481 31118 net.cpp:522] conv2_1_0 -> conv2_1_0
I1012 11:55:11.980005 31118 net.cpp:172] Setting up conv2_1_0
I1012 11:55:11.980072 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.980087 31118 net.cpp:194] Memory required for data: 30146816
I1012 11:55:11.980120 31118 layer_factory.hpp:77] Creating layer conv2_1_bn0
I1012 11:55:11.980149 31118 net.cpp:128] Creating Layer conv2_1_bn0
I1012 11:55:11.980166 31118 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I1012 11:55:11.980191 31118 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I1012 11:55:11.980433 31118 net.cpp:172] Setting up conv2_1_bn0
I1012 11:55:11.980456 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.980471 31118 net.cpp:194] Memory required for data: 34341120
I1012 11:55:11.980494 31118 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1012 11:55:11.980517 31118 net.cpp:128] Creating Layer conv2_1_scale0
I1012 11:55:11.980533 31118 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I1012 11:55:11.980552 31118 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I1012 11:55:11.980613 31118 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1012 11:55:11.980775 31118 net.cpp:172] Setting up conv2_1_scale0
I1012 11:55:11.980789 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.980794 31118 net.cpp:194] Memory required for data: 38535424
I1012 11:55:11.980803 31118 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I1012 11:55:11.980810 31118 net.cpp:128] Creating Layer conv2_1_ReLU0
I1012 11:55:11.980834 31118 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I1012 11:55:11.980850 31118 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I1012 11:55:11.982067 31118 net.cpp:172] Setting up conv2_1_ReLU0
I1012 11:55:11.982097 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.982115 31118 net.cpp:194] Memory required for data: 42729728
I1012 11:55:11.982144 31118 layer_factory.hpp:77] Creating layer conv2_1_1
I1012 11:55:11.982177 31118 net.cpp:128] Creating Layer conv2_1_1
I1012 11:55:11.982195 31118 net.cpp:558] conv2_1_1 <- conv2_1_0
I1012 11:55:11.982213 31118 net.cpp:522] conv2_1_1 -> conv2_1_1
I1012 11:55:11.988832 31118 net.cpp:172] Setting up conv2_1_1
I1012 11:55:11.988878 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.988893 31118 net.cpp:194] Memory required for data: 46924032
I1012 11:55:11.988915 31118 layer_factory.hpp:77] Creating layer conv2_1_bn1
I1012 11:55:11.988936 31118 net.cpp:128] Creating Layer conv2_1_bn1
I1012 11:55:11.988955 31118 net.cpp:558] conv2_1_bn1 <- conv2_1_1
I1012 11:55:11.988970 31118 net.cpp:509] conv2_1_bn1 -> conv2_1_1 (in-place)
I1012 11:55:11.989209 31118 net.cpp:172] Setting up conv2_1_bn1
I1012 11:55:11.989233 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.989248 31118 net.cpp:194] Memory required for data: 51118336
I1012 11:55:11.989272 31118 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1012 11:55:11.989293 31118 net.cpp:128] Creating Layer conv2_1_scale1
I1012 11:55:11.989310 31118 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I1012 11:55:11.989327 31118 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I1012 11:55:11.989388 31118 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1012 11:55:11.989542 31118 net.cpp:172] Setting up conv2_1_scale1
I1012 11:55:11.989553 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.989557 31118 net.cpp:194] Memory required for data: 55312640
I1012 11:55:11.989565 31118 layer_factory.hpp:77] Creating layer conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:11.989573 31118 net.cpp:128] Creating Layer conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:11.989578 31118 net.cpp:558] conv2_1_1_conv2_1_scale1_0_split <- conv2_1_1
I1012 11:55:11.989588 31118 net.cpp:522] conv2_1_1_conv2_1_scale1_0_split -> conv2_1_1_conv2_1_scale1_0_split_0
I1012 11:55:11.989616 31118 net.cpp:522] conv2_1_1_conv2_1_scale1_0_split -> conv2_1_1_conv2_1_scale1_0_split_1
I1012 11:55:11.989678 31118 net.cpp:172] Setting up conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:11.989699 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.989715 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:11.989740 31118 net.cpp:194] Memory required for data: 63701248
I1012 11:55:11.989755 31118 layer_factory.hpp:77] Creating layer conv2_1_Pooling
I1012 11:55:11.989779 31118 net.cpp:128] Creating Layer conv2_1_Pooling
I1012 11:55:11.989794 31118 net.cpp:558] conv2_1_Pooling <- conv2_1_1_conv2_1_scale1_0_split_0
I1012 11:55:11.989810 31118 net.cpp:522] conv2_1_Pooling -> conv2_1_Pooling
I1012 11:55:11.989878 31118 net.cpp:172] Setting up conv2_1_Pooling
I1012 11:55:11.989898 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:11.989912 31118 net.cpp:194] Memory required for data: 63705344
I1012 11:55:11.989925 31118 layer_factory.hpp:77] Creating layer conv2_1_2
I1012 11:55:11.989950 31118 net.cpp:128] Creating Layer conv2_1_2
I1012 11:55:11.989966 31118 net.cpp:558] conv2_1_2 <- conv2_1_Pooling
I1012 11:55:11.989984 31118 net.cpp:522] conv2_1_2 -> conv2_1_2
I1012 11:55:11.995800 31118 net.cpp:172] Setting up conv2_1_2
I1012 11:55:11.995854 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:11.995872 31118 net.cpp:194] Memory required for data: 63709440
I1012 11:55:11.995908 31118 layer_factory.hpp:77] Creating layer conv2_1_ReLU2
I1012 11:55:11.995935 31118 net.cpp:128] Creating Layer conv2_1_ReLU2
I1012 11:55:11.995954 31118 net.cpp:558] conv2_1_ReLU2 <- conv2_1_2
I1012 11:55:11.995971 31118 net.cpp:509] conv2_1_ReLU2 -> conv2_1_2 (in-place)
I1012 11:55:11.997535 31118 net.cpp:172] Setting up conv2_1_ReLU2
I1012 11:55:11.997579 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:11.997596 31118 net.cpp:194] Memory required for data: 63713536
I1012 11:55:11.997612 31118 layer_factory.hpp:77] Creating layer conv2_1_3
I1012 11:55:11.997638 31118 net.cpp:128] Creating Layer conv2_1_3
I1012 11:55:11.997670 31118 net.cpp:558] conv2_1_3 <- conv2_1_2
I1012 11:55:11.997692 31118 net.cpp:522] conv2_1_3 -> conv2_1_3
I1012 11:55:12.004297 31118 net.cpp:172] Setting up conv2_1_3
I1012 11:55:12.004333 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.004338 31118 net.cpp:194] Memory required for data: 63717632
I1012 11:55:12.004348 31118 layer_factory.hpp:77] Creating layer conv2_1_Prob3
I1012 11:55:12.004359 31118 net.cpp:128] Creating Layer conv2_1_Prob3
I1012 11:55:12.004397 31118 net.cpp:558] conv2_1_Prob3 <- conv2_1_3
I1012 11:55:12.004410 31118 net.cpp:509] conv2_1_Prob3 -> conv2_1_3 (in-place)
I1012 11:55:12.005697 31118 net.cpp:172] Setting up conv2_1_Prob3
I1012 11:55:12.005717 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.005729 31118 net.cpp:194] Memory required for data: 63721728
I1012 11:55:12.005734 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_1
I1012 11:55:12.005746 31118 net.cpp:128] Creating Layer conv2_Axpy_1
I1012 11:55:12.005750 31118 net.cpp:558] conv2_Axpy_1 <- conv2_1_3
I1012 11:55:12.005756 31118 net.cpp:558] conv2_Axpy_1 <- conv2_1_1_conv2_1_scale1_0_split_1
I1012 11:55:12.005762 31118 net.cpp:558] conv2_Axpy_1 <- conv1_conv1/ReLU_0_split_1
I1012 11:55:12.005795 31118 net.cpp:522] conv2_Axpy_1 -> conv2_Axpy_1
I1012 11:55:12.005874 31118 net.cpp:172] Setting up conv2_Axpy_1
I1012 11:55:12.005894 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.005908 31118 net.cpp:194] Memory required for data: 67916032
I1012 11:55:12.005921 31118 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I1012 11:55:12.005944 31118 net.cpp:128] Creating Layer conv2_1ReLU_1
I1012 11:55:12.005957 31118 net.cpp:558] conv2_1ReLU_1 <- conv2_Axpy_1
I1012 11:55:12.005972 31118 net.cpp:509] conv2_1ReLU_1 -> conv2_Axpy_1 (in-place)
I1012 11:55:12.006223 31118 net.cpp:172] Setting up conv2_1ReLU_1
I1012 11:55:12.006237 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.006242 31118 net.cpp:194] Memory required for data: 72110336
I1012 11:55:12.006247 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.006258 31118 net.cpp:128] Creating Layer conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.006283 31118 net.cpp:558] conv2_Axpy_1_conv2_1ReLU_1_0_split <- conv2_Axpy_1
I1012 11:55:12.006304 31118 net.cpp:522] conv2_Axpy_1_conv2_1ReLU_1_0_split -> conv2_Axpy_1_conv2_1ReLU_1_0_split_0
I1012 11:55:12.006323 31118 net.cpp:522] conv2_Axpy_1_conv2_1ReLU_1_0_split -> conv2_Axpy_1_conv2_1ReLU_1_0_split_1
I1012 11:55:12.006382 31118 net.cpp:172] Setting up conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.006404 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.006420 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.006433 31118 net.cpp:194] Memory required for data: 80498944
I1012 11:55:12.006448 31118 layer_factory.hpp:77] Creating layer conv2_2_0
I1012 11:55:12.006471 31118 net.cpp:128] Creating Layer conv2_2_0
I1012 11:55:12.006487 31118 net.cpp:558] conv2_2_0 <- conv2_Axpy_1_conv2_1ReLU_1_0_split_0
I1012 11:55:12.006505 31118 net.cpp:522] conv2_2_0 -> conv2_2_0
I1012 11:55:12.007874 31118 net.cpp:172] Setting up conv2_2_0
I1012 11:55:12.007901 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.007905 31118 net.cpp:194] Memory required for data: 84693248
I1012 11:55:12.007915 31118 layer_factory.hpp:77] Creating layer conv2_2_bn0
I1012 11:55:12.007927 31118 net.cpp:128] Creating Layer conv2_2_bn0
I1012 11:55:12.007932 31118 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I1012 11:55:12.007938 31118 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I1012 11:55:12.008169 31118 net.cpp:172] Setting up conv2_2_bn0
I1012 11:55:12.008178 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.008183 31118 net.cpp:194] Memory required for data: 88887552
I1012 11:55:12.008193 31118 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1012 11:55:12.008227 31118 net.cpp:128] Creating Layer conv2_2_scale0
I1012 11:55:12.008244 31118 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I1012 11:55:12.008276 31118 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I1012 11:55:12.008342 31118 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1012 11:55:12.008489 31118 net.cpp:172] Setting up conv2_2_scale0
I1012 11:55:12.008500 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.008504 31118 net.cpp:194] Memory required for data: 93081856
I1012 11:55:12.008513 31118 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I1012 11:55:12.008535 31118 net.cpp:128] Creating Layer conv2_2_ReLU0
I1012 11:55:12.008549 31118 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I1012 11:55:12.008569 31118 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I1012 11:55:12.009131 31118 net.cpp:172] Setting up conv2_2_ReLU0
I1012 11:55:12.009174 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.009189 31118 net.cpp:194] Memory required for data: 97276160
I1012 11:55:12.009205 31118 layer_factory.hpp:77] Creating layer conv2_2_1
I1012 11:55:12.009233 31118 net.cpp:128] Creating Layer conv2_2_1
I1012 11:55:12.009250 31118 net.cpp:558] conv2_2_1 <- conv2_2_0
I1012 11:55:12.009270 31118 net.cpp:522] conv2_2_1 -> conv2_2_1
I1012 11:55:12.010685 31118 net.cpp:172] Setting up conv2_2_1
I1012 11:55:12.010715 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.010718 31118 net.cpp:194] Memory required for data: 101470464
I1012 11:55:12.010737 31118 layer_factory.hpp:77] Creating layer conv2_2_bn1
I1012 11:55:12.010773 31118 net.cpp:128] Creating Layer conv2_2_bn1
I1012 11:55:12.010788 31118 net.cpp:558] conv2_2_bn1 <- conv2_2_1
I1012 11:55:12.010808 31118 net.cpp:509] conv2_2_bn1 -> conv2_2_1 (in-place)
I1012 11:55:12.011051 31118 net.cpp:172] Setting up conv2_2_bn1
I1012 11:55:12.011062 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.011066 31118 net.cpp:194] Memory required for data: 105664768
I1012 11:55:12.011077 31118 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1012 11:55:12.011102 31118 net.cpp:128] Creating Layer conv2_2_scale1
I1012 11:55:12.011116 31118 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I1012 11:55:12.011135 31118 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I1012 11:55:12.011190 31118 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1012 11:55:12.011332 31118 net.cpp:172] Setting up conv2_2_scale1
I1012 11:55:12.011345 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.011350 31118 net.cpp:194] Memory required for data: 109859072
I1012 11:55:12.011358 31118 layer_factory.hpp:77] Creating layer conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.011380 31118 net.cpp:128] Creating Layer conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.011394 31118 net.cpp:558] conv2_2_1_conv2_2_scale1_0_split <- conv2_2_1
I1012 11:55:12.011412 31118 net.cpp:522] conv2_2_1_conv2_2_scale1_0_split -> conv2_2_1_conv2_2_scale1_0_split_0
I1012 11:55:12.011435 31118 net.cpp:522] conv2_2_1_conv2_2_scale1_0_split -> conv2_2_1_conv2_2_scale1_0_split_1
I1012 11:55:12.011493 31118 net.cpp:172] Setting up conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.011512 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.011529 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.011543 31118 net.cpp:194] Memory required for data: 118247680
I1012 11:55:12.011555 31118 layer_factory.hpp:77] Creating layer conv2_2_Pooling
I1012 11:55:12.011579 31118 net.cpp:128] Creating Layer conv2_2_Pooling
I1012 11:55:12.011602 31118 net.cpp:558] conv2_2_Pooling <- conv2_2_1_conv2_2_scale1_0_split_0
I1012 11:55:12.011620 31118 net.cpp:522] conv2_2_Pooling -> conv2_2_Pooling
I1012 11:55:12.011660 31118 net.cpp:172] Setting up conv2_2_Pooling
I1012 11:55:12.011677 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.011690 31118 net.cpp:194] Memory required for data: 118251776
I1012 11:55:12.011706 31118 layer_factory.hpp:77] Creating layer conv2_2_2
I1012 11:55:12.011729 31118 net.cpp:128] Creating Layer conv2_2_2
I1012 11:55:12.011742 31118 net.cpp:558] conv2_2_2 <- conv2_2_Pooling
I1012 11:55:12.011759 31118 net.cpp:522] conv2_2_2 -> conv2_2_2
I1012 11:55:12.013183 31118 net.cpp:172] Setting up conv2_2_2
I1012 11:55:12.013207 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.013212 31118 net.cpp:194] Memory required for data: 118255872
I1012 11:55:12.013222 31118 layer_factory.hpp:77] Creating layer conv2_2_ReLU2
I1012 11:55:12.013279 31118 net.cpp:128] Creating Layer conv2_2_ReLU2
I1012 11:55:12.013295 31118 net.cpp:558] conv2_2_ReLU2 <- conv2_2_2
I1012 11:55:12.013314 31118 net.cpp:509] conv2_2_ReLU2 -> conv2_2_2 (in-place)
I1012 11:55:12.013581 31118 net.cpp:172] Setting up conv2_2_ReLU2
I1012 11:55:12.013594 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.013598 31118 net.cpp:194] Memory required for data: 118259968
I1012 11:55:12.013603 31118 layer_factory.hpp:77] Creating layer conv2_2_3
I1012 11:55:12.013618 31118 net.cpp:128] Creating Layer conv2_2_3
I1012 11:55:12.013645 31118 net.cpp:558] conv2_2_3 <- conv2_2_2
I1012 11:55:12.013665 31118 net.cpp:522] conv2_2_3 -> conv2_2_3
I1012 11:55:12.016619 31118 net.cpp:172] Setting up conv2_2_3
I1012 11:55:12.016647 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.016651 31118 net.cpp:194] Memory required for data: 118264064
I1012 11:55:12.016661 31118 layer_factory.hpp:77] Creating layer conv2_2_Prob3
I1012 11:55:12.016672 31118 net.cpp:128] Creating Layer conv2_2_Prob3
I1012 11:55:12.016680 31118 net.cpp:558] conv2_2_Prob3 <- conv2_2_3
I1012 11:55:12.016687 31118 net.cpp:509] conv2_2_Prob3 -> conv2_2_3 (in-place)
I1012 11:55:12.016927 31118 net.cpp:172] Setting up conv2_2_Prob3
I1012 11:55:12.016938 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.016943 31118 net.cpp:194] Memory required for data: 118268160
I1012 11:55:12.016948 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_2
I1012 11:55:12.016960 31118 net.cpp:128] Creating Layer conv2_Axpy_2
I1012 11:55:12.016966 31118 net.cpp:558] conv2_Axpy_2 <- conv2_2_3
I1012 11:55:12.016973 31118 net.cpp:558] conv2_Axpy_2 <- conv2_2_1_conv2_2_scale1_0_split_1
I1012 11:55:12.016979 31118 net.cpp:558] conv2_Axpy_2 <- conv2_Axpy_1_conv2_1ReLU_1_0_split_1
I1012 11:55:12.016988 31118 net.cpp:522] conv2_Axpy_2 -> conv2_Axpy_2
I1012 11:55:12.017050 31118 net.cpp:172] Setting up conv2_Axpy_2
I1012 11:55:12.017060 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.017066 31118 net.cpp:194] Memory required for data: 122462464
I1012 11:55:12.017069 31118 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I1012 11:55:12.017079 31118 net.cpp:128] Creating Layer conv2_2ReLU_1
I1012 11:55:12.017086 31118 net.cpp:558] conv2_2ReLU_1 <- conv2_Axpy_2
I1012 11:55:12.017091 31118 net.cpp:509] conv2_2ReLU_1 -> conv2_Axpy_2 (in-place)
I1012 11:55:12.017315 31118 net.cpp:172] Setting up conv2_2ReLU_1
I1012 11:55:12.017325 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.017331 31118 net.cpp:194] Memory required for data: 126656768
I1012 11:55:12.017335 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.017346 31118 net.cpp:128] Creating Layer conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.017352 31118 net.cpp:558] conv2_Axpy_2_conv2_2ReLU_1_0_split <- conv2_Axpy_2
I1012 11:55:12.017359 31118 net.cpp:522] conv2_Axpy_2_conv2_2ReLU_1_0_split -> conv2_Axpy_2_conv2_2ReLU_1_0_split_0
I1012 11:55:12.017371 31118 net.cpp:522] conv2_Axpy_2_conv2_2ReLU_1_0_split -> conv2_Axpy_2_conv2_2ReLU_1_0_split_1
I1012 11:55:12.017419 31118 net.cpp:172] Setting up conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.017428 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.017436 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.017441 31118 net.cpp:194] Memory required for data: 135045376
I1012 11:55:12.017446 31118 layer_factory.hpp:77] Creating layer conv2_3_0
I1012 11:55:12.017459 31118 net.cpp:128] Creating Layer conv2_3_0
I1012 11:55:12.017465 31118 net.cpp:558] conv2_3_0 <- conv2_Axpy_2_conv2_2ReLU_1_0_split_0
I1012 11:55:12.017474 31118 net.cpp:522] conv2_3_0 -> conv2_3_0
I1012 11:55:12.018921 31118 net.cpp:172] Setting up conv2_3_0
I1012 11:55:12.018963 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.018968 31118 net.cpp:194] Memory required for data: 139239680
I1012 11:55:12.018978 31118 layer_factory.hpp:77] Creating layer conv2_3_bn0
I1012 11:55:12.018990 31118 net.cpp:128] Creating Layer conv2_3_bn0
I1012 11:55:12.019003 31118 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I1012 11:55:12.019011 31118 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I1012 11:55:12.019249 31118 net.cpp:172] Setting up conv2_3_bn0
I1012 11:55:12.019259 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.019263 31118 net.cpp:194] Memory required for data: 143433984
I1012 11:55:12.019273 31118 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1012 11:55:12.019280 31118 net.cpp:128] Creating Layer conv2_3_scale0
I1012 11:55:12.019287 31118 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I1012 11:55:12.019294 31118 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I1012 11:55:12.019340 31118 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1012 11:55:12.019474 31118 net.cpp:172] Setting up conv2_3_scale0
I1012 11:55:12.019484 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.019490 31118 net.cpp:194] Memory required for data: 147628288
I1012 11:55:12.019496 31118 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I1012 11:55:12.019506 31118 net.cpp:128] Creating Layer conv2_3_ReLU0
I1012 11:55:12.019511 31118 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I1012 11:55:12.019518 31118 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I1012 11:55:12.020097 31118 net.cpp:172] Setting up conv2_3_ReLU0
I1012 11:55:12.020117 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.020123 31118 net.cpp:194] Memory required for data: 151822592
I1012 11:55:12.020128 31118 layer_factory.hpp:77] Creating layer conv2_3_1
I1012 11:55:12.020149 31118 net.cpp:128] Creating Layer conv2_3_1
I1012 11:55:12.020155 31118 net.cpp:558] conv2_3_1 <- conv2_3_0
I1012 11:55:12.020164 31118 net.cpp:522] conv2_3_1 -> conv2_3_1
I1012 11:55:12.021558 31118 net.cpp:172] Setting up conv2_3_1
I1012 11:55:12.021587 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.021592 31118 net.cpp:194] Memory required for data: 156016896
I1012 11:55:12.021601 31118 layer_factory.hpp:77] Creating layer conv2_3_bn1
I1012 11:55:12.021610 31118 net.cpp:128] Creating Layer conv2_3_bn1
I1012 11:55:12.021615 31118 net.cpp:558] conv2_3_bn1 <- conv2_3_1
I1012 11:55:12.021623 31118 net.cpp:509] conv2_3_bn1 -> conv2_3_1 (in-place)
I1012 11:55:12.021878 31118 net.cpp:172] Setting up conv2_3_bn1
I1012 11:55:12.021889 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.021893 31118 net.cpp:194] Memory required for data: 160211200
I1012 11:55:12.021903 31118 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1012 11:55:12.021935 31118 net.cpp:128] Creating Layer conv2_3_scale1
I1012 11:55:12.021950 31118 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I1012 11:55:12.021968 31118 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I1012 11:55:12.022028 31118 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1012 11:55:12.022164 31118 net.cpp:172] Setting up conv2_3_scale1
I1012 11:55:12.022177 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.022184 31118 net.cpp:194] Memory required for data: 164405504
I1012 11:55:12.022192 31118 layer_factory.hpp:77] Creating layer conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.022198 31118 net.cpp:128] Creating Layer conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.022203 31118 net.cpp:558] conv2_3_1_conv2_3_scale1_0_split <- conv2_3_1
I1012 11:55:12.022209 31118 net.cpp:522] conv2_3_1_conv2_3_scale1_0_split -> conv2_3_1_conv2_3_scale1_0_split_0
I1012 11:55:12.022219 31118 net.cpp:522] conv2_3_1_conv2_3_scale1_0_split -> conv2_3_1_conv2_3_scale1_0_split_1
I1012 11:55:12.022264 31118 net.cpp:172] Setting up conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.022287 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.022306 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.022313 31118 net.cpp:194] Memory required for data: 172794112
I1012 11:55:12.022328 31118 layer_factory.hpp:77] Creating layer conv2_3_Pooling
I1012 11:55:12.022338 31118 net.cpp:128] Creating Layer conv2_3_Pooling
I1012 11:55:12.022346 31118 net.cpp:558] conv2_3_Pooling <- conv2_3_1_conv2_3_scale1_0_split_0
I1012 11:55:12.022352 31118 net.cpp:522] conv2_3_Pooling -> conv2_3_Pooling
I1012 11:55:12.022384 31118 net.cpp:172] Setting up conv2_3_Pooling
I1012 11:55:12.022408 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.022414 31118 net.cpp:194] Memory required for data: 172798208
I1012 11:55:12.022418 31118 layer_factory.hpp:77] Creating layer conv2_3_2
I1012 11:55:12.022431 31118 net.cpp:128] Creating Layer conv2_3_2
I1012 11:55:12.022450 31118 net.cpp:558] conv2_3_2 <- conv2_3_Pooling
I1012 11:55:12.022467 31118 net.cpp:522] conv2_3_2 -> conv2_3_2
I1012 11:55:12.023839 31118 net.cpp:172] Setting up conv2_3_2
I1012 11:55:12.023865 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.023870 31118 net.cpp:194] Memory required for data: 172802304
I1012 11:55:12.023885 31118 layer_factory.hpp:77] Creating layer conv2_3_ReLU2
I1012 11:55:12.023895 31118 net.cpp:128] Creating Layer conv2_3_ReLU2
I1012 11:55:12.023927 31118 net.cpp:558] conv2_3_ReLU2 <- conv2_3_2
I1012 11:55:12.023938 31118 net.cpp:509] conv2_3_ReLU2 -> conv2_3_2 (in-place)
I1012 11:55:12.024185 31118 net.cpp:172] Setting up conv2_3_ReLU2
I1012 11:55:12.024197 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.024202 31118 net.cpp:194] Memory required for data: 172806400
I1012 11:55:12.024206 31118 layer_factory.hpp:77] Creating layer conv2_3_3
I1012 11:55:12.024224 31118 net.cpp:128] Creating Layer conv2_3_3
I1012 11:55:12.024231 31118 net.cpp:558] conv2_3_3 <- conv2_3_2
I1012 11:55:12.024238 31118 net.cpp:522] conv2_3_3 -> conv2_3_3
I1012 11:55:12.025631 31118 net.cpp:172] Setting up conv2_3_3
I1012 11:55:12.025655 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.025658 31118 net.cpp:194] Memory required for data: 172810496
I1012 11:55:12.025668 31118 layer_factory.hpp:77] Creating layer conv2_3_Prob3
I1012 11:55:12.025682 31118 net.cpp:128] Creating Layer conv2_3_Prob3
I1012 11:55:12.025687 31118 net.cpp:558] conv2_3_Prob3 <- conv2_3_3
I1012 11:55:12.025694 31118 net.cpp:509] conv2_3_Prob3 -> conv2_3_3 (in-place)
I1012 11:55:12.025946 31118 net.cpp:172] Setting up conv2_3_Prob3
I1012 11:55:12.025960 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.025965 31118 net.cpp:194] Memory required for data: 172814592
I1012 11:55:12.025969 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_3
I1012 11:55:12.025979 31118 net.cpp:128] Creating Layer conv2_Axpy_3
I1012 11:55:12.025985 31118 net.cpp:558] conv2_Axpy_3 <- conv2_3_3
I1012 11:55:12.025990 31118 net.cpp:558] conv2_Axpy_3 <- conv2_3_1_conv2_3_scale1_0_split_1
I1012 11:55:12.025995 31118 net.cpp:558] conv2_Axpy_3 <- conv2_Axpy_2_conv2_2ReLU_1_0_split_1
I1012 11:55:12.026001 31118 net.cpp:522] conv2_Axpy_3 -> conv2_Axpy_3
I1012 11:55:12.026068 31118 net.cpp:172] Setting up conv2_Axpy_3
I1012 11:55:12.026082 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.026085 31118 net.cpp:194] Memory required for data: 177008896
I1012 11:55:12.026089 31118 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I1012 11:55:12.026098 31118 net.cpp:128] Creating Layer conv2_3ReLU_1
I1012 11:55:12.026103 31118 net.cpp:558] conv2_3ReLU_1 <- conv2_Axpy_3
I1012 11:55:12.026108 31118 net.cpp:509] conv2_3ReLU_1 -> conv2_Axpy_3 (in-place)
I1012 11:55:12.026681 31118 net.cpp:172] Setting up conv2_3ReLU_1
I1012 11:55:12.026705 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.026710 31118 net.cpp:194] Memory required for data: 181203200
I1012 11:55:12.026715 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.026726 31118 net.cpp:128] Creating Layer conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.026731 31118 net.cpp:558] conv2_Axpy_3_conv2_3ReLU_1_0_split <- conv2_Axpy_3
I1012 11:55:12.026738 31118 net.cpp:522] conv2_Axpy_3_conv2_3ReLU_1_0_split -> conv2_Axpy_3_conv2_3ReLU_1_0_split_0
I1012 11:55:12.026765 31118 net.cpp:522] conv2_Axpy_3_conv2_3ReLU_1_0_split -> conv2_Axpy_3_conv2_3ReLU_1_0_split_1
I1012 11:55:12.026818 31118 net.cpp:172] Setting up conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.026829 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.026835 31118 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1012 11:55:12.026839 31118 net.cpp:194] Memory required for data: 189591808
I1012 11:55:12.026844 31118 layer_factory.hpp:77] Creating layer conv3_1_0
I1012 11:55:12.026859 31118 net.cpp:128] Creating Layer conv3_1_0
I1012 11:55:12.026865 31118 net.cpp:558] conv3_1_0 <- conv2_Axpy_3_conv2_3ReLU_1_0_split_0
I1012 11:55:12.026872 31118 net.cpp:522] conv3_1_0 -> conv3_1_0
I1012 11:55:12.029623 31118 net.cpp:172] Setting up conv3_1_0
I1012 11:55:12.029650 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.029655 31118 net.cpp:194] Memory required for data: 191688960
I1012 11:55:12.029665 31118 layer_factory.hpp:77] Creating layer conv3_1_bn0
I1012 11:55:12.029675 31118 net.cpp:128] Creating Layer conv3_1_bn0
I1012 11:55:12.029687 31118 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I1012 11:55:12.029696 31118 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I1012 11:55:12.029959 31118 net.cpp:172] Setting up conv3_1_bn0
I1012 11:55:12.029973 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.029978 31118 net.cpp:194] Memory required for data: 193786112
I1012 11:55:12.029994 31118 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1012 11:55:12.030006 31118 net.cpp:128] Creating Layer conv3_1_scale0
I1012 11:55:12.030014 31118 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I1012 11:55:12.030020 31118 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I1012 11:55:12.030069 31118 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1012 11:55:12.030239 31118 net.cpp:172] Setting up conv3_1_scale0
I1012 11:55:12.030251 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.030254 31118 net.cpp:194] Memory required for data: 195883264
I1012 11:55:12.030262 31118 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I1012 11:55:12.030272 31118 net.cpp:128] Creating Layer conv3_1_ReLU0
I1012 11:55:12.030277 31118 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I1012 11:55:12.030283 31118 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I1012 11:55:12.030545 31118 net.cpp:172] Setting up conv3_1_ReLU0
I1012 11:55:12.030560 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.030565 31118 net.cpp:194] Memory required for data: 197980416
I1012 11:55:12.030570 31118 layer_factory.hpp:77] Creating layer conv3_1_1
I1012 11:55:12.030582 31118 net.cpp:128] Creating Layer conv3_1_1
I1012 11:55:12.030589 31118 net.cpp:558] conv3_1_1 <- conv3_1_0
I1012 11:55:12.030599 31118 net.cpp:522] conv3_1_1 -> conv3_1_1
I1012 11:55:12.032136 31118 net.cpp:172] Setting up conv3_1_1
I1012 11:55:12.032160 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.032163 31118 net.cpp:194] Memory required for data: 200077568
I1012 11:55:12.032173 31118 layer_factory.hpp:77] Creating layer conv3_1_bn1
I1012 11:55:12.032186 31118 net.cpp:128] Creating Layer conv3_1_bn1
I1012 11:55:12.032191 31118 net.cpp:558] conv3_1_bn1 <- conv3_1_1
I1012 11:55:12.032197 31118 net.cpp:509] conv3_1_bn1 -> conv3_1_1 (in-place)
I1012 11:55:12.032440 31118 net.cpp:172] Setting up conv3_1_bn1
I1012 11:55:12.032451 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.032455 31118 net.cpp:194] Memory required for data: 202174720
I1012 11:55:12.032465 31118 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1012 11:55:12.032471 31118 net.cpp:128] Creating Layer conv3_1_scale1
I1012 11:55:12.032475 31118 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I1012 11:55:12.032483 31118 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I1012 11:55:12.032560 31118 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1012 11:55:12.032716 31118 net.cpp:172] Setting up conv3_1_scale1
I1012 11:55:12.032727 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.032743 31118 net.cpp:194] Memory required for data: 204271872
I1012 11:55:12.032752 31118 layer_factory.hpp:77] Creating layer conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.032775 31118 net.cpp:128] Creating Layer conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.032783 31118 net.cpp:558] conv3_1_1_conv3_1_scale1_0_split <- conv3_1_1
I1012 11:55:12.032793 31118 net.cpp:522] conv3_1_1_conv3_1_scale1_0_split -> conv3_1_1_conv3_1_scale1_0_split_0
I1012 11:55:12.032802 31118 net.cpp:522] conv3_1_1_conv3_1_scale1_0_split -> conv3_1_1_conv3_1_scale1_0_split_1
I1012 11:55:12.032862 31118 net.cpp:172] Setting up conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.032872 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.032878 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.032883 31118 net.cpp:194] Memory required for data: 208466176
I1012 11:55:12.032887 31118 layer_factory.hpp:77] Creating layer conv3_1_Pooling
I1012 11:55:12.032897 31118 net.cpp:128] Creating Layer conv3_1_Pooling
I1012 11:55:12.032904 31118 net.cpp:558] conv3_1_Pooling <- conv3_1_1_conv3_1_scale1_0_split_0
I1012 11:55:12.032910 31118 net.cpp:522] conv3_1_Pooling -> conv3_1_Pooling
I1012 11:55:12.032936 31118 net.cpp:172] Setting up conv3_1_Pooling
I1012 11:55:12.032945 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.032949 31118 net.cpp:194] Memory required for data: 208474368
I1012 11:55:12.032953 31118 layer_factory.hpp:77] Creating layer conv3_1_2
I1012 11:55:12.032965 31118 net.cpp:128] Creating Layer conv3_1_2
I1012 11:55:12.032971 31118 net.cpp:558] conv3_1_2 <- conv3_1_Pooling
I1012 11:55:12.032979 31118 net.cpp:522] conv3_1_2 -> conv3_1_2
I1012 11:55:12.034374 31118 net.cpp:172] Setting up conv3_1_2
I1012 11:55:12.034401 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.034405 31118 net.cpp:194] Memory required for data: 208478464
I1012 11:55:12.034415 31118 layer_factory.hpp:77] Creating layer conv3_1_ReLU2
I1012 11:55:12.034431 31118 net.cpp:128] Creating Layer conv3_1_ReLU2
I1012 11:55:12.034466 31118 net.cpp:558] conv3_1_ReLU2 <- conv3_1_2
I1012 11:55:12.034477 31118 net.cpp:509] conv3_1_ReLU2 -> conv3_1_2 (in-place)
I1012 11:55:12.035048 31118 net.cpp:172] Setting up conv3_1_ReLU2
I1012 11:55:12.035069 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.035079 31118 net.cpp:194] Memory required for data: 208482560
I1012 11:55:12.035085 31118 layer_factory.hpp:77] Creating layer conv3_1_3
I1012 11:55:12.035099 31118 net.cpp:128] Creating Layer conv3_1_3
I1012 11:55:12.035104 31118 net.cpp:558] conv3_1_3 <- conv3_1_2
I1012 11:55:12.035115 31118 net.cpp:522] conv3_1_3 -> conv3_1_3
I1012 11:55:12.036489 31118 net.cpp:172] Setting up conv3_1_3
I1012 11:55:12.036515 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.036520 31118 net.cpp:194] Memory required for data: 208490752
I1012 11:55:12.036530 31118 layer_factory.hpp:77] Creating layer conv3_1_Prob3
I1012 11:55:12.036537 31118 net.cpp:128] Creating Layer conv3_1_Prob3
I1012 11:55:12.036568 31118 net.cpp:558] conv3_1_Prob3 <- conv3_1_3
I1012 11:55:12.036581 31118 net.cpp:509] conv3_1_Prob3 -> conv3_1_3 (in-place)
I1012 11:55:12.037541 31118 net.cpp:172] Setting up conv3_1_Prob3
I1012 11:55:12.037564 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.037569 31118 net.cpp:194] Memory required for data: 208498944
I1012 11:55:12.037573 31118 layer_factory.hpp:77] Creating layer conv3_1_down
I1012 11:55:12.037587 31118 net.cpp:128] Creating Layer conv3_1_down
I1012 11:55:12.037592 31118 net.cpp:558] conv3_1_down <- conv2_Axpy_3_conv2_3ReLU_1_0_split_1
I1012 11:55:12.037603 31118 net.cpp:522] conv3_1_down -> conv3_1_down
I1012 11:55:12.039007 31118 net.cpp:172] Setting up conv3_1_down
I1012 11:55:12.039033 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.039038 31118 net.cpp:194] Memory required for data: 210596096
I1012 11:55:12.039048 31118 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I1012 11:55:12.039058 31118 net.cpp:128] Creating Layer conv3_1_bn_down
I1012 11:55:12.039083 31118 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I1012 11:55:12.039093 31118 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I1012 11:55:12.039333 31118 net.cpp:172] Setting up conv3_1_bn_down
I1012 11:55:12.039345 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.039350 31118 net.cpp:194] Memory required for data: 212693248
I1012 11:55:12.039360 31118 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1012 11:55:12.039366 31118 net.cpp:128] Creating Layer conv3_1_scale_down
I1012 11:55:12.039371 31118 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I1012 11:55:12.039378 31118 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I1012 11:55:12.039424 31118 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1012 11:55:12.039566 31118 net.cpp:172] Setting up conv3_1_scale_down
I1012 11:55:12.039577 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.039582 31118 net.cpp:194] Memory required for data: 214790400
I1012 11:55:12.039588 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_1
I1012 11:55:12.039597 31118 net.cpp:128] Creating Layer conv3_Axpy_1
I1012 11:55:12.039605 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_3
I1012 11:55:12.039610 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_1_conv3_1_scale1_0_split_1
I1012 11:55:12.039615 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_down
I1012 11:55:12.039623 31118 net.cpp:522] conv3_Axpy_1 -> conv3_Axpy_1
I1012 11:55:12.039723 31118 net.cpp:172] Setting up conv3_Axpy_1
I1012 11:55:12.039736 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.039739 31118 net.cpp:194] Memory required for data: 216887552
I1012 11:55:12.039743 31118 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I1012 11:55:12.039750 31118 net.cpp:128] Creating Layer conv3_1ReLU_1
I1012 11:55:12.039754 31118 net.cpp:558] conv3_1ReLU_1 <- conv3_Axpy_1
I1012 11:55:12.039760 31118 net.cpp:509] conv3_1ReLU_1 -> conv3_Axpy_1 (in-place)
I1012 11:55:12.039994 31118 net.cpp:172] Setting up conv3_1ReLU_1
I1012 11:55:12.040006 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.040011 31118 net.cpp:194] Memory required for data: 218984704
I1012 11:55:12.040015 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.040024 31118 net.cpp:128] Creating Layer conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.040032 31118 net.cpp:558] conv3_Axpy_1_conv3_1ReLU_1_0_split <- conv3_Axpy_1
I1012 11:55:12.040040 31118 net.cpp:522] conv3_Axpy_1_conv3_1ReLU_1_0_split -> conv3_Axpy_1_conv3_1ReLU_1_0_split_0
I1012 11:55:12.040047 31118 net.cpp:522] conv3_Axpy_1_conv3_1ReLU_1_0_split -> conv3_Axpy_1_conv3_1ReLU_1_0_split_1
I1012 11:55:12.040093 31118 net.cpp:172] Setting up conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.040103 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.040110 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.040114 31118 net.cpp:194] Memory required for data: 223179008
I1012 11:55:12.040118 31118 layer_factory.hpp:77] Creating layer conv3_2_0
I1012 11:55:12.040130 31118 net.cpp:128] Creating Layer conv3_2_0
I1012 11:55:12.040135 31118 net.cpp:558] conv3_2_0 <- conv3_Axpy_1_conv3_1ReLU_1_0_split_0
I1012 11:55:12.040144 31118 net.cpp:522] conv3_2_0 -> conv3_2_0
I1012 11:55:12.041656 31118 net.cpp:172] Setting up conv3_2_0
I1012 11:55:12.041682 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.041685 31118 net.cpp:194] Memory required for data: 225276160
I1012 11:55:12.041695 31118 layer_factory.hpp:77] Creating layer conv3_2_bn0
I1012 11:55:12.041707 31118 net.cpp:128] Creating Layer conv3_2_bn0
I1012 11:55:12.041712 31118 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I1012 11:55:12.041718 31118 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I1012 11:55:12.041975 31118 net.cpp:172] Setting up conv3_2_bn0
I1012 11:55:12.041986 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.041990 31118 net.cpp:194] Memory required for data: 227373312
I1012 11:55:12.042001 31118 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1012 11:55:12.042026 31118 net.cpp:128] Creating Layer conv3_2_scale0
I1012 11:55:12.042034 31118 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I1012 11:55:12.042042 31118 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I1012 11:55:12.042088 31118 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1012 11:55:12.042232 31118 net.cpp:172] Setting up conv3_2_scale0
I1012 11:55:12.042243 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.042246 31118 net.cpp:194] Memory required for data: 229470464
I1012 11:55:12.042254 31118 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I1012 11:55:12.042263 31118 net.cpp:128] Creating Layer conv3_2_ReLU0
I1012 11:55:12.042268 31118 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I1012 11:55:12.042274 31118 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I1012 11:55:12.042516 31118 net.cpp:172] Setting up conv3_2_ReLU0
I1012 11:55:12.042528 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.042533 31118 net.cpp:194] Memory required for data: 231567616
I1012 11:55:12.042537 31118 layer_factory.hpp:77] Creating layer conv3_2_1
I1012 11:55:12.042551 31118 net.cpp:128] Creating Layer conv3_2_1
I1012 11:55:12.042556 31118 net.cpp:558] conv3_2_1 <- conv3_2_0
I1012 11:55:12.042564 31118 net.cpp:522] conv3_2_1 -> conv3_2_1
I1012 11:55:12.044092 31118 net.cpp:172] Setting up conv3_2_1
I1012 11:55:12.044121 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.044126 31118 net.cpp:194] Memory required for data: 233664768
I1012 11:55:12.044137 31118 layer_factory.hpp:77] Creating layer conv3_2_bn1
I1012 11:55:12.044145 31118 net.cpp:128] Creating Layer conv3_2_bn1
I1012 11:55:12.044150 31118 net.cpp:558] conv3_2_bn1 <- conv3_2_1
I1012 11:55:12.044159 31118 net.cpp:509] conv3_2_bn1 -> conv3_2_1 (in-place)
I1012 11:55:12.044414 31118 net.cpp:172] Setting up conv3_2_bn1
I1012 11:55:12.044425 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.044428 31118 net.cpp:194] Memory required for data: 235761920
I1012 11:55:12.044438 31118 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1012 11:55:12.044447 31118 net.cpp:128] Creating Layer conv3_2_scale1
I1012 11:55:12.044454 31118 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I1012 11:55:12.044461 31118 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I1012 11:55:12.044505 31118 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1012 11:55:12.044646 31118 net.cpp:172] Setting up conv3_2_scale1
I1012 11:55:12.044656 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.044661 31118 net.cpp:194] Memory required for data: 237859072
I1012 11:55:12.044668 31118 layer_factory.hpp:77] Creating layer conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.044680 31118 net.cpp:128] Creating Layer conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.044693 31118 net.cpp:558] conv3_2_1_conv3_2_scale1_0_split <- conv3_2_1
I1012 11:55:12.044699 31118 net.cpp:522] conv3_2_1_conv3_2_scale1_0_split -> conv3_2_1_conv3_2_scale1_0_split_0
I1012 11:55:12.044709 31118 net.cpp:522] conv3_2_1_conv3_2_scale1_0_split -> conv3_2_1_conv3_2_scale1_0_split_1
I1012 11:55:12.044749 31118 net.cpp:172] Setting up conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.044756 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.044762 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.044766 31118 net.cpp:194] Memory required for data: 242053376
I1012 11:55:12.044770 31118 layer_factory.hpp:77] Creating layer conv3_2_Pooling
I1012 11:55:12.044777 31118 net.cpp:128] Creating Layer conv3_2_Pooling
I1012 11:55:12.044781 31118 net.cpp:558] conv3_2_Pooling <- conv3_2_1_conv3_2_scale1_0_split_0
I1012 11:55:12.044790 31118 net.cpp:522] conv3_2_Pooling -> conv3_2_Pooling
I1012 11:55:12.044817 31118 net.cpp:172] Setting up conv3_2_Pooling
I1012 11:55:12.044826 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.044831 31118 net.cpp:194] Memory required for data: 242061568
I1012 11:55:12.044836 31118 layer_factory.hpp:77] Creating layer conv3_2_2
I1012 11:55:12.044847 31118 net.cpp:128] Creating Layer conv3_2_2
I1012 11:55:12.044868 31118 net.cpp:558] conv3_2_2 <- conv3_2_Pooling
I1012 11:55:12.044878 31118 net.cpp:522] conv3_2_2 -> conv3_2_2
I1012 11:55:12.046277 31118 net.cpp:172] Setting up conv3_2_2
I1012 11:55:12.046303 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.046308 31118 net.cpp:194] Memory required for data: 242065664
I1012 11:55:12.046317 31118 layer_factory.hpp:77] Creating layer conv3_2_ReLU2
I1012 11:55:12.046325 31118 net.cpp:128] Creating Layer conv3_2_ReLU2
I1012 11:55:12.046330 31118 net.cpp:558] conv3_2_ReLU2 <- conv3_2_2
I1012 11:55:12.046339 31118 net.cpp:509] conv3_2_ReLU2 -> conv3_2_2 (in-place)
I1012 11:55:12.046587 31118 net.cpp:172] Setting up conv3_2_ReLU2
I1012 11:55:12.046598 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.046602 31118 net.cpp:194] Memory required for data: 242069760
I1012 11:55:12.046607 31118 layer_factory.hpp:77] Creating layer conv3_2_3
I1012 11:55:12.046618 31118 net.cpp:128] Creating Layer conv3_2_3
I1012 11:55:12.046627 31118 net.cpp:558] conv3_2_3 <- conv3_2_2
I1012 11:55:12.046635 31118 net.cpp:522] conv3_2_3 -> conv3_2_3
I1012 11:55:12.048008 31118 net.cpp:172] Setting up conv3_2_3
I1012 11:55:12.048034 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.048039 31118 net.cpp:194] Memory required for data: 242077952
I1012 11:55:12.048049 31118 layer_factory.hpp:77] Creating layer conv3_2_Prob3
I1012 11:55:12.048061 31118 net.cpp:128] Creating Layer conv3_2_Prob3
I1012 11:55:12.048074 31118 net.cpp:558] conv3_2_Prob3 <- conv3_2_3
I1012 11:55:12.048081 31118 net.cpp:509] conv3_2_Prob3 -> conv3_2_3 (in-place)
I1012 11:55:12.048642 31118 net.cpp:172] Setting up conv3_2_Prob3
I1012 11:55:12.048662 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.048667 31118 net.cpp:194] Memory required for data: 242086144
I1012 11:55:12.048671 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_2
I1012 11:55:12.048681 31118 net.cpp:128] Creating Layer conv3_Axpy_2
I1012 11:55:12.048687 31118 net.cpp:558] conv3_Axpy_2 <- conv3_2_3
I1012 11:55:12.048693 31118 net.cpp:558] conv3_Axpy_2 <- conv3_2_1_conv3_2_scale1_0_split_1
I1012 11:55:12.048704 31118 net.cpp:558] conv3_Axpy_2 <- conv3_Axpy_1_conv3_1ReLU_1_0_split_1
I1012 11:55:12.048710 31118 net.cpp:522] conv3_Axpy_2 -> conv3_Axpy_2
I1012 11:55:12.048780 31118 net.cpp:172] Setting up conv3_Axpy_2
I1012 11:55:12.048790 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.048794 31118 net.cpp:194] Memory required for data: 244183296
I1012 11:55:12.048799 31118 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I1012 11:55:12.048806 31118 net.cpp:128] Creating Layer conv3_2ReLU_1
I1012 11:55:12.048813 31118 net.cpp:558] conv3_2ReLU_1 <- conv3_Axpy_2
I1012 11:55:12.048820 31118 net.cpp:509] conv3_2ReLU_1 -> conv3_Axpy_2 (in-place)
I1012 11:55:12.049051 31118 net.cpp:172] Setting up conv3_2ReLU_1
I1012 11:55:12.049062 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.049065 31118 net.cpp:194] Memory required for data: 246280448
I1012 11:55:12.049069 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.049078 31118 net.cpp:128] Creating Layer conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.049087 31118 net.cpp:558] conv3_Axpy_2_conv3_2ReLU_1_0_split <- conv3_Axpy_2
I1012 11:55:12.049094 31118 net.cpp:522] conv3_Axpy_2_conv3_2ReLU_1_0_split -> conv3_Axpy_2_conv3_2ReLU_1_0_split_0
I1012 11:55:12.049103 31118 net.cpp:522] conv3_Axpy_2_conv3_2ReLU_1_0_split -> conv3_Axpy_2_conv3_2ReLU_1_0_split_1
I1012 11:55:12.049144 31118 net.cpp:172] Setting up conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.049154 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.049160 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.049163 31118 net.cpp:194] Memory required for data: 250474752
I1012 11:55:12.049167 31118 layer_factory.hpp:77] Creating layer conv3_3_0
I1012 11:55:12.049180 31118 net.cpp:128] Creating Layer conv3_3_0
I1012 11:55:12.049185 31118 net.cpp:558] conv3_3_0 <- conv3_Axpy_2_conv3_2ReLU_1_0_split_0
I1012 11:55:12.049207 31118 net.cpp:522] conv3_3_0 -> conv3_3_0
I1012 11:55:12.050765 31118 net.cpp:172] Setting up conv3_3_0
I1012 11:55:12.050791 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.050796 31118 net.cpp:194] Memory required for data: 252571904
I1012 11:55:12.050809 31118 layer_factory.hpp:77] Creating layer conv3_3_bn0
I1012 11:55:12.050820 31118 net.cpp:128] Creating Layer conv3_3_bn0
I1012 11:55:12.050825 31118 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I1012 11:55:12.050834 31118 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I1012 11:55:12.051086 31118 net.cpp:172] Setting up conv3_3_bn0
I1012 11:55:12.051097 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.051101 31118 net.cpp:194] Memory required for data: 254669056
I1012 11:55:12.051110 31118 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1012 11:55:12.051117 31118 net.cpp:128] Creating Layer conv3_3_scale0
I1012 11:55:12.051122 31118 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I1012 11:55:12.051127 31118 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I1012 11:55:12.051173 31118 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1012 11:55:12.051312 31118 net.cpp:172] Setting up conv3_3_scale0
I1012 11:55:12.051322 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.051326 31118 net.cpp:194] Memory required for data: 256766208
I1012 11:55:12.051333 31118 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I1012 11:55:12.051342 31118 net.cpp:128] Creating Layer conv3_3_ReLU0
I1012 11:55:12.051347 31118 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I1012 11:55:12.051352 31118 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I1012 11:55:12.051589 31118 net.cpp:172] Setting up conv3_3_ReLU0
I1012 11:55:12.051600 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.051604 31118 net.cpp:194] Memory required for data: 258863360
I1012 11:55:12.051609 31118 layer_factory.hpp:77] Creating layer conv3_3_1
I1012 11:55:12.051621 31118 net.cpp:128] Creating Layer conv3_3_1
I1012 11:55:12.051626 31118 net.cpp:558] conv3_3_1 <- conv3_3_0
I1012 11:55:12.051635 31118 net.cpp:522] conv3_3_1 -> conv3_3_1
I1012 11:55:12.053150 31118 net.cpp:172] Setting up conv3_3_1
I1012 11:55:12.053176 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.053180 31118 net.cpp:194] Memory required for data: 260960512
I1012 11:55:12.053194 31118 layer_factory.hpp:77] Creating layer conv3_3_bn1
I1012 11:55:12.053203 31118 net.cpp:128] Creating Layer conv3_3_bn1
I1012 11:55:12.053208 31118 net.cpp:558] conv3_3_bn1 <- conv3_3_1
I1012 11:55:12.053215 31118 net.cpp:509] conv3_3_bn1 -> conv3_3_1 (in-place)
I1012 11:55:12.053459 31118 net.cpp:172] Setting up conv3_3_bn1
I1012 11:55:12.053469 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.053473 31118 net.cpp:194] Memory required for data: 263057664
I1012 11:55:12.053483 31118 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1012 11:55:12.053490 31118 net.cpp:128] Creating Layer conv3_3_scale1
I1012 11:55:12.053495 31118 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I1012 11:55:12.053501 31118 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I1012 11:55:12.053547 31118 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1012 11:55:12.053690 31118 net.cpp:172] Setting up conv3_3_scale1
I1012 11:55:12.053700 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.053704 31118 net.cpp:194] Memory required for data: 265154816
I1012 11:55:12.053712 31118 layer_factory.hpp:77] Creating layer conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.053726 31118 net.cpp:128] Creating Layer conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.053732 31118 net.cpp:558] conv3_3_1_conv3_3_scale1_0_split <- conv3_3_1
I1012 11:55:12.053741 31118 net.cpp:522] conv3_3_1_conv3_3_scale1_0_split -> conv3_3_1_conv3_3_scale1_0_split_0
I1012 11:55:12.053750 31118 net.cpp:522] conv3_3_1_conv3_3_scale1_0_split -> conv3_3_1_conv3_3_scale1_0_split_1
I1012 11:55:12.053789 31118 net.cpp:172] Setting up conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.053797 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.053817 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.053822 31118 net.cpp:194] Memory required for data: 269349120
I1012 11:55:12.053827 31118 layer_factory.hpp:77] Creating layer conv3_3_Pooling
I1012 11:55:12.053834 31118 net.cpp:128] Creating Layer conv3_3_Pooling
I1012 11:55:12.053839 31118 net.cpp:558] conv3_3_Pooling <- conv3_3_1_conv3_3_scale1_0_split_0
I1012 11:55:12.053845 31118 net.cpp:522] conv3_3_Pooling -> conv3_3_Pooling
I1012 11:55:12.053874 31118 net.cpp:172] Setting up conv3_3_Pooling
I1012 11:55:12.053881 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.053885 31118 net.cpp:194] Memory required for data: 269357312
I1012 11:55:12.053889 31118 layer_factory.hpp:77] Creating layer conv3_3_2
I1012 11:55:12.053900 31118 net.cpp:128] Creating Layer conv3_3_2
I1012 11:55:12.053905 31118 net.cpp:558] conv3_3_2 <- conv3_3_Pooling
I1012 11:55:12.053915 31118 net.cpp:522] conv3_3_2 -> conv3_3_2
I1012 11:55:12.059216 31118 net.cpp:172] Setting up conv3_3_2
I1012 11:55:12.059242 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.059247 31118 net.cpp:194] Memory required for data: 269361408
I1012 11:55:12.059257 31118 layer_factory.hpp:77] Creating layer conv3_3_ReLU2
I1012 11:55:12.059269 31118 net.cpp:128] Creating Layer conv3_3_ReLU2
I1012 11:55:12.059274 31118 net.cpp:558] conv3_3_ReLU2 <- conv3_3_2
I1012 11:55:12.059280 31118 net.cpp:509] conv3_3_ReLU2 -> conv3_3_2 (in-place)
I1012 11:55:12.061281 31118 net.cpp:172] Setting up conv3_3_ReLU2
I1012 11:55:12.061306 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.061311 31118 net.cpp:194] Memory required for data: 269365504
I1012 11:55:12.061316 31118 layer_factory.hpp:77] Creating layer conv3_3_3
I1012 11:55:12.061336 31118 net.cpp:128] Creating Layer conv3_3_3
I1012 11:55:12.061342 31118 net.cpp:558] conv3_3_3 <- conv3_3_2
I1012 11:55:12.061353 31118 net.cpp:522] conv3_3_3 -> conv3_3_3
I1012 11:55:12.068073 31118 net.cpp:172] Setting up conv3_3_3
I1012 11:55:12.068100 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.068105 31118 net.cpp:194] Memory required for data: 269373696
I1012 11:55:12.068114 31118 layer_factory.hpp:77] Creating layer conv3_3_Prob3
I1012 11:55:12.068125 31118 net.cpp:128] Creating Layer conv3_3_Prob3
I1012 11:55:12.068131 31118 net.cpp:558] conv3_3_Prob3 <- conv3_3_3
I1012 11:55:12.068137 31118 net.cpp:509] conv3_3_Prob3 -> conv3_3_3 (in-place)
I1012 11:55:12.070152 31118 net.cpp:172] Setting up conv3_3_Prob3
I1012 11:55:12.070169 31118 net.cpp:186] Top shape: 64 32 1 1 (2048)
I1012 11:55:12.070174 31118 net.cpp:194] Memory required for data: 269381888
I1012 11:55:12.070178 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_3
I1012 11:55:12.070188 31118 net.cpp:128] Creating Layer conv3_Axpy_3
I1012 11:55:12.070194 31118 net.cpp:558] conv3_Axpy_3 <- conv3_3_3
I1012 11:55:12.070199 31118 net.cpp:558] conv3_Axpy_3 <- conv3_3_1_conv3_3_scale1_0_split_1
I1012 11:55:12.070205 31118 net.cpp:558] conv3_Axpy_3 <- conv3_Axpy_2_conv3_2ReLU_1_0_split_1
I1012 11:55:12.070214 31118 net.cpp:522] conv3_Axpy_3 -> conv3_Axpy_3
I1012 11:55:12.070286 31118 net.cpp:172] Setting up conv3_Axpy_3
I1012 11:55:12.070292 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.070297 31118 net.cpp:194] Memory required for data: 271479040
I1012 11:55:12.070302 31118 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I1012 11:55:12.070307 31118 net.cpp:128] Creating Layer conv3_3ReLU_1
I1012 11:55:12.070312 31118 net.cpp:558] conv3_3ReLU_1 <- conv3_Axpy_3
I1012 11:55:12.070320 31118 net.cpp:509] conv3_3ReLU_1 -> conv3_Axpy_3 (in-place)
I1012 11:55:12.072360 31118 net.cpp:172] Setting up conv3_3ReLU_1
I1012 11:55:12.072376 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.072381 31118 net.cpp:194] Memory required for data: 273576192
I1012 11:55:12.072386 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.072391 31118 net.cpp:128] Creating Layer conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.072413 31118 net.cpp:558] conv3_Axpy_3_conv3_3ReLU_1_0_split <- conv3_Axpy_3
I1012 11:55:12.072427 31118 net.cpp:522] conv3_Axpy_3_conv3_3ReLU_1_0_split -> conv3_Axpy_3_conv3_3ReLU_1_0_split_0
I1012 11:55:12.072446 31118 net.cpp:522] conv3_Axpy_3_conv3_3ReLU_1_0_split -> conv3_Axpy_3_conv3_3ReLU_1_0_split_1
I1012 11:55:12.072490 31118 net.cpp:172] Setting up conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.072499 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.072505 31118 net.cpp:186] Top shape: 64 32 16 16 (524288)
I1012 11:55:12.072510 31118 net.cpp:194] Memory required for data: 277770496
I1012 11:55:12.072513 31118 layer_factory.hpp:77] Creating layer conv4_1_0
I1012 11:55:12.072526 31118 net.cpp:128] Creating Layer conv4_1_0
I1012 11:55:12.072531 31118 net.cpp:558] conv4_1_0 <- conv3_Axpy_3_conv3_3ReLU_1_0_split_0
I1012 11:55:12.072540 31118 net.cpp:522] conv4_1_0 -> conv4_1_0
I1012 11:55:12.079483 31118 net.cpp:172] Setting up conv4_1_0
I1012 11:55:12.079515 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.079521 31118 net.cpp:194] Memory required for data: 278819072
I1012 11:55:12.079533 31118 layer_factory.hpp:77] Creating layer conv4_1_bn0
I1012 11:55:12.079545 31118 net.cpp:128] Creating Layer conv4_1_bn0
I1012 11:55:12.079552 31118 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I1012 11:55:12.079562 31118 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I1012 11:55:12.079838 31118 net.cpp:172] Setting up conv4_1_bn0
I1012 11:55:12.079850 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.079854 31118 net.cpp:194] Memory required for data: 279867648
I1012 11:55:12.079864 31118 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1012 11:55:12.079874 31118 net.cpp:128] Creating Layer conv4_1_scale0
I1012 11:55:12.079877 31118 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I1012 11:55:12.079882 31118 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I1012 11:55:12.079934 31118 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1012 11:55:12.080092 31118 net.cpp:172] Setting up conv4_1_scale0
I1012 11:55:12.080101 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.080104 31118 net.cpp:194] Memory required for data: 280916224
I1012 11:55:12.080126 31118 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I1012 11:55:12.080132 31118 net.cpp:128] Creating Layer conv4_1_ReLU0
I1012 11:55:12.080137 31118 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I1012 11:55:12.080142 31118 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I1012 11:55:12.081189 31118 net.cpp:172] Setting up conv4_1_ReLU0
I1012 11:55:12.081212 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.081218 31118 net.cpp:194] Memory required for data: 281964800
I1012 11:55:12.081223 31118 layer_factory.hpp:77] Creating layer conv4_1_1
I1012 11:55:12.081238 31118 net.cpp:128] Creating Layer conv4_1_1
I1012 11:55:12.081243 31118 net.cpp:558] conv4_1_1 <- conv4_1_0
I1012 11:55:12.081259 31118 net.cpp:522] conv4_1_1 -> conv4_1_1
I1012 11:55:12.088304 31118 net.cpp:172] Setting up conv4_1_1
I1012 11:55:12.088330 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.088335 31118 net.cpp:194] Memory required for data: 283013376
I1012 11:55:12.088346 31118 layer_factory.hpp:77] Creating layer conv4_1_bn1
I1012 11:55:12.088358 31118 net.cpp:128] Creating Layer conv4_1_bn1
I1012 11:55:12.088363 31118 net.cpp:558] conv4_1_bn1 <- conv4_1_1
I1012 11:55:12.088376 31118 net.cpp:509] conv4_1_bn1 -> conv4_1_1 (in-place)
I1012 11:55:12.088649 31118 net.cpp:172] Setting up conv4_1_bn1
I1012 11:55:12.088665 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.088668 31118 net.cpp:194] Memory required for data: 284061952
I1012 11:55:12.088677 31118 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1012 11:55:12.088685 31118 net.cpp:128] Creating Layer conv4_1_scale1
I1012 11:55:12.088688 31118 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I1012 11:55:12.088695 31118 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I1012 11:55:12.088742 31118 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1012 11:55:12.088918 31118 net.cpp:172] Setting up conv4_1_scale1
I1012 11:55:12.088927 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.088930 31118 net.cpp:194] Memory required for data: 285110528
I1012 11:55:12.088938 31118 layer_factory.hpp:77] Creating layer conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.088948 31118 net.cpp:128] Creating Layer conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.088953 31118 net.cpp:558] conv4_1_1_conv4_1_scale1_0_split <- conv4_1_1
I1012 11:55:12.088960 31118 net.cpp:522] conv4_1_1_conv4_1_scale1_0_split -> conv4_1_1_conv4_1_scale1_0_split_0
I1012 11:55:12.088968 31118 net.cpp:522] conv4_1_1_conv4_1_scale1_0_split -> conv4_1_1_conv4_1_scale1_0_split_1
I1012 11:55:12.089013 31118 net.cpp:172] Setting up conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.089021 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.089027 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.089030 31118 net.cpp:194] Memory required for data: 287207680
I1012 11:55:12.089035 31118 layer_factory.hpp:77] Creating layer conv4_1_Pooling
I1012 11:55:12.089046 31118 net.cpp:128] Creating Layer conv4_1_Pooling
I1012 11:55:12.089051 31118 net.cpp:558] conv4_1_Pooling <- conv4_1_1_conv4_1_scale1_0_split_0
I1012 11:55:12.089056 31118 net.cpp:522] conv4_1_Pooling -> conv4_1_Pooling
I1012 11:55:12.089084 31118 net.cpp:172] Setting up conv4_1_Pooling
I1012 11:55:12.089092 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.089095 31118 net.cpp:194] Memory required for data: 287224064
I1012 11:55:12.089100 31118 layer_factory.hpp:77] Creating layer conv4_1_2
I1012 11:55:12.089113 31118 net.cpp:128] Creating Layer conv4_1_2
I1012 11:55:12.089118 31118 net.cpp:558] conv4_1_2 <- conv4_1_Pooling
I1012 11:55:12.089126 31118 net.cpp:522] conv4_1_2 -> conv4_1_2
I1012 11:55:12.094646 31118 net.cpp:172] Setting up conv4_1_2
I1012 11:55:12.094672 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.094677 31118 net.cpp:194] Memory required for data: 287228160
I1012 11:55:12.094687 31118 layer_factory.hpp:77] Creating layer conv4_1_ReLU2
I1012 11:55:12.094696 31118 net.cpp:128] Creating Layer conv4_1_ReLU2
I1012 11:55:12.094700 31118 net.cpp:558] conv4_1_ReLU2 <- conv4_1_2
I1012 11:55:12.094709 31118 net.cpp:509] conv4_1_ReLU2 -> conv4_1_2 (in-place)
I1012 11:55:12.096699 31118 net.cpp:172] Setting up conv4_1_ReLU2
I1012 11:55:12.096715 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.096720 31118 net.cpp:194] Memory required for data: 287232256
I1012 11:55:12.096724 31118 layer_factory.hpp:77] Creating layer conv4_1_3
I1012 11:55:12.096740 31118 net.cpp:128] Creating Layer conv4_1_3
I1012 11:55:12.096745 31118 net.cpp:558] conv4_1_3 <- conv4_1_2
I1012 11:55:12.096753 31118 net.cpp:522] conv4_1_3 -> conv4_1_3
I1012 11:55:12.103447 31118 net.cpp:172] Setting up conv4_1_3
I1012 11:55:12.103473 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.103478 31118 net.cpp:194] Memory required for data: 287248640
I1012 11:55:12.103487 31118 layer_factory.hpp:77] Creating layer conv4_1_Prob3
I1012 11:55:12.103499 31118 net.cpp:128] Creating Layer conv4_1_Prob3
I1012 11:55:12.103504 31118 net.cpp:558] conv4_1_Prob3 <- conv4_1_3
I1012 11:55:12.103510 31118 net.cpp:509] conv4_1_Prob3 -> conv4_1_3 (in-place)
I1012 11:55:12.105525 31118 net.cpp:172] Setting up conv4_1_Prob3
I1012 11:55:12.105541 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.105546 31118 net.cpp:194] Memory required for data: 287265024
I1012 11:55:12.105551 31118 layer_factory.hpp:77] Creating layer conv4_1_down
I1012 11:55:12.105566 31118 net.cpp:128] Creating Layer conv4_1_down
I1012 11:55:12.105571 31118 net.cpp:558] conv4_1_down <- conv3_Axpy_3_conv3_3ReLU_1_0_split_1
I1012 11:55:12.105579 31118 net.cpp:522] conv4_1_down -> conv4_1_down
I1012 11:55:12.112356 31118 net.cpp:172] Setting up conv4_1_down
I1012 11:55:12.112386 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.112391 31118 net.cpp:194] Memory required for data: 288313600
I1012 11:55:12.112403 31118 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I1012 11:55:12.112439 31118 net.cpp:128] Creating Layer conv4_1_bn_down
I1012 11:55:12.112447 31118 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I1012 11:55:12.112460 31118 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I1012 11:55:12.112793 31118 net.cpp:172] Setting up conv4_1_bn_down
I1012 11:55:12.112802 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.112807 31118 net.cpp:194] Memory required for data: 289362176
I1012 11:55:12.112819 31118 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1012 11:55:12.112828 31118 net.cpp:128] Creating Layer conv4_1_scale_down
I1012 11:55:12.112834 31118 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I1012 11:55:12.112841 31118 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I1012 11:55:12.112902 31118 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1012 11:55:12.113091 31118 net.cpp:172] Setting up conv4_1_scale_down
I1012 11:55:12.113104 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.113111 31118 net.cpp:194] Memory required for data: 290410752
I1012 11:55:12.113119 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_1
I1012 11:55:12.113128 31118 net.cpp:128] Creating Layer conv4_Axpy_1
I1012 11:55:12.113133 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_3
I1012 11:55:12.113140 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_1_conv4_1_scale1_0_split_1
I1012 11:55:12.113147 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_down
I1012 11:55:12.113154 31118 net.cpp:522] conv4_Axpy_1 -> conv4_Axpy_1
I1012 11:55:12.113229 31118 net.cpp:172] Setting up conv4_Axpy_1
I1012 11:55:12.113237 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.113242 31118 net.cpp:194] Memory required for data: 291459328
I1012 11:55:12.113247 31118 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I1012 11:55:12.113258 31118 net.cpp:128] Creating Layer conv4_1ReLU_1
I1012 11:55:12.113265 31118 net.cpp:558] conv4_1ReLU_1 <- conv4_Axpy_1
I1012 11:55:12.113271 31118 net.cpp:509] conv4_1ReLU_1 -> conv4_Axpy_1 (in-place)
I1012 11:55:12.114385 31118 net.cpp:172] Setting up conv4_1ReLU_1
I1012 11:55:12.114404 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.114409 31118 net.cpp:194] Memory required for data: 292507904
I1012 11:55:12.114414 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.114429 31118 net.cpp:128] Creating Layer conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.114435 31118 net.cpp:558] conv4_Axpy_1_conv4_1ReLU_1_0_split <- conv4_Axpy_1
I1012 11:55:12.114444 31118 net.cpp:522] conv4_Axpy_1_conv4_1ReLU_1_0_split -> conv4_Axpy_1_conv4_1ReLU_1_0_split_0
I1012 11:55:12.114456 31118 net.cpp:522] conv4_Axpy_1_conv4_1ReLU_1_0_split -> conv4_Axpy_1_conv4_1ReLU_1_0_split_1
I1012 11:55:12.114511 31118 net.cpp:172] Setting up conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.114518 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.114524 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.114528 31118 net.cpp:194] Memory required for data: 294605056
I1012 11:55:12.114532 31118 layer_factory.hpp:77] Creating layer conv4_2_0
I1012 11:55:12.114545 31118 net.cpp:128] Creating Layer conv4_2_0
I1012 11:55:12.114550 31118 net.cpp:558] conv4_2_0 <- conv4_Axpy_1_conv4_1ReLU_1_0_split_0
I1012 11:55:12.114560 31118 net.cpp:522] conv4_2_0 -> conv4_2_0
I1012 11:55:12.121134 31118 net.cpp:172] Setting up conv4_2_0
I1012 11:55:12.121163 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.121167 31118 net.cpp:194] Memory required for data: 295653632
I1012 11:55:12.121177 31118 layer_factory.hpp:77] Creating layer conv4_2_bn0
I1012 11:55:12.121186 31118 net.cpp:128] Creating Layer conv4_2_bn0
I1012 11:55:12.121191 31118 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I1012 11:55:12.121201 31118 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I1012 11:55:12.121466 31118 net.cpp:172] Setting up conv4_2_bn0
I1012 11:55:12.121479 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.121484 31118 net.cpp:194] Memory required for data: 296702208
I1012 11:55:12.121508 31118 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1012 11:55:12.121516 31118 net.cpp:128] Creating Layer conv4_2_scale0
I1012 11:55:12.121521 31118 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I1012 11:55:12.121529 31118 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I1012 11:55:12.121578 31118 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1012 11:55:12.121737 31118 net.cpp:172] Setting up conv4_2_scale0
I1012 11:55:12.121749 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.121753 31118 net.cpp:194] Memory required for data: 297750784
I1012 11:55:12.121762 31118 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I1012 11:55:12.121770 31118 net.cpp:128] Creating Layer conv4_2_ReLU0
I1012 11:55:12.121775 31118 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I1012 11:55:12.121780 31118 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I1012 11:55:12.123205 31118 net.cpp:172] Setting up conv4_2_ReLU0
I1012 11:55:12.123227 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.123231 31118 net.cpp:194] Memory required for data: 298799360
I1012 11:55:12.123236 31118 layer_factory.hpp:77] Creating layer conv4_2_1
I1012 11:55:12.123267 31118 net.cpp:128] Creating Layer conv4_2_1
I1012 11:55:12.123275 31118 net.cpp:558] conv4_2_1 <- conv4_2_0
I1012 11:55:12.123282 31118 net.cpp:522] conv4_2_1 -> conv4_2_1
I1012 11:55:12.130328 31118 net.cpp:172] Setting up conv4_2_1
I1012 11:55:12.130354 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.130359 31118 net.cpp:194] Memory required for data: 299847936
I1012 11:55:12.130369 31118 layer_factory.hpp:77] Creating layer conv4_2_bn1
I1012 11:55:12.130380 31118 net.cpp:128] Creating Layer conv4_2_bn1
I1012 11:55:12.130386 31118 net.cpp:558] conv4_2_bn1 <- conv4_2_1
I1012 11:55:12.130396 31118 net.cpp:509] conv4_2_bn1 -> conv4_2_1 (in-place)
I1012 11:55:12.130676 31118 net.cpp:172] Setting up conv4_2_bn1
I1012 11:55:12.130694 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.130698 31118 net.cpp:194] Memory required for data: 300896512
I1012 11:55:12.130708 31118 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1012 11:55:12.130715 31118 net.cpp:128] Creating Layer conv4_2_scale1
I1012 11:55:12.130719 31118 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I1012 11:55:12.130725 31118 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I1012 11:55:12.130774 31118 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1012 11:55:12.130928 31118 net.cpp:172] Setting up conv4_2_scale1
I1012 11:55:12.130935 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.130939 31118 net.cpp:194] Memory required for data: 301945088
I1012 11:55:12.130946 31118 layer_factory.hpp:77] Creating layer conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.130955 31118 net.cpp:128] Creating Layer conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.130960 31118 net.cpp:558] conv4_2_1_conv4_2_scale1_0_split <- conv4_2_1
I1012 11:55:12.130966 31118 net.cpp:522] conv4_2_1_conv4_2_scale1_0_split -> conv4_2_1_conv4_2_scale1_0_split_0
I1012 11:55:12.130975 31118 net.cpp:522] conv4_2_1_conv4_2_scale1_0_split -> conv4_2_1_conv4_2_scale1_0_split_1
I1012 11:55:12.131021 31118 net.cpp:172] Setting up conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.131027 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.131033 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.131037 31118 net.cpp:194] Memory required for data: 304042240
I1012 11:55:12.131042 31118 layer_factory.hpp:77] Creating layer conv4_2_Pooling
I1012 11:55:12.131049 31118 net.cpp:128] Creating Layer conv4_2_Pooling
I1012 11:55:12.131054 31118 net.cpp:558] conv4_2_Pooling <- conv4_2_1_conv4_2_scale1_0_split_0
I1012 11:55:12.131062 31118 net.cpp:522] conv4_2_Pooling -> conv4_2_Pooling
I1012 11:55:12.131088 31118 net.cpp:172] Setting up conv4_2_Pooling
I1012 11:55:12.131095 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.131099 31118 net.cpp:194] Memory required for data: 304058624
I1012 11:55:12.131103 31118 layer_factory.hpp:77] Creating layer conv4_2_2
I1012 11:55:12.131130 31118 net.cpp:128] Creating Layer conv4_2_2
I1012 11:55:12.131136 31118 net.cpp:558] conv4_2_2 <- conv4_2_Pooling
I1012 11:55:12.131144 31118 net.cpp:522] conv4_2_2 -> conv4_2_2
I1012 11:55:12.136611 31118 net.cpp:172] Setting up conv4_2_2
I1012 11:55:12.136637 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.136642 31118 net.cpp:194] Memory required for data: 304062720
I1012 11:55:12.136652 31118 layer_factory.hpp:77] Creating layer conv4_2_ReLU2
I1012 11:55:12.136660 31118 net.cpp:128] Creating Layer conv4_2_ReLU2
I1012 11:55:12.136665 31118 net.cpp:558] conv4_2_ReLU2 <- conv4_2_2
I1012 11:55:12.136674 31118 net.cpp:509] conv4_2_ReLU2 -> conv4_2_2 (in-place)
I1012 11:55:12.138675 31118 net.cpp:172] Setting up conv4_2_ReLU2
I1012 11:55:12.138694 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.138697 31118 net.cpp:194] Memory required for data: 304066816
I1012 11:55:12.138702 31118 layer_factory.hpp:77] Creating layer conv4_2_3
I1012 11:55:12.138715 31118 net.cpp:128] Creating Layer conv4_2_3
I1012 11:55:12.138720 31118 net.cpp:558] conv4_2_3 <- conv4_2_2
I1012 11:55:12.138731 31118 net.cpp:522] conv4_2_3 -> conv4_2_3
I1012 11:55:12.145424 31118 net.cpp:172] Setting up conv4_2_3
I1012 11:55:12.145452 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.145455 31118 net.cpp:194] Memory required for data: 304083200
I1012 11:55:12.145465 31118 layer_factory.hpp:77] Creating layer conv4_2_Prob3
I1012 11:55:12.145476 31118 net.cpp:128] Creating Layer conv4_2_Prob3
I1012 11:55:12.145481 31118 net.cpp:558] conv4_2_Prob3 <- conv4_2_3
I1012 11:55:12.145488 31118 net.cpp:509] conv4_2_Prob3 -> conv4_2_3 (in-place)
I1012 11:55:12.147500 31118 net.cpp:172] Setting up conv4_2_Prob3
I1012 11:55:12.147517 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.147521 31118 net.cpp:194] Memory required for data: 304099584
I1012 11:55:12.147526 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_2
I1012 11:55:12.147536 31118 net.cpp:128] Creating Layer conv4_Axpy_2
I1012 11:55:12.147541 31118 net.cpp:558] conv4_Axpy_2 <- conv4_2_3
I1012 11:55:12.147547 31118 net.cpp:558] conv4_Axpy_2 <- conv4_2_1_conv4_2_scale1_0_split_1
I1012 11:55:12.147552 31118 net.cpp:558] conv4_Axpy_2 <- conv4_Axpy_1_conv4_1ReLU_1_0_split_1
I1012 11:55:12.147558 31118 net.cpp:522] conv4_Axpy_2 -> conv4_Axpy_2
I1012 11:55:12.147635 31118 net.cpp:172] Setting up conv4_Axpy_2
I1012 11:55:12.147645 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.147650 31118 net.cpp:194] Memory required for data: 305148160
I1012 11:55:12.147653 31118 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I1012 11:55:12.147660 31118 net.cpp:128] Creating Layer conv4_2ReLU_1
I1012 11:55:12.147665 31118 net.cpp:558] conv4_2ReLU_1 <- conv4_Axpy_2
I1012 11:55:12.147670 31118 net.cpp:509] conv4_2ReLU_1 -> conv4_Axpy_2 (in-place)
I1012 11:55:12.149730 31118 net.cpp:172] Setting up conv4_2ReLU_1
I1012 11:55:12.149752 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.149757 31118 net.cpp:194] Memory required for data: 306196736
I1012 11:55:12.149762 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.149773 31118 net.cpp:128] Creating Layer conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.149778 31118 net.cpp:558] conv4_Axpy_2_conv4_2ReLU_1_0_split <- conv4_Axpy_2
I1012 11:55:12.149790 31118 net.cpp:522] conv4_Axpy_2_conv4_2ReLU_1_0_split -> conv4_Axpy_2_conv4_2ReLU_1_0_split_0
I1012 11:55:12.149801 31118 net.cpp:522] conv4_Axpy_2_conv4_2ReLU_1_0_split -> conv4_Axpy_2_conv4_2ReLU_1_0_split_1
I1012 11:55:12.149858 31118 net.cpp:172] Setting up conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.149868 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.149874 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.149878 31118 net.cpp:194] Memory required for data: 308293888
I1012 11:55:12.149883 31118 layer_factory.hpp:77] Creating layer conv4_3_0
I1012 11:55:12.149894 31118 net.cpp:128] Creating Layer conv4_3_0
I1012 11:55:12.149899 31118 net.cpp:558] conv4_3_0 <- conv4_Axpy_2_conv4_2ReLU_1_0_split_0
I1012 11:55:12.149924 31118 net.cpp:522] conv4_3_0 -> conv4_3_0
I1012 11:55:12.156509 31118 net.cpp:172] Setting up conv4_3_0
I1012 11:55:12.156538 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.156543 31118 net.cpp:194] Memory required for data: 309342464
I1012 11:55:12.156553 31118 layer_factory.hpp:77] Creating layer conv4_3_bn0
I1012 11:55:12.156561 31118 net.cpp:128] Creating Layer conv4_3_bn0
I1012 11:55:12.156566 31118 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I1012 11:55:12.156575 31118 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I1012 11:55:12.156850 31118 net.cpp:172] Setting up conv4_3_bn0
I1012 11:55:12.156863 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.156867 31118 net.cpp:194] Memory required for data: 310391040
I1012 11:55:12.156877 31118 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1012 11:55:12.156884 31118 net.cpp:128] Creating Layer conv4_3_scale0
I1012 11:55:12.156888 31118 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I1012 11:55:12.156896 31118 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I1012 11:55:12.156941 31118 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1012 11:55:12.157097 31118 net.cpp:172] Setting up conv4_3_scale0
I1012 11:55:12.157105 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.157109 31118 net.cpp:194] Memory required for data: 311439616
I1012 11:55:12.157117 31118 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I1012 11:55:12.157126 31118 net.cpp:128] Creating Layer conv4_3_ReLU0
I1012 11:55:12.157130 31118 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I1012 11:55:12.157136 31118 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I1012 11:55:12.158586 31118 net.cpp:172] Setting up conv4_3_ReLU0
I1012 11:55:12.158604 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.158608 31118 net.cpp:194] Memory required for data: 312488192
I1012 11:55:12.158613 31118 layer_factory.hpp:77] Creating layer conv4_3_1
I1012 11:55:12.158632 31118 net.cpp:128] Creating Layer conv4_3_1
I1012 11:55:12.158638 31118 net.cpp:558] conv4_3_1 <- conv4_3_0
I1012 11:55:12.158645 31118 net.cpp:522] conv4_3_1 -> conv4_3_1
I1012 11:55:12.165696 31118 net.cpp:172] Setting up conv4_3_1
I1012 11:55:12.165731 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.165736 31118 net.cpp:194] Memory required for data: 313536768
I1012 11:55:12.165746 31118 layer_factory.hpp:77] Creating layer conv4_3_bn1
I1012 11:55:12.165755 31118 net.cpp:128] Creating Layer conv4_3_bn1
I1012 11:55:12.165760 31118 net.cpp:558] conv4_3_bn1 <- conv4_3_1
I1012 11:55:12.165772 31118 net.cpp:509] conv4_3_bn1 -> conv4_3_1 (in-place)
I1012 11:55:12.166056 31118 net.cpp:172] Setting up conv4_3_bn1
I1012 11:55:12.166070 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.166074 31118 net.cpp:194] Memory required for data: 314585344
I1012 11:55:12.166085 31118 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1012 11:55:12.166091 31118 net.cpp:128] Creating Layer conv4_3_scale1
I1012 11:55:12.166095 31118 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I1012 11:55:12.166105 31118 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I1012 11:55:12.166151 31118 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1012 11:55:12.166309 31118 net.cpp:172] Setting up conv4_3_scale1
I1012 11:55:12.166317 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.166321 31118 net.cpp:194] Memory required for data: 315633920
I1012 11:55:12.166328 31118 layer_factory.hpp:77] Creating layer conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.166338 31118 net.cpp:128] Creating Layer conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.166343 31118 net.cpp:558] conv4_3_1_conv4_3_scale1_0_split <- conv4_3_1
I1012 11:55:12.166349 31118 net.cpp:522] conv4_3_1_conv4_3_scale1_0_split -> conv4_3_1_conv4_3_scale1_0_split_0
I1012 11:55:12.166359 31118 net.cpp:522] conv4_3_1_conv4_3_scale1_0_split -> conv4_3_1_conv4_3_scale1_0_split_1
I1012 11:55:12.166406 31118 net.cpp:172] Setting up conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.166430 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.166436 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.166440 31118 net.cpp:194] Memory required for data: 317731072
I1012 11:55:12.166445 31118 layer_factory.hpp:77] Creating layer conv4_3_Pooling
I1012 11:55:12.166451 31118 net.cpp:128] Creating Layer conv4_3_Pooling
I1012 11:55:12.166456 31118 net.cpp:558] conv4_3_Pooling <- conv4_3_1_conv4_3_scale1_0_split_0
I1012 11:55:12.166462 31118 net.cpp:522] conv4_3_Pooling -> conv4_3_Pooling
I1012 11:55:12.166493 31118 net.cpp:172] Setting up conv4_3_Pooling
I1012 11:55:12.166501 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.166504 31118 net.cpp:194] Memory required for data: 317747456
I1012 11:55:12.166508 31118 layer_factory.hpp:77] Creating layer conv4_3_2
I1012 11:55:12.166518 31118 net.cpp:128] Creating Layer conv4_3_2
I1012 11:55:12.166523 31118 net.cpp:558] conv4_3_2 <- conv4_3_Pooling
I1012 11:55:12.166532 31118 net.cpp:522] conv4_3_2 -> conv4_3_2
I1012 11:55:12.171980 31118 net.cpp:172] Setting up conv4_3_2
I1012 11:55:12.172005 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.172010 31118 net.cpp:194] Memory required for data: 317751552
I1012 11:55:12.172020 31118 layer_factory.hpp:77] Creating layer conv4_3_ReLU2
I1012 11:55:12.172029 31118 net.cpp:128] Creating Layer conv4_3_ReLU2
I1012 11:55:12.172036 31118 net.cpp:558] conv4_3_ReLU2 <- conv4_3_2
I1012 11:55:12.172044 31118 net.cpp:509] conv4_3_ReLU2 -> conv4_3_2 (in-place)
I1012 11:55:12.174053 31118 net.cpp:172] Setting up conv4_3_ReLU2
I1012 11:55:12.174072 31118 net.cpp:186] Top shape: 64 16 1 1 (1024)
I1012 11:55:12.174075 31118 net.cpp:194] Memory required for data: 317755648
I1012 11:55:12.174079 31118 layer_factory.hpp:77] Creating layer conv4_3_3
I1012 11:55:12.174093 31118 net.cpp:128] Creating Layer conv4_3_3
I1012 11:55:12.174098 31118 net.cpp:558] conv4_3_3 <- conv4_3_2
I1012 11:55:12.174108 31118 net.cpp:522] conv4_3_3 -> conv4_3_3
I1012 11:55:12.180842 31118 net.cpp:172] Setting up conv4_3_3
I1012 11:55:12.180868 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.180873 31118 net.cpp:194] Memory required for data: 317772032
I1012 11:55:12.180882 31118 layer_factory.hpp:77] Creating layer conv4_3_Prob3
I1012 11:55:12.180893 31118 net.cpp:128] Creating Layer conv4_3_Prob3
I1012 11:55:12.180899 31118 net.cpp:558] conv4_3_Prob3 <- conv4_3_3
I1012 11:55:12.180905 31118 net.cpp:509] conv4_3_Prob3 -> conv4_3_3 (in-place)
I1012 11:55:12.182902 31118 net.cpp:172] Setting up conv4_3_Prob3
I1012 11:55:12.182926 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.182931 31118 net.cpp:194] Memory required for data: 317788416
I1012 11:55:12.182936 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_3
I1012 11:55:12.182946 31118 net.cpp:128] Creating Layer conv4_Axpy_3
I1012 11:55:12.182952 31118 net.cpp:558] conv4_Axpy_3 <- conv4_3_3
I1012 11:55:12.182958 31118 net.cpp:558] conv4_Axpy_3 <- conv4_3_1_conv4_3_scale1_0_split_1
I1012 11:55:12.182965 31118 net.cpp:558] conv4_Axpy_3 <- conv4_Axpy_2_conv4_2ReLU_1_0_split_1
I1012 11:55:12.182971 31118 net.cpp:522] conv4_Axpy_3 -> conv4_Axpy_3
I1012 11:55:12.183048 31118 net.cpp:172] Setting up conv4_Axpy_3
I1012 11:55:12.183058 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.183061 31118 net.cpp:194] Memory required for data: 318836992
I1012 11:55:12.183065 31118 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I1012 11:55:12.183073 31118 net.cpp:128] Creating Layer conv4_3ReLU_1
I1012 11:55:12.183076 31118 net.cpp:558] conv4_3ReLU_1 <- conv4_Axpy_3
I1012 11:55:12.183081 31118 net.cpp:509] conv4_3ReLU_1 -> conv4_Axpy_3 (in-place)
I1012 11:55:12.185101 31118 net.cpp:172] Setting up conv4_3ReLU_1
I1012 11:55:12.185118 31118 net.cpp:186] Top shape: 64 64 8 8 (262144)
I1012 11:55:12.185122 31118 net.cpp:194] Memory required for data: 319885568
I1012 11:55:12.185127 31118 layer_factory.hpp:77] Creating layer Pooling1
I1012 11:55:12.185134 31118 net.cpp:128] Creating Layer Pooling1
I1012 11:55:12.185154 31118 net.cpp:558] Pooling1 <- conv4_Axpy_3
I1012 11:55:12.185168 31118 net.cpp:522] Pooling1 -> Pooling1
I1012 11:55:12.185202 31118 net.cpp:172] Setting up Pooling1
I1012 11:55:12.185214 31118 net.cpp:186] Top shape: 64 64 1 1 (4096)
I1012 11:55:12.185217 31118 net.cpp:194] Memory required for data: 319901952
I1012 11:55:12.185221 31118 layer_factory.hpp:77] Creating layer fc1
I1012 11:55:12.185232 31118 net.cpp:128] Creating Layer fc1
I1012 11:55:12.185236 31118 net.cpp:558] fc1 <- Pooling1
I1012 11:55:12.185245 31118 net.cpp:522] fc1 -> fc1
I1012 11:55:12.185420 31118 net.cpp:172] Setting up fc1
I1012 11:55:12.185436 31118 net.cpp:186] Top shape: 64 10 (640)
I1012 11:55:12.185441 31118 net.cpp:194] Memory required for data: 319904512
I1012 11:55:12.185449 31118 layer_factory.hpp:77] Creating layer Softmax1
I1012 11:55:12.185459 31118 net.cpp:128] Creating Layer Softmax1
I1012 11:55:12.185463 31118 net.cpp:558] Softmax1 <- fc1
I1012 11:55:12.185468 31118 net.cpp:558] Softmax1 <- label
I1012 11:55:12.185477 31118 net.cpp:522] Softmax1 -> Softmax1
I1012 11:55:12.185489 31118 layer_factory.hpp:77] Creating layer Softmax1
I1012 11:55:12.187443 31118 net.cpp:172] Setting up Softmax1
I1012 11:55:12.187464 31118 net.cpp:186] Top shape: (1)
I1012 11:55:12.187469 31118 net.cpp:189]     with loss weight 1
I1012 11:55:12.187513 31118 net.cpp:194] Memory required for data: 319904516
I1012 11:55:12.187518 31118 net.cpp:301] Softmax1 needs backward computation.
I1012 11:55:12.187525 31118 net.cpp:301] fc1 needs backward computation.
I1012 11:55:12.187528 31118 net.cpp:301] Pooling1 needs backward computation.
I1012 11:55:12.187532 31118 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I1012 11:55:12.187537 31118 net.cpp:301] conv4_Axpy_3 needs backward computation.
I1012 11:55:12.187542 31118 net.cpp:301] conv4_3_Prob3 needs backward computation.
I1012 11:55:12.187546 31118 net.cpp:301] conv4_3_3 needs backward computation.
I1012 11:55:12.187551 31118 net.cpp:301] conv4_3_ReLU2 needs backward computation.
I1012 11:55:12.187556 31118 net.cpp:301] conv4_3_2 needs backward computation.
I1012 11:55:12.187559 31118 net.cpp:301] conv4_3_Pooling needs backward computation.
I1012 11:55:12.187564 31118 net.cpp:301] conv4_3_1_conv4_3_scale1_0_split needs backward computation.
I1012 11:55:12.187568 31118 net.cpp:301] conv4_3_scale1 needs backward computation.
I1012 11:55:12.187572 31118 net.cpp:301] conv4_3_bn1 needs backward computation.
I1012 11:55:12.187577 31118 net.cpp:301] conv4_3_1 needs backward computation.
I1012 11:55:12.187582 31118 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I1012 11:55:12.187585 31118 net.cpp:301] conv4_3_scale0 needs backward computation.
I1012 11:55:12.187589 31118 net.cpp:301] conv4_3_bn0 needs backward computation.
I1012 11:55:12.187593 31118 net.cpp:301] conv4_3_0 needs backward computation.
I1012 11:55:12.187597 31118 net.cpp:301] conv4_Axpy_2_conv4_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.187602 31118 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I1012 11:55:12.187606 31118 net.cpp:301] conv4_Axpy_2 needs backward computation.
I1012 11:55:12.187614 31118 net.cpp:301] conv4_2_Prob3 needs backward computation.
I1012 11:55:12.187618 31118 net.cpp:301] conv4_2_3 needs backward computation.
I1012 11:55:12.187623 31118 net.cpp:301] conv4_2_ReLU2 needs backward computation.
I1012 11:55:12.187628 31118 net.cpp:301] conv4_2_2 needs backward computation.
I1012 11:55:12.187631 31118 net.cpp:301] conv4_2_Pooling needs backward computation.
I1012 11:55:12.187636 31118 net.cpp:301] conv4_2_1_conv4_2_scale1_0_split needs backward computation.
I1012 11:55:12.187640 31118 net.cpp:301] conv4_2_scale1 needs backward computation.
I1012 11:55:12.187644 31118 net.cpp:301] conv4_2_bn1 needs backward computation.
I1012 11:55:12.187649 31118 net.cpp:301] conv4_2_1 needs backward computation.
I1012 11:55:12.187654 31118 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I1012 11:55:12.187657 31118 net.cpp:301] conv4_2_scale0 needs backward computation.
I1012 11:55:12.187661 31118 net.cpp:301] conv4_2_bn0 needs backward computation.
I1012 11:55:12.187680 31118 net.cpp:301] conv4_2_0 needs backward computation.
I1012 11:55:12.187685 31118 net.cpp:301] conv4_Axpy_1_conv4_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.187690 31118 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I1012 11:55:12.187695 31118 net.cpp:301] conv4_Axpy_1 needs backward computation.
I1012 11:55:12.187700 31118 net.cpp:301] conv4_1_scale_down needs backward computation.
I1012 11:55:12.187705 31118 net.cpp:301] conv4_1_bn_down needs backward computation.
I1012 11:55:12.187708 31118 net.cpp:301] conv4_1_down needs backward computation.
I1012 11:55:12.187713 31118 net.cpp:301] conv4_1_Prob3 needs backward computation.
I1012 11:55:12.187717 31118 net.cpp:301] conv4_1_3 needs backward computation.
I1012 11:55:12.187722 31118 net.cpp:301] conv4_1_ReLU2 needs backward computation.
I1012 11:55:12.187726 31118 net.cpp:301] conv4_1_2 needs backward computation.
I1012 11:55:12.187731 31118 net.cpp:301] conv4_1_Pooling needs backward computation.
I1012 11:55:12.187736 31118 net.cpp:301] conv4_1_1_conv4_1_scale1_0_split needs backward computation.
I1012 11:55:12.187741 31118 net.cpp:301] conv4_1_scale1 needs backward computation.
I1012 11:55:12.187747 31118 net.cpp:301] conv4_1_bn1 needs backward computation.
I1012 11:55:12.187752 31118 net.cpp:301] conv4_1_1 needs backward computation.
I1012 11:55:12.187757 31118 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I1012 11:55:12.187760 31118 net.cpp:301] conv4_1_scale0 needs backward computation.
I1012 11:55:12.187765 31118 net.cpp:301] conv4_1_bn0 needs backward computation.
I1012 11:55:12.187769 31118 net.cpp:301] conv4_1_0 needs backward computation.
I1012 11:55:12.187774 31118 net.cpp:301] conv3_Axpy_3_conv3_3ReLU_1_0_split needs backward computation.
I1012 11:55:12.187778 31118 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I1012 11:55:12.187783 31118 net.cpp:301] conv3_Axpy_3 needs backward computation.
I1012 11:55:12.187789 31118 net.cpp:301] conv3_3_Prob3 needs backward computation.
I1012 11:55:12.187793 31118 net.cpp:301] conv3_3_3 needs backward computation.
I1012 11:55:12.187798 31118 net.cpp:301] conv3_3_ReLU2 needs backward computation.
I1012 11:55:12.187803 31118 net.cpp:301] conv3_3_2 needs backward computation.
I1012 11:55:12.187806 31118 net.cpp:301] conv3_3_Pooling needs backward computation.
I1012 11:55:12.187811 31118 net.cpp:301] conv3_3_1_conv3_3_scale1_0_split needs backward computation.
I1012 11:55:12.187816 31118 net.cpp:301] conv3_3_scale1 needs backward computation.
I1012 11:55:12.187820 31118 net.cpp:301] conv3_3_bn1 needs backward computation.
I1012 11:55:12.187824 31118 net.cpp:301] conv3_3_1 needs backward computation.
I1012 11:55:12.187829 31118 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I1012 11:55:12.187834 31118 net.cpp:301] conv3_3_scale0 needs backward computation.
I1012 11:55:12.187837 31118 net.cpp:301] conv3_3_bn0 needs backward computation.
I1012 11:55:12.187841 31118 net.cpp:301] conv3_3_0 needs backward computation.
I1012 11:55:12.187846 31118 net.cpp:301] conv3_Axpy_2_conv3_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.187851 31118 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I1012 11:55:12.187856 31118 net.cpp:301] conv3_Axpy_2 needs backward computation.
I1012 11:55:12.187862 31118 net.cpp:301] conv3_2_Prob3 needs backward computation.
I1012 11:55:12.187867 31118 net.cpp:301] conv3_2_3 needs backward computation.
I1012 11:55:12.187871 31118 net.cpp:301] conv3_2_ReLU2 needs backward computation.
I1012 11:55:12.187876 31118 net.cpp:301] conv3_2_2 needs backward computation.
I1012 11:55:12.187880 31118 net.cpp:301] conv3_2_Pooling needs backward computation.
I1012 11:55:12.187885 31118 net.cpp:301] conv3_2_1_conv3_2_scale1_0_split needs backward computation.
I1012 11:55:12.187891 31118 net.cpp:301] conv3_2_scale1 needs backward computation.
I1012 11:55:12.187896 31118 net.cpp:301] conv3_2_bn1 needs backward computation.
I1012 11:55:12.187899 31118 net.cpp:301] conv3_2_1 needs backward computation.
I1012 11:55:12.187911 31118 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I1012 11:55:12.187916 31118 net.cpp:301] conv3_2_scale0 needs backward computation.
I1012 11:55:12.187921 31118 net.cpp:301] conv3_2_bn0 needs backward computation.
I1012 11:55:12.187924 31118 net.cpp:301] conv3_2_0 needs backward computation.
I1012 11:55:12.187929 31118 net.cpp:301] conv3_Axpy_1_conv3_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.187934 31118 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I1012 11:55:12.187938 31118 net.cpp:301] conv3_Axpy_1 needs backward computation.
I1012 11:55:12.187947 31118 net.cpp:301] conv3_1_scale_down needs backward computation.
I1012 11:55:12.187952 31118 net.cpp:301] conv3_1_bn_down needs backward computation.
I1012 11:55:12.187957 31118 net.cpp:301] conv3_1_down needs backward computation.
I1012 11:55:12.187961 31118 net.cpp:301] conv3_1_Prob3 needs backward computation.
I1012 11:55:12.187966 31118 net.cpp:301] conv3_1_3 needs backward computation.
I1012 11:55:12.187970 31118 net.cpp:301] conv3_1_ReLU2 needs backward computation.
I1012 11:55:12.187974 31118 net.cpp:301] conv3_1_2 needs backward computation.
I1012 11:55:12.187979 31118 net.cpp:301] conv3_1_Pooling needs backward computation.
I1012 11:55:12.187984 31118 net.cpp:301] conv3_1_1_conv3_1_scale1_0_split needs backward computation.
I1012 11:55:12.187989 31118 net.cpp:301] conv3_1_scale1 needs backward computation.
I1012 11:55:12.187994 31118 net.cpp:301] conv3_1_bn1 needs backward computation.
I1012 11:55:12.187999 31118 net.cpp:301] conv3_1_1 needs backward computation.
I1012 11:55:12.188002 31118 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I1012 11:55:12.188007 31118 net.cpp:301] conv3_1_scale0 needs backward computation.
I1012 11:55:12.188012 31118 net.cpp:301] conv3_1_bn0 needs backward computation.
I1012 11:55:12.188016 31118 net.cpp:301] conv3_1_0 needs backward computation.
I1012 11:55:12.188021 31118 net.cpp:301] conv2_Axpy_3_conv2_3ReLU_1_0_split needs backward computation.
I1012 11:55:12.188026 31118 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I1012 11:55:12.188030 31118 net.cpp:301] conv2_Axpy_3 needs backward computation.
I1012 11:55:12.188036 31118 net.cpp:301] conv2_3_Prob3 needs backward computation.
I1012 11:55:12.188040 31118 net.cpp:301] conv2_3_3 needs backward computation.
I1012 11:55:12.188045 31118 net.cpp:301] conv2_3_ReLU2 needs backward computation.
I1012 11:55:12.188050 31118 net.cpp:301] conv2_3_2 needs backward computation.
I1012 11:55:12.188055 31118 net.cpp:301] conv2_3_Pooling needs backward computation.
I1012 11:55:12.188060 31118 net.cpp:301] conv2_3_1_conv2_3_scale1_0_split needs backward computation.
I1012 11:55:12.188064 31118 net.cpp:301] conv2_3_scale1 needs backward computation.
I1012 11:55:12.188068 31118 net.cpp:301] conv2_3_bn1 needs backward computation.
I1012 11:55:12.188073 31118 net.cpp:301] conv2_3_1 needs backward computation.
I1012 11:55:12.188077 31118 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I1012 11:55:12.188084 31118 net.cpp:301] conv2_3_scale0 needs backward computation.
I1012 11:55:12.188089 31118 net.cpp:301] conv2_3_bn0 needs backward computation.
I1012 11:55:12.188093 31118 net.cpp:301] conv2_3_0 needs backward computation.
I1012 11:55:12.188098 31118 net.cpp:301] conv2_Axpy_2_conv2_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.188103 31118 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I1012 11:55:12.188108 31118 net.cpp:301] conv2_Axpy_2 needs backward computation.
I1012 11:55:12.188114 31118 net.cpp:301] conv2_2_Prob3 needs backward computation.
I1012 11:55:12.188119 31118 net.cpp:301] conv2_2_3 needs backward computation.
I1012 11:55:12.188123 31118 net.cpp:301] conv2_2_ReLU2 needs backward computation.
I1012 11:55:12.188128 31118 net.cpp:301] conv2_2_2 needs backward computation.
I1012 11:55:12.188133 31118 net.cpp:301] conv2_2_Pooling needs backward computation.
I1012 11:55:12.188138 31118 net.cpp:301] conv2_2_1_conv2_2_scale1_0_split needs backward computation.
I1012 11:55:12.188150 31118 net.cpp:301] conv2_2_scale1 needs backward computation.
I1012 11:55:12.188155 31118 net.cpp:301] conv2_2_bn1 needs backward computation.
I1012 11:55:12.188159 31118 net.cpp:301] conv2_2_1 needs backward computation.
I1012 11:55:12.188163 31118 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I1012 11:55:12.188169 31118 net.cpp:301] conv2_2_scale0 needs backward computation.
I1012 11:55:12.188172 31118 net.cpp:301] conv2_2_bn0 needs backward computation.
I1012 11:55:12.188176 31118 net.cpp:301] conv2_2_0 needs backward computation.
I1012 11:55:12.188181 31118 net.cpp:301] conv2_Axpy_1_conv2_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.188186 31118 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I1012 11:55:12.188190 31118 net.cpp:301] conv2_Axpy_1 needs backward computation.
I1012 11:55:12.188196 31118 net.cpp:301] conv2_1_Prob3 needs backward computation.
I1012 11:55:12.188201 31118 net.cpp:301] conv2_1_3 needs backward computation.
I1012 11:55:12.188205 31118 net.cpp:301] conv2_1_ReLU2 needs backward computation.
I1012 11:55:12.188210 31118 net.cpp:301] conv2_1_2 needs backward computation.
I1012 11:55:12.188215 31118 net.cpp:301] conv2_1_Pooling needs backward computation.
I1012 11:55:12.188220 31118 net.cpp:301] conv2_1_1_conv2_1_scale1_0_split needs backward computation.
I1012 11:55:12.188225 31118 net.cpp:301] conv2_1_scale1 needs backward computation.
I1012 11:55:12.188230 31118 net.cpp:301] conv2_1_bn1 needs backward computation.
I1012 11:55:12.188233 31118 net.cpp:301] conv2_1_1 needs backward computation.
I1012 11:55:12.188238 31118 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I1012 11:55:12.188242 31118 net.cpp:301] conv2_1_scale0 needs backward computation.
I1012 11:55:12.188246 31118 net.cpp:301] conv2_1_bn0 needs backward computation.
I1012 11:55:12.188251 31118 net.cpp:301] conv2_1_0 needs backward computation.
I1012 11:55:12.188256 31118 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I1012 11:55:12.188261 31118 net.cpp:301] conv1/ReLU needs backward computation.
I1012 11:55:12.188266 31118 net.cpp:301] conv1/scale needs backward computation.
I1012 11:55:12.188271 31118 net.cpp:301] conv1/bn needs backward computation.
I1012 11:55:12.188274 31118 net.cpp:301] conv1 needs backward computation.
I1012 11:55:12.188282 31118 net.cpp:303] Data1 does not need backward computation.
I1012 11:55:12.188287 31118 net.cpp:348] This network produces output Softmax1
I1012 11:55:12.188388 31118 net.cpp:363] Network initialization done.
I1012 11:55:12.190143 31118 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_SE-ResNet_20.prototxt
I1012 11:55:12.190160 31118 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I1012 11:55:12.190169 31118 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_SE-ResNet_20.prototxt
I1012 11:55:12.191267 31118 net.cpp:82] Initializing net from parameters: 
name: "ResNet-20"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.00390625
    mirror: false
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
  image_data_param {
    shuffle: false
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_Pooling"
  type: "Pooling"
  bottom: "conv2_1_1"
  top: "conv2_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_1_2"
  type: "Convolution"
  bottom: "conv2_1_Pooling"
  top: "conv2_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_ReLU2"
  type: "ReLU"
  bottom: "conv2_1_2"
  top: "conv2_1_2"
}
layer {
  name: "conv2_1_3"
  type: "Convolution"
  bottom: "conv2_1_2"
  top: "conv2_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_Prob3"
  type: "Sigmoid"
  bottom: "conv2_1_3"
  top: "conv2_1_3"
}
layer {
  name: "conv2_Axpy_1"
  type: "Axpy"
  bottom: "conv2_1_3"
  bottom: "conv2_1_1"
  bottom: "conv1"
  top: "conv2_Axpy_1"
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_1"
  top: "conv2_Axpy_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Axpy_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_Pooling"
  type: "Pooling"
  bottom: "conv2_2_1"
  top: "conv2_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_2_2"
  type: "Convolution"
  bottom: "conv2_2_Pooling"
  top: "conv2_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_ReLU2"
  type: "ReLU"
  bottom: "conv2_2_2"
  top: "conv2_2_2"
}
layer {
  name: "conv2_2_3"
  type: "Convolution"
  bottom: "conv2_2_2"
  top: "conv2_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_Prob3"
  type: "Sigmoid"
  bottom: "conv2_2_3"
  top: "conv2_2_3"
}
layer {
  name: "conv2_Axpy_2"
  type: "Axpy"
  bottom: "conv2_2_3"
  bottom: "conv2_2_1"
  bottom: "conv2_Axpy_1"
  top: "conv2_Axpy_2"
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_2"
  top: "conv2_Axpy_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Axpy_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_Pooling"
  type: "Pooling"
  bottom: "conv2_3_1"
  top: "conv2_3_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv2_3_2"
  type: "Convolution"
  bottom: "conv2_3_Pooling"
  top: "conv2_3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_ReLU2"
  type: "ReLU"
  bottom: "conv2_3_2"
  top: "conv2_3_2"
}
layer {
  name: "conv2_3_3"
  type: "Convolution"
  bottom: "conv2_3_2"
  top: "conv2_3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_Prob3"
  type: "Sigmoid"
  bottom: "conv2_3_3"
  top: "conv2_3_3"
}
layer {
  name: "conv2_Axpy_3"
  type: "Axpy"
  bottom: "conv2_3_3"
  bottom: "conv2_3_1"
  bottom: "conv2_Axpy_2"
  top: "conv2_Axpy_3"
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Axpy_3"
  top: "conv2_Axpy_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Axpy_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_Pooling"
  type: "Pooling"
  bottom: "conv3_1_1"
  top: "conv3_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_1_2"
  type: "Convolution"
  bottom: "conv3_1_Pooling"
  top: "conv3_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_ReLU2"
  type: "ReLU"
  bottom: "conv3_1_2"
  top: "conv3_1_2"
}
layer {
  name: "conv3_1_3"
  type: "Convolution"
  bottom: "conv3_1_2"
  top: "conv3_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_Prob3"
  type: "Sigmoid"
  bottom: "conv3_1_3"
  top: "conv3_1_3"
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Axpy_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Axpy_1"
  type: "Axpy"
  bottom: "conv3_1_3"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Axpy_1"
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_1"
  top: "conv3_Axpy_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Axpy_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_Pooling"
  type: "Pooling"
  bottom: "conv3_2_1"
  top: "conv3_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_2_2"
  type: "Convolution"
  bottom: "conv3_2_Pooling"
  top: "conv3_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_ReLU2"
  type: "ReLU"
  bottom: "conv3_2_2"
  top: "conv3_2_2"
}
layer {
  name: "conv3_2_3"
  type: "Convolution"
  bottom: "conv3_2_2"
  top: "conv3_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_Prob3"
  type: "Sigmoid"
  bottom: "conv3_2_3"
  top: "conv3_2_3"
}
layer {
  name: "conv3_Axpy_2"
  type: "Axpy"
  bottom: "conv3_2_3"
  bottom: "conv3_2_1"
  bottom: "conv3_Axpy_1"
  top: "conv3_Axpy_2"
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_2"
  top: "conv3_Axpy_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Axpy_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_Pooling"
  type: "Pooling"
  bottom: "conv3_3_1"
  top: "conv3_3_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv3_3_2"
  type: "Convolution"
  bottom: "conv3_3_Pooling"
  top: "conv3_3_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_ReLU2"
  type: "ReLU"
  bottom: "conv3_3_2"
  top: "conv3_3_2"
}
layer {
  name: "conv3_3_3"
  type: "Convolution"
  bottom: "conv3_3_2"
  top: "conv3_3_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_Prob3"
  type: "Sigmoid"
  bottom: "conv3_3_3"
  top: "conv3_3_3"
}
layer {
  name: "conv3_Axpy_3"
  type: "Axpy"
  bottom: "conv3_3_3"
  bottom: "conv3_3_1"
  bottom: "conv3_Axpy_2"
  top: "conv3_Axpy_3"
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Axpy_3"
  top: "conv3_Axpy_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Axpy_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_Pooling"
  type: "Pooling"
  bottom: "conv4_1_1"
  top: "conv4_1_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv4_1_2"
  type: "Convolution"
  bottom: "conv4_1_Pooling"
  top: "conv4_1_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_ReLU2"
  type: "ReLU"
  bottom: "conv4_1_2"
  top: "conv4_1_2"
}
layer {
  name: "conv4_1_3"
  type: "Convolution"
  bottom: "conv4_1_2"
  top: "conv4_1_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_Prob3"
  type: "Sigmoid"
  bottom: "conv4_1_3"
  top: "conv4_1_3"
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Axpy_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Axpy_1"
  type: "Axpy"
  bottom: "conv4_1_3"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Axpy_1"
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Axpy_1"
  top: "conv4_Axpy_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Axpy_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_Pooling"
  type: "Pooling"
  bottom: "conv4_2_1"
  top: "conv4_2_Pooling"
  pooling_param {
    pool: AVE
    engine: CAFFE
    global_pooling: true
  }
}
layer {
  name: "conv4_2_2"
  type: "Convolution"
  bottom: "conv4_2_Pooling"
  top: "conv4_2_2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_ReLU2"
  type: "ReLU"
  bottom: "conv4_2_2"
  top: "conv4_2_2"
}
layer {
  name: "conv4_2_3"
  type: "Convolution"
  bottom: "conv4_2_2"
  top: "conv4_2_3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_Prob3"
  type: "Sigmoid"
  bottom: "conv4_2_3"
  top: "conv4_2_3"
}
layer {
  name: "conv4_Axpy_2"
  type: "Axpy"
  bottom: "conv4_2_3"
  bottom: "conv4_2_1"
  bottom: "conv4_Axpy_1"
  top: "conv4_Axpy_2"
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Axpy_2"
  top: "conv4_Axpy_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Axpy_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_Pooling"
  type: "Pooling"
  bottom: "conv4_3_1"
 
I1012 11:55:12.191859 31118 layer_factory.hpp:77] Creating layer Data1
I1012 11:55:12.191953 31118 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I1012 11:55:12.191970 31118 net.cpp:128] Creating Layer Data1
I1012 11:55:12.191977 31118 net.cpp:522] Data1 -> data
I1012 11:55:12.191988 31118 net.cpp:522] Data1 -> label
I1012 11:55:12.192140 31118 data_layer.cpp:45] output data size: 10,3,32,32
I1012 11:55:12.200493 31118 net.cpp:172] Setting up Data1
I1012 11:55:12.200512 31118 net.cpp:186] Top shape: 10 3 32 32 (30720)
I1012 11:55:12.200518 31118 net.cpp:186] Top shape: 10 (10)
I1012 11:55:12.200522 31118 net.cpp:194] Memory required for data: 122920
I1012 11:55:12.200527 31118 layer_factory.hpp:77] Creating layer label_Data1_1_split
I1012 11:55:12.200536 31118 net.cpp:128] Creating Layer label_Data1_1_split
I1012 11:55:12.200541 31118 net.cpp:558] label_Data1_1_split <- label
I1012 11:55:12.200547 31118 net.cpp:522] label_Data1_1_split -> label_Data1_1_split_0
I1012 11:55:12.200556 31118 net.cpp:522] label_Data1_1_split -> label_Data1_1_split_1
I1012 11:55:12.200795 31118 net.cpp:172] Setting up label_Data1_1_split
I1012 11:55:12.200803 31118 net.cpp:186] Top shape: 10 (10)
I1012 11:55:12.200809 31118 net.cpp:186] Top shape: 10 (10)
I1012 11:55:12.200812 31118 net.cpp:194] Memory required for data: 123000
I1012 11:55:12.200817 31118 layer_factory.hpp:77] Creating layer conv1
I1012 11:55:12.200829 31118 net.cpp:128] Creating Layer conv1
I1012 11:55:12.200834 31118 net.cpp:558] conv1 <- data
I1012 11:55:12.200842 31118 net.cpp:522] conv1 -> conv1
I1012 11:55:12.209553 31118 net.cpp:172] Setting up conv1
I1012 11:55:12.209585 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.209590 31118 net.cpp:194] Memory required for data: 778360
I1012 11:55:12.209607 31118 layer_factory.hpp:77] Creating layer conv1/bn
I1012 11:55:12.209617 31118 net.cpp:128] Creating Layer conv1/bn
I1012 11:55:12.209623 31118 net.cpp:558] conv1/bn <- conv1
I1012 11:55:12.209633 31118 net.cpp:509] conv1/bn -> conv1 (in-place)
I1012 11:55:12.210060 31118 net.cpp:172] Setting up conv1/bn
I1012 11:55:12.210084 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.210090 31118 net.cpp:194] Memory required for data: 1433720
I1012 11:55:12.210111 31118 layer_factory.hpp:77] Creating layer conv1/scale
I1012 11:55:12.210186 31118 net.cpp:128] Creating Layer conv1/scale
I1012 11:55:12.210192 31118 net.cpp:558] conv1/scale <- conv1
I1012 11:55:12.210204 31118 net.cpp:509] conv1/scale -> conv1 (in-place)
I1012 11:55:12.210294 31118 layer_factory.hpp:77] Creating layer conv1/scale
I1012 11:55:12.210525 31118 net.cpp:172] Setting up conv1/scale
I1012 11:55:12.210537 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.210543 31118 net.cpp:194] Memory required for data: 2089080
I1012 11:55:12.210553 31118 layer_factory.hpp:77] Creating layer conv1/ReLU
I1012 11:55:12.210569 31118 net.cpp:128] Creating Layer conv1/ReLU
I1012 11:55:12.210575 31118 net.cpp:558] conv1/ReLU <- conv1
I1012 11:55:12.210582 31118 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I1012 11:55:12.211570 31118 net.cpp:172] Setting up conv1/ReLU
I1012 11:55:12.211601 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.211606 31118 net.cpp:194] Memory required for data: 2744440
I1012 11:55:12.211611 31118 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I1012 11:55:12.211621 31118 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I1012 11:55:12.211627 31118 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I1012 11:55:12.211637 31118 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I1012 11:55:12.211659 31118 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I1012 11:55:12.211733 31118 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I1012 11:55:12.211745 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.211772 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.211777 31118 net.cpp:194] Memory required for data: 4055160
I1012 11:55:12.211782 31118 layer_factory.hpp:77] Creating layer conv2_1_0
I1012 11:55:12.211807 31118 net.cpp:128] Creating Layer conv2_1_0
I1012 11:55:12.211817 31118 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I1012 11:55:12.211825 31118 net.cpp:522] conv2_1_0 -> conv2_1_0
I1012 11:55:12.216158 31118 net.cpp:172] Setting up conv2_1_0
I1012 11:55:12.216182 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.216188 31118 net.cpp:194] Memory required for data: 4710520
I1012 11:55:12.216202 31118 layer_factory.hpp:77] Creating layer conv2_1_bn0
I1012 11:55:12.216223 31118 net.cpp:128] Creating Layer conv2_1_bn0
I1012 11:55:12.216230 31118 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I1012 11:55:12.216239 31118 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I1012 11:55:12.216589 31118 net.cpp:172] Setting up conv2_1_bn0
I1012 11:55:12.216603 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.216609 31118 net.cpp:194] Memory required for data: 5365880
I1012 11:55:12.216620 31118 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1012 11:55:12.216629 31118 net.cpp:128] Creating Layer conv2_1_scale0
I1012 11:55:12.216634 31118 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I1012 11:55:12.216641 31118 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I1012 11:55:12.216704 31118 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1012 11:55:12.216898 31118 net.cpp:172] Setting up conv2_1_scale0
I1012 11:55:12.216912 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.216917 31118 net.cpp:194] Memory required for data: 6021240
I1012 11:55:12.216926 31118 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I1012 11:55:12.216936 31118 net.cpp:128] Creating Layer conv2_1_ReLU0
I1012 11:55:12.216943 31118 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I1012 11:55:12.216951 31118 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I1012 11:55:12.218191 31118 net.cpp:172] Setting up conv2_1_ReLU0
I1012 11:55:12.218216 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.218221 31118 net.cpp:194] Memory required for data: 6676600
I1012 11:55:12.218225 31118 layer_factory.hpp:77] Creating layer conv2_1_1
I1012 11:55:12.218240 31118 net.cpp:128] Creating Layer conv2_1_1
I1012 11:55:12.218245 31118 net.cpp:558] conv2_1_1 <- conv2_1_0
I1012 11:55:12.218256 31118 net.cpp:522] conv2_1_1 -> conv2_1_1
I1012 11:55:12.224989 31118 net.cpp:172] Setting up conv2_1_1
I1012 11:55:12.225019 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.225024 31118 net.cpp:194] Memory required for data: 7331960
I1012 11:55:12.225034 31118 layer_factory.hpp:77] Creating layer conv2_1_bn1
I1012 11:55:12.225042 31118 net.cpp:128] Creating Layer conv2_1_bn1
I1012 11:55:12.225047 31118 net.cpp:558] conv2_1_bn1 <- conv2_1_1
I1012 11:55:12.225056 31118 net.cpp:509] conv2_1_bn1 -> conv2_1_1 (in-place)
I1012 11:55:12.225338 31118 net.cpp:172] Setting up conv2_1_bn1
I1012 11:55:12.225353 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.225358 31118 net.cpp:194] Memory required for data: 7987320
I1012 11:55:12.225369 31118 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1012 11:55:12.225378 31118 net.cpp:128] Creating Layer conv2_1_scale1
I1012 11:55:12.225383 31118 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I1012 11:55:12.225387 31118 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I1012 11:55:12.225437 31118 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1012 11:55:12.225594 31118 net.cpp:172] Setting up conv2_1_scale1
I1012 11:55:12.225601 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.225605 31118 net.cpp:194] Memory required for data: 8642680
I1012 11:55:12.225612 31118 layer_factory.hpp:77] Creating layer conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:12.225620 31118 net.cpp:128] Creating Layer conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:12.225623 31118 net.cpp:558] conv2_1_1_conv2_1_scale1_0_split <- conv2_1_1
I1012 11:55:12.225647 31118 net.cpp:522] conv2_1_1_conv2_1_scale1_0_split -> conv2_1_1_conv2_1_scale1_0_split_0
I1012 11:55:12.225656 31118 net.cpp:522] conv2_1_1_conv2_1_scale1_0_split -> conv2_1_1_conv2_1_scale1_0_split_1
I1012 11:55:12.225705 31118 net.cpp:172] Setting up conv2_1_1_conv2_1_scale1_0_split
I1012 11:55:12.225713 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.225718 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.225731 31118 net.cpp:194] Memory required for data: 9953400
I1012 11:55:12.225735 31118 layer_factory.hpp:77] Creating layer conv2_1_Pooling
I1012 11:55:12.225744 31118 net.cpp:128] Creating Layer conv2_1_Pooling
I1012 11:55:12.225747 31118 net.cpp:558] conv2_1_Pooling <- conv2_1_1_conv2_1_scale1_0_split_0
I1012 11:55:12.225754 31118 net.cpp:522] conv2_1_Pooling -> conv2_1_Pooling
I1012 11:55:12.225785 31118 net.cpp:172] Setting up conv2_1_Pooling
I1012 11:55:12.225792 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.225796 31118 net.cpp:194] Memory required for data: 9954040
I1012 11:55:12.225800 31118 layer_factory.hpp:77] Creating layer conv2_1_2
I1012 11:55:12.225813 31118 net.cpp:128] Creating Layer conv2_1_2
I1012 11:55:12.225818 31118 net.cpp:558] conv2_1_2 <- conv2_1_Pooling
I1012 11:55:12.225824 31118 net.cpp:522] conv2_1_2 -> conv2_1_2
I1012 11:55:12.231626 31118 net.cpp:172] Setting up conv2_1_2
I1012 11:55:12.231652 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.231657 31118 net.cpp:194] Memory required for data: 9954680
I1012 11:55:12.231667 31118 layer_factory.hpp:77] Creating layer conv2_1_ReLU2
I1012 11:55:12.231676 31118 net.cpp:128] Creating Layer conv2_1_ReLU2
I1012 11:55:12.231681 31118 net.cpp:558] conv2_1_ReLU2 <- conv2_1_2
I1012 11:55:12.231688 31118 net.cpp:509] conv2_1_ReLU2 -> conv2_1_2 (in-place)
I1012 11:55:12.233680 31118 net.cpp:172] Setting up conv2_1_ReLU2
I1012 11:55:12.233700 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.233705 31118 net.cpp:194] Memory required for data: 9955320
I1012 11:55:12.233709 31118 layer_factory.hpp:77] Creating layer conv2_1_3
I1012 11:55:12.233732 31118 net.cpp:128] Creating Layer conv2_1_3
I1012 11:55:12.233738 31118 net.cpp:558] conv2_1_3 <- conv2_1_2
I1012 11:55:12.233745 31118 net.cpp:522] conv2_1_3 -> conv2_1_3
I1012 11:55:12.240468 31118 net.cpp:172] Setting up conv2_1_3
I1012 11:55:12.240496 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.240501 31118 net.cpp:194] Memory required for data: 9955960
I1012 11:55:12.240510 31118 layer_factory.hpp:77] Creating layer conv2_1_Prob3
I1012 11:55:12.240520 31118 net.cpp:128] Creating Layer conv2_1_Prob3
I1012 11:55:12.240525 31118 net.cpp:558] conv2_1_Prob3 <- conv2_1_3
I1012 11:55:12.240530 31118 net.cpp:509] conv2_1_Prob3 -> conv2_1_3 (in-place)
I1012 11:55:12.242534 31118 net.cpp:172] Setting up conv2_1_Prob3
I1012 11:55:12.242553 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.242558 31118 net.cpp:194] Memory required for data: 9956600
I1012 11:55:12.242563 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_1
I1012 11:55:12.242571 31118 net.cpp:128] Creating Layer conv2_Axpy_1
I1012 11:55:12.242576 31118 net.cpp:558] conv2_Axpy_1 <- conv2_1_3
I1012 11:55:12.242581 31118 net.cpp:558] conv2_Axpy_1 <- conv2_1_1_conv2_1_scale1_0_split_1
I1012 11:55:12.242588 31118 net.cpp:558] conv2_Axpy_1 <- conv1_conv1/ReLU_0_split_1
I1012 11:55:12.242594 31118 net.cpp:522] conv2_Axpy_1 -> conv2_Axpy_1
I1012 11:55:12.242667 31118 net.cpp:172] Setting up conv2_Axpy_1
I1012 11:55:12.242674 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.242678 31118 net.cpp:194] Memory required for data: 10611960
I1012 11:55:12.242682 31118 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I1012 11:55:12.242691 31118 net.cpp:128] Creating Layer conv2_1ReLU_1
I1012 11:55:12.242696 31118 net.cpp:558] conv2_1ReLU_1 <- conv2_Axpy_1
I1012 11:55:12.242702 31118 net.cpp:509] conv2_1ReLU_1 -> conv2_Axpy_1 (in-place)
I1012 11:55:12.244740 31118 net.cpp:172] Setting up conv2_1ReLU_1
I1012 11:55:12.244768 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.244787 31118 net.cpp:194] Memory required for data: 11267320
I1012 11:55:12.244793 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.244801 31118 net.cpp:128] Creating Layer conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.244808 31118 net.cpp:558] conv2_Axpy_1_conv2_1ReLU_1_0_split <- conv2_Axpy_1
I1012 11:55:12.244817 31118 net.cpp:522] conv2_Axpy_1_conv2_1ReLU_1_0_split -> conv2_Axpy_1_conv2_1ReLU_1_0_split_0
I1012 11:55:12.244827 31118 net.cpp:522] conv2_Axpy_1_conv2_1ReLU_1_0_split -> conv2_Axpy_1_conv2_1ReLU_1_0_split_1
I1012 11:55:12.244887 31118 net.cpp:172] Setting up conv2_Axpy_1_conv2_1ReLU_1_0_split
I1012 11:55:12.244895 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.244901 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.244905 31118 net.cpp:194] Memory required for data: 12578040
I1012 11:55:12.244910 31118 layer_factory.hpp:77] Creating layer conv2_2_0
I1012 11:55:12.244922 31118 net.cpp:128] Creating Layer conv2_2_0
I1012 11:55:12.244926 31118 net.cpp:558] conv2_2_0 <- conv2_Axpy_1_conv2_1ReLU_1_0_split_0
I1012 11:55:12.244940 31118 net.cpp:522] conv2_2_0 -> conv2_2_0
I1012 11:55:12.251534 31118 net.cpp:172] Setting up conv2_2_0
I1012 11:55:12.251559 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.251564 31118 net.cpp:194] Memory required for data: 13233400
I1012 11:55:12.251574 31118 layer_factory.hpp:77] Creating layer conv2_2_bn0
I1012 11:55:12.251585 31118 net.cpp:128] Creating Layer conv2_2_bn0
I1012 11:55:12.251590 31118 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I1012 11:55:12.251600 31118 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I1012 11:55:12.251880 31118 net.cpp:172] Setting up conv2_2_bn0
I1012 11:55:12.251893 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.251897 31118 net.cpp:194] Memory required for data: 13888760
I1012 11:55:12.251907 31118 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1012 11:55:12.251916 31118 net.cpp:128] Creating Layer conv2_2_scale0
I1012 11:55:12.251920 31118 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I1012 11:55:12.251926 31118 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I1012 11:55:12.251977 31118 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1012 11:55:12.252136 31118 net.cpp:172] Setting up conv2_2_scale0
I1012 11:55:12.252146 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.252151 31118 net.cpp:194] Memory required for data: 14544120
I1012 11:55:12.252158 31118 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I1012 11:55:12.252167 31118 net.cpp:128] Creating Layer conv2_2_ReLU0
I1012 11:55:12.252172 31118 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I1012 11:55:12.252178 31118 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I1012 11:55:12.253582 31118 net.cpp:172] Setting up conv2_2_ReLU0
I1012 11:55:12.253600 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.253604 31118 net.cpp:194] Memory required for data: 15199480
I1012 11:55:12.253609 31118 layer_factory.hpp:77] Creating layer conv2_2_1
I1012 11:55:12.253624 31118 net.cpp:128] Creating Layer conv2_2_1
I1012 11:55:12.253629 31118 net.cpp:558] conv2_2_1 <- conv2_2_0
I1012 11:55:12.253636 31118 net.cpp:522] conv2_2_1 -> conv2_2_1
I1012 11:55:12.260367 31118 net.cpp:172] Setting up conv2_2_1
I1012 11:55:12.260393 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.260398 31118 net.cpp:194] Memory required for data: 15854840
I1012 11:55:12.260414 31118 layer_factory.hpp:77] Creating layer conv2_2_bn1
I1012 11:55:12.260424 31118 net.cpp:128] Creating Layer conv2_2_bn1
I1012 11:55:12.260429 31118 net.cpp:558] conv2_2_bn1 <- conv2_2_1
I1012 11:55:12.260442 31118 net.cpp:509] conv2_2_bn1 -> conv2_2_1 (in-place)
I1012 11:55:12.260730 31118 net.cpp:172] Setting up conv2_2_bn1
I1012 11:55:12.260740 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.260746 31118 net.cpp:194] Memory required for data: 16510200
I1012 11:55:12.260754 31118 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1012 11:55:12.260780 31118 net.cpp:128] Creating Layer conv2_2_scale1
I1012 11:55:12.260787 31118 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I1012 11:55:12.260794 31118 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I1012 11:55:12.260844 31118 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1012 11:55:12.260998 31118 net.cpp:172] Setting up conv2_2_scale1
I1012 11:55:12.261013 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.261016 31118 net.cpp:194] Memory required for data: 17165560
I1012 11:55:12.261024 31118 layer_factory.hpp:77] Creating layer conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.261030 31118 net.cpp:128] Creating Layer conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.261035 31118 net.cpp:558] conv2_2_1_conv2_2_scale1_0_split <- conv2_2_1
I1012 11:55:12.261042 31118 net.cpp:522] conv2_2_1_conv2_2_scale1_0_split -> conv2_2_1_conv2_2_scale1_0_split_0
I1012 11:55:12.261054 31118 net.cpp:522] conv2_2_1_conv2_2_scale1_0_split -> conv2_2_1_conv2_2_scale1_0_split_1
I1012 11:55:12.261101 31118 net.cpp:172] Setting up conv2_2_1_conv2_2_scale1_0_split
I1012 11:55:12.261111 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.261117 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.261121 31118 net.cpp:194] Memory required for data: 18476280
I1012 11:55:12.261126 31118 layer_factory.hpp:77] Creating layer conv2_2_Pooling
I1012 11:55:12.261134 31118 net.cpp:128] Creating Layer conv2_2_Pooling
I1012 11:55:12.261139 31118 net.cpp:558] conv2_2_Pooling <- conv2_2_1_conv2_2_scale1_0_split_0
I1012 11:55:12.261145 31118 net.cpp:522] conv2_2_Pooling -> conv2_2_Pooling
I1012 11:55:12.261173 31118 net.cpp:172] Setting up conv2_2_Pooling
I1012 11:55:12.261179 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.261183 31118 net.cpp:194] Memory required for data: 18476920
I1012 11:55:12.261188 31118 layer_factory.hpp:77] Creating layer conv2_2_2
I1012 11:55:12.261201 31118 net.cpp:128] Creating Layer conv2_2_2
I1012 11:55:12.261206 31118 net.cpp:558] conv2_2_2 <- conv2_2_Pooling
I1012 11:55:12.261214 31118 net.cpp:522] conv2_2_2 -> conv2_2_2
I1012 11:55:12.267014 31118 net.cpp:172] Setting up conv2_2_2
I1012 11:55:12.267040 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.267043 31118 net.cpp:194] Memory required for data: 18477560
I1012 11:55:12.267053 31118 layer_factory.hpp:77] Creating layer conv2_2_ReLU2
I1012 11:55:12.267065 31118 net.cpp:128] Creating Layer conv2_2_ReLU2
I1012 11:55:12.267069 31118 net.cpp:558] conv2_2_ReLU2 <- conv2_2_2
I1012 11:55:12.267081 31118 net.cpp:509] conv2_2_ReLU2 -> conv2_2_2 (in-place)
I1012 11:55:12.269078 31118 net.cpp:172] Setting up conv2_2_ReLU2
I1012 11:55:12.269094 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.269098 31118 net.cpp:194] Memory required for data: 18478200
I1012 11:55:12.269104 31118 layer_factory.hpp:77] Creating layer conv2_2_3
I1012 11:55:12.269120 31118 net.cpp:128] Creating Layer conv2_2_3
I1012 11:55:12.269126 31118 net.cpp:558] conv2_2_3 <- conv2_2_2
I1012 11:55:12.269146 31118 net.cpp:522] conv2_2_3 -> conv2_2_3
I1012 11:55:12.275842 31118 net.cpp:172] Setting up conv2_2_3
I1012 11:55:12.275867 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.275872 31118 net.cpp:194] Memory required for data: 18478840
I1012 11:55:12.275882 31118 layer_factory.hpp:77] Creating layer conv2_2_Prob3
I1012 11:55:12.275889 31118 net.cpp:128] Creating Layer conv2_2_Prob3
I1012 11:55:12.275894 31118 net.cpp:558] conv2_2_Prob3 <- conv2_2_3
I1012 11:55:12.275903 31118 net.cpp:509] conv2_2_Prob3 -> conv2_2_3 (in-place)
I1012 11:55:12.280217 31118 net.cpp:172] Setting up conv2_2_Prob3
I1012 11:55:12.280237 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.280242 31118 net.cpp:194] Memory required for data: 18479480
I1012 11:55:12.280251 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_2
I1012 11:55:12.280261 31118 net.cpp:128] Creating Layer conv2_Axpy_2
I1012 11:55:12.280269 31118 net.cpp:558] conv2_Axpy_2 <- conv2_2_3
I1012 11:55:12.280277 31118 net.cpp:558] conv2_Axpy_2 <- conv2_2_1_conv2_2_scale1_0_split_1
I1012 11:55:12.280299 31118 net.cpp:558] conv2_Axpy_2 <- conv2_Axpy_1_conv2_1ReLU_1_0_split_1
I1012 11:55:12.280308 31118 net.cpp:522] conv2_Axpy_2 -> conv2_Axpy_2
I1012 11:55:12.280393 31118 net.cpp:172] Setting up conv2_Axpy_2
I1012 11:55:12.280405 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.280408 31118 net.cpp:194] Memory required for data: 19134840
I1012 11:55:12.280412 31118 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I1012 11:55:12.280421 31118 net.cpp:128] Creating Layer conv2_2ReLU_1
I1012 11:55:12.280426 31118 net.cpp:558] conv2_2ReLU_1 <- conv2_Axpy_2
I1012 11:55:12.280431 31118 net.cpp:509] conv2_2ReLU_1 -> conv2_Axpy_2 (in-place)
I1012 11:55:12.280681 31118 net.cpp:172] Setting up conv2_2ReLU_1
I1012 11:55:12.280694 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.280699 31118 net.cpp:194] Memory required for data: 19790200
I1012 11:55:12.280704 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.280712 31118 net.cpp:128] Creating Layer conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.280717 31118 net.cpp:558] conv2_Axpy_2_conv2_2ReLU_1_0_split <- conv2_Axpy_2
I1012 11:55:12.280724 31118 net.cpp:522] conv2_Axpy_2_conv2_2ReLU_1_0_split -> conv2_Axpy_2_conv2_2ReLU_1_0_split_0
I1012 11:55:12.280733 31118 net.cpp:522] conv2_Axpy_2_conv2_2ReLU_1_0_split -> conv2_Axpy_2_conv2_2ReLU_1_0_split_1
I1012 11:55:12.280787 31118 net.cpp:172] Setting up conv2_Axpy_2_conv2_2ReLU_1_0_split
I1012 11:55:12.280797 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.280803 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.280807 31118 net.cpp:194] Memory required for data: 21100920
I1012 11:55:12.280812 31118 layer_factory.hpp:77] Creating layer conv2_3_0
I1012 11:55:12.280823 31118 net.cpp:128] Creating Layer conv2_3_0
I1012 11:55:12.280828 31118 net.cpp:558] conv2_3_0 <- conv2_Axpy_2_conv2_2ReLU_1_0_split_0
I1012 11:55:12.280838 31118 net.cpp:522] conv2_3_0 -> conv2_3_0
I1012 11:55:12.286896 31118 net.cpp:172] Setting up conv2_3_0
I1012 11:55:12.286926 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.286931 31118 net.cpp:194] Memory required for data: 21756280
I1012 11:55:12.286947 31118 layer_factory.hpp:77] Creating layer conv2_3_bn0
I1012 11:55:12.286959 31118 net.cpp:128] Creating Layer conv2_3_bn0
I1012 11:55:12.286974 31118 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I1012 11:55:12.286984 31118 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I1012 11:55:12.287348 31118 net.cpp:172] Setting up conv2_3_bn0
I1012 11:55:12.287361 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.287366 31118 net.cpp:194] Memory required for data: 22411640
I1012 11:55:12.287379 31118 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1012 11:55:12.287386 31118 net.cpp:128] Creating Layer conv2_3_scale0
I1012 11:55:12.287392 31118 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I1012 11:55:12.287400 31118 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I1012 11:55:12.287467 31118 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1012 11:55:12.287667 31118 net.cpp:172] Setting up conv2_3_scale0
I1012 11:55:12.287678 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.287683 31118 net.cpp:194] Memory required for data: 23067000
I1012 11:55:12.287693 31118 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I1012 11:55:12.287701 31118 net.cpp:128] Creating Layer conv2_3_ReLU0
I1012 11:55:12.287706 31118 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I1012 11:55:12.287715 31118 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I1012 11:55:12.288913 31118 net.cpp:172] Setting up conv2_3_ReLU0
I1012 11:55:12.288933 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.288938 31118 net.cpp:194] Memory required for data: 23722360
I1012 11:55:12.288944 31118 layer_factory.hpp:77] Creating layer conv2_3_1
I1012 11:55:12.288960 31118 net.cpp:128] Creating Layer conv2_3_1
I1012 11:55:12.288967 31118 net.cpp:558] conv2_3_1 <- conv2_3_0
I1012 11:55:12.288996 31118 net.cpp:522] conv2_3_1 -> conv2_3_1
I1012 11:55:12.295676 31118 net.cpp:172] Setting up conv2_3_1
I1012 11:55:12.295702 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.295706 31118 net.cpp:194] Memory required for data: 24377720
I1012 11:55:12.295717 31118 layer_factory.hpp:77] Creating layer conv2_3_bn1
I1012 11:55:12.295727 31118 net.cpp:128] Creating Layer conv2_3_bn1
I1012 11:55:12.295732 31118 net.cpp:558] conv2_3_bn1 <- conv2_3_1
I1012 11:55:12.295742 31118 net.cpp:509] conv2_3_bn1 -> conv2_3_1 (in-place)
I1012 11:55:12.296036 31118 net.cpp:172] Setting up conv2_3_bn1
I1012 11:55:12.296047 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.296053 31118 net.cpp:194] Memory required for data: 25033080
I1012 11:55:12.296062 31118 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1012 11:55:12.296072 31118 net.cpp:128] Creating Layer conv2_3_scale1
I1012 11:55:12.296077 31118 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I1012 11:55:12.296082 31118 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I1012 11:55:12.296133 31118 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1012 11:55:12.296298 31118 net.cpp:172] Setting up conv2_3_scale1
I1012 11:55:12.296308 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.296311 31118 net.cpp:194] Memory required for data: 25688440
I1012 11:55:12.296319 31118 layer_factory.hpp:77] Creating layer conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.296329 31118 net.cpp:128] Creating Layer conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.296332 31118 net.cpp:558] conv2_3_1_conv2_3_scale1_0_split <- conv2_3_1
I1012 11:55:12.296341 31118 net.cpp:522] conv2_3_1_conv2_3_scale1_0_split -> conv2_3_1_conv2_3_scale1_0_split_0
I1012 11:55:12.296350 31118 net.cpp:522] conv2_3_1_conv2_3_scale1_0_split -> conv2_3_1_conv2_3_scale1_0_split_1
I1012 11:55:12.296398 31118 net.cpp:172] Setting up conv2_3_1_conv2_3_scale1_0_split
I1012 11:55:12.296406 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.296411 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.296416 31118 net.cpp:194] Memory required for data: 26999160
I1012 11:55:12.296419 31118 layer_factory.hpp:77] Creating layer conv2_3_Pooling
I1012 11:55:12.296425 31118 net.cpp:128] Creating Layer conv2_3_Pooling
I1012 11:55:12.296430 31118 net.cpp:558] conv2_3_Pooling <- conv2_3_1_conv2_3_scale1_0_split_0
I1012 11:55:12.296435 31118 net.cpp:522] conv2_3_Pooling -> conv2_3_Pooling
I1012 11:55:12.296466 31118 net.cpp:172] Setting up conv2_3_Pooling
I1012 11:55:12.296473 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.296476 31118 net.cpp:194] Memory required for data: 26999800
I1012 11:55:12.296480 31118 layer_factory.hpp:77] Creating layer conv2_3_2
I1012 11:55:12.296489 31118 net.cpp:128] Creating Layer conv2_3_2
I1012 11:55:12.296494 31118 net.cpp:558] conv2_3_2 <- conv2_3_Pooling
I1012 11:55:12.296504 31118 net.cpp:522] conv2_3_2 -> conv2_3_2
I1012 11:55:12.302390 31118 net.cpp:172] Setting up conv2_3_2
I1012 11:55:12.302418 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.302423 31118 net.cpp:194] Memory required for data: 27000440
I1012 11:55:12.302435 31118 layer_factory.hpp:77] Creating layer conv2_3_ReLU2
I1012 11:55:12.302445 31118 net.cpp:128] Creating Layer conv2_3_ReLU2
I1012 11:55:12.302451 31118 net.cpp:558] conv2_3_ReLU2 <- conv2_3_2
I1012 11:55:12.302466 31118 net.cpp:509] conv2_3_ReLU2 -> conv2_3_2 (in-place)
I1012 11:55:12.304396 31118 net.cpp:172] Setting up conv2_3_ReLU2
I1012 11:55:12.304415 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.304420 31118 net.cpp:194] Memory required for data: 27001080
I1012 11:55:12.304426 31118 layer_factory.hpp:77] Creating layer conv2_3_3
I1012 11:55:12.304442 31118 net.cpp:128] Creating Layer conv2_3_3
I1012 11:55:12.304448 31118 net.cpp:558] conv2_3_3 <- conv2_3_2
I1012 11:55:12.304466 31118 net.cpp:522] conv2_3_3 -> conv2_3_3
I1012 11:55:12.311226 31118 net.cpp:172] Setting up conv2_3_3
I1012 11:55:12.311256 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.311288 31118 net.cpp:194] Memory required for data: 27001720
I1012 11:55:12.311301 31118 layer_factory.hpp:77] Creating layer conv2_3_Prob3
I1012 11:55:12.311314 31118 net.cpp:128] Creating Layer conv2_3_Prob3
I1012 11:55:12.311326 31118 net.cpp:558] conv2_3_Prob3 <- conv2_3_3
I1012 11:55:12.311334 31118 net.cpp:509] conv2_3_Prob3 -> conv2_3_3 (in-place)
I1012 11:55:12.313251 31118 net.cpp:172] Setting up conv2_3_Prob3
I1012 11:55:12.313278 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.313284 31118 net.cpp:194] Memory required for data: 27002360
I1012 11:55:12.313290 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_3
I1012 11:55:12.313307 31118 net.cpp:128] Creating Layer conv2_Axpy_3
I1012 11:55:12.313313 31118 net.cpp:558] conv2_Axpy_3 <- conv2_3_3
I1012 11:55:12.313323 31118 net.cpp:558] conv2_Axpy_3 <- conv2_3_1_conv2_3_scale1_0_split_1
I1012 11:55:12.313329 31118 net.cpp:558] conv2_Axpy_3 <- conv2_Axpy_2_conv2_2ReLU_1_0_split_1
I1012 11:55:12.313338 31118 net.cpp:522] conv2_Axpy_3 -> conv2_Axpy_3
I1012 11:55:12.313437 31118 net.cpp:172] Setting up conv2_Axpy_3
I1012 11:55:12.313446 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.313452 31118 net.cpp:194] Memory required for data: 27657720
I1012 11:55:12.313457 31118 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I1012 11:55:12.313467 31118 net.cpp:128] Creating Layer conv2_3ReLU_1
I1012 11:55:12.313473 31118 net.cpp:558] conv2_3ReLU_1 <- conv2_Axpy_3
I1012 11:55:12.313479 31118 net.cpp:509] conv2_3ReLU_1 -> conv2_Axpy_3 (in-place)
I1012 11:55:12.315440 31118 net.cpp:172] Setting up conv2_3ReLU_1
I1012 11:55:12.315459 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.315464 31118 net.cpp:194] Memory required for data: 28313080
I1012 11:55:12.315467 31118 layer_factory.hpp:77] Creating layer conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.315480 31118 net.cpp:128] Creating Layer conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.315485 31118 net.cpp:558] conv2_Axpy_3_conv2_3ReLU_1_0_split <- conv2_Axpy_3
I1012 11:55:12.315491 31118 net.cpp:522] conv2_Axpy_3_conv2_3ReLU_1_0_split -> conv2_Axpy_3_conv2_3ReLU_1_0_split_0
I1012 11:55:12.315503 31118 net.cpp:522] conv2_Axpy_3_conv2_3ReLU_1_0_split -> conv2_Axpy_3_conv2_3ReLU_1_0_split_1
I1012 11:55:12.315558 31118 net.cpp:172] Setting up conv2_Axpy_3_conv2_3ReLU_1_0_split
I1012 11:55:12.315565 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.315570 31118 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1012 11:55:12.315575 31118 net.cpp:194] Memory required for data: 29623800
I1012 11:55:12.315579 31118 layer_factory.hpp:77] Creating layer conv3_1_0
I1012 11:55:12.315593 31118 net.cpp:128] Creating Layer conv3_1_0
I1012 11:55:12.315598 31118 net.cpp:558] conv3_1_0 <- conv2_Axpy_3_conv2_3ReLU_1_0_split_0
I1012 11:55:12.315605 31118 net.cpp:522] conv3_1_0 -> conv3_1_0
I1012 11:55:12.326458 31118 net.cpp:172] Setting up conv3_1_0
I1012 11:55:12.326489 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.326496 31118 net.cpp:194] Memory required for data: 29951480
I1012 11:55:12.326510 31118 layer_factory.hpp:77] Creating layer conv3_1_bn0
I1012 11:55:12.326522 31118 net.cpp:128] Creating Layer conv3_1_bn0
I1012 11:55:12.326535 31118 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I1012 11:55:12.326545 31118 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I1012 11:55:12.326964 31118 net.cpp:172] Setting up conv3_1_bn0
I1012 11:55:12.326980 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.326987 31118 net.cpp:194] Memory required for data: 30279160
I1012 11:55:12.327016 31118 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1012 11:55:12.327031 31118 net.cpp:128] Creating Layer conv3_1_scale0
I1012 11:55:12.327039 31118 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I1012 11:55:12.327047 31118 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I1012 11:55:12.327128 31118 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1012 11:55:12.327373 31118 net.cpp:172] Setting up conv3_1_scale0
I1012 11:55:12.327384 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.327414 31118 net.cpp:194] Memory required for data: 30606840
I1012 11:55:12.327426 31118 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I1012 11:55:12.327435 31118 net.cpp:128] Creating Layer conv3_1_ReLU0
I1012 11:55:12.327442 31118 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I1012 11:55:12.327455 31118 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I1012 11:55:12.328420 31118 net.cpp:172] Setting up conv3_1_ReLU0
I1012 11:55:12.328444 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.328450 31118 net.cpp:194] Memory required for data: 30934520
I1012 11:55:12.328459 31118 layer_factory.hpp:77] Creating layer conv3_1_1
I1012 11:55:12.328477 31118 net.cpp:128] Creating Layer conv3_1_1
I1012 11:55:12.328490 31118 net.cpp:558] conv3_1_1 <- conv3_1_0
I1012 11:55:12.328503 31118 net.cpp:522] conv3_1_1 -> conv3_1_1
I1012 11:55:12.335253 31118 net.cpp:172] Setting up conv3_1_1
I1012 11:55:12.335289 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.335296 31118 net.cpp:194] Memory required for data: 31262200
I1012 11:55:12.335310 31118 layer_factory.hpp:77] Creating layer conv3_1_bn1
I1012 11:55:12.335322 31118 net.cpp:128] Creating Layer conv3_1_bn1
I1012 11:55:12.335332 31118 net.cpp:558] conv3_1_bn1 <- conv3_1_1
I1012 11:55:12.335345 31118 net.cpp:509] conv3_1_bn1 -> conv3_1_1 (in-place)
I1012 11:55:12.335785 31118 net.cpp:172] Setting up conv3_1_bn1
I1012 11:55:12.335800 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.335808 31118 net.cpp:194] Memory required for data: 31589880
I1012 11:55:12.335821 31118 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1012 11:55:12.335832 31118 net.cpp:128] Creating Layer conv3_1_scale1
I1012 11:55:12.335839 31118 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I1012 11:55:12.335847 31118 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I1012 11:55:12.335927 31118 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1012 11:55:12.336172 31118 net.cpp:172] Setting up conv3_1_scale1
I1012 11:55:12.336186 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.336194 31118 net.cpp:194] Memory required for data: 31917560
I1012 11:55:12.336205 31118 layer_factory.hpp:77] Creating layer conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.336215 31118 net.cpp:128] Creating Layer conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.336221 31118 net.cpp:558] conv3_1_1_conv3_1_scale1_0_split <- conv3_1_1
I1012 11:55:12.336230 31118 net.cpp:522] conv3_1_1_conv3_1_scale1_0_split -> conv3_1_1_conv3_1_scale1_0_split_0
I1012 11:55:12.336246 31118 net.cpp:522] conv3_1_1_conv3_1_scale1_0_split -> conv3_1_1_conv3_1_scale1_0_split_1
I1012 11:55:12.336313 31118 net.cpp:172] Setting up conv3_1_1_conv3_1_scale1_0_split
I1012 11:55:12.336324 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.336333 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.336338 31118 net.cpp:194] Memory required for data: 32572920
I1012 11:55:12.336345 31118 layer_factory.hpp:77] Creating layer conv3_1_Pooling
I1012 11:55:12.336357 31118 net.cpp:128] Creating Layer conv3_1_Pooling
I1012 11:55:12.336365 31118 net.cpp:558] conv3_1_Pooling <- conv3_1_1_conv3_1_scale1_0_split_0
I1012 11:55:12.336374 31118 net.cpp:522] conv3_1_Pooling -> conv3_1_Pooling
I1012 11:55:12.336417 31118 net.cpp:172] Setting up conv3_1_Pooling
I1012 11:55:12.336428 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.336433 31118 net.cpp:194] Memory required for data: 32574200
I1012 11:55:12.336441 31118 layer_factory.hpp:77] Creating layer conv3_1_2
I1012 11:55:12.336463 31118 net.cpp:128] Creating Layer conv3_1_2
I1012 11:55:12.336470 31118 net.cpp:558] conv3_1_2 <- conv3_1_Pooling
I1012 11:55:12.336482 31118 net.cpp:522] conv3_1_2 -> conv3_1_2
I1012 11:55:12.343879 31118 net.cpp:172] Setting up conv3_1_2
I1012 11:55:12.343905 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.343909 31118 net.cpp:194] Memory required for data: 32574840
I1012 11:55:12.343920 31118 layer_factory.hpp:77] Creating layer conv3_1_ReLU2
I1012 11:55:12.343943 31118 net.cpp:128] Creating Layer conv3_1_ReLU2
I1012 11:55:12.343953 31118 net.cpp:558] conv3_1_ReLU2 <- conv3_1_2
I1012 11:55:12.343963 31118 net.cpp:509] conv3_1_ReLU2 -> conv3_1_2 (in-place)
I1012 11:55:12.345939 31118 net.cpp:172] Setting up conv3_1_ReLU2
I1012 11:55:12.345968 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.345973 31118 net.cpp:194] Memory required for data: 32575480
I1012 11:55:12.345978 31118 layer_factory.hpp:77] Creating layer conv3_1_3
I1012 11:55:12.346002 31118 net.cpp:128] Creating Layer conv3_1_3
I1012 11:55:12.346014 31118 net.cpp:558] conv3_1_3 <- conv3_1_2
I1012 11:55:12.346026 31118 net.cpp:522] conv3_1_3 -> conv3_1_3
I1012 11:55:12.352753 31118 net.cpp:172] Setting up conv3_1_3
I1012 11:55:12.352782 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.352787 31118 net.cpp:194] Memory required for data: 32576760
I1012 11:55:12.352799 31118 layer_factory.hpp:77] Creating layer conv3_1_Prob3
I1012 11:55:12.352809 31118 net.cpp:128] Creating Layer conv3_1_Prob3
I1012 11:55:12.352815 31118 net.cpp:558] conv3_1_Prob3 <- conv3_1_3
I1012 11:55:12.352829 31118 net.cpp:509] conv3_1_Prob3 -> conv3_1_3 (in-place)
I1012 11:55:12.354743 31118 net.cpp:172] Setting up conv3_1_Prob3
I1012 11:55:12.354763 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.354768 31118 net.cpp:194] Memory required for data: 32578040
I1012 11:55:12.354774 31118 layer_factory.hpp:77] Creating layer conv3_1_down
I1012 11:55:12.354791 31118 net.cpp:128] Creating Layer conv3_1_down
I1012 11:55:12.354797 31118 net.cpp:558] conv3_1_down <- conv2_Axpy_3_conv2_3ReLU_1_0_split_1
I1012 11:55:12.354811 31118 net.cpp:522] conv3_1_down -> conv3_1_down
I1012 11:55:12.361528 31118 net.cpp:172] Setting up conv3_1_down
I1012 11:55:12.361554 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.361558 31118 net.cpp:194] Memory required for data: 32905720
I1012 11:55:12.361568 31118 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I1012 11:55:12.361580 31118 net.cpp:128] Creating Layer conv3_1_bn_down
I1012 11:55:12.361585 31118 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I1012 11:55:12.361593 31118 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I1012 11:55:12.361896 31118 net.cpp:172] Setting up conv3_1_bn_down
I1012 11:55:12.361909 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.361915 31118 net.cpp:194] Memory required for data: 33233400
I1012 11:55:12.361925 31118 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1012 11:55:12.361933 31118 net.cpp:128] Creating Layer conv3_1_scale_down
I1012 11:55:12.361937 31118 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I1012 11:55:12.361945 31118 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I1012 11:55:12.362000 31118 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1012 11:55:12.362160 31118 net.cpp:172] Setting up conv3_1_scale_down
I1012 11:55:12.362205 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.362210 31118 net.cpp:194] Memory required for data: 33561080
I1012 11:55:12.362217 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_1
I1012 11:55:12.362223 31118 net.cpp:128] Creating Layer conv3_Axpy_1
I1012 11:55:12.362231 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_3
I1012 11:55:12.362236 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_1_conv3_1_scale1_0_split_1
I1012 11:55:12.362241 31118 net.cpp:558] conv3_Axpy_1 <- conv3_1_down
I1012 11:55:12.362247 31118 net.cpp:522] conv3_Axpy_1 -> conv3_Axpy_1
I1012 11:55:12.362309 31118 net.cpp:172] Setting up conv3_Axpy_1
I1012 11:55:12.362316 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.362320 31118 net.cpp:194] Memory required for data: 33888760
I1012 11:55:12.362324 31118 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I1012 11:55:12.362330 31118 net.cpp:128] Creating Layer conv3_1ReLU_1
I1012 11:55:12.362334 31118 net.cpp:558] conv3_1ReLU_1 <- conv3_Axpy_1
I1012 11:55:12.362342 31118 net.cpp:509] conv3_1ReLU_1 -> conv3_Axpy_1 (in-place)
I1012 11:55:12.363585 31118 net.cpp:172] Setting up conv3_1ReLU_1
I1012 11:55:12.363620 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.363624 31118 net.cpp:194] Memory required for data: 34216440
I1012 11:55:12.363628 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.363638 31118 net.cpp:128] Creating Layer conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.363643 31118 net.cpp:558] conv3_Axpy_1_conv3_1ReLU_1_0_split <- conv3_Axpy_1
I1012 11:55:12.363654 31118 net.cpp:522] conv3_Axpy_1_conv3_1ReLU_1_0_split -> conv3_Axpy_1_conv3_1ReLU_1_0_split_0
I1012 11:55:12.363667 31118 net.cpp:522] conv3_Axpy_1_conv3_1ReLU_1_0_split -> conv3_Axpy_1_conv3_1ReLU_1_0_split_1
I1012 11:55:12.363723 31118 net.cpp:172] Setting up conv3_Axpy_1_conv3_1ReLU_1_0_split
I1012 11:55:12.363729 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.363735 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.363739 31118 net.cpp:194] Memory required for data: 34871800
I1012 11:55:12.363744 31118 layer_factory.hpp:77] Creating layer conv3_2_0
I1012 11:55:12.363755 31118 net.cpp:128] Creating Layer conv3_2_0
I1012 11:55:12.363760 31118 net.cpp:558] conv3_2_0 <- conv3_Axpy_1_conv3_1ReLU_1_0_split_0
I1012 11:55:12.363770 31118 net.cpp:522] conv3_2_0 -> conv3_2_0
I1012 11:55:12.370498 31118 net.cpp:172] Setting up conv3_2_0
I1012 11:55:12.370527 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.370532 31118 net.cpp:194] Memory required for data: 35199480
I1012 11:55:12.370544 31118 layer_factory.hpp:77] Creating layer conv3_2_bn0
I1012 11:55:12.370558 31118 net.cpp:128] Creating Layer conv3_2_bn0
I1012 11:55:12.370566 31118 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I1012 11:55:12.370574 31118 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I1012 11:55:12.370928 31118 net.cpp:172] Setting up conv3_2_bn0
I1012 11:55:12.370940 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.370945 31118 net.cpp:194] Memory required for data: 35527160
I1012 11:55:12.370956 31118 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1012 11:55:12.370971 31118 net.cpp:128] Creating Layer conv3_2_scale0
I1012 11:55:12.370978 31118 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I1012 11:55:12.370985 31118 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I1012 11:55:12.371052 31118 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1012 11:55:12.371254 31118 net.cpp:172] Setting up conv3_2_scale0
I1012 11:55:12.371264 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.371269 31118 net.cpp:194] Memory required for data: 35854840
I1012 11:55:12.371279 31118 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I1012 11:55:12.371289 31118 net.cpp:128] Creating Layer conv3_2_ReLU0
I1012 11:55:12.371294 31118 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I1012 11:55:12.371304 31118 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I1012 11:55:12.372622 31118 net.cpp:172] Setting up conv3_2_ReLU0
I1012 11:55:12.372653 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.372658 31118 net.cpp:194] Memory required for data: 36182520
I1012 11:55:12.372664 31118 layer_factory.hpp:77] Creating layer conv3_2_1
I1012 11:55:12.372680 31118 net.cpp:128] Creating Layer conv3_2_1
I1012 11:55:12.372687 31118 net.cpp:558] conv3_2_1 <- conv3_2_0
I1012 11:55:12.372699 31118 net.cpp:522] conv3_2_1 -> conv3_2_1
I1012 11:55:12.375963 31118 net.cpp:172] Setting up conv3_2_1
I1012 11:55:12.375990 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.375994 31118 net.cpp:194] Memory required for data: 36510200
I1012 11:55:12.376004 31118 layer_factory.hpp:77] Creating layer conv3_2_bn1
I1012 11:55:12.376015 31118 net.cpp:128] Creating Layer conv3_2_bn1
I1012 11:55:12.376021 31118 net.cpp:558] conv3_2_bn1 <- conv3_2_1
I1012 11:55:12.376030 31118 net.cpp:509] conv3_2_bn1 -> conv3_2_1 (in-place)
I1012 11:55:12.376322 31118 net.cpp:172] Setting up conv3_2_bn1
I1012 11:55:12.376334 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.376339 31118 net.cpp:194] Memory required for data: 36837880
I1012 11:55:12.376348 31118 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1012 11:55:12.376382 31118 net.cpp:128] Creating Layer conv3_2_scale1
I1012 11:55:12.376387 31118 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I1012 11:55:12.376394 31118 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I1012 11:55:12.376448 31118 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1012 11:55:12.376617 31118 net.cpp:172] Setting up conv3_2_scale1
I1012 11:55:12.376629 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.376633 31118 net.cpp:194] Memory required for data: 37165560
I1012 11:55:12.376641 31118 layer_factory.hpp:77] Creating layer conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.376652 31118 net.cpp:128] Creating Layer conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.376655 31118 net.cpp:558] conv3_2_1_conv3_2_scale1_0_split <- conv3_2_1
I1012 11:55:12.376664 31118 net.cpp:522] conv3_2_1_conv3_2_scale1_0_split -> conv3_2_1_conv3_2_scale1_0_split_0
I1012 11:55:12.376672 31118 net.cpp:522] conv3_2_1_conv3_2_scale1_0_split -> conv3_2_1_conv3_2_scale1_0_split_1
I1012 11:55:12.376720 31118 net.cpp:172] Setting up conv3_2_1_conv3_2_scale1_0_split
I1012 11:55:12.376730 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.376736 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.376740 31118 net.cpp:194] Memory required for data: 37820920
I1012 11:55:12.376744 31118 layer_factory.hpp:77] Creating layer conv3_2_Pooling
I1012 11:55:12.376751 31118 net.cpp:128] Creating Layer conv3_2_Pooling
I1012 11:55:12.376755 31118 net.cpp:558] conv3_2_Pooling <- conv3_2_1_conv3_2_scale1_0_split_0
I1012 11:55:12.376765 31118 net.cpp:522] conv3_2_Pooling -> conv3_2_Pooling
I1012 11:55:12.376796 31118 net.cpp:172] Setting up conv3_2_Pooling
I1012 11:55:12.376806 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.376811 31118 net.cpp:194] Memory required for data: 37822200
I1012 11:55:12.376814 31118 layer_factory.hpp:77] Creating layer conv3_2_2
I1012 11:55:12.376828 31118 net.cpp:128] Creating Layer conv3_2_2
I1012 11:55:12.376833 31118 net.cpp:558] conv3_2_2 <- conv3_2_Pooling
I1012 11:55:12.376839 31118 net.cpp:522] conv3_2_2 -> conv3_2_2
I1012 11:55:12.382122 31118 net.cpp:172] Setting up conv3_2_2
I1012 11:55:12.382143 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.382148 31118 net.cpp:194] Memory required for data: 37822840
I1012 11:55:12.382158 31118 layer_factory.hpp:77] Creating layer conv3_2_ReLU2
I1012 11:55:12.382165 31118 net.cpp:128] Creating Layer conv3_2_ReLU2
I1012 11:55:12.382170 31118 net.cpp:558] conv3_2_ReLU2 <- conv3_2_2
I1012 11:55:12.382179 31118 net.cpp:509] conv3_2_ReLU2 -> conv3_2_2 (in-place)
I1012 11:55:12.384145 31118 net.cpp:172] Setting up conv3_2_ReLU2
I1012 11:55:12.384162 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.384166 31118 net.cpp:194] Memory required for data: 37823480
I1012 11:55:12.384171 31118 layer_factory.hpp:77] Creating layer conv3_2_3
I1012 11:55:12.384184 31118 net.cpp:128] Creating Layer conv3_2_3
I1012 11:55:12.384188 31118 net.cpp:558] conv3_2_3 <- conv3_2_2
I1012 11:55:12.384198 31118 net.cpp:522] conv3_2_3 -> conv3_2_3
I1012 11:55:12.390928 31118 net.cpp:172] Setting up conv3_2_3
I1012 11:55:12.390951 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.390956 31118 net.cpp:194] Memory required for data: 37824760
I1012 11:55:12.390966 31118 layer_factory.hpp:77] Creating layer conv3_2_Prob3
I1012 11:55:12.390974 31118 net.cpp:128] Creating Layer conv3_2_Prob3
I1012 11:55:12.390980 31118 net.cpp:558] conv3_2_Prob3 <- conv3_2_3
I1012 11:55:12.390988 31118 net.cpp:509] conv3_2_Prob3 -> conv3_2_3 (in-place)
I1012 11:55:12.392976 31118 net.cpp:172] Setting up conv3_2_Prob3
I1012 11:55:12.392985 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.392989 31118 net.cpp:194] Memory required for data: 37826040
I1012 11:55:12.392993 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_2
I1012 11:55:12.393002 31118 net.cpp:128] Creating Layer conv3_Axpy_2
I1012 11:55:12.393005 31118 net.cpp:558] conv3_Axpy_2 <- conv3_2_3
I1012 11:55:12.393029 31118 net.cpp:558] conv3_Axpy_2 <- conv3_2_1_conv3_2_scale1_0_split_1
I1012 11:55:12.393035 31118 net.cpp:558] conv3_Axpy_2 <- conv3_Axpy_1_conv3_1ReLU_1_0_split_1
I1012 11:55:12.393044 31118 net.cpp:522] conv3_Axpy_2 -> conv3_Axpy_2
I1012 11:55:12.393126 31118 net.cpp:172] Setting up conv3_Axpy_2
I1012 11:55:12.393136 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.393139 31118 net.cpp:194] Memory required for data: 38153720
I1012 11:55:12.393143 31118 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I1012 11:55:12.393149 31118 net.cpp:128] Creating Layer conv3_2ReLU_1
I1012 11:55:12.393154 31118 net.cpp:558] conv3_2ReLU_1 <- conv3_Axpy_2
I1012 11:55:12.393159 31118 net.cpp:509] conv3_2ReLU_1 -> conv3_Axpy_2 (in-place)
I1012 11:55:12.395184 31118 net.cpp:172] Setting up conv3_2ReLU_1
I1012 11:55:12.395202 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.395207 31118 net.cpp:194] Memory required for data: 38481400
I1012 11:55:12.395211 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.395220 31118 net.cpp:128] Creating Layer conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.395223 31118 net.cpp:558] conv3_Axpy_2_conv3_2ReLU_1_0_split <- conv3_Axpy_2
I1012 11:55:12.395231 31118 net.cpp:522] conv3_Axpy_2_conv3_2ReLU_1_0_split -> conv3_Axpy_2_conv3_2ReLU_1_0_split_0
I1012 11:55:12.395241 31118 net.cpp:522] conv3_Axpy_2_conv3_2ReLU_1_0_split -> conv3_Axpy_2_conv3_2ReLU_1_0_split_1
I1012 11:55:12.395292 31118 net.cpp:172] Setting up conv3_Axpy_2_conv3_2ReLU_1_0_split
I1012 11:55:12.395300 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.395305 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.395309 31118 net.cpp:194] Memory required for data: 39136760
I1012 11:55:12.395314 31118 layer_factory.hpp:77] Creating layer conv3_3_0
I1012 11:55:12.395328 31118 net.cpp:128] Creating Layer conv3_3_0
I1012 11:55:12.395332 31118 net.cpp:558] conv3_3_0 <- conv3_Axpy_2_conv3_2ReLU_1_0_split_0
I1012 11:55:12.395341 31118 net.cpp:522] conv3_3_0 -> conv3_3_0
I1012 11:55:12.401965 31118 net.cpp:172] Setting up conv3_3_0
I1012 11:55:12.401991 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.401996 31118 net.cpp:194] Memory required for data: 39464440
I1012 11:55:12.402006 31118 layer_factory.hpp:77] Creating layer conv3_3_bn0
I1012 11:55:12.402020 31118 net.cpp:128] Creating Layer conv3_3_bn0
I1012 11:55:12.402025 31118 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I1012 11:55:12.402034 31118 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I1012 11:55:12.402336 31118 net.cpp:172] Setting up conv3_3_bn0
I1012 11:55:12.402346 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.402350 31118 net.cpp:194] Memory required for data: 39792120
I1012 11:55:12.402361 31118 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1012 11:55:12.402372 31118 net.cpp:128] Creating Layer conv3_3_scale0
I1012 11:55:12.402377 31118 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I1012 11:55:12.402386 31118 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I1012 11:55:12.402444 31118 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1012 11:55:12.402617 31118 net.cpp:172] Setting up conv3_3_scale0
I1012 11:55:12.402627 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.402631 31118 net.cpp:194] Memory required for data: 40119800
I1012 11:55:12.402639 31118 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I1012 11:55:12.402647 31118 net.cpp:128] Creating Layer conv3_3_ReLU0
I1012 11:55:12.402655 31118 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I1012 11:55:12.402667 31118 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I1012 11:55:12.404016 31118 net.cpp:172] Setting up conv3_3_ReLU0
I1012 11:55:12.404042 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.404047 31118 net.cpp:194] Memory required for data: 40447480
I1012 11:55:12.404052 31118 layer_factory.hpp:77] Creating layer conv3_3_1
I1012 11:55:12.404067 31118 net.cpp:128] Creating Layer conv3_3_1
I1012 11:55:12.404075 31118 net.cpp:558] conv3_3_1 <- conv3_3_0
I1012 11:55:12.404103 31118 net.cpp:522] conv3_3_1 -> conv3_3_1
I1012 11:55:12.410843 31118 net.cpp:172] Setting up conv3_3_1
I1012 11:55:12.410874 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.410878 31118 net.cpp:194] Memory required for data: 40775160
I1012 11:55:12.410892 31118 layer_factory.hpp:77] Creating layer conv3_3_bn1
I1012 11:55:12.410907 31118 net.cpp:128] Creating Layer conv3_3_bn1
I1012 11:55:12.410912 31118 net.cpp:558] conv3_3_bn1 <- conv3_3_1
I1012 11:55:12.410923 31118 net.cpp:509] conv3_3_bn1 -> conv3_3_1 (in-place)
I1012 11:55:12.411218 31118 net.cpp:172] Setting up conv3_3_bn1
I1012 11:55:12.411231 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.411234 31118 net.cpp:194] Memory required for data: 41102840
I1012 11:55:12.411244 31118 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1012 11:55:12.411254 31118 net.cpp:128] Creating Layer conv3_3_scale1
I1012 11:55:12.411258 31118 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I1012 11:55:12.411264 31118 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I1012 11:55:12.411325 31118 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1012 11:55:12.411504 31118 net.cpp:172] Setting up conv3_3_scale1
I1012 11:55:12.411515 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.411520 31118 net.cpp:194] Memory required for data: 41430520
I1012 11:55:12.411527 31118 layer_factory.hpp:77] Creating layer conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.411536 31118 net.cpp:128] Creating Layer conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.411541 31118 net.cpp:558] conv3_3_1_conv3_3_scale1_0_split <- conv3_3_1
I1012 11:55:12.411550 31118 net.cpp:522] conv3_3_1_conv3_3_scale1_0_split -> conv3_3_1_conv3_3_scale1_0_split_0
I1012 11:55:12.411559 31118 net.cpp:522] conv3_3_1_conv3_3_scale1_0_split -> conv3_3_1_conv3_3_scale1_0_split_1
I1012 11:55:12.411604 31118 net.cpp:172] Setting up conv3_3_1_conv3_3_scale1_0_split
I1012 11:55:12.411612 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.411617 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.411622 31118 net.cpp:194] Memory required for data: 42085880
I1012 11:55:12.411625 31118 layer_factory.hpp:77] Creating layer conv3_3_Pooling
I1012 11:55:12.411634 31118 net.cpp:128] Creating Layer conv3_3_Pooling
I1012 11:55:12.411638 31118 net.cpp:558] conv3_3_Pooling <- conv3_3_1_conv3_3_scale1_0_split_0
I1012 11:55:12.411644 31118 net.cpp:522] conv3_3_Pooling -> conv3_3_Pooling
I1012 11:55:12.411679 31118 net.cpp:172] Setting up conv3_3_Pooling
I1012 11:55:12.411685 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.411689 31118 net.cpp:194] Memory required for data: 42087160
I1012 11:55:12.411693 31118 layer_factory.hpp:77] Creating layer conv3_3_2
I1012 11:55:12.411703 31118 net.cpp:128] Creating Layer conv3_3_2
I1012 11:55:12.411708 31118 net.cpp:558] conv3_3_2 <- conv3_3_Pooling
I1012 11:55:12.411720 31118 net.cpp:522] conv3_3_2 -> conv3_3_2
I1012 11:55:12.417471 31118 net.cpp:172] Setting up conv3_3_2
I1012 11:55:12.417496 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.417500 31118 net.cpp:194] Memory required for data: 42087800
I1012 11:55:12.417511 31118 layer_factory.hpp:77] Creating layer conv3_3_ReLU2
I1012 11:55:12.417523 31118 net.cpp:128] Creating Layer conv3_3_ReLU2
I1012 11:55:12.417529 31118 net.cpp:558] conv3_3_ReLU2 <- conv3_3_2
I1012 11:55:12.417536 31118 net.cpp:509] conv3_3_ReLU2 -> conv3_3_2 (in-place)
I1012 11:55:12.419489 31118 net.cpp:172] Setting up conv3_3_ReLU2
I1012 11:55:12.419507 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.419512 31118 net.cpp:194] Memory required for data: 42088440
I1012 11:55:12.419517 31118 layer_factory.hpp:77] Creating layer conv3_3_3
I1012 11:55:12.419529 31118 net.cpp:128] Creating Layer conv3_3_3
I1012 11:55:12.419534 31118 net.cpp:558] conv3_3_3 <- conv3_3_2
I1012 11:55:12.419544 31118 net.cpp:522] conv3_3_3 -> conv3_3_3
I1012 11:55:12.426337 31118 net.cpp:172] Setting up conv3_3_3
I1012 11:55:12.426371 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.426406 31118 net.cpp:194] Memory required for data: 42089720
I1012 11:55:12.426419 31118 layer_factory.hpp:77] Creating layer conv3_3_Prob3
I1012 11:55:12.426432 31118 net.cpp:128] Creating Layer conv3_3_Prob3
I1012 11:55:12.426440 31118 net.cpp:558] conv3_3_Prob3 <- conv3_3_3
I1012 11:55:12.426450 31118 net.cpp:509] conv3_3_Prob3 -> conv3_3_3 (in-place)
I1012 11:55:12.428333 31118 net.cpp:172] Setting up conv3_3_Prob3
I1012 11:55:12.428349 31118 net.cpp:186] Top shape: 10 32 1 1 (320)
I1012 11:55:12.428354 31118 net.cpp:194] Memory required for data: 42091000
I1012 11:55:12.428359 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_3
I1012 11:55:12.428367 31118 net.cpp:128] Creating Layer conv3_Axpy_3
I1012 11:55:12.428371 31118 net.cpp:558] conv3_Axpy_3 <- conv3_3_3
I1012 11:55:12.428378 31118 net.cpp:558] conv3_Axpy_3 <- conv3_3_1_conv3_3_scale1_0_split_1
I1012 11:55:12.428385 31118 net.cpp:558] conv3_Axpy_3 <- conv3_Axpy_2_conv3_2ReLU_1_0_split_1
I1012 11:55:12.428401 31118 net.cpp:522] conv3_Axpy_3 -> conv3_Axpy_3
I1012 11:55:12.428484 31118 net.cpp:172] Setting up conv3_Axpy_3
I1012 11:55:12.428496 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.428500 31118 net.cpp:194] Memory required for data: 42418680
I1012 11:55:12.428504 31118 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I1012 11:55:12.428514 31118 net.cpp:128] Creating Layer conv3_3ReLU_1
I1012 11:55:12.428519 31118 net.cpp:558] conv3_3ReLU_1 <- conv3_Axpy_3
I1012 11:55:12.428524 31118 net.cpp:509] conv3_3ReLU_1 -> conv3_Axpy_3 (in-place)
I1012 11:55:12.430554 31118 net.cpp:172] Setting up conv3_3ReLU_1
I1012 11:55:12.430578 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.430583 31118 net.cpp:194] Memory required for data: 42746360
I1012 11:55:12.430588 31118 layer_factory.hpp:77] Creating layer conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.430599 31118 net.cpp:128] Creating Layer conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.430609 31118 net.cpp:558] conv3_Axpy_3_conv3_3ReLU_1_0_split <- conv3_Axpy_3
I1012 11:55:12.430618 31118 net.cpp:522] conv3_Axpy_3_conv3_3ReLU_1_0_split -> conv3_Axpy_3_conv3_3ReLU_1_0_split_0
I1012 11:55:12.430630 31118 net.cpp:522] conv3_Axpy_3_conv3_3ReLU_1_0_split -> conv3_Axpy_3_conv3_3ReLU_1_0_split_1
I1012 11:55:12.430687 31118 net.cpp:172] Setting up conv3_Axpy_3_conv3_3ReLU_1_0_split
I1012 11:55:12.430694 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.430701 31118 net.cpp:186] Top shape: 10 32 16 16 (81920)
I1012 11:55:12.430704 31118 net.cpp:194] Memory required for data: 43401720
I1012 11:55:12.430708 31118 layer_factory.hpp:77] Creating layer conv4_1_0
I1012 11:55:12.430724 31118 net.cpp:128] Creating Layer conv4_1_0
I1012 11:55:12.430729 31118 net.cpp:558] conv4_1_0 <- conv3_Axpy_3_conv3_3ReLU_1_0_split_0
I1012 11:55:12.430739 31118 net.cpp:522] conv4_1_0 -> conv4_1_0
I1012 11:55:12.437373 31118 net.cpp:172] Setting up conv4_1_0
I1012 11:55:12.437404 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.437408 31118 net.cpp:194] Memory required for data: 43565560
I1012 11:55:12.437423 31118 layer_factory.hpp:77] Creating layer conv4_1_bn0
I1012 11:55:12.437438 31118 net.cpp:128] Creating Layer conv4_1_bn0
I1012 11:55:12.437449 31118 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I1012 11:55:12.437456 31118 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I1012 11:55:12.437798 31118 net.cpp:172] Setting up conv4_1_bn0
I1012 11:55:12.437810 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.437815 31118 net.cpp:194] Memory required for data: 43729400
I1012 11:55:12.437825 31118 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1012 11:55:12.437839 31118 net.cpp:128] Creating Layer conv4_1_scale0
I1012 11:55:12.437844 31118 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I1012 11:55:12.437849 31118 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I1012 11:55:12.437922 31118 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1012 11:55:12.438110 31118 net.cpp:172] Setting up conv4_1_scale0
I1012 11:55:12.438120 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.438154 31118 net.cpp:194] Memory required for data: 43893240
I1012 11:55:12.438184 31118 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I1012 11:55:12.438194 31118 net.cpp:128] Creating Layer conv4_1_ReLU0
I1012 11:55:12.438201 31118 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I1012 11:55:12.438210 31118 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I1012 11:55:12.439380 31118 net.cpp:172] Setting up conv4_1_ReLU0
I1012 11:55:12.439399 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.439404 31118 net.cpp:194] Memory required for data: 44057080
I1012 11:55:12.439409 31118 layer_factory.hpp:77] Creating layer conv4_1_1
I1012 11:55:12.439424 31118 net.cpp:128] Creating Layer conv4_1_1
I1012 11:55:12.439435 31118 net.cpp:558] conv4_1_1 <- conv4_1_0
I1012 11:55:12.439445 31118 net.cpp:522] conv4_1_1 -> conv4_1_1
I1012 11:55:12.446193 31118 net.cpp:172] Setting up conv4_1_1
I1012 11:55:12.446220 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.446225 31118 net.cpp:194] Memory required for data: 44220920
I1012 11:55:12.446235 31118 layer_factory.hpp:77] Creating layer conv4_1_bn1
I1012 11:55:12.446249 31118 net.cpp:128] Creating Layer conv4_1_bn1
I1012 11:55:12.446257 31118 net.cpp:558] conv4_1_bn1 <- conv4_1_1
I1012 11:55:12.446264 31118 net.cpp:509] conv4_1_bn1 -> conv4_1_1 (in-place)
I1012 11:55:12.446573 31118 net.cpp:172] Setting up conv4_1_bn1
I1012 11:55:12.446583 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.446588 31118 net.cpp:194] Memory required for data: 44384760
I1012 11:55:12.446597 31118 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1012 11:55:12.446607 31118 net.cpp:128] Creating Layer conv4_1_scale1
I1012 11:55:12.446612 31118 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I1012 11:55:12.446617 31118 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I1012 11:55:12.446681 31118 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1012 11:55:12.446856 31118 net.cpp:172] Setting up conv4_1_scale1
I1012 11:55:12.446869 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.446873 31118 net.cpp:194] Memory required for data: 44548600
I1012 11:55:12.446882 31118 layer_factory.hpp:77] Creating layer conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.446892 31118 net.cpp:128] Creating Layer conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.446897 31118 net.cpp:558] conv4_1_1_conv4_1_scale1_0_split <- conv4_1_1
I1012 11:55:12.446905 31118 net.cpp:522] conv4_1_1_conv4_1_scale1_0_split -> conv4_1_1_conv4_1_scale1_0_split_0
I1012 11:55:12.446918 31118 net.cpp:522] conv4_1_1_conv4_1_scale1_0_split -> conv4_1_1_conv4_1_scale1_0_split_1
I1012 11:55:12.446969 31118 net.cpp:172] Setting up conv4_1_1_conv4_1_scale1_0_split
I1012 11:55:12.446976 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.446981 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.446985 31118 net.cpp:194] Memory required for data: 44876280
I1012 11:55:12.446990 31118 layer_factory.hpp:77] Creating layer conv4_1_Pooling
I1012 11:55:12.447000 31118 net.cpp:128] Creating Layer conv4_1_Pooling
I1012 11:55:12.447005 31118 net.cpp:558] conv4_1_Pooling <- conv4_1_1_conv4_1_scale1_0_split_0
I1012 11:55:12.447011 31118 net.cpp:522] conv4_1_Pooling -> conv4_1_Pooling
I1012 11:55:12.447043 31118 net.cpp:172] Setting up conv4_1_Pooling
I1012 11:55:12.447051 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.447054 31118 net.cpp:194] Memory required for data: 44878840
I1012 11:55:12.447058 31118 layer_factory.hpp:77] Creating layer conv4_1_2
I1012 11:55:12.447075 31118 net.cpp:128] Creating Layer conv4_1_2
I1012 11:55:12.447084 31118 net.cpp:558] conv4_1_2 <- conv4_1_Pooling
I1012 11:55:12.447091 31118 net.cpp:522] conv4_1_2 -> conv4_1_2
I1012 11:55:12.452813 31118 net.cpp:172] Setting up conv4_1_2
I1012 11:55:12.452847 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.452852 31118 net.cpp:194] Memory required for data: 44879480
I1012 11:55:12.452864 31118 layer_factory.hpp:77] Creating layer conv4_1_ReLU2
I1012 11:55:12.452903 31118 net.cpp:128] Creating Layer conv4_1_ReLU2
I1012 11:55:12.452910 31118 net.cpp:558] conv4_1_ReLU2 <- conv4_1_2
I1012 11:55:12.452917 31118 net.cpp:509] conv4_1_ReLU2 -> conv4_1_2 (in-place)
I1012 11:55:12.454811 31118 net.cpp:172] Setting up conv4_1_ReLU2
I1012 11:55:12.454830 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.454834 31118 net.cpp:194] Memory required for data: 44880120
I1012 11:55:12.454839 31118 layer_factory.hpp:77] Creating layer conv4_1_3
I1012 11:55:12.454856 31118 net.cpp:128] Creating Layer conv4_1_3
I1012 11:55:12.454862 31118 net.cpp:558] conv4_1_3 <- conv4_1_2
I1012 11:55:12.454870 31118 net.cpp:522] conv4_1_3 -> conv4_1_3
I1012 11:55:12.461623 31118 net.cpp:172] Setting up conv4_1_3
I1012 11:55:12.461650 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.461655 31118 net.cpp:194] Memory required for data: 44882680
I1012 11:55:12.461665 31118 layer_factory.hpp:77] Creating layer conv4_1_Prob3
I1012 11:55:12.461678 31118 net.cpp:128] Creating Layer conv4_1_Prob3
I1012 11:55:12.461683 31118 net.cpp:558] conv4_1_Prob3 <- conv4_1_3
I1012 11:55:12.461693 31118 net.cpp:509] conv4_1_Prob3 -> conv4_1_3 (in-place)
I1012 11:55:12.463659 31118 net.cpp:172] Setting up conv4_1_Prob3
I1012 11:55:12.463685 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.463690 31118 net.cpp:194] Memory required for data: 44885240
I1012 11:55:12.463695 31118 layer_factory.hpp:77] Creating layer conv4_1_down
I1012 11:55:12.463708 31118 net.cpp:128] Creating Layer conv4_1_down
I1012 11:55:12.463718 31118 net.cpp:558] conv4_1_down <- conv3_Axpy_3_conv3_3ReLU_1_0_split_1
I1012 11:55:12.463729 31118 net.cpp:522] conv4_1_down -> conv4_1_down
I1012 11:55:12.470499 31118 net.cpp:172] Setting up conv4_1_down
I1012 11:55:12.470528 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.470532 31118 net.cpp:194] Memory required for data: 45049080
I1012 11:55:12.470546 31118 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I1012 11:55:12.470559 31118 net.cpp:128] Creating Layer conv4_1_bn_down
I1012 11:55:12.470568 31118 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I1012 11:55:12.470577 31118 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I1012 11:55:12.470903 31118 net.cpp:172] Setting up conv4_1_bn_down
I1012 11:55:12.470914 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.470918 31118 net.cpp:194] Memory required for data: 45212920
I1012 11:55:12.470929 31118 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1012 11:55:12.470943 31118 net.cpp:128] Creating Layer conv4_1_scale_down
I1012 11:55:12.470948 31118 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I1012 11:55:12.470954 31118 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I1012 11:55:12.471019 31118 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1012 11:55:12.471205 31118 net.cpp:172] Setting up conv4_1_scale_down
I1012 11:55:12.471216 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.471220 31118 net.cpp:194] Memory required for data: 45376760
I1012 11:55:12.471228 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_1
I1012 11:55:12.471240 31118 net.cpp:128] Creating Layer conv4_Axpy_1
I1012 11:55:12.471246 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_3
I1012 11:55:12.471252 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_1_conv4_1_scale1_0_split_1
I1012 11:55:12.471257 31118 net.cpp:558] conv4_Axpy_1 <- conv4_1_down
I1012 11:55:12.471266 31118 net.cpp:522] conv4_Axpy_1 -> conv4_Axpy_1
I1012 11:55:12.471338 31118 net.cpp:172] Setting up conv4_Axpy_1
I1012 11:55:12.471348 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.471351 31118 net.cpp:194] Memory required for data: 45540600
I1012 11:55:12.471356 31118 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I1012 11:55:12.471364 31118 net.cpp:128] Creating Layer conv4_1ReLU_1
I1012 11:55:12.471367 31118 net.cpp:558] conv4_1ReLU_1 <- conv4_Axpy_1
I1012 11:55:12.471374 31118 net.cpp:509] conv4_1ReLU_1 -> conv4_Axpy_1 (in-place)
I1012 11:55:12.472518 31118 net.cpp:172] Setting up conv4_1ReLU_1
I1012 11:55:12.472558 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.472563 31118 net.cpp:194] Memory required for data: 45704440
I1012 11:55:12.472568 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.472576 31118 net.cpp:128] Creating Layer conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.472581 31118 net.cpp:558] conv4_Axpy_1_conv4_1ReLU_1_0_split <- conv4_Axpy_1
I1012 11:55:12.472590 31118 net.cpp:522] conv4_Axpy_1_conv4_1ReLU_1_0_split -> conv4_Axpy_1_conv4_1ReLU_1_0_split_0
I1012 11:55:12.472600 31118 net.cpp:522] conv4_Axpy_1_conv4_1ReLU_1_0_split -> conv4_Axpy_1_conv4_1ReLU_1_0_split_1
I1012 11:55:12.472669 31118 net.cpp:172] Setting up conv4_Axpy_1_conv4_1ReLU_1_0_split
I1012 11:55:12.472682 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.472687 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.472692 31118 net.cpp:194] Memory required for data: 46032120
I1012 11:55:12.472697 31118 layer_factory.hpp:77] Creating layer conv4_2_0
I1012 11:55:12.472709 31118 net.cpp:128] Creating Layer conv4_2_0
I1012 11:55:12.472714 31118 net.cpp:558] conv4_2_0 <- conv4_Axpy_1_conv4_1ReLU_1_0_split_0
I1012 11:55:12.472723 31118 net.cpp:522] conv4_2_0 -> conv4_2_0
I1012 11:55:12.481088 31118 net.cpp:172] Setting up conv4_2_0
I1012 11:55:12.481110 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.481115 31118 net.cpp:194] Memory required for data: 46195960
I1012 11:55:12.481127 31118 layer_factory.hpp:77] Creating layer conv4_2_bn0
I1012 11:55:12.481142 31118 net.cpp:128] Creating Layer conv4_2_bn0
I1012 11:55:12.481148 31118 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I1012 11:55:12.481158 31118 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I1012 11:55:12.481485 31118 net.cpp:172] Setting up conv4_2_bn0
I1012 11:55:12.481495 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.481499 31118 net.cpp:194] Memory required for data: 46359800
I1012 11:55:12.481509 31118 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1012 11:55:12.481521 31118 net.cpp:128] Creating Layer conv4_2_scale0
I1012 11:55:12.481526 31118 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I1012 11:55:12.481532 31118 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I1012 11:55:12.481588 31118 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1012 11:55:12.481782 31118 net.cpp:172] Setting up conv4_2_scale0
I1012 11:55:12.481793 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.481797 31118 net.cpp:194] Memory required for data: 46523640
I1012 11:55:12.481806 31118 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I1012 11:55:12.481823 31118 net.cpp:128] Creating Layer conv4_2_ReLU0
I1012 11:55:12.481828 31118 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I1012 11:55:12.481834 31118 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I1012 11:55:12.482094 31118 net.cpp:172] Setting up conv4_2_ReLU0
I1012 11:55:12.482107 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.482111 31118 net.cpp:194] Memory required for data: 46687480
I1012 11:55:12.482116 31118 layer_factory.hpp:77] Creating layer conv4_2_1
I1012 11:55:12.482138 31118 net.cpp:128] Creating Layer conv4_2_1
I1012 11:55:12.482146 31118 net.cpp:558] conv4_2_1 <- conv4_2_0
I1012 11:55:12.482156 31118 net.cpp:522] conv4_2_1 -> conv4_2_1
I1012 11:55:12.488196 31118 net.cpp:172] Setting up conv4_2_1
I1012 11:55:12.488224 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.488229 31118 net.cpp:194] Memory required for data: 46851320
I1012 11:55:12.488242 31118 layer_factory.hpp:77] Creating layer conv4_2_bn1
I1012 11:55:12.488255 31118 net.cpp:128] Creating Layer conv4_2_bn1
I1012 11:55:12.488262 31118 net.cpp:558] conv4_2_bn1 <- conv4_2_1
I1012 11:55:12.488270 31118 net.cpp:509] conv4_2_bn1 -> conv4_2_1 (in-place)
I1012 11:55:12.488595 31118 net.cpp:172] Setting up conv4_2_bn1
I1012 11:55:12.488605 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.488608 31118 net.cpp:194] Memory required for data: 47015160
I1012 11:55:12.488620 31118 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1012 11:55:12.488658 31118 net.cpp:128] Creating Layer conv4_2_scale1
I1012 11:55:12.488664 31118 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I1012 11:55:12.488669 31118 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I1012 11:55:12.488731 31118 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1012 11:55:12.488919 31118 net.cpp:172] Setting up conv4_2_scale1
I1012 11:55:12.488927 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.488932 31118 net.cpp:194] Memory required for data: 47179000
I1012 11:55:12.488940 31118 layer_factory.hpp:77] Creating layer conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.488952 31118 net.cpp:128] Creating Layer conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.488957 31118 net.cpp:558] conv4_2_1_conv4_2_scale1_0_split <- conv4_2_1
I1012 11:55:12.488965 31118 net.cpp:522] conv4_2_1_conv4_2_scale1_0_split -> conv4_2_1_conv4_2_scale1_0_split_0
I1012 11:55:12.488975 31118 net.cpp:522] conv4_2_1_conv4_2_scale1_0_split -> conv4_2_1_conv4_2_scale1_0_split_1
I1012 11:55:12.489027 31118 net.cpp:172] Setting up conv4_2_1_conv4_2_scale1_0_split
I1012 11:55:12.489037 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.489042 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.489046 31118 net.cpp:194] Memory required for data: 47506680
I1012 11:55:12.489050 31118 layer_factory.hpp:77] Creating layer conv4_2_Pooling
I1012 11:55:12.489059 31118 net.cpp:128] Creating Layer conv4_2_Pooling
I1012 11:55:12.489063 31118 net.cpp:558] conv4_2_Pooling <- conv4_2_1_conv4_2_scale1_0_split_0
I1012 11:55:12.489069 31118 net.cpp:522] conv4_2_Pooling -> conv4_2_Pooling
I1012 11:55:12.489104 31118 net.cpp:172] Setting up conv4_2_Pooling
I1012 11:55:12.489111 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.489115 31118 net.cpp:194] Memory required for data: 47509240
I1012 11:55:12.489120 31118 layer_factory.hpp:77] Creating layer conv4_2_2
I1012 11:55:12.489132 31118 net.cpp:128] Creating Layer conv4_2_2
I1012 11:55:12.489137 31118 net.cpp:558] conv4_2_2 <- conv4_2_Pooling
I1012 11:55:12.489145 31118 net.cpp:522] conv4_2_2 -> conv4_2_2
I1012 11:55:12.494792 31118 net.cpp:172] Setting up conv4_2_2
I1012 11:55:12.494818 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.494822 31118 net.cpp:194] Memory required for data: 47509880
I1012 11:55:12.494833 31118 layer_factory.hpp:77] Creating layer conv4_2_ReLU2
I1012 11:55:12.494843 31118 net.cpp:128] Creating Layer conv4_2_ReLU2
I1012 11:55:12.494850 31118 net.cpp:558] conv4_2_ReLU2 <- conv4_2_2
I1012 11:55:12.494858 31118 net.cpp:509] conv4_2_ReLU2 -> conv4_2_2 (in-place)
I1012 11:55:12.496814 31118 net.cpp:172] Setting up conv4_2_ReLU2
I1012 11:55:12.496830 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.496835 31118 net.cpp:194] Memory required for data: 47510520
I1012 11:55:12.496840 31118 layer_factory.hpp:77] Creating layer conv4_2_3
I1012 11:55:12.496853 31118 net.cpp:128] Creating Layer conv4_2_3
I1012 11:55:12.496862 31118 net.cpp:558] conv4_2_3 <- conv4_2_2
I1012 11:55:12.496870 31118 net.cpp:522] conv4_2_3 -> conv4_2_3
I1012 11:55:12.503664 31118 net.cpp:172] Setting up conv4_2_3
I1012 11:55:12.503701 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.503706 31118 net.cpp:194] Memory required for data: 47513080
I1012 11:55:12.503720 31118 layer_factory.hpp:77] Creating layer conv4_2_Prob3
I1012 11:55:12.503732 31118 net.cpp:128] Creating Layer conv4_2_Prob3
I1012 11:55:12.503738 31118 net.cpp:558] conv4_2_Prob3 <- conv4_2_3
I1012 11:55:12.503746 31118 net.cpp:509] conv4_2_Prob3 -> conv4_2_3 (in-place)
I1012 11:55:12.505650 31118 net.cpp:172] Setting up conv4_2_Prob3
I1012 11:55:12.505676 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.505679 31118 net.cpp:194] Memory required for data: 47515640
I1012 11:55:12.505684 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_2
I1012 11:55:12.505695 31118 net.cpp:128] Creating Layer conv4_Axpy_2
I1012 11:55:12.505702 31118 net.cpp:558] conv4_Axpy_2 <- conv4_2_3
I1012 11:55:12.505707 31118 net.cpp:558] conv4_Axpy_2 <- conv4_2_1_conv4_2_scale1_0_split_1
I1012 11:55:12.505749 31118 net.cpp:558] conv4_Axpy_2 <- conv4_Axpy_1_conv4_1ReLU_1_0_split_1
I1012 11:55:12.505759 31118 net.cpp:522] conv4_Axpy_2 -> conv4_Axpy_2
I1012 11:55:12.505854 31118 net.cpp:172] Setting up conv4_Axpy_2
I1012 11:55:12.505862 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.505867 31118 net.cpp:194] Memory required for data: 47679480
I1012 11:55:12.505870 31118 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I1012 11:55:12.505878 31118 net.cpp:128] Creating Layer conv4_2ReLU_1
I1012 11:55:12.505882 31118 net.cpp:558] conv4_2ReLU_1 <- conv4_Axpy_2
I1012 11:55:12.505890 31118 net.cpp:509] conv4_2ReLU_1 -> conv4_Axpy_2 (in-place)
I1012 11:55:12.507855 31118 net.cpp:172] Setting up conv4_2ReLU_1
I1012 11:55:12.507869 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.507874 31118 net.cpp:194] Memory required for data: 47843320
I1012 11:55:12.507879 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.507885 31118 net.cpp:128] Creating Layer conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.507890 31118 net.cpp:558] conv4_Axpy_2_conv4_2ReLU_1_0_split <- conv4_Axpy_2
I1012 11:55:12.507899 31118 net.cpp:522] conv4_Axpy_2_conv4_2ReLU_1_0_split -> conv4_Axpy_2_conv4_2ReLU_1_0_split_0
I1012 11:55:12.507911 31118 net.cpp:522] conv4_Axpy_2_conv4_2ReLU_1_0_split -> conv4_Axpy_2_conv4_2ReLU_1_0_split_1
I1012 11:55:12.507975 31118 net.cpp:172] Setting up conv4_Axpy_2_conv4_2ReLU_1_0_split
I1012 11:55:12.507982 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.507988 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.507992 31118 net.cpp:194] Memory required for data: 48171000
I1012 11:55:12.507997 31118 layer_factory.hpp:77] Creating layer conv4_3_0
I1012 11:55:12.508011 31118 net.cpp:128] Creating Layer conv4_3_0
I1012 11:55:12.508016 31118 net.cpp:558] conv4_3_0 <- conv4_Axpy_2_conv4_2ReLU_1_0_split_0
I1012 11:55:12.508030 31118 net.cpp:522] conv4_3_0 -> conv4_3_0
I1012 11:55:12.515002 31118 net.cpp:172] Setting up conv4_3_0
I1012 11:55:12.515027 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.515031 31118 net.cpp:194] Memory required for data: 48334840
I1012 11:55:12.515043 31118 layer_factory.hpp:77] Creating layer conv4_3_bn0
I1012 11:55:12.515056 31118 net.cpp:128] Creating Layer conv4_3_bn0
I1012 11:55:12.515066 31118 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I1012 11:55:12.515075 31118 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I1012 11:55:12.515413 31118 net.cpp:172] Setting up conv4_3_bn0
I1012 11:55:12.515426 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.515430 31118 net.cpp:194] Memory required for data: 48498680
I1012 11:55:12.515440 31118 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1012 11:55:12.515452 31118 net.cpp:128] Creating Layer conv4_3_scale0
I1012 11:55:12.515457 31118 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I1012 11:55:12.515462 31118 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I1012 11:55:12.515532 31118 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1012 11:55:12.515722 31118 net.cpp:172] Setting up conv4_3_scale0
I1012 11:55:12.515733 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.515736 31118 net.cpp:194] Memory required for data: 48662520
I1012 11:55:12.515744 31118 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I1012 11:55:12.515753 31118 net.cpp:128] Creating Layer conv4_3_ReLU0
I1012 11:55:12.515758 31118 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I1012 11:55:12.515765 31118 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I1012 11:55:12.516716 31118 net.cpp:172] Setting up conv4_3_ReLU0
I1012 11:55:12.516734 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.516738 31118 net.cpp:194] Memory required for data: 48826360
I1012 11:55:12.516743 31118 layer_factory.hpp:77] Creating layer conv4_3_1
I1012 11:55:12.516758 31118 net.cpp:128] Creating Layer conv4_3_1
I1012 11:55:12.516763 31118 net.cpp:558] conv4_3_1 <- conv4_3_0
I1012 11:55:12.516774 31118 net.cpp:522] conv4_3_1 -> conv4_3_1
I1012 11:55:12.523821 31118 net.cpp:172] Setting up conv4_3_1
I1012 11:55:12.523864 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.523870 31118 net.cpp:194] Memory required for data: 48990200
I1012 11:55:12.523890 31118 layer_factory.hpp:77] Creating layer conv4_3_bn1
I1012 11:55:12.523908 31118 net.cpp:128] Creating Layer conv4_3_bn1
I1012 11:55:12.523918 31118 net.cpp:558] conv4_3_bn1 <- conv4_3_1
I1012 11:55:12.523931 31118 net.cpp:509] conv4_3_bn1 -> conv4_3_1 (in-place)
I1012 11:55:12.524260 31118 net.cpp:172] Setting up conv4_3_bn1
I1012 11:55:12.524269 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.524273 31118 net.cpp:194] Memory required for data: 49154040
I1012 11:55:12.524283 31118 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1012 11:55:12.524294 31118 net.cpp:128] Creating Layer conv4_3_scale1
I1012 11:55:12.524302 31118 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I1012 11:55:12.524308 31118 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I1012 11:55:12.524370 31118 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1012 11:55:12.524557 31118 net.cpp:172] Setting up conv4_3_scale1
I1012 11:55:12.524567 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.524571 31118 net.cpp:194] Memory required for data: 49317880
I1012 11:55:12.524579 31118 layer_factory.hpp:77] Creating layer conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.524585 31118 net.cpp:128] Creating Layer conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.524595 31118 net.cpp:558] conv4_3_1_conv4_3_scale1_0_split <- conv4_3_1
I1012 11:55:12.524603 31118 net.cpp:522] conv4_3_1_conv4_3_scale1_0_split -> conv4_3_1_conv4_3_scale1_0_split_0
I1012 11:55:12.524615 31118 net.cpp:522] conv4_3_1_conv4_3_scale1_0_split -> conv4_3_1_conv4_3_scale1_0_split_1
I1012 11:55:12.524670 31118 net.cpp:172] Setting up conv4_3_1_conv4_3_scale1_0_split
I1012 11:55:12.524679 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.524685 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.524689 31118 net.cpp:194] Memory required for data: 49645560
I1012 11:55:12.524693 31118 layer_factory.hpp:77] Creating layer conv4_3_Pooling
I1012 11:55:12.524701 31118 net.cpp:128] Creating Layer conv4_3_Pooling
I1012 11:55:12.524706 31118 net.cpp:558] conv4_3_Pooling <- conv4_3_1_conv4_3_scale1_0_split_0
I1012 11:55:12.524713 31118 net.cpp:522] conv4_3_Pooling -> conv4_3_Pooling
I1012 11:55:12.524750 31118 net.cpp:172] Setting up conv4_3_Pooling
I1012 11:55:12.524760 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.524763 31118 net.cpp:194] Memory required for data: 49648120
I1012 11:55:12.524768 31118 layer_factory.hpp:77] Creating layer conv4_3_2
I1012 11:55:12.524781 31118 net.cpp:128] Creating Layer conv4_3_2
I1012 11:55:12.524786 31118 net.cpp:558] conv4_3_2 <- conv4_3_Pooling
I1012 11:55:12.524793 31118 net.cpp:522] conv4_3_2 -> conv4_3_2
I1012 11:55:12.526379 31118 net.cpp:172] Setting up conv4_3_2
I1012 11:55:12.526405 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.526409 31118 net.cpp:194] Memory required for data: 49648760
I1012 11:55:12.526423 31118 layer_factory.hpp:77] Creating layer conv4_3_ReLU2
I1012 11:55:12.526432 31118 net.cpp:128] Creating Layer conv4_3_ReLU2
I1012 11:55:12.526437 31118 net.cpp:558] conv4_3_ReLU2 <- conv4_3_2
I1012 11:55:12.526446 31118 net.cpp:509] conv4_3_ReLU2 -> conv4_3_2 (in-place)
I1012 11:55:12.527055 31118 net.cpp:172] Setting up conv4_3_ReLU2
I1012 11:55:12.527076 31118 net.cpp:186] Top shape: 10 16 1 1 (160)
I1012 11:55:12.527082 31118 net.cpp:194] Memory required for data: 49649400
I1012 11:55:12.527088 31118 layer_factory.hpp:77] Creating layer conv4_3_3
I1012 11:55:12.527102 31118 net.cpp:128] Creating Layer conv4_3_3
I1012 11:55:12.527112 31118 net.cpp:558] conv4_3_3 <- conv4_3_2
I1012 11:55:12.527122 31118 net.cpp:522] conv4_3_3 -> conv4_3_3
I1012 11:55:12.528673 31118 net.cpp:172] Setting up conv4_3_3
I1012 11:55:12.528698 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.528702 31118 net.cpp:194] Memory required for data: 49651960
I1012 11:55:12.528738 31118 layer_factory.hpp:77] Creating layer conv4_3_Prob3
I1012 11:55:12.528748 31118 net.cpp:128] Creating Layer conv4_3_Prob3
I1012 11:55:12.528756 31118 net.cpp:558] conv4_3_Prob3 <- conv4_3_3
I1012 11:55:12.528766 31118 net.cpp:509] conv4_3_Prob3 -> conv4_3_3 (in-place)
I1012 11:55:12.529033 31118 net.cpp:172] Setting up conv4_3_Prob3
I1012 11:55:12.529047 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.529050 31118 net.cpp:194] Memory required for data: 49654520
I1012 11:55:12.529055 31118 layer_factory.hpp:77] Creating layer conv4_Axpy_3
I1012 11:55:12.529062 31118 net.cpp:128] Creating Layer conv4_Axpy_3
I1012 11:55:12.529067 31118 net.cpp:558] conv4_Axpy_3 <- conv4_3_3
I1012 11:55:12.529075 31118 net.cpp:558] conv4_Axpy_3 <- conv4_3_1_conv4_3_scale1_0_split_1
I1012 11:55:12.529088 31118 net.cpp:558] conv4_Axpy_3 <- conv4_Axpy_2_conv4_2ReLU_1_0_split_1
I1012 11:55:12.529094 31118 net.cpp:522] conv4_Axpy_3 -> conv4_Axpy_3
I1012 11:55:12.529182 31118 net.cpp:172] Setting up conv4_Axpy_3
I1012 11:55:12.529192 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.529196 31118 net.cpp:194] Memory required for data: 49818360
I1012 11:55:12.529201 31118 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I1012 11:55:12.529208 31118 net.cpp:128] Creating Layer conv4_3ReLU_1
I1012 11:55:12.529213 31118 net.cpp:558] conv4_3ReLU_1 <- conv4_Axpy_3
I1012 11:55:12.529218 31118 net.cpp:509] conv4_3ReLU_1 -> conv4_Axpy_3 (in-place)
I1012 11:55:12.529462 31118 net.cpp:172] Setting up conv4_3ReLU_1
I1012 11:55:12.529474 31118 net.cpp:186] Top shape: 10 64 8 8 (40960)
I1012 11:55:12.529479 31118 net.cpp:194] Memory required for data: 49982200
I1012 11:55:12.529484 31118 layer_factory.hpp:77] Creating layer Pooling1
I1012 11:55:12.529492 31118 net.cpp:128] Creating Layer Pooling1
I1012 11:55:12.529500 31118 net.cpp:558] Pooling1 <- conv4_Axpy_3
I1012 11:55:12.529507 31118 net.cpp:522] Pooling1 -> Pooling1
I1012 11:55:12.529549 31118 net.cpp:172] Setting up Pooling1
I1012 11:55:12.529558 31118 net.cpp:186] Top shape: 10 64 1 1 (640)
I1012 11:55:12.529561 31118 net.cpp:194] Memory required for data: 49984760
I1012 11:55:12.529565 31118 layer_factory.hpp:77] Creating layer fc1
I1012 11:55:12.529577 31118 net.cpp:128] Creating Layer fc1
I1012 11:55:12.529583 31118 net.cpp:558] fc1 <- Pooling1
I1012 11:55:12.529590 31118 net.cpp:522] fc1 -> fc1
I1012 11:55:12.529801 31118 net.cpp:172] Setting up fc1
I1012 11:55:12.529814 31118 net.cpp:186] Top shape: 10 10 (100)
I1012 11:55:12.529817 31118 net.cpp:194] Memory required for data: 49985160
I1012 11:55:12.529827 31118 layer_factory.hpp:77] Creating layer fc1_fc1_0_split
I1012 11:55:12.529834 31118 net.cpp:128] Creating Layer fc1_fc1_0_split
I1012 11:55:12.529844 31118 net.cpp:558] fc1_fc1_0_split <- fc1
I1012 11:55:12.529851 31118 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_0
I1012 11:55:12.529861 31118 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_1
I1012 11:55:12.529918 31118 net.cpp:172] Setting up fc1_fc1_0_split
I1012 11:55:12.529927 31118 net.cpp:186] Top shape: 10 10 (100)
I1012 11:55:12.529933 31118 net.cpp:186] Top shape: 10 10 (100)
I1012 11:55:12.529937 31118 net.cpp:194] Memory required for data: 49985960
I1012 11:55:12.529942 31118 layer_factory.hpp:77] Creating layer Softmax1
I1012 11:55:12.529952 31118 net.cpp:128] Creating Layer Softmax1
I1012 11:55:12.529956 31118 net.cpp:558] Softmax1 <- fc1_fc1_0_split_0
I1012 11:55:12.529961 31118 net.cpp:558] Softmax1 <- label_Data1_1_split_0
I1012 11:55:12.529969 31118 net.cpp:522] Softmax1 -> Softmax1
I1012 11:55:12.529979 31118 layer_factory.hpp:77] Creating layer Softmax1
I1012 11:55:12.530728 31118 net.cpp:172] Setting up Softmax1
I1012 11:55:12.530748 31118 net.cpp:186] Top shape: (1)
I1012 11:55:12.530755 31118 net.cpp:189]     with loss weight 1
I1012 11:55:12.530773 31118 net.cpp:194] Memory required for data: 49985964
I1012 11:55:12.530778 31118 layer_factory.hpp:77] Creating layer prob
I1012 11:55:12.530786 31118 net.cpp:128] Creating Layer prob
I1012 11:55:12.530808 31118 net.cpp:558] prob <- fc1_fc1_0_split_1
I1012 11:55:12.530815 31118 net.cpp:558] prob <- label_Data1_1_split_1
I1012 11:55:12.530827 31118 net.cpp:522] prob -> prob
I1012 11:55:12.530838 31118 net.cpp:172] Setting up prob
I1012 11:55:12.530843 31118 net.cpp:186] Top shape: (1)
I1012 11:55:12.530846 31118 net.cpp:194] Memory required for data: 49985968
I1012 11:55:12.530851 31118 net.cpp:303] prob does not need backward computation.
I1012 11:55:12.530856 31118 net.cpp:301] Softmax1 needs backward computation.
I1012 11:55:12.530861 31118 net.cpp:301] fc1_fc1_0_split needs backward computation.
I1012 11:55:12.530865 31118 net.cpp:301] fc1 needs backward computation.
I1012 11:55:12.530869 31118 net.cpp:301] Pooling1 needs backward computation.
I1012 11:55:12.530874 31118 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I1012 11:55:12.530879 31118 net.cpp:301] conv4_Axpy_3 needs backward computation.
I1012 11:55:12.530884 31118 net.cpp:301] conv4_3_Prob3 needs backward computation.
I1012 11:55:12.530889 31118 net.cpp:301] conv4_3_3 needs backward computation.
I1012 11:55:12.530894 31118 net.cpp:301] conv4_3_ReLU2 needs backward computation.
I1012 11:55:12.530897 31118 net.cpp:301] conv4_3_2 needs backward computation.
I1012 11:55:12.530908 31118 net.cpp:301] conv4_3_Pooling needs backward computation.
I1012 11:55:12.530913 31118 net.cpp:301] conv4_3_1_conv4_3_scale1_0_split needs backward computation.
I1012 11:55:12.530917 31118 net.cpp:301] conv4_3_scale1 needs backward computation.
I1012 11:55:12.530921 31118 net.cpp:301] conv4_3_bn1 needs backward computation.
I1012 11:55:12.530925 31118 net.cpp:301] conv4_3_1 needs backward computation.
I1012 11:55:12.530930 31118 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I1012 11:55:12.530935 31118 net.cpp:301] conv4_3_scale0 needs backward computation.
I1012 11:55:12.530938 31118 net.cpp:301] conv4_3_bn0 needs backward computation.
I1012 11:55:12.530942 31118 net.cpp:301] conv4_3_0 needs backward computation.
I1012 11:55:12.530947 31118 net.cpp:301] conv4_Axpy_2_conv4_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.530952 31118 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I1012 11:55:12.530957 31118 net.cpp:301] conv4_Axpy_2 needs backward computation.
I1012 11:55:12.530966 31118 net.cpp:301] conv4_2_Prob3 needs backward computation.
I1012 11:55:12.530970 31118 net.cpp:301] conv4_2_3 needs backward computation.
I1012 11:55:12.530974 31118 net.cpp:301] conv4_2_ReLU2 needs backward computation.
I1012 11:55:12.530979 31118 net.cpp:301] conv4_2_2 needs backward computation.
I1012 11:55:12.530984 31118 net.cpp:301] conv4_2_Pooling needs backward computation.
I1012 11:55:12.530989 31118 net.cpp:301] conv4_2_1_conv4_2_scale1_0_split needs backward computation.
I1012 11:55:12.530993 31118 net.cpp:301] conv4_2_scale1 needs backward computation.
I1012 11:55:12.530998 31118 net.cpp:301] conv4_2_bn1 needs backward computation.
I1012 11:55:12.531002 31118 net.cpp:301] conv4_2_1 needs backward computation.
I1012 11:55:12.531006 31118 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I1012 11:55:12.531011 31118 net.cpp:301] conv4_2_scale0 needs backward computation.
I1012 11:55:12.531015 31118 net.cpp:301] conv4_2_bn0 needs backward computation.
I1012 11:55:12.531020 31118 net.cpp:301] conv4_2_0 needs backward computation.
I1012 11:55:12.531025 31118 net.cpp:301] conv4_Axpy_1_conv4_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.531029 31118 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I1012 11:55:12.531034 31118 net.cpp:301] conv4_Axpy_1 needs backward computation.
I1012 11:55:12.531039 31118 net.cpp:301] conv4_1_scale_down needs backward computation.
I1012 11:55:12.531044 31118 net.cpp:301] conv4_1_bn_down needs backward computation.
I1012 11:55:12.531049 31118 net.cpp:301] conv4_1_down needs backward computation.
I1012 11:55:12.531054 31118 net.cpp:301] conv4_1_Prob3 needs backward computation.
I1012 11:55:12.531059 31118 net.cpp:301] conv4_1_3 needs backward computation.
I1012 11:55:12.531062 31118 net.cpp:301] conv4_1_ReLU2 needs backward computation.
I1012 11:55:12.531078 31118 net.cpp:301] conv4_1_2 needs backward computation.
I1012 11:55:12.531083 31118 net.cpp:301] conv4_1_Pooling needs backward computation.
I1012 11:55:12.531088 31118 net.cpp:301] conv4_1_1_conv4_1_scale1_0_split needs backward computation.
I1012 11:55:12.531093 31118 net.cpp:301] conv4_1_scale1 needs backward computation.
I1012 11:55:12.531100 31118 net.cpp:301] conv4_1_bn1 needs backward computation.
I1012 11:55:12.531105 31118 net.cpp:301] conv4_1_1 needs backward computation.
I1012 11:55:12.531111 31118 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I1012 11:55:12.531117 31118 net.cpp:301] conv4_1_scale0 needs backward computation.
I1012 11:55:12.531121 31118 net.cpp:301] conv4_1_bn0 needs backward computation.
I1012 11:55:12.531126 31118 net.cpp:301] conv4_1_0 needs backward computation.
I1012 11:55:12.531133 31118 net.cpp:301] conv3_Axpy_3_conv3_3ReLU_1_0_split needs backward computation.
I1012 11:55:12.531142 31118 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I1012 11:55:12.531147 31118 net.cpp:301] conv3_Axpy_3 needs backward computation.
I1012 11:55:12.531157 31118 net.cpp:301] conv3_3_Prob3 needs backward computation.
I1012 11:55:12.531160 31118 net.cpp:301] conv3_3_3 needs backward computation.
I1012 11:55:12.531167 31118 net.cpp:301] conv3_3_ReLU2 needs backward computation.
I1012 11:55:12.531172 31118 net.cpp:301] conv3_3_2 needs backward computation.
I1012 11:55:12.531180 31118 net.cpp:301] conv3_3_Pooling needs backward computation.
I1012 11:55:12.531185 31118 net.cpp:301] conv3_3_1_conv3_3_scale1_0_split needs backward computation.
I1012 11:55:12.531193 31118 net.cpp:301] conv3_3_scale1 needs backward computation.
I1012 11:55:12.531196 31118 net.cpp:301] conv3_3_bn1 needs backward computation.
I1012 11:55:12.531201 31118 net.cpp:301] conv3_3_1 needs backward computation.
I1012 11:55:12.531208 31118 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I1012 11:55:12.531213 31118 net.cpp:301] conv3_3_scale0 needs backward computation.
I1012 11:55:12.531216 31118 net.cpp:301] conv3_3_bn0 needs backward computation.
I1012 11:55:12.531224 31118 net.cpp:301] conv3_3_0 needs backward computation.
I1012 11:55:12.531229 31118 net.cpp:301] conv3_Axpy_2_conv3_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.531235 31118 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I1012 11:55:12.531240 31118 net.cpp:301] conv3_Axpy_2 needs backward computation.
I1012 11:55:12.531249 31118 net.cpp:301] conv3_2_Prob3 needs backward computation.
I1012 11:55:12.531255 31118 net.cpp:301] conv3_2_3 needs backward computation.
I1012 11:55:12.531260 31118 net.cpp:301] conv3_2_ReLU2 needs backward computation.
I1012 11:55:12.531266 31118 net.cpp:301] conv3_2_2 needs backward computation.
I1012 11:55:12.531271 31118 net.cpp:301] conv3_2_Pooling needs backward computation.
I1012 11:55:12.531278 31118 net.cpp:301] conv3_2_1_conv3_2_scale1_0_split needs backward computation.
I1012 11:55:12.531283 31118 net.cpp:301] conv3_2_scale1 needs backward computation.
I1012 11:55:12.531291 31118 net.cpp:301] conv3_2_bn1 needs backward computation.
I1012 11:55:12.531294 31118 net.cpp:301] conv3_2_1 needs backward computation.
I1012 11:55:12.531299 31118 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I1012 11:55:12.531306 31118 net.cpp:301] conv3_2_scale0 needs backward computation.
I1012 11:55:12.531311 31118 net.cpp:301] conv3_2_bn0 needs backward computation.
I1012 11:55:12.531314 31118 net.cpp:301] conv3_2_0 needs backward computation.
I1012 11:55:12.531319 31118 net.cpp:301] conv3_Axpy_1_conv3_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.531324 31118 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I1012 11:55:12.531328 31118 net.cpp:301] conv3_Axpy_1 needs backward computation.
I1012 11:55:12.531338 31118 net.cpp:301] conv3_1_scale_down needs backward computation.
I1012 11:55:12.531345 31118 net.cpp:301] conv3_1_bn_down needs backward computation.
I1012 11:55:12.531349 31118 net.cpp:301] conv3_1_down needs backward computation.
I1012 11:55:12.531365 31118 net.cpp:301] conv3_1_Prob3 needs backward computation.
I1012 11:55:12.531370 31118 net.cpp:301] conv3_1_3 needs backward computation.
I1012 11:55:12.531378 31118 net.cpp:301] conv3_1_ReLU2 needs backward computation.
I1012 11:55:12.531383 31118 net.cpp:301] conv3_1_2 needs backward computation.
I1012 11:55:12.531386 31118 net.cpp:301] conv3_1_Pooling needs backward computation.
I1012 11:55:12.531394 31118 net.cpp:301] conv3_1_1_conv3_1_scale1_0_split needs backward computation.
I1012 11:55:12.531399 31118 net.cpp:301] conv3_1_scale1 needs backward computation.
I1012 11:55:12.531405 31118 net.cpp:301] conv3_1_bn1 needs backward computation.
I1012 11:55:12.531410 31118 net.cpp:301] conv3_1_1 needs backward computation.
I1012 11:55:12.531415 31118 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I1012 11:55:12.531421 31118 net.cpp:301] conv3_1_scale0 needs backward computation.
I1012 11:55:12.531425 31118 net.cpp:301] conv3_1_bn0 needs backward computation.
I1012 11:55:12.531430 31118 net.cpp:301] conv3_1_0 needs backward computation.
I1012 11:55:12.531437 31118 net.cpp:301] conv2_Axpy_3_conv2_3ReLU_1_0_split needs backward computation.
I1012 11:55:12.531441 31118 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I1012 11:55:12.531448 31118 net.cpp:301] conv2_Axpy_3 needs backward computation.
I1012 11:55:12.531455 31118 net.cpp:301] conv2_3_Prob3 needs backward computation.
I1012 11:55:12.531461 31118 net.cpp:301] conv2_3_3 needs backward computation.
I1012 11:55:12.531466 31118 net.cpp:301] conv2_3_ReLU2 needs backward computation.
I1012 11:55:12.531473 31118 net.cpp:301] conv2_3_2 needs backward computation.
I1012 11:55:12.531478 31118 net.cpp:301] conv2_3_Pooling needs backward computation.
I1012 11:55:12.531486 31118 net.cpp:301] conv2_3_1_conv2_3_scale1_0_split needs backward computation.
I1012 11:55:12.531491 31118 net.cpp:301] conv2_3_scale1 needs backward computation.
I1012 11:55:12.531499 31118 net.cpp:301] conv2_3_bn1 needs backward computation.
I1012 11:55:12.531503 31118 net.cpp:301] conv2_3_1 needs backward computation.
I1012 11:55:12.531508 31118 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I1012 11:55:12.531514 31118 net.cpp:301] conv2_3_scale0 needs backward computation.
I1012 11:55:12.531518 31118 net.cpp:301] conv2_3_bn0 needs backward computation.
I1012 11:55:12.531522 31118 net.cpp:301] conv2_3_0 needs backward computation.
I1012 11:55:12.531530 31118 net.cpp:301] conv2_Axpy_2_conv2_2ReLU_1_0_split needs backward computation.
I1012 11:55:12.531540 31118 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I1012 11:55:12.531545 31118 net.cpp:301] conv2_Axpy_2 needs backward computation.
I1012 11:55:12.531555 31118 net.cpp:301] conv2_2_Prob3 needs backward computation.
I1012 11:55:12.531563 31118 net.cpp:301] conv2_2_3 needs backward computation.
I1012 11:55:12.531567 31118 net.cpp:301] conv2_2_ReLU2 needs backward computation.
I1012 11:55:12.531572 31118 net.cpp:301] conv2_2_2 needs backward computation.
I1012 11:55:12.531579 31118 net.cpp:301] conv2_2_Pooling needs backward computation.
I1012 11:55:12.531584 31118 net.cpp:301] conv2_2_1_conv2_2_scale1_0_split needs backward computation.
I1012 11:55:12.531591 31118 net.cpp:301] conv2_2_scale1 needs backward computation.
I1012 11:55:12.531595 31118 net.cpp:301] conv2_2_bn1 needs backward computation.
I1012 11:55:12.531605 31118 net.cpp:301] conv2_2_1 needs backward computation.
I1012 11:55:12.531608 31118 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I1012 11:55:12.531613 31118 net.cpp:301] conv2_2_scale0 needs backward computation.
I1012 11:55:12.531617 31118 net.cpp:301] conv2_2_bn0 needs backward computation.
I1012 11:55:12.531623 31118 net.cpp:301] conv2_2_0 needs backward computation.
I1012 11:55:12.531630 31118 net.cpp:301] conv2_Axpy_1_conv2_1ReLU_1_0_split needs backward computation.
I1012 11:55:12.531635 31118 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I1012 11:55:12.531641 31118 net.cpp:301] conv2_Axpy_1 needs backward computation.
I1012 11:55:12.531647 31118 net.cpp:301] conv2_1_Prob3 needs backward computation.
I1012 11:55:12.531661 31118 net.cpp:301] conv2_1_3 needs backward computation.
I1012 11:55:12.531666 31118 net.cpp:301] conv2_1_ReLU2 needs backward computation.
I1012 11:55:12.531673 31118 net.cpp:301] conv2_1_2 needs backward computation.
I1012 11:55:12.531678 31118 net.cpp:301] conv2_1_Pooling needs backward computation.
I1012 11:55:12.531687 31118 net.cpp:301] conv2_1_1_conv2_1_scale1_0_split needs backward computation.
I1012 11:55:12.531692 31118 net.cpp:301] conv2_1_scale1 needs backward computation.
I1012 11:55:12.531698 31118 net.cpp:301] conv2_1_bn1 needs backward computation.
I1012 11:55:12.531703 31118 net.cpp:301] conv2_1_1 needs backward computation.
I1012 11:55:12.531708 31118 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I1012 11:55:12.531714 31118 net.cpp:301] conv2_1_scale0 needs backward computation.
I1012 11:55:12.531719 31118 net.cpp:301] conv2_1_bn0 needs backward computation.
I1012 11:55:12.531723 31118 net.cpp:301] conv2_1_0 needs backward computation.
I1012 11:55:12.531728 31118 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I1012 11:55:12.531733 31118 net.cpp:301] conv1/ReLU needs backward computation.
I1012 11:55:12.531738 31118 net.cpp:301] conv1/scale needs backward computation.
I1012 11:55:12.531742 31118 net.cpp:301] conv1/bn needs backward computation.
I1012 11:55:12.531746 31118 net.cpp:301] conv1 needs backward computation.
I1012 11:55:12.531751 31118 net.cpp:303] label_Data1_1_split does not need backward computation.
I1012 11:55:12.531759 31118 net.cpp:303] Data1 does not need backward computation.
I1012 11:55:12.531764 31118 net.cpp:348] This network produces output Softmax1
I1012 11:55:12.531767 31118 net.cpp:348] This network produces output prob
I1012 11:55:12.531860 31118 net.cpp:363] Network initialization done.
I1012 11:55:12.532382 31118 solver.cpp:110] Solver scaffolding done.
I1012 11:55:12.543331 31118 caffe.cpp:313] Starting Optimization
I1012 11:55:12.543352 31118 solver.cpp:425] Solving ResNet-20
I1012 11:55:12.543357 31118 solver.cpp:427] Learning Rate Policy: multistep
I1012 11:55:12.547565 31118 solver.cpp:514] Iteration 0, Testing net (#0)
I1012 11:55:35.098706 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 11:55:35.113369 31118 solver.cpp:580]     Test net output #0: Softmax1 = 87.3361 (* 1 = 87.3361 loss)
I1012 11:55:35.113397 31118 solver.cpp:580]     Test net output #1: prob = 1
I1012 11:55:35.557447 31118 solver.cpp:357] Iteration 0 (-1.14392e+09 iter/s, 23.0149s/100 iters), loss = 2.64368
I1012 11:55:35.557519 31118 solver.cpp:376]     Train net output #0: Softmax1 = 2.66689 (* 1 = 2.66689 loss)
I1012 11:55:35.557559 31118 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I1012 11:56:08.690893 31118 solver.cpp:357] Iteration 100 (3.01819 iter/s, 33.1325s/100 iters), loss = 1.57964
I1012 11:56:08.691058 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.64978 (* 1 = 1.64978 loss)
I1012 11:56:08.691071 31118 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I1012 11:56:42.049839 31118 solver.cpp:357] Iteration 200 (2.9976 iter/s, 33.36s/100 iters), loss = 1.31355
I1012 11:56:42.050000 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.30046 (* 1 = 1.30046 loss)
I1012 11:56:42.050014 31118 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I1012 11:57:15.339923 31118 solver.cpp:357] Iteration 300 (3.00399 iter/s, 33.2891s/100 iters), loss = 1.23468
I1012 11:57:15.340065 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.46395 (* 1 = 1.46395 loss)
I1012 11:57:15.340080 31118 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I1012 11:57:44.711948 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 11:57:48.659905 31118 solver.cpp:357] Iteration 400 (3.00129 iter/s, 33.319s/100 iters), loss = 1.18308
I1012 11:57:48.660120 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.975229 (* 1 = 0.975229 loss)
I1012 11:57:48.660135 31118 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I1012 11:58:21.566783 31118 solver.cpp:514] Iteration 500, Testing net (#0)
I1012 11:58:44.120625 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 11:58:44.136050 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.4875 (* 1 = 2.4875 loss)
I1012 11:58:44.136080 31118 solver.cpp:580]     Test net output #1: prob = 0.3144
I1012 11:58:44.433290 31118 solver.cpp:357] Iteration 500 (1.79297 iter/s, 55.7733s/100 iters), loss = 1.03669
I1012 11:58:44.433349 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.13703 (* 1 = 1.13703 loss)
I1012 11:58:44.433362 31118 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I1012 11:59:17.611845 31118 solver.cpp:357] Iteration 600 (3.01408 iter/s, 33.1776s/100 iters), loss = 0.993813
I1012 11:59:17.611997 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.1452 (* 1 = 1.1452 loss)
I1012 11:59:17.612010 31118 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I1012 11:59:50.938199 31118 solver.cpp:357] Iteration 700 (3.00072 iter/s, 33.3254s/100 iters), loss = 0.82107
I1012 11:59:50.938369 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.967488 (* 1 = 0.967488 loss)
I1012 11:59:50.938381 31118 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I1012 12:00:16.911639 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:00:24.013548 31118 solver.cpp:357] Iteration 800 (3.02349 iter/s, 33.0744s/100 iters), loss = 0.837136
I1012 12:00:24.013715 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.766868 (* 1 = 0.766868 loss)
I1012 12:00:24.013731 31118 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I1012 12:00:57.205420 31118 solver.cpp:357] Iteration 900 (3.01287 iter/s, 33.1909s/100 iters), loss = 0.949163
I1012 12:00:57.205600 31118 solver.cpp:376]     Train net output #0: Softmax1 = 1.0257 (* 1 = 1.0257 loss)
I1012 12:00:57.205613 31118 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I1012 12:01:30.229043 31118 solver.cpp:514] Iteration 1000, Testing net (#0)
I1012 12:01:52.729216 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:01:52.852327 31118 solver.cpp:580]     Test net output #0: Softmax1 = 3.22144 (* 1 = 3.22144 loss)
I1012 12:01:52.852355 31118 solver.cpp:580]     Test net output #1: prob = 0.2147
I1012 12:01:53.161852 31118 solver.cpp:357] Iteration 1000 (1.78711 iter/s, 55.9563s/100 iters), loss = 0.767782
I1012 12:01:53.162004 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.635458 (* 1 = 0.635458 loss)
I1012 12:01:53.162032 31118 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I1012 12:02:26.378222 31118 solver.cpp:357] Iteration 1100 (3.01065 iter/s, 33.2154s/100 iters), loss = 0.613754
I1012 12:02:26.378417 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.525559 (* 1 = 0.525559 loss)
I1012 12:02:26.378430 31118 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I1012 12:02:49.311424 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:02:59.597368 31118 solver.cpp:357] Iteration 1200 (3.0104 iter/s, 33.2182s/100 iters), loss = 0.857478
I1012 12:02:59.597561 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.916445 (* 1 = 0.916445 loss)
I1012 12:02:59.597576 31118 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I1012 12:03:32.771020 31118 solver.cpp:357] Iteration 1300 (3.01453 iter/s, 33.1727s/100 iters), loss = 0.701013
I1012 12:03:32.771195 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.743093 (* 1 = 0.743093 loss)
I1012 12:03:32.771209 31118 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I1012 12:04:05.952523 31118 solver.cpp:357] Iteration 1400 (3.01382 iter/s, 33.1805s/100 iters), loss = 0.662911
I1012 12:04:05.952670 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.59429 (* 1 = 0.59429 loss)
I1012 12:04:05.952682 31118 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I1012 12:04:38.908825 31118 solver.cpp:514] Iteration 1500, Testing net (#0)
I1012 12:05:01.333865 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:05:01.456830 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.93623 (* 1 = 2.93623 loss)
I1012 12:05:01.456858 31118 solver.cpp:580]     Test net output #1: prob = 0.2438
I1012 12:05:01.456879 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_1500.caffemodel
I1012 12:05:01.476230 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_1500.solverstate
I1012 12:05:01.478895 31118 solver.cpp:593]     Max_acc: 0.2438  with iter: 1500
I1012 12:05:01.779810 31118 solver.cpp:357] Iteration 1500 (1.79124 iter/s, 55.8272s/100 iters), loss = 0.702353
I1012 12:05:01.779881 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.679036 (* 1 = 0.679036 loss)
I1012 12:05:01.779892 31118 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I1012 12:05:21.502749 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:05:34.842039 31118 solver.cpp:357] Iteration 1600 (3.02449 iter/s, 33.0634s/100 iters), loss = 0.663948
I1012 12:05:34.842114 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.686583 (* 1 = 0.686583 loss)
I1012 12:05:34.842128 31118 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I1012 12:06:08.211815 31118 solver.cpp:357] Iteration 1700 (2.99681 iter/s, 33.3688s/100 iters), loss = 0.813297
I1012 12:06:08.211958 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.767586 (* 1 = 0.767586 loss)
I1012 12:06:08.211974 31118 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I1012 12:06:41.224359 31118 solver.cpp:357] Iteration 1800 (3.02905 iter/s, 33.0136s/100 iters), loss = 0.804633
I1012 12:06:41.224536 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.787609 (* 1 = 0.787609 loss)
I1012 12:06:41.224550 31118 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I1012 12:07:14.465554 31118 solver.cpp:357] Iteration 1900 (3.0084 iter/s, 33.2402s/100 iters), loss = 0.718156
I1012 12:07:14.465718 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.741116 (* 1 = 0.741116 loss)
I1012 12:07:14.465734 31118 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I1012 12:07:31.175716 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:07:47.211558 31118 solver.cpp:514] Iteration 2000, Testing net (#0)
I1012 12:08:09.630741 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:08:09.754678 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.47124 (* 1 = 2.47124 loss)
I1012 12:08:09.754705 31118 solver.cpp:580]     Test net output #1: prob = 0.2195
I1012 12:08:09.754712 31118 solver.cpp:593]     Max_acc: 0.2438  with iter: 1500
I1012 12:08:10.068079 31118 solver.cpp:357] Iteration 2000 (1.79848 iter/s, 55.6024s/100 iters), loss = 0.514547
I1012 12:08:10.068166 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.502836 (* 1 = 0.502836 loss)
I1012 12:08:10.068179 31118 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I1012 12:08:43.302731 31118 solver.cpp:357] Iteration 2100 (3.009 iter/s, 33.2337s/100 iters), loss = 0.655395
I1012 12:08:43.302902 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.644128 (* 1 = 0.644128 loss)
I1012 12:08:43.302915 31118 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I1012 12:09:16.524642 31118 solver.cpp:357] Iteration 2200 (3.01015 iter/s, 33.2209s/100 iters), loss = 0.713487
I1012 12:09:16.524793 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.68443 (* 1 = 0.68443 loss)
I1012 12:09:16.524807 31118 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I1012 12:09:49.716542 31118 solver.cpp:357] Iteration 2300 (3.01287 iter/s, 33.1909s/100 iters), loss = 0.65718
I1012 12:09:49.716724 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.600215 (* 1 = 0.600215 loss)
I1012 12:09:49.716737 31118 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I1012 12:10:03.476161 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:10:23.143600 31118 solver.cpp:357] Iteration 2400 (2.99168 iter/s, 33.4261s/100 iters), loss = 0.635485
I1012 12:10:23.143803 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.675323 (* 1 = 0.675323 loss)
I1012 12:10:23.143816 31118 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I1012 12:10:55.785585 31118 solver.cpp:514] Iteration 2500, Testing net (#0)
I1012 12:11:18.240880 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:11:18.365293 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.22494 (* 1 = 1.22494 loss)
I1012 12:11:18.365321 31118 solver.cpp:580]     Test net output #1: prob = 0.6014
I1012 12:11:18.365337 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_2500.caffemodel
I1012 12:11:18.375972 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_2500.solverstate
I1012 12:11:18.378381 31118 solver.cpp:593]     Max_acc: 0.6014  with iter: 2500
I1012 12:11:18.684113 31118 solver.cpp:357] Iteration 2500 (1.80049 iter/s, 55.5404s/100 iters), loss = 0.628775
I1012 12:11:18.684207 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.616359 (* 1 = 0.616359 loss)
I1012 12:11:18.684221 31118 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I1012 12:11:51.799551 31118 solver.cpp:357] Iteration 2600 (3.01983 iter/s, 33.1145s/100 iters), loss = 0.714261
I1012 12:11:51.799882 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.592774 (* 1 = 0.592774 loss)
I1012 12:11:51.799896 31118 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I1012 12:12:24.995007 31118 solver.cpp:357] Iteration 2700 (3.01255 iter/s, 33.1945s/100 iters), loss = 0.760166
I1012 12:12:24.995151 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.667783 (* 1 = 0.667783 loss)
I1012 12:12:24.995162 31118 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I1012 12:12:35.474319 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:12:58.213162 31118 solver.cpp:357] Iteration 2800 (3.01049 iter/s, 33.2172s/100 iters), loss = 0.524149
I1012 12:12:58.213320 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.513272 (* 1 = 0.513272 loss)
I1012 12:12:58.213332 31118 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I1012 12:13:31.290606 31118 solver.cpp:357] Iteration 2900 (3.02311 iter/s, 33.0785s/100 iters), loss = 0.529015
I1012 12:13:31.290925 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.454882 (* 1 = 0.454882 loss)
I1012 12:13:31.290938 31118 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I1012 12:14:04.080739 31118 solver.cpp:514] Iteration 3000, Testing net (#0)
I1012 12:14:26.432446 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:14:26.518812 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.19195 (* 1 = 1.19195 loss)
I1012 12:14:26.518852 31118 solver.cpp:580]     Test net output #1: prob = 0.588701
I1012 12:14:26.518859 31118 solver.cpp:593]     Max_acc: 0.6014  with iter: 2500
I1012 12:14:26.772459 31118 solver.cpp:357] Iteration 3000 (1.80239 iter/s, 55.4817s/100 iters), loss = 0.568503
I1012 12:14:26.772536 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.662343 (* 1 = 0.662343 loss)
I1012 12:14:26.772547 31118 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I1012 12:15:00.126526 31118 solver.cpp:357] Iteration 3100 (2.99822 iter/s, 33.3531s/100 iters), loss = 0.590149
I1012 12:15:00.126636 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.705716 (* 1 = 0.705716 loss)
I1012 12:15:00.126647 31118 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I1012 12:15:07.426647 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:15:33.207809 31118 solver.cpp:357] Iteration 3200 (3.02276 iter/s, 33.0824s/100 iters), loss = 0.588928
I1012 12:15:33.207936 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.336744 (* 1 = 0.336744 loss)
I1012 12:15:33.207947 31118 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I1012 12:16:06.442100 31118 solver.cpp:357] Iteration 3300 (3.00903 iter/s, 33.2333s/100 iters), loss = 0.599342
I1012 12:16:06.442304 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.755797 (* 1 = 0.755797 loss)
I1012 12:16:06.442333 31118 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I1012 12:16:39.639329 31118 solver.cpp:357] Iteration 3400 (3.01239 iter/s, 33.1962s/100 iters), loss = 0.486378
I1012 12:16:39.639585 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.444546 (* 1 = 0.444546 loss)
I1012 12:16:39.639597 31118 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I1012 12:17:12.713246 31118 solver.cpp:514] Iteration 3500, Testing net (#0)
I1012 12:17:34.947993 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:17:34.963184 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.96541 (* 1 = 1.96541 loss)
I1012 12:17:34.963212 31118 solver.cpp:580]     Test net output #1: prob = 0.4465
I1012 12:17:34.963219 31118 solver.cpp:593]     Max_acc: 0.6014  with iter: 2500
I1012 12:17:35.260913 31118 solver.cpp:357] Iteration 3500 (1.79787 iter/s, 55.6214s/100 iters), loss = 0.554399
I1012 12:17:35.260977 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.613659 (* 1 = 0.613659 loss)
I1012 12:17:35.260990 31118 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I1012 12:17:39.723866 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:18:08.581923 31118 solver.cpp:357] Iteration 3600 (3.0012 iter/s, 33.32s/100 iters), loss = 0.583509
I1012 12:18:08.582062 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.50356 (* 1 = 0.50356 loss)
I1012 12:18:08.582075 31118 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I1012 12:18:41.927484 31118 solver.cpp:357] Iteration 3700 (2.99899 iter/s, 33.3446s/100 iters), loss = 0.486299
I1012 12:18:41.927587 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.53338 (* 1 = 0.53338 loss)
I1012 12:18:41.927597 31118 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I1012 12:19:15.198151 31118 solver.cpp:357] Iteration 3800 (3.00575 iter/s, 33.2696s/100 iters), loss = 0.363922
I1012 12:19:15.198339 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.453909 (* 1 = 0.453909 loss)
I1012 12:19:15.198352 31118 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I1012 12:19:48.384512 31118 solver.cpp:357] Iteration 3900 (3.01338 iter/s, 33.1854s/100 iters), loss = 0.663678
I1012 12:19:48.384794 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.772371 (* 1 = 0.772371 loss)
I1012 12:19:48.384827 31118 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I1012 12:19:49.541494 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:20:21.285027 31118 solver.cpp:514] Iteration 4000, Testing net (#0)
I1012 12:20:43.559022 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:20:43.681640 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.63063 (* 1 = 1.63063 loss)
I1012 12:20:43.681668 31118 solver.cpp:580]     Test net output #1: prob = 0.5052
I1012 12:20:43.681677 31118 solver.cpp:593]     Max_acc: 0.6014  with iter: 2500
I1012 12:20:43.993331 31118 solver.cpp:357] Iteration 4000 (1.79828 iter/s, 55.6087s/100 iters), loss = 0.683905
I1012 12:20:43.993422 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.609885 (* 1 = 0.609885 loss)
I1012 12:20:43.993434 31118 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I1012 12:21:17.182564 31118 solver.cpp:357] Iteration 4100 (3.01311 iter/s, 33.1883s/100 iters), loss = 0.513052
I1012 12:21:17.182691 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.545337 (* 1 = 0.545337 loss)
I1012 12:21:17.182704 31118 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I1012 12:21:50.535023 31118 solver.cpp:357] Iteration 4200 (2.99837 iter/s, 33.3515s/100 iters), loss = 0.640603
I1012 12:21:50.535200 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.459705 (* 1 = 0.459705 loss)
I1012 12:21:50.535214 31118 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I1012 12:22:21.868108 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:22:23.828318 31118 solver.cpp:357] Iteration 4300 (3.0037 iter/s, 33.2923s/100 iters), loss = 0.494751
I1012 12:22:23.828382 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.504039 (* 1 = 0.504039 loss)
I1012 12:22:23.828395 31118 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I1012 12:22:57.100612 31118 solver.cpp:357] Iteration 4400 (3.0056 iter/s, 33.2713s/100 iters), loss = 0.552061
I1012 12:22:57.100823 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.546898 (* 1 = 0.546898 loss)
I1012 12:22:57.100836 31118 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I1012 12:23:30.131436 31118 solver.cpp:514] Iteration 4500, Testing net (#0)
I1012 12:23:52.243633 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:23:52.258944 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.0595 (* 1 = 1.0595 loss)
I1012 12:23:52.258971 31118 solver.cpp:580]     Test net output #1: prob = 0.651
I1012 12:23:52.258985 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_4500.caffemodel
I1012 12:23:52.267997 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_4500.solverstate
I1012 12:23:52.270337 31118 solver.cpp:593]     Max_acc: 0.651  with iter: 4500
I1012 12:23:52.569523 31118 solver.cpp:357] Iteration 4500 (1.80282 iter/s, 55.4688s/100 iters), loss = 0.592115
I1012 12:23:52.569582 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.413912 (* 1 = 0.413912 loss)
I1012 12:23:52.569597 31118 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I1012 12:24:25.890282 31118 solver.cpp:357] Iteration 4600 (3.00122 iter/s, 33.3198s/100 iters), loss = 0.457561
I1012 12:24:25.890420 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.493371 (* 1 = 0.493371 loss)
I1012 12:24:25.890434 31118 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I1012 12:24:54.009665 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:24:59.006175 31118 solver.cpp:357] Iteration 4700 (3.01978 iter/s, 33.1149s/100 iters), loss = 0.382435
I1012 12:24:59.006351 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.346772 (* 1 = 0.346772 loss)
I1012 12:24:59.006364 31118 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I1012 12:25:32.350507 31118 solver.cpp:357] Iteration 4800 (2.9991 iter/s, 33.3434s/100 iters), loss = 0.49293
I1012 12:25:32.350642 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.590057 (* 1 = 0.590057 loss)
I1012 12:25:32.350654 31118 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I1012 12:26:05.610618 31118 solver.cpp:357] Iteration 4900 (3.00651 iter/s, 33.2612s/100 iters), loss = 0.512165
I1012 12:26:05.610728 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.438922 (* 1 = 0.438922 loss)
I1012 12:26:05.610739 31118 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I1012 12:26:38.552465 31118 solver.cpp:514] Iteration 5000, Testing net (#0)
I1012 12:27:01.282299 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:27:01.297442 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.15641 (* 1 = 1.15641 loss)
I1012 12:27:01.297472 31118 solver.cpp:580]     Test net output #1: prob = 0.636501
I1012 12:27:01.297479 31118 solver.cpp:593]     Max_acc: 0.651  with iter: 4500
I1012 12:27:01.560065 31118 solver.cpp:357] Iteration 5000 (1.78733 iter/s, 55.9493s/100 iters), loss = 0.513866
I1012 12:27:01.560132 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.705815 (* 1 = 0.705815 loss)
I1012 12:27:01.560144 31118 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I1012 12:27:26.627144 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:27:34.724258 31118 solver.cpp:357] Iteration 5100 (3.01539 iter/s, 33.1632s/100 iters), loss = 0.437015
I1012 12:27:34.724330 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.307717 (* 1 = 0.307717 loss)
I1012 12:27:34.724340 31118 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I1012 12:28:07.993443 31118 solver.cpp:357] Iteration 5200 (3.00587 iter/s, 33.2682s/100 iters), loss = 0.693375
I1012 12:28:07.993623 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.725972 (* 1 = 0.725972 loss)
I1012 12:28:07.993634 31118 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I1012 12:28:41.217911 31118 solver.cpp:357] Iteration 5300 (3.00992 iter/s, 33.2235s/100 iters), loss = 0.641466
I1012 12:28:41.218113 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.673704 (* 1 = 0.673704 loss)
I1012 12:28:41.218123 31118 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I1012 12:29:14.357719 31118 solver.cpp:357] Iteration 5400 (3.01743 iter/s, 33.1408s/100 iters), loss = 0.397667
I1012 12:29:14.357848 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.418566 (* 1 = 0.418566 loss)
I1012 12:29:14.357861 31118 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I1012 12:29:36.378708 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:29:47.190919 31118 solver.cpp:514] Iteration 5500, Testing net (#0)
I1012 12:30:09.570580 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:30:09.699565 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.09799 (* 1 = 1.09799 loss)
I1012 12:30:09.699611 31118 solver.cpp:580]     Test net output #1: prob = 0.6347
I1012 12:30:09.699620 31118 solver.cpp:593]     Max_acc: 0.651  with iter: 4500
I1012 12:30:09.958770 31118 solver.cpp:357] Iteration 5500 (1.79847 iter/s, 55.603s/100 iters), loss = 0.411407
I1012 12:30:09.958845 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.410733 (* 1 = 0.410733 loss)
I1012 12:30:09.958858 31118 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I1012 12:30:43.258939 31118 solver.cpp:357] Iteration 5600 (3.00308 iter/s, 33.2992s/100 iters), loss = 0.532415
I1012 12:30:43.259096 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.410415 (* 1 = 0.410415 loss)
I1012 12:30:43.259109 31118 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I1012 12:31:14.738878 31118 solver.cpp:357] Iteration 5700 (3.17673 iter/s, 31.4789s/100 iters), loss = 0.441989
I1012 12:31:14.739269 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.35118 (* 1 = 0.35118 loss)
I1012 12:31:14.739341 31118 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I1012 12:31:46.903743 31118 solver.cpp:357] Iteration 5800 (3.1089 iter/s, 32.1657s/100 iters), loss = 0.367649
I1012 12:31:46.903931 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.41384 (* 1 = 0.41384 loss)
I1012 12:31:46.903945 31118 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I1012 12:32:05.736623 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:32:20.200973 31118 solver.cpp:357] Iteration 5900 (3.00334 iter/s, 33.2962s/100 iters), loss = 0.634583
I1012 12:32:20.201084 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.4998 (* 1 = 0.4998 loss)
I1012 12:32:20.201095 31118 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I1012 12:32:53.174921 31118 solver.cpp:514] Iteration 6000, Testing net (#0)
I1012 12:33:15.637979 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:33:15.770025 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.10144 (* 1 = 1.10144 loss)
I1012 12:33:15.770073 31118 solver.cpp:580]     Test net output #1: prob = 0.637099
I1012 12:33:15.770081 31118 solver.cpp:593]     Max_acc: 0.651  with iter: 4500
I1012 12:33:16.028228 31118 solver.cpp:357] Iteration 6000 (1.79125 iter/s, 55.827s/100 iters), loss = 0.50778
I1012 12:33:16.028301 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.480554 (* 1 = 0.480554 loss)
I1012 12:33:16.028313 31118 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I1012 12:33:49.116852 31118 solver.cpp:357] Iteration 6100 (3.02228 iter/s, 33.0876s/100 iters), loss = 0.447227
I1012 12:33:49.116971 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.399356 (* 1 = 0.399356 loss)
I1012 12:33:49.116983 31118 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I1012 12:34:22.423655 31118 solver.cpp:357] Iteration 6200 (3.00248 iter/s, 33.3058s/100 iters), loss = 0.412604
I1012 12:34:22.423780 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.393277 (* 1 = 0.393277 loss)
I1012 12:34:22.423797 31118 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I1012 12:34:38.146558 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:34:55.593036 31118 solver.cpp:357] Iteration 6300 (3.01473 iter/s, 33.1705s/100 iters), loss = 0.487065
I1012 12:34:55.593231 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.398708 (* 1 = 0.398708 loss)
I1012 12:34:55.593245 31118 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I1012 12:35:28.808784 31118 solver.cpp:357] Iteration 6400 (3.01071 iter/s, 33.2148s/100 iters), loss = 0.523392
I1012 12:35:28.808974 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.560953 (* 1 = 0.560953 loss)
I1012 12:35:28.808989 31118 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I1012 12:36:01.640421 31118 solver.cpp:514] Iteration 6500, Testing net (#0)
I1012 12:36:24.210013 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:36:24.317219 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.971971 (* 1 = 0.971971 loss)
I1012 12:36:24.317260 31118 solver.cpp:580]     Test net output #1: prob = 0.6857
I1012 12:36:24.317272 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_6500.caffemodel
I1012 12:36:24.327121 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_6500.solverstate
I1012 12:36:24.329438 31118 solver.cpp:593]     Max_acc: 0.6857  with iter: 6500
I1012 12:36:24.584666 31118 solver.cpp:357] Iteration 6500 (1.79289 iter/s, 55.7757s/100 iters), loss = 0.452939
I1012 12:36:24.584743 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.492501 (* 1 = 0.492501 loss)
I1012 12:36:24.584755 31118 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I1012 12:36:57.714489 31118 solver.cpp:357] Iteration 6600 (3.01852 iter/s, 33.1288s/100 iters), loss = 0.560233
I1012 12:36:57.714648 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.571316 (* 1 = 0.571316 loss)
I1012 12:36:57.714658 31118 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I1012 12:37:10.368408 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:37:31.151710 31118 solver.cpp:357] Iteration 6700 (2.99076 iter/s, 33.4363s/100 iters), loss = 0.519212
I1012 12:37:31.151880 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.489479 (* 1 = 0.489479 loss)
I1012 12:37:31.151891 31118 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I1012 12:38:04.396356 31118 solver.cpp:357] Iteration 6800 (3.00809 iter/s, 33.2437s/100 iters), loss = 0.486847
I1012 12:38:04.396484 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.567577 (* 1 = 0.567577 loss)
I1012 12:38:04.396497 31118 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I1012 12:38:37.528614 31118 solver.cpp:357] Iteration 6900 (3.01959 iter/s, 33.1171s/100 iters), loss = 0.463292
I1012 12:38:37.528761 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.369901 (* 1 = 0.369901 loss)
I1012 12:38:37.528774 31118 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I1012 12:39:10.528326 31118 solver.cpp:514] Iteration 7000, Testing net (#0)
I1012 12:39:32.841071 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:39:32.963469 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.806942 (* 1 = 0.806942 loss)
I1012 12:39:32.963500 31118 solver.cpp:580]     Test net output #1: prob = 0.7151
I1012 12:39:32.963515 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_7000.caffemodel
I1012 12:39:32.974864 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_7000.solverstate
I1012 12:39:32.977298 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:39:33.290652 31118 solver.cpp:357] Iteration 7000 (1.79421 iter/s, 55.7349s/100 iters), loss = 0.498424
I1012 12:39:33.290704 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.644725 (* 1 = 0.644725 loss)
I1012 12:39:33.290717 31118 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I1012 12:39:42.883345 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:40:06.477954 31118 solver.cpp:357] Iteration 7100 (3.01455 iter/s, 33.1725s/100 iters), loss = 0.503399
I1012 12:40:06.478018 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.530011 (* 1 = 0.530011 loss)
I1012 12:40:06.478029 31118 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I1012 12:40:39.818461 31118 solver.cpp:357] Iteration 7200 (3.00037 iter/s, 33.3292s/100 iters), loss = 0.650446
I1012 12:40:39.818776 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.611285 (* 1 = 0.611285 loss)
I1012 12:40:39.818821 31118 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I1012 12:41:12.957216 31118 solver.cpp:357] Iteration 7300 (3.01872 iter/s, 33.1266s/100 iters), loss = 0.428102
I1012 12:41:12.957392 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.449293 (* 1 = 0.449293 loss)
I1012 12:41:12.957406 31118 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I1012 12:41:46.049083 31118 solver.cpp:357] Iteration 7400 (3.02286 iter/s, 33.0813s/100 iters), loss = 0.47988
I1012 12:41:46.049229 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.420245 (* 1 = 0.420245 loss)
I1012 12:41:46.049243 31118 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I1012 12:41:52.449798 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:42:19.148007 31118 solver.cpp:514] Iteration 7500, Testing net (#0)
I1012 12:42:41.468222 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:42:41.592173 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.18381 (* 1 = 2.18381 loss)
I1012 12:42:41.592200 31118 solver.cpp:580]     Test net output #1: prob = 0.4759
I1012 12:42:41.592208 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:42:41.902904 31118 solver.cpp:357] Iteration 7500 (1.79087 iter/s, 55.8387s/100 iters), loss = 0.460948
I1012 12:42:41.902997 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.503799 (* 1 = 0.503799 loss)
I1012 12:42:41.903012 31118 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I1012 12:43:15.122723 31118 solver.cpp:357] Iteration 7600 (3.01105 iter/s, 33.211s/100 iters), loss = 0.445819
I1012 12:43:15.122845 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.394075 (* 1 = 0.394075 loss)
I1012 12:43:15.122860 31118 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I1012 12:43:48.308012 31118 solver.cpp:357] Iteration 7700 (3.01412 iter/s, 33.1771s/100 iters), loss = 0.410266
I1012 12:43:48.308128 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.500358 (* 1 = 0.500358 loss)
I1012 12:43:48.308141 31118 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I1012 12:44:21.500967 31118 solver.cpp:357] Iteration 7800 (3.01337 iter/s, 33.1854s/100 iters), loss = 0.415465
I1012 12:44:21.501149 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.454113 (* 1 = 0.454113 loss)
I1012 12:44:21.501163 31118 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I1012 12:44:24.731128 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:44:54.805783 31118 solver.cpp:357] Iteration 7900 (3.0032 iter/s, 33.2978s/100 iters), loss = 0.493462
I1012 12:44:54.805878 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.565449 (* 1 = 0.565449 loss)
I1012 12:44:54.805888 31118 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I1012 12:45:27.680229 31118 solver.cpp:514] Iteration 8000, Testing net (#0)
I1012 12:45:50.144419 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:45:50.262887 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.42407 (* 1 = 2.42407 loss)
I1012 12:45:50.262915 31118 solver.cpp:580]     Test net output #1: prob = 0.434
I1012 12:45:50.262923 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:45:50.547308 31118 solver.cpp:357] Iteration 8000 (1.79423 iter/s, 55.7343s/100 iters), loss = 0.389178
I1012 12:45:50.547375 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.381444 (* 1 = 0.381444 loss)
I1012 12:45:50.547387 31118 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I1012 12:46:23.641039 31118 solver.cpp:357] Iteration 8100 (3.02207 iter/s, 33.0899s/100 iters), loss = 0.469877
I1012 12:46:23.641166 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.46126 (* 1 = 0.46126 loss)
I1012 12:46:23.641178 31118 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I1012 12:46:56.953307 31118 solver.cpp:357] Iteration 8200 (3.0024 iter/s, 33.3066s/100 iters), loss = 0.542046
I1012 12:46:56.953524 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.555311 (* 1 = 0.555311 loss)
I1012 12:46:56.953538 31118 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I1012 12:46:57.107067 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:47:30.056715 31118 solver.cpp:357] Iteration 8300 (3.02132 iter/s, 33.0981s/100 iters), loss = 0.518532
I1012 12:47:30.056820 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.459383 (* 1 = 0.459383 loss)
I1012 12:47:30.056834 31118 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I1012 12:48:03.354085 31118 solver.cpp:357] Iteration 8400 (3.0037 iter/s, 33.2923s/100 iters), loss = 0.415105
I1012 12:48:03.354209 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.459519 (* 1 = 0.459519 loss)
I1012 12:48:03.354223 31118 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I1012 12:48:36.246664 31118 solver.cpp:514] Iteration 8500, Testing net (#0)
I1012 12:48:58.593938 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:48:58.717358 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.03361 (* 1 = 1.03361 loss)
I1012 12:48:58.717386 31118 solver.cpp:580]     Test net output #1: prob = 0.6835
I1012 12:48:58.717394 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:48:59.022078 31118 solver.cpp:357] Iteration 8500 (1.79658 iter/s, 55.6614s/100 iters), loss = 0.391374
I1012 12:48:59.022150 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.475593 (* 1 = 0.475593 loss)
I1012 12:48:59.022161 31118 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I1012 12:49:29.295347 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:49:32.267172 31118 solver.cpp:357] Iteration 8600 (3.00818 iter/s, 33.2427s/100 iters), loss = 0.440127
I1012 12:49:32.267241 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.447522 (* 1 = 0.447522 loss)
I1012 12:49:32.267252 31118 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I1012 12:50:05.602138 31118 solver.cpp:357] Iteration 8700 (3.00025 iter/s, 33.3306s/100 iters), loss = 0.375949
I1012 12:50:05.602262 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.390319 (* 1 = 0.390319 loss)
I1012 12:50:05.602273 31118 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I1012 12:50:38.805842 31118 solver.cpp:357] Iteration 8800 (3.0121 iter/s, 33.1994s/100 iters), loss = 0.467395
I1012 12:50:38.805970 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.443656 (* 1 = 0.443656 loss)
I1012 12:50:38.805980 31118 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I1012 12:51:11.935561 31118 solver.cpp:357] Iteration 8900 (3.01863 iter/s, 33.1276s/100 iters), loss = 0.56677
I1012 12:51:11.935667 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.548182 (* 1 = 0.548182 loss)
I1012 12:51:11.935679 31118 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I1012 12:51:39.068142 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:51:44.949874 31118 solver.cpp:514] Iteration 9000, Testing net (#0)
I1012 12:52:07.707715 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:52:07.833148 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.872524 (* 1 = 0.872524 loss)
I1012 12:52:07.833196 31118 solver.cpp:580]     Test net output #1: prob = 0.6988
I1012 12:52:07.833204 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:52:08.082762 31118 solver.cpp:357] Iteration 9000 (1.78113 iter/s, 56.144s/100 iters), loss = 0.283553
I1012 12:52:08.082842 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.267143 (* 1 = 0.267143 loss)
I1012 12:52:08.082855 31118 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I1012 12:52:41.364490 31118 solver.cpp:357] Iteration 9100 (3.005 iter/s, 33.2778s/100 iters), loss = 0.4804
I1012 12:52:41.364606 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.597026 (* 1 = 0.597026 loss)
I1012 12:52:41.364616 31118 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I1012 12:53:14.548898 31118 solver.cpp:357] Iteration 9200 (3.01381 iter/s, 33.1806s/100 iters), loss = 0.382913
I1012 12:53:14.549111 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.426415 (* 1 = 0.426415 loss)
I1012 12:53:14.549124 31118 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I1012 12:53:47.810405 31118 solver.cpp:357] Iteration 9300 (3.00664 iter/s, 33.2597s/100 iters), loss = 0.441768
I1012 12:53:47.810515 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.485639 (* 1 = 0.485639 loss)
I1012 12:53:47.810526 31118 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I1012 12:54:11.793020 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:54:21.112365 31118 solver.cpp:357] Iteration 9400 (3.00316 iter/s, 33.2983s/100 iters), loss = 0.525912
I1012 12:54:21.112540 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.287124 (* 1 = 0.287124 loss)
I1012 12:54:21.112552 31118 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I1012 12:54:54.011355 31118 solver.cpp:514] Iteration 9500, Testing net (#0)
I1012 12:55:16.407832 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:55:16.422783 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.39218 (* 1 = 1.39218 loss)
I1012 12:55:16.422819 31118 solver.cpp:580]     Test net output #1: prob = 0.628701
I1012 12:55:16.422827 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:55:16.731118 31118 solver.cpp:357] Iteration 9500 (1.7981 iter/s, 55.6142s/100 iters), loss = 0.40558
I1012 12:55:16.731171 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.423145 (* 1 = 0.423145 loss)
I1012 12:55:16.731185 31118 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I1012 12:55:50.021708 31118 solver.cpp:357] Iteration 9600 (3.00331 iter/s, 33.2966s/100 iters), loss = 0.478812
I1012 12:55:50.021893 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.54732 (* 1 = 0.54732 loss)
I1012 12:55:50.021906 31118 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I1012 12:56:23.054270 31118 solver.cpp:357] Iteration 9700 (3.02685 iter/s, 33.0376s/100 iters), loss = 0.414539
I1012 12:56:23.054451 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.385706 (* 1 = 0.385706 loss)
I1012 12:56:23.054464 31118 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I1012 12:56:44.007400 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:56:56.107347 31118 solver.cpp:357] Iteration 9800 (3.02505 iter/s, 33.0573s/100 iters), loss = 0.435385
I1012 12:56:56.107468 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.575471 (* 1 = 0.575471 loss)
I1012 12:56:56.107482 31118 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I1012 12:57:29.338089 31118 solver.cpp:357] Iteration 9900 (3.00895 iter/s, 33.2342s/100 iters), loss = 0.371834
I1012 12:57:29.338238 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.430763 (* 1 = 0.430763 loss)
I1012 12:57:29.338253 31118 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I1012 12:58:02.367431 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_10000.caffemodel
I1012 12:58:02.380784 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_10000.solverstate
I1012 12:58:02.383844 31118 solver.cpp:514] Iteration 10000, Testing net (#0)
I1012 12:58:24.738281 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:58:24.796555 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.01772 (* 1 = 1.01772 loss)
I1012 12:58:24.796592 31118 solver.cpp:580]     Test net output #1: prob = 0.6796
I1012 12:58:24.796599 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 12:58:25.047593 31118 solver.cpp:357] Iteration 10000 (1.79484 iter/s, 55.7153s/100 iters), loss = 0.429465
I1012 12:58:25.047667 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.431177 (* 1 = 0.431177 loss)
I1012 12:58:25.047679 31118 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I1012 12:58:58.396240 31118 solver.cpp:357] Iteration 10100 (2.99846 iter/s, 33.3505s/100 iters), loss = 0.488766
I1012 12:58:58.396409 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.453493 (* 1 = 0.453493 loss)
I1012 12:58:58.396420 31118 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I1012 12:59:16.102567 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 12:59:31.636792 31118 solver.cpp:357] Iteration 10200 (3.00807 iter/s, 33.2439s/100 iters), loss = 0.383691
I1012 12:59:31.636965 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.456293 (* 1 = 0.456293 loss)
I1012 12:59:31.636976 31118 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I1012 13:00:04.864975 31118 solver.cpp:357] Iteration 10300 (3.00941 iter/s, 33.229s/100 iters), loss = 0.422067
I1012 13:00:04.865128 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.450297 (* 1 = 0.450297 loss)
I1012 13:00:04.865140 31118 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I1012 13:00:38.138483 31118 solver.cpp:357] Iteration 10400 (3.00535 iter/s, 33.274s/100 iters), loss = 0.483857
I1012 13:00:38.138645 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.574894 (* 1 = 0.574894 loss)
I1012 13:00:38.138656 31118 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I1012 13:01:10.833212 31118 solver.cpp:514] Iteration 10500, Testing net (#0)
I1012 13:01:33.198222 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:01:33.321632 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.51745 (* 1 = 1.51745 loss)
I1012 13:01:33.321660 31118 solver.cpp:580]     Test net output #1: prob = 0.6331
I1012 13:01:33.321666 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:01:33.635715 31118 solver.cpp:357] Iteration 10500 (1.80184 iter/s, 55.4988s/100 iters), loss = 0.532085
I1012 13:01:33.635767 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.576104 (* 1 = 0.576104 loss)
I1012 13:01:33.635782 31118 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I1012 13:01:48.222395 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:02:06.909735 31118 solver.cpp:357] Iteration 10600 (3.00537 iter/s, 33.2737s/100 iters), loss = 0.479254
I1012 13:02:06.909813 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.422743 (* 1 = 0.422743 loss)
I1012 13:02:06.909826 31118 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I1012 13:02:40.162032 31118 solver.cpp:357] Iteration 10700 (3.00736 iter/s, 33.2518s/100 iters), loss = 0.412855
I1012 13:02:40.162187 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.445247 (* 1 = 0.445247 loss)
I1012 13:02:40.162201 31118 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I1012 13:03:13.363420 31118 solver.cpp:357] Iteration 10800 (3.01199 iter/s, 33.2007s/100 iters), loss = 0.433441
I1012 13:03:13.363546 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.47596 (* 1 = 0.47596 loss)
I1012 13:03:13.363561 31118 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I1012 13:03:46.695994 31118 solver.cpp:357] Iteration 10900 (3.00015 iter/s, 33.3317s/100 iters), loss = 0.444826
I1012 13:03:46.696105 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.522721 (* 1 = 0.522721 loss)
I1012 13:03:46.696115 31118 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I1012 13:03:58.201714 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:04:19.721527 31118 solver.cpp:514] Iteration 11000, Testing net (#0)
I1012 13:04:42.106078 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:04:42.229753 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.75275 (* 1 = 1.75275 loss)
I1012 13:04:42.229777 31118 solver.cpp:580]     Test net output #1: prob = 0.5234
I1012 13:04:42.229784 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:04:42.544454 31118 solver.cpp:357] Iteration 11000 (1.7905 iter/s, 55.8502s/100 iters), loss = 0.41329
I1012 13:04:42.544505 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.422055 (* 1 = 0.422055 loss)
I1012 13:04:42.544519 31118 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I1012 13:05:15.810252 31118 solver.cpp:357] Iteration 11100 (3.00621 iter/s, 33.2645s/100 iters), loss = 0.42392
I1012 13:05:15.810500 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.523925 (* 1 = 0.523925 loss)
I1012 13:05:15.810513 31118 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I1012 13:05:48.984848 31118 solver.cpp:357] Iteration 11200 (3.01448 iter/s, 33.1732s/100 iters), loss = 0.275454
I1012 13:05:48.985100 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.27772 (* 1 = 0.27772 loss)
I1012 13:05:48.985118 31118 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I1012 13:06:22.203632 31118 solver.cpp:357] Iteration 11300 (3.01048 iter/s, 33.2173s/100 iters), loss = 0.458772
I1012 13:06:22.203959 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.508031 (* 1 = 0.508031 loss)
I1012 13:06:22.204022 31118 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I1012 13:06:30.709887 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:06:55.564411 31118 solver.cpp:357] Iteration 11400 (2.99767 iter/s, 33.3592s/100 iters), loss = 0.463206
I1012 13:06:55.564568 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.348422 (* 1 = 0.348422 loss)
I1012 13:06:55.564580 31118 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I1012 13:07:28.437083 31118 solver.cpp:514] Iteration 11500, Testing net (#0)
I1012 13:07:50.467026 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:07:50.559353 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.07144 (* 1 = 2.07144 loss)
I1012 13:07:50.559384 31118 solver.cpp:580]     Test net output #1: prob = 0.4716
I1012 13:07:50.559393 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:07:50.871238 31118 solver.cpp:357] Iteration 11500 (1.80814 iter/s, 55.3055s/100 iters), loss = 0.515584
I1012 13:07:50.871292 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.545901 (* 1 = 0.545901 loss)
I1012 13:07:50.871304 31118 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I1012 13:08:24.098654 31118 solver.cpp:357] Iteration 11600 (3.00972 iter/s, 33.2257s/100 iters), loss = 0.367775
I1012 13:08:24.098783 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.385452 (* 1 = 0.385452 loss)
I1012 13:08:24.098794 31118 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I1012 13:08:57.193272 31118 solver.cpp:357] Iteration 11700 (3.02162 iter/s, 33.0949s/100 iters), loss = 0.452395
I1012 13:08:57.193423 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.297262 (* 1 = 0.297262 loss)
I1012 13:08:57.193436 31118 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I1012 13:09:02.626046 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:09:30.544353 31118 solver.cpp:357] Iteration 11800 (2.99857 iter/s, 33.3492s/100 iters), loss = 0.628454
I1012 13:09:30.544510 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.52449 (* 1 = 0.52449 loss)
I1012 13:09:30.544523 31118 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I1012 13:10:03.863412 31118 solver.cpp:357] Iteration 11900 (3.00127 iter/s, 33.3192s/100 iters), loss = 0.397896
I1012 13:10:03.863574 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.52219 (* 1 = 0.52219 loss)
I1012 13:10:03.863590 31118 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I1012 13:10:36.788216 31118 solver.cpp:514] Iteration 12000, Testing net (#0)
I1012 13:10:59.128777 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:10:59.252120 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.92152 (* 1 = 0.92152 loss)
I1012 13:10:59.252176 31118 solver.cpp:580]     Test net output #1: prob = 0.689801
I1012 13:10:59.252182 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:10:59.512182 31118 solver.cpp:357] Iteration 12000 (1.79704 iter/s, 55.647s/100 iters), loss = 0.477033
I1012 13:10:59.512264 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.516186 (* 1 = 0.516186 loss)
I1012 13:10:59.512277 31118 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I1012 13:11:32.637949 31118 solver.cpp:357] Iteration 12100 (3.01898 iter/s, 33.1238s/100 iters), loss = 0.389609
I1012 13:11:32.638186 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.47901 (* 1 = 0.47901 loss)
I1012 13:11:32.638200 31118 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I1012 13:11:34.803141 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:12:06.004719 31118 solver.cpp:357] Iteration 12200 (2.99718 iter/s, 33.3647s/100 iters), loss = 0.376941
I1012 13:12:06.004870 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.41275 (* 1 = 0.41275 loss)
I1012 13:12:06.004884 31118 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I1012 13:12:39.272714 31118 solver.cpp:357] Iteration 12300 (3.00593 iter/s, 33.2676s/100 iters), loss = 0.373867
I1012 13:12:39.272826 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.315024 (* 1 = 0.315024 loss)
I1012 13:12:39.272837 31118 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I1012 13:13:12.440521 31118 solver.cpp:357] Iteration 12400 (3.0149 iter/s, 33.1686s/100 iters), loss = 0.579724
I1012 13:13:12.440665 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.663041 (* 1 = 0.663041 loss)
I1012 13:13:12.440677 31118 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I1012 13:13:44.635448 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:13:45.298691 31118 solver.cpp:514] Iteration 12500, Testing net (#0)
I1012 13:14:07.473350 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:14:07.600405 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.89116 (* 1 = 0.89116 loss)
I1012 13:14:07.600433 31118 solver.cpp:580]     Test net output #1: prob = 0.707199
I1012 13:14:07.600440 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:14:07.913832 31118 solver.cpp:357] Iteration 12500 (1.80252 iter/s, 55.4779s/100 iters), loss = 0.400129
I1012 13:14:07.913924 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.409875 (* 1 = 0.409875 loss)
I1012 13:14:07.913938 31118 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I1012 13:14:41.136415 31118 solver.cpp:357] Iteration 12600 (3.00996 iter/s, 33.223s/100 iters), loss = 0.530169
I1012 13:14:41.136580 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.432137 (* 1 = 0.432137 loss)
I1012 13:14:41.136592 31118 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I1012 13:15:14.185907 31118 solver.cpp:357] Iteration 12700 (3.02574 iter/s, 33.0498s/100 iters), loss = 0.513579
I1012 13:15:14.186065 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.546847 (* 1 = 0.546847 loss)
I1012 13:15:14.186076 31118 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I1012 13:15:47.303623 31118 solver.cpp:357] Iteration 12800 (3.01952 iter/s, 33.1179s/100 iters), loss = 0.464255
I1012 13:15:47.303869 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.56776 (* 1 = 0.56776 loss)
I1012 13:15:47.303884 31118 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I1012 13:16:16.683205 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:16:20.498585 31118 solver.cpp:357] Iteration 12900 (3.0125 iter/s, 33.195s/100 iters), loss = 0.50829
I1012 13:16:20.498756 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.354951 (* 1 = 0.354951 loss)
I1012 13:16:20.498769 31118 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I1012 13:16:53.590770 31118 solver.cpp:514] Iteration 13000, Testing net (#0)
I1012 13:17:15.962260 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:17:16.083024 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.25576 (* 1 = 1.25576 loss)
I1012 13:17:16.083055 31118 solver.cpp:580]     Test net output #1: prob = 0.6371
I1012 13:17:16.083063 31118 solver.cpp:593]     Max_acc: 0.7151  with iter: 7000
I1012 13:17:16.395231 31118 solver.cpp:357] Iteration 13000 (1.78898 iter/s, 55.8979s/100 iters), loss = 0.403953
I1012 13:17:16.395309 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.42573 (* 1 = 0.42573 loss)
I1012 13:17:16.395323 31118 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I1012 13:17:49.512807 31118 solver.cpp:357] Iteration 13100 (3.01957 iter/s, 33.1173s/100 iters), loss = 0.469557
I1012 13:17:49.513010 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.391064 (* 1 = 0.391064 loss)
I1012 13:17:49.513023 31118 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I1012 13:18:22.738533 31118 solver.cpp:357] Iteration 13200 (3.00975 iter/s, 33.2253s/100 iters), loss = 0.375085
I1012 13:18:22.738714 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.437295 (* 1 = 0.437295 loss)
I1012 13:18:22.738730 31118 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I1012 13:18:48.791719 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:18:55.881950 31118 solver.cpp:357] Iteration 13300 (3.01723 iter/s, 33.143s/100 iters), loss = 0.545857
I1012 13:18:55.882087 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.439716 (* 1 = 0.439716 loss)
I1012 13:18:55.882100 31118 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I1012 13:19:29.267262 31118 solver.cpp:357] Iteration 13400 (2.99538 iter/s, 33.3848s/100 iters), loss = 0.541305
I1012 13:19:29.267487 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.50993 (* 1 = 0.50993 loss)
I1012 13:19:29.267515 31118 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I1012 13:20:02.049933 31118 solver.cpp:514] Iteration 13500, Testing net (#0)
I1012 13:20:24.206617 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:20:24.222456 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.660766 (* 1 = 0.660766 loss)
I1012 13:20:24.222484 31118 solver.cpp:580]     Test net output #1: prob = 0.775501
I1012 13:20:24.222497 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_13500.caffemodel
I1012 13:20:24.231436 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_13500.solverstate
I1012 13:20:24.233758 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:20:24.533466 31118 solver.cpp:357] Iteration 13500 (1.80941 iter/s, 55.2666s/100 iters), loss = 0.366527
I1012 13:20:24.533529 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.408828 (* 1 = 0.408828 loss)
I1012 13:20:24.533541 31118 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I1012 13:20:57.712699 31118 solver.cpp:357] Iteration 13600 (3.014 iter/s, 33.1785s/100 iters), loss = 0.298833
I1012 13:20:57.712841 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.220917 (* 1 = 0.220917 loss)
I1012 13:20:57.712855 31118 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I1012 13:21:20.774475 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:21:30.854930 31118 solver.cpp:357] Iteration 13700 (3.01737 iter/s, 33.1414s/100 iters), loss = 0.42233
I1012 13:21:30.855060 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.398576 (* 1 = 0.398576 loss)
I1012 13:21:30.855073 31118 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I1012 13:22:04.071591 31118 solver.cpp:357] Iteration 13800 (3.01062 iter/s, 33.2158s/100 iters), loss = 0.333851
I1012 13:22:04.071710 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.351806 (* 1 = 0.351806 loss)
I1012 13:22:04.071722 31118 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I1012 13:22:37.282675 31118 solver.cpp:357] Iteration 13900 (3.01113 iter/s, 33.2101s/100 iters), loss = 0.375221
I1012 13:22:37.282853 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.317311 (* 1 = 0.317311 loss)
I1012 13:22:37.282866 31118 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I1012 13:23:10.199029 31118 solver.cpp:514] Iteration 14000, Testing net (#0)
I1012 13:23:32.508273 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:23:32.622248 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.78298 (* 1 = 1.78298 loss)
I1012 13:23:32.622289 31118 solver.cpp:580]     Test net output #1: prob = 0.5092
I1012 13:23:32.622297 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:23:32.868520 31118 solver.cpp:357] Iteration 14000 (1.79903 iter/s, 55.5856s/100 iters), loss = 0.406717
I1012 13:23:32.868595 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.390609 (* 1 = 0.390609 loss)
I1012 13:23:32.868608 31118 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I1012 13:23:52.667095 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:24:06.197525 31118 solver.cpp:357] Iteration 14100 (3.00049 iter/s, 33.3279s/100 iters), loss = 0.369532
I1012 13:24:06.197613 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.377229 (* 1 = 0.377229 loss)
I1012 13:24:06.197625 31118 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I1012 13:24:39.376380 31118 solver.cpp:357] Iteration 14200 (3.01407 iter/s, 33.1777s/100 iters), loss = 0.482607
I1012 13:24:39.376689 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.507379 (* 1 = 0.507379 loss)
I1012 13:24:39.376701 31118 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I1012 13:25:12.704562 31118 solver.cpp:357] Iteration 14300 (3.00057 iter/s, 33.327s/100 iters), loss = 0.539569
I1012 13:25:12.704695 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.376273 (* 1 = 0.376273 loss)
I1012 13:25:12.704708 31118 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I1012 13:25:45.921748 31118 solver.cpp:357] Iteration 14400 (3.0106 iter/s, 33.216s/100 iters), loss = 0.430416
I1012 13:25:45.921883 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.410829 (* 1 = 0.410829 loss)
I1012 13:25:45.921895 31118 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I1012 13:26:02.580920 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:26:18.825829 31118 solver.cpp:514] Iteration 14500, Testing net (#0)
I1012 13:26:41.113991 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:26:41.237138 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.744934 (* 1 = 0.744934 loss)
I1012 13:26:41.237164 31118 solver.cpp:580]     Test net output #1: prob = 0.7576
I1012 13:26:41.237170 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:26:41.546867 31118 solver.cpp:357] Iteration 14500 (1.79777 iter/s, 55.6245s/100 iters), loss = 0.408411
I1012 13:26:41.546918 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.299099 (* 1 = 0.299099 loss)
I1012 13:26:41.546933 31118 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I1012 13:27:14.634722 31118 solver.cpp:357] Iteration 14600 (3.02238 iter/s, 33.0865s/100 iters), loss = 0.418982
I1012 13:27:14.634872 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.476009 (* 1 = 0.476009 loss)
I1012 13:27:14.634887 31118 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I1012 13:27:47.800889 31118 solver.cpp:357] Iteration 14700 (3.01524 iter/s, 33.1648s/100 iters), loss = 0.381462
I1012 13:27:47.801065 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.389311 (* 1 = 0.389311 loss)
I1012 13:27:47.801079 31118 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I1012 13:28:21.013394 31118 solver.cpp:357] Iteration 14800 (3.01104 iter/s, 33.2111s/100 iters), loss = 0.379709
I1012 13:28:21.013514 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.316657 (* 1 = 0.316657 loss)
I1012 13:28:21.013526 31118 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I1012 13:28:34.844812 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:28:54.191392 31118 solver.cpp:357] Iteration 14900 (3.01417 iter/s, 33.1766s/100 iters), loss = 0.302791
I1012 13:28:54.191582 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.3648 (* 1 = 0.3648 loss)
I1012 13:28:54.191596 31118 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I1012 13:29:27.084065 31118 solver.cpp:514] Iteration 15000, Testing net (#0)
I1012 13:29:49.405786 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:29:49.528956 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.32992 (* 1 = 1.32992 loss)
I1012 13:29:49.529007 31118 solver.cpp:580]     Test net output #1: prob = 0.5473
I1012 13:29:49.529016 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:29:49.763177 31118 solver.cpp:357] Iteration 15000 (1.79944 iter/s, 55.5728s/100 iters), loss = 0.549581
I1012 13:29:49.763258 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.504748 (* 1 = 0.504748 loss)
I1012 13:29:49.763272 31118 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I1012 13:30:23.042624 31118 solver.cpp:357] Iteration 15100 (3.00499 iter/s, 33.278s/100 iters), loss = 0.375581
I1012 13:30:23.042800 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.368578 (* 1 = 0.368578 loss)
I1012 13:30:23.042812 31118 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I1012 13:30:56.297276 31118 solver.cpp:357] Iteration 15200 (3.00723 iter/s, 33.2532s/100 iters), loss = 0.536507
I1012 13:30:56.297487 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.495624 (* 1 = 0.495624 loss)
I1012 13:30:56.297516 31118 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I1012 13:31:06.794059 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:31:29.666085 31118 solver.cpp:357] Iteration 15300 (2.99695 iter/s, 33.3673s/100 iters), loss = 0.34676
I1012 13:31:29.666270 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.379883 (* 1 = 0.379883 loss)
I1012 13:31:29.666282 31118 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I1012 13:32:02.914481 31118 solver.cpp:357] Iteration 15400 (3.0078 iter/s, 33.2469s/100 iters), loss = 0.461594
I1012 13:32:02.914598 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.442098 (* 1 = 0.442098 loss)
I1012 13:32:02.914610 31118 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I1012 13:32:35.809288 31118 solver.cpp:514] Iteration 15500, Testing net (#0)
I1012 13:32:58.003252 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:32:58.019121 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.901267 (* 1 = 0.901267 loss)
I1012 13:32:58.019151 31118 solver.cpp:580]     Test net output #1: prob = 0.727999
I1012 13:32:58.019157 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:32:58.320996 31118 solver.cpp:357] Iteration 15500 (1.80488 iter/s, 55.4054s/100 iters), loss = 0.464802
I1012 13:32:58.321058 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.496838 (* 1 = 0.496838 loss)
I1012 13:32:58.321070 31118 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I1012 13:33:31.481245 31118 solver.cpp:357] Iteration 15600 (3.0158 iter/s, 33.1586s/100 iters), loss = 0.399618
I1012 13:33:31.481390 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.415377 (* 1 = 0.415377 loss)
I1012 13:33:31.481405 31118 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I1012 13:33:38.789566 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:34:04.510155 31118 solver.cpp:357] Iteration 15700 (3.0278 iter/s, 33.0273s/100 iters), loss = 0.49266
I1012 13:34:04.510320 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.369974 (* 1 = 0.369974 loss)
I1012 13:34:04.510334 31118 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I1012 13:34:37.820767 31118 solver.cpp:357] Iteration 15800 (3.00219 iter/s, 33.309s/100 iters), loss = 0.385146
I1012 13:34:37.820886 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.460453 (* 1 = 0.460453 loss)
I1012 13:34:37.820897 31118 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I1012 13:35:11.034421 31118 solver.cpp:357] Iteration 15900 (3.01096 iter/s, 33.212s/100 iters), loss = 0.389128
I1012 13:35:11.034556 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.338395 (* 1 = 0.338395 loss)
I1012 13:35:11.034571 31118 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I1012 13:35:44.034826 31118 solver.cpp:514] Iteration 16000, Testing net (#0)
I1012 13:36:06.422825 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:36:06.545866 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.07449 (* 1 = 1.07449 loss)
I1012 13:36:06.545892 31118 solver.cpp:580]     Test net output #1: prob = 0.639901
I1012 13:36:06.545899 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:36:06.857667 31118 solver.cpp:357] Iteration 16000 (1.79141 iter/s, 55.822s/100 iters), loss = 0.339376
I1012 13:36:06.857728 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.340874 (* 1 = 0.340874 loss)
I1012 13:36:06.857740 31118 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I1012 13:36:11.249068 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:36:40.105590 31118 solver.cpp:357] Iteration 16100 (3.00786 iter/s, 33.2463s/100 iters), loss = 0.453871
I1012 13:36:40.105834 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.431401 (* 1 = 0.431401 loss)
I1012 13:36:40.105847 31118 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I1012 13:37:13.277308 31118 solver.cpp:357] Iteration 16200 (3.01477 iter/s, 33.17s/100 iters), loss = 0.304724
I1012 13:37:13.277479 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.24021 (* 1 = 0.24021 loss)
I1012 13:37:13.277492 31118 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I1012 13:37:46.422382 31118 solver.cpp:357] Iteration 16300 (3.01719 iter/s, 33.1434s/100 iters), loss = 0.356653
I1012 13:37:46.422539 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.45067 (* 1 = 0.45067 loss)
I1012 13:37:46.422554 31118 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I1012 13:38:19.778244 31118 solver.cpp:357] Iteration 16400 (2.99813 iter/s, 33.3542s/100 iters), loss = 0.437873
I1012 13:38:19.778365 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.490338 (* 1 = 0.490338 loss)
I1012 13:38:19.778378 31118 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I1012 13:38:20.895900 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:38:52.730873 31118 solver.cpp:514] Iteration 16500, Testing net (#0)
I1012 13:39:14.870399 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:39:14.951426 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.52009 (* 1 = 1.52009 loss)
I1012 13:39:14.951455 31118 solver.cpp:580]     Test net output #1: prob = 0.5984
I1012 13:39:14.951463 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:39:15.266242 31118 solver.cpp:357] Iteration 16500 (1.80224 iter/s, 55.4867s/100 iters), loss = 0.420455
I1012 13:39:15.266297 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.301198 (* 1 = 0.301198 loss)
I1012 13:39:15.266309 31118 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I1012 13:39:48.394039 31118 solver.cpp:357] Iteration 16600 (3.01877 iter/s, 33.1261s/100 iters), loss = 0.423897
I1012 13:39:48.394215 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.406714 (* 1 = 0.406714 loss)
I1012 13:39:48.394229 31118 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I1012 13:40:21.488560 31118 solver.cpp:357] Iteration 16700 (3.0218 iter/s, 33.0928s/100 iters), loss = 0.46307
I1012 13:40:21.488730 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.320951 (* 1 = 0.320951 loss)
I1012 13:40:21.488744 31118 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I1012 13:40:52.967854 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:40:54.923897 31118 solver.cpp:357] Iteration 16800 (2.991 iter/s, 33.4336s/100 iters), loss = 0.348545
I1012 13:40:54.923967 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.346885 (* 1 = 0.346885 loss)
I1012 13:40:54.923979 31118 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I1012 13:41:28.094784 31118 solver.cpp:357] Iteration 16900 (3.01466 iter/s, 33.1713s/100 iters), loss = 0.339571
I1012 13:41:28.094902 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.326243 (* 1 = 0.326243 loss)
I1012 13:41:28.094914 31118 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I1012 13:42:01.202203 31118 solver.cpp:514] Iteration 17000, Testing net (#0)
I1012 13:42:23.320958 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:42:23.444375 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.797159 (* 1 = 0.797159 loss)
I1012 13:42:23.444403 31118 solver.cpp:580]     Test net output #1: prob = 0.736099
I1012 13:42:23.444408 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:42:23.755897 31118 solver.cpp:357] Iteration 17000 (1.79663 iter/s, 55.6597s/100 iters), loss = 0.518635
I1012 13:42:23.755947 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.351719 (* 1 = 0.351719 loss)
I1012 13:42:23.755962 31118 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I1012 13:42:56.920260 31118 solver.cpp:357] Iteration 17100 (3.01544 iter/s, 33.1626s/100 iters), loss = 0.334698
I1012 13:42:56.920467 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.351609 (* 1 = 0.351609 loss)
I1012 13:42:56.920481 31118 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I1012 13:43:25.114670 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:43:30.241966 31118 solver.cpp:357] Iteration 17200 (3.00103 iter/s, 33.3219s/100 iters), loss = 0.335274
I1012 13:43:30.242120 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.286622 (* 1 = 0.286622 loss)
I1012 13:43:30.242132 31118 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I1012 13:44:03.441198 31118 solver.cpp:357] Iteration 17300 (3.01228 iter/s, 33.1975s/100 iters), loss = 0.38524
I1012 13:44:03.442929 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.481009 (* 1 = 0.481009 loss)
I1012 13:44:03.442947 31118 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I1012 13:44:36.637941 31118 solver.cpp:357] Iteration 17400 (3.0125 iter/s, 33.195s/100 iters), loss = 0.430131
I1012 13:44:36.638051 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.468972 (* 1 = 0.468972 loss)
I1012 13:44:36.638064 31118 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I1012 13:45:09.616780 31118 solver.cpp:514] Iteration 17500, Testing net (#0)
I1012 13:45:31.866108 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:45:31.988545 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.55856 (* 1 = 1.55856 loss)
I1012 13:45:31.988574 31118 solver.cpp:580]     Test net output #1: prob = 0.520801
I1012 13:45:31.988581 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:45:32.306186 31118 solver.cpp:357] Iteration 17500 (1.7964 iter/s, 55.6668s/100 iters), loss = 0.291644
I1012 13:45:32.306277 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.368724 (* 1 = 0.368724 loss)
I1012 13:45:32.306290 31118 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I1012 13:45:57.445363 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:46:05.557909 31118 solver.cpp:357] Iteration 17600 (3.00752 iter/s, 33.2499s/100 iters), loss = 0.349051
I1012 13:46:05.557981 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.293644 (* 1 = 0.293644 loss)
I1012 13:46:05.557991 31118 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I1012 13:46:38.745178 31118 solver.cpp:357] Iteration 17700 (3.01332 iter/s, 33.186s/100 iters), loss = 0.468714
I1012 13:46:38.745306 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.593022 (* 1 = 0.593022 loss)
I1012 13:46:38.745316 31118 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I1012 13:47:12.079152 31118 solver.cpp:357] Iteration 17800 (2.99975 iter/s, 33.3361s/100 iters), loss = 0.507891
I1012 13:47:12.079286 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.52585 (* 1 = 0.52585 loss)
I1012 13:47:12.079299 31118 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I1012 13:47:45.269224 31118 solver.cpp:357] Iteration 17900 (3.01296 iter/s, 33.19s/100 iters), loss = 0.315134
I1012 13:47:45.269353 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.268467 (* 1 = 0.268467 loss)
I1012 13:47:45.269366 31118 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I1012 13:48:07.259495 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:48:18.161842 31118 solver.cpp:514] Iteration 18000, Testing net (#0)
I1012 13:48:40.574637 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:48:40.600000 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.13596 (* 1 = 1.13596 loss)
I1012 13:48:40.600033 31118 solver.cpp:580]     Test net output #1: prob = 0.6304
I1012 13:48:40.600040 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:48:40.855450 31118 solver.cpp:357] Iteration 18000 (1.79897 iter/s, 55.5873s/100 iters), loss = 0.375524
I1012 13:48:40.855517 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.407655 (* 1 = 0.407655 loss)
I1012 13:48:40.855530 31118 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I1012 13:49:14.065601 31118 solver.cpp:357] Iteration 18100 (3.01115 iter/s, 33.2099s/100 iters), loss = 0.366478
I1012 13:49:14.065819 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.248535 (* 1 = 0.248535 loss)
I1012 13:49:14.065832 31118 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I1012 13:49:47.154889 31118 solver.cpp:357] Iteration 18200 (3.02217 iter/s, 33.0889s/100 iters), loss = 0.446248
I1012 13:49:47.155056 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.343246 (* 1 = 0.343246 loss)
I1012 13:49:47.155071 31118 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I1012 13:50:20.166015 31118 solver.cpp:357] Iteration 18300 (3.02933 iter/s, 33.0106s/100 iters), loss = 0.249372
I1012 13:50:20.166151 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.251635 (* 1 = 0.251635 loss)
I1012 13:50:20.166163 31118 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I1012 13:50:38.994859 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:50:53.424602 31118 solver.cpp:357] Iteration 18400 (3.00679 iter/s, 33.2581s/100 iters), loss = 0.681048
I1012 13:50:53.424717 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.56766 (* 1 = 0.56766 loss)
I1012 13:50:53.424731 31118 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I1012 13:51:26.382899 31118 solver.cpp:514] Iteration 18500, Testing net (#0)
I1012 13:51:48.647784 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:51:48.771694 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.05605 (* 1 = 1.05605 loss)
I1012 13:51:48.771721 31118 solver.cpp:580]     Test net output #1: prob = 0.6515
I1012 13:51:48.771728 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:51:49.086701 31118 solver.cpp:357] Iteration 18500 (1.79654 iter/s, 55.6625s/100 iters), loss = 0.483209
I1012 13:51:49.086788 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.407311 (* 1 = 0.407311 loss)
I1012 13:51:49.086802 31118 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I1012 13:52:22.183593 31118 solver.cpp:357] Iteration 18600 (3.0215 iter/s, 33.0962s/100 iters), loss = 0.423202
I1012 13:52:22.183763 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.34054 (* 1 = 0.34054 loss)
I1012 13:52:22.183775 31118 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I1012 13:52:55.633005 31118 solver.cpp:357] Iteration 18700 (2.98966 iter/s, 33.4486s/100 iters), loss = 0.321342
I1012 13:52:55.633149 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.254614 (* 1 = 0.254614 loss)
I1012 13:52:55.633162 31118 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I1012 13:53:11.246912 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:53:28.775426 31118 solver.cpp:357] Iteration 18800 (3.01717 iter/s, 33.1436s/100 iters), loss = 0.357442
I1012 13:53:28.775595 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.36861 (* 1 = 0.36861 loss)
I1012 13:53:28.775609 31118 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I1012 13:54:01.944628 31118 solver.cpp:357] Iteration 18900 (3.01493 iter/s, 33.1683s/100 iters), loss = 0.458279
I1012 13:54:01.944794 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.536014 (* 1 = 0.536014 loss)
I1012 13:54:01.944804 31118 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I1012 13:54:34.965520 31118 solver.cpp:514] Iteration 19000, Testing net (#0)
I1012 13:54:57.447072 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:54:57.491222 31118 solver.cpp:580]     Test net output #0: Softmax1 = 2.16624 (* 1 = 2.16624 loss)
I1012 13:54:57.491257 31118 solver.cpp:580]     Test net output #1: prob = 0.5117
I1012 13:54:57.491264 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:54:57.756202 31118 solver.cpp:357] Iteration 19000 (1.79175 iter/s, 55.8115s/100 iters), loss = 0.325174
I1012 13:54:57.756274 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.430429 (* 1 = 0.430429 loss)
I1012 13:54:57.756285 31118 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I1012 13:55:31.138285 31118 solver.cpp:357] Iteration 19100 (2.99571 iter/s, 33.3811s/100 iters), loss = 0.457388
I1012 13:55:31.138551 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.461589 (* 1 = 0.461589 loss)
I1012 13:55:31.138566 31118 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I1012 13:55:43.726727 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:56:04.207185 31118 solver.cpp:357] Iteration 19200 (3.02409 iter/s, 33.0678s/100 iters), loss = 0.503363
I1012 13:56:04.207350 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.465521 (* 1 = 0.465521 loss)
I1012 13:56:04.207362 31118 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I1012 13:56:37.426656 31118 solver.cpp:357] Iteration 19300 (3.01038 iter/s, 33.2184s/100 iters), loss = 0.418963
I1012 13:56:37.426803 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.359207 (* 1 = 0.359207 loss)
I1012 13:56:37.426813 31118 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I1012 13:57:10.557873 31118 solver.cpp:357] Iteration 19400 (3.01841 iter/s, 33.13s/100 iters), loss = 0.373509
I1012 13:57:10.558058 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.350684 (* 1 = 0.350684 loss)
I1012 13:57:10.558070 31118 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I1012 13:57:43.604660 31118 solver.cpp:514] Iteration 19500, Testing net (#0)
I1012 13:58:05.899392 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:58:05.914474 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.668382 (* 1 = 0.668382 loss)
I1012 13:58:05.914501 31118 solver.cpp:580]     Test net output #1: prob = 0.773101
I1012 13:58:05.914508 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 13:58:06.181077 31118 solver.cpp:357] Iteration 19500 (1.79783 iter/s, 55.6228s/100 iters), loss = 0.443744
I1012 13:58:06.181140 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.558515 (* 1 = 0.558515 loss)
I1012 13:58:06.181152 31118 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I1012 13:58:15.745553 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 13:58:39.527011 31118 solver.cpp:357] Iteration 19600 (2.99897 iter/s, 33.3447s/100 iters), loss = 0.479936
I1012 13:58:39.527081 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.548926 (* 1 = 0.548926 loss)
I1012 13:58:39.527093 31118 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I1012 13:59:12.823968 31118 solver.cpp:357] Iteration 19700 (3.0032 iter/s, 33.2979s/100 iters), loss = 0.489531
I1012 13:59:12.824133 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.403762 (* 1 = 0.403762 loss)
I1012 13:59:12.824146 31118 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I1012 13:59:46.068346 31118 solver.cpp:357] Iteration 19800 (3.00814 iter/s, 33.2431s/100 iters), loss = 0.374637
I1012 13:59:46.068473 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.424108 (* 1 = 0.424108 loss)
I1012 13:59:46.068486 31118 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I1012 14:00:19.280143 31118 solver.cpp:357] Iteration 19900 (3.0111 iter/s, 33.2105s/100 iters), loss = 0.448993
I1012 14:00:19.280304 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.440348 (* 1 = 0.440348 loss)
I1012 14:00:19.280315 31118 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I1012 14:00:25.590456 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:00:52.115828 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_20000.caffemodel
I1012 14:00:52.127331 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_20000.solverstate
I1012 14:00:52.130268 31118 solver.cpp:514] Iteration 20000, Testing net (#0)
I1012 14:01:14.287374 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:01:14.396015 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.97653 (* 1 = 0.97653 loss)
I1012 14:01:14.396045 31118 solver.cpp:580]     Test net output #1: prob = 0.6887
I1012 14:01:14.396051 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:01:14.712771 31118 solver.cpp:357] Iteration 20000 (1.80402 iter/s, 55.4319s/100 iters), loss = 0.346953
I1012 14:01:14.712824 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.377159 (* 1 = 0.377159 loss)
I1012 14:01:14.712836 31118 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I1012 14:01:48.014793 31118 solver.cpp:357] Iteration 20100 (3.00294 iter/s, 33.3007s/100 iters), loss = 0.404203
I1012 14:01:48.015043 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.323703 (* 1 = 0.323703 loss)
I1012 14:01:48.015058 31118 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I1012 14:02:21.393360 31118 solver.cpp:357] Iteration 20200 (2.99605 iter/s, 33.3772s/100 iters), loss = 0.395786
I1012 14:02:21.393522 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.341217 (* 1 = 0.341217 loss)
I1012 14:02:21.393533 31118 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I1012 14:02:54.495630 31118 solver.cpp:357] Iteration 20300 (3.02088 iter/s, 33.1029s/100 iters), loss = 0.354504
I1012 14:02:54.495775 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.38684 (* 1 = 0.38684 loss)
I1012 14:02:54.495787 31118 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I1012 14:02:57.635064 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:03:27.676779 31118 solver.cpp:357] Iteration 20400 (3.01389 iter/s, 33.1798s/100 iters), loss = 0.414917
I1012 14:03:27.676916 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.397895 (* 1 = 0.397895 loss)
I1012 14:03:27.676928 31118 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I1012 14:04:00.697760 31118 solver.cpp:514] Iteration 20500, Testing net (#0)
I1012 14:04:23.245976 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:04:23.260812 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.955994 (* 1 = 0.955994 loss)
I1012 14:04:23.260840 31118 solver.cpp:580]     Test net output #1: prob = 0.7165
I1012 14:04:23.260846 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:04:23.539019 31118 solver.cpp:357] Iteration 20500 (1.79009 iter/s, 55.8631s/100 iters), loss = 0.230035
I1012 14:04:23.539084 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.179447 (* 1 = 0.179447 loss)
I1012 14:04:23.539096 31118 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I1012 14:04:56.765525 31118 solver.cpp:357] Iteration 20600 (3.00978 iter/s, 33.2251s/100 iters), loss = 0.373819
I1012 14:04:56.765681 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.410669 (* 1 = 0.410669 loss)
I1012 14:04:56.765696 31118 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I1012 14:05:29.917774 31118 solver.cpp:357] Iteration 20700 (3.01652 iter/s, 33.1508s/100 iters), loss = 0.456874
I1012 14:05:29.917901 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.399975 (* 1 = 0.399975 loss)
I1012 14:05:29.917914 31118 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I1012 14:05:30.144455 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:06:03.156903 31118 solver.cpp:357] Iteration 20800 (3.00863 iter/s, 33.2377s/100 iters), loss = 0.419673
I1012 14:06:03.157014 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.44512 (* 1 = 0.44512 loss)
I1012 14:06:03.157027 31118 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I1012 14:06:36.295616 31118 solver.cpp:357] Iteration 20900 (3.01775 iter/s, 33.1372s/100 iters), loss = 0.386408
I1012 14:06:36.295724 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.439232 (* 1 = 0.439232 loss)
I1012 14:06:36.295738 31118 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I1012 14:07:09.211693 31118 solver.cpp:514] Iteration 21000, Testing net (#0)
I1012 14:07:31.582548 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:07:31.662701 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.36323 (* 1 = 1.36323 loss)
I1012 14:07:31.662744 31118 solver.cpp:580]     Test net output #1: prob = 0.6444
I1012 14:07:31.662753 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:07:31.917887 31118 solver.cpp:357] Iteration 21000 (1.7978 iter/s, 55.6234s/100 iters), loss = 0.40257
I1012 14:07:31.917961 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.454 (* 1 = 0.454 loss)
I1012 14:07:31.917974 31118 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I1012 14:08:02.202239 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:08:05.254607 31118 solver.cpp:357] Iteration 21100 (2.99983 iter/s, 33.3353s/100 iters), loss = 0.44686
I1012 14:08:05.254693 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.50016 (* 1 = 0.50016 loss)
I1012 14:08:05.254704 31118 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I1012 14:08:38.409492 31118 solver.cpp:357] Iteration 21200 (3.01628 iter/s, 33.1534s/100 iters), loss = 0.380555
I1012 14:08:38.409626 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.348064 (* 1 = 0.348064 loss)
I1012 14:08:38.409639 31118 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I1012 14:09:11.762183 31118 solver.cpp:357] Iteration 21300 (2.99839 iter/s, 33.3512s/100 iters), loss = 0.410595
I1012 14:09:11.762290 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.320957 (* 1 = 0.320957 loss)
I1012 14:09:11.762302 31118 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I1012 14:09:44.855082 31118 solver.cpp:357] Iteration 21400 (3.02174 iter/s, 33.0935s/100 iters), loss = 0.441374
I1012 14:09:44.855252 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.452176 (* 1 = 0.452176 loss)
I1012 14:09:44.855265 31118 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I1012 14:10:11.957231 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:10:17.714320 31118 solver.cpp:514] Iteration 21500, Testing net (#0)
I1012 14:10:40.238874 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:10:40.361552 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.10705 (* 1 = 1.10705 loss)
I1012 14:10:40.361580 31118 solver.cpp:580]     Test net output #1: prob = 0.639201
I1012 14:10:40.361588 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:10:40.672772 31118 solver.cpp:357] Iteration 21500 (1.79158 iter/s, 55.8166s/100 iters), loss = 0.27439
I1012 14:10:40.672855 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.357929 (* 1 = 0.357929 loss)
I1012 14:10:40.672869 31118 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I1012 14:11:13.750938 31118 solver.cpp:357] Iteration 21600 (3.02328 iter/s, 33.0766s/100 iters), loss = 0.44467
I1012 14:11:13.751091 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.460539 (* 1 = 0.460539 loss)
I1012 14:11:13.751104 31118 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I1012 14:11:47.099210 31118 solver.cpp:357] Iteration 21700 (2.99865 iter/s, 33.3483s/100 iters), loss = 0.332427
I1012 14:11:47.099359 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.321733 (* 1 = 0.321733 loss)
I1012 14:11:47.099370 31118 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I1012 14:12:20.401468 31118 solver.cpp:357] Iteration 21800 (3.00294 iter/s, 33.3006s/100 iters), loss = 0.422873
I1012 14:12:20.401638 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.501528 (* 1 = 0.501528 loss)
I1012 14:12:20.401649 31118 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I1012 14:12:44.263134 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:12:53.577986 31118 solver.cpp:357] Iteration 21900 (3.01432 iter/s, 33.175s/100 iters), loss = 0.587077
I1012 14:12:53.578186 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.37431 (* 1 = 0.37431 loss)
I1012 14:12:53.578241 31118 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I1012 14:13:26.537011 31118 solver.cpp:514] Iteration 22000, Testing net (#0)
I1012 14:13:48.877862 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:13:48.999433 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.31993 (* 1 = 1.31993 loss)
I1012 14:13:48.999480 31118 solver.cpp:580]     Test net output #1: prob = 0.5293
I1012 14:13:48.999487 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:13:49.259300 31118 solver.cpp:357] Iteration 22000 (1.79597 iter/s, 55.6802s/100 iters), loss = 0.405462
I1012 14:13:49.259388 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.501977 (* 1 = 0.501977 loss)
I1012 14:13:49.259402 31118 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I1012 14:14:22.569547 31118 solver.cpp:357] Iteration 22100 (3.00222 iter/s, 33.3087s/100 iters), loss = 0.378428
I1012 14:14:22.569754 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.327941 (* 1 = 0.327941 loss)
I1012 14:14:22.569766 31118 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I1012 14:14:55.731540 31118 solver.cpp:357] Iteration 22200 (3.01553 iter/s, 33.1616s/100 iters), loss = 0.399609
I1012 14:14:55.731678 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.465707 (* 1 = 0.465707 loss)
I1012 14:14:55.731693 31118 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I1012 14:15:16.659070 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:15:28.903940 31118 solver.cpp:357] Iteration 22300 (3.0147 iter/s, 33.1708s/100 iters), loss = 0.397528
I1012 14:15:28.904144 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.492605 (* 1 = 0.492605 loss)
I1012 14:15:28.904155 31118 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I1012 14:16:02.056550 31118 solver.cpp:357] Iteration 22400 (3.0165 iter/s, 33.151s/100 iters), loss = 0.344348
I1012 14:16:02.056691 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.334984 (* 1 = 0.334984 loss)
I1012 14:16:02.056705 31118 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I1012 14:16:34.940673 31118 solver.cpp:514] Iteration 22500, Testing net (#0)
I1012 14:16:57.092321 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:16:57.216356 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.884898 (* 1 = 0.884898 loss)
I1012 14:16:57.216384 31118 solver.cpp:580]     Test net output #1: prob = 0.6929
I1012 14:16:57.216390 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:16:57.526202 31118 solver.cpp:357] Iteration 22500 (1.80282 iter/s, 55.4685s/100 iters), loss = 0.340089
I1012 14:16:57.526358 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.295897 (* 1 = 0.295897 loss)
I1012 14:16:57.526391 31118 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I1012 14:17:30.649085 31118 solver.cpp:357] Iteration 22600 (3.0192 iter/s, 33.1213s/100 iters), loss = 0.462505
I1012 14:17:30.649237 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.509865 (* 1 = 0.509865 loss)
I1012 14:17:30.649250 31118 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I1012 14:17:45.675957 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:18:01.168596 31118 solver.cpp:357] Iteration 22700 (3.27655 iter/s, 30.5199s/100 iters), loss = 0.388829
I1012 14:18:01.168737 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.317311 (* 1 = 0.317311 loss)
I1012 14:18:01.168751 31118 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I1012 14:18:34.330816 31118 solver.cpp:357] Iteration 22800 (3.01562 iter/s, 33.1606s/100 iters), loss = 0.337654
I1012 14:18:34.330935 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.406145 (* 1 = 0.406145 loss)
I1012 14:18:34.330946 31118 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I1012 14:19:07.666226 31118 solver.cpp:357] Iteration 22900 (2.99996 iter/s, 33.3338s/100 iters), loss = 0.351686
I1012 14:19:07.666373 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.400075 (* 1 = 0.400075 loss)
I1012 14:19:07.666384 31118 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I1012 14:19:40.539664 31118 solver.cpp:514] Iteration 23000, Testing net (#0)
I1012 14:20:03.044952 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:20:03.151160 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.83981 (* 1 = 0.83981 loss)
I1012 14:20:03.151202 31118 solver.cpp:580]     Test net output #1: prob = 0.7297
I1012 14:20:03.151209 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:20:03.397418 31118 solver.cpp:357] Iteration 23000 (1.7943 iter/s, 55.7321s/100 iters), loss = 0.493182
I1012 14:20:03.397487 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.498046 (* 1 = 0.498046 loss)
I1012 14:20:03.397500 31118 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I1012 14:20:18.166810 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:20:36.664131 31118 solver.cpp:357] Iteration 23100 (3.00615 iter/s, 33.2651s/100 iters), loss = 0.478988
I1012 14:20:36.664247 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.391982 (* 1 = 0.391982 loss)
I1012 14:20:36.664263 31118 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I1012 14:21:09.779532 31118 solver.cpp:357] Iteration 23200 (3.02007 iter/s, 33.1118s/100 iters), loss = 0.339688
I1012 14:21:09.779738 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.321138 (* 1 = 0.321138 loss)
I1012 14:21:09.779752 31118 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I1012 14:21:42.841389 31118 solver.cpp:357] Iteration 23300 (3.02497 iter/s, 33.0582s/100 iters), loss = 0.417312
I1012 14:21:42.841560 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.47044 (* 1 = 0.47044 loss)
I1012 14:21:42.841573 31118 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I1012 14:22:16.099478 31118 solver.cpp:357] Iteration 23400 (3.00711 iter/s, 33.2546s/100 iters), loss = 0.373343
I1012 14:22:16.099647 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.418417 (* 1 = 0.418417 loss)
I1012 14:22:16.099661 31118 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I1012 14:22:27.663616 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:22:49.070668 31118 solver.cpp:514] Iteration 23500, Testing net (#0)
I1012 14:23:11.564328 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:23:11.600802 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.709737 (* 1 = 0.709737 loss)
I1012 14:23:11.600836 31118 solver.cpp:580]     Test net output #1: prob = 0.758899
I1012 14:23:11.600842 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:23:11.912979 31118 solver.cpp:357] Iteration 23500 (1.79181 iter/s, 55.8093s/100 iters), loss = 0.367756
I1012 14:23:11.913028 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.390726 (* 1 = 0.390726 loss)
I1012 14:23:11.913041 31118 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I1012 14:23:45.016453 31118 solver.cpp:357] Iteration 23600 (3.02113 iter/s, 33.1002s/100 iters), loss = 0.422286
I1012 14:23:45.016572 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.5054 (* 1 = 0.5054 loss)
I1012 14:23:45.016583 31118 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I1012 14:24:18.157735 31118 solver.cpp:357] Iteration 23700 (3.01767 iter/s, 33.1381s/100 iters), loss = 0.420232
I1012 14:24:18.157891 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.515382 (* 1 = 0.515382 loss)
I1012 14:24:18.157903 31118 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I1012 14:24:51.420835 31118 solver.cpp:357] Iteration 23800 (3.00643 iter/s, 33.2621s/100 iters), loss = 0.360843
I1012 14:24:51.420979 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.285076 (* 1 = 0.285076 loss)
I1012 14:24:51.420989 31118 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I1012 14:24:59.779719 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:25:24.626514 31118 solver.cpp:357] Iteration 23900 (3.0118 iter/s, 33.2027s/100 iters), loss = 0.467492
I1012 14:25:24.626654 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.343964 (* 1 = 0.343964 loss)
I1012 14:25:24.626667 31118 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I1012 14:25:57.631422 31118 solver.cpp:514] Iteration 24000, Testing net (#0)
I1012 14:26:19.931954 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:26:19.982403 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.866683 (* 1 = 0.866683 loss)
I1012 14:26:19.982434 31118 solver.cpp:580]     Test net output #1: prob = 0.7228
I1012 14:26:19.982440 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:26:20.233674 31118 solver.cpp:357] Iteration 24000 (1.79844 iter/s, 55.6038s/100 iters), loss = 0.422742
I1012 14:26:20.233747 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.352157 (* 1 = 0.352157 loss)
I1012 14:26:20.233760 31118 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I1012 14:26:53.550735 31118 solver.cpp:357] Iteration 24100 (3.00172 iter/s, 33.3143s/100 iters), loss = 0.530968
I1012 14:26:53.550941 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.49587 (* 1 = 0.49587 loss)
I1012 14:26:53.550953 31118 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I1012 14:27:26.832329 31118 solver.cpp:357] Iteration 24200 (3.00473 iter/s, 33.2808s/100 iters), loss = 0.39199
I1012 14:27:26.832495 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.300715 (* 1 = 0.300715 loss)
I1012 14:27:26.832506 31118 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I1012 14:27:32.186271 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:28:00.120862 31118 solver.cpp:357] Iteration 24300 (3.00428 iter/s, 33.2858s/100 iters), loss = 0.546659
I1012 14:28:00.121037 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.468039 (* 1 = 0.468039 loss)
I1012 14:28:00.121049 31118 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I1012 14:28:33.495501 31118 solver.cpp:357] Iteration 24400 (2.99653 iter/s, 33.372s/100 iters), loss = 0.376816
I1012 14:28:33.495651 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.496556 (* 1 = 0.496556 loss)
I1012 14:28:33.495662 31118 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I1012 14:29:06.345573 31118 solver.cpp:514] Iteration 24500, Testing net (#0)
I1012 14:29:28.819427 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:29:28.903034 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.29711 (* 1 = 1.29711 loss)
I1012 14:29:28.903074 31118 solver.cpp:580]     Test net output #1: prob = 0.6604
I1012 14:29:28.903081 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:29:29.158651 31118 solver.cpp:357] Iteration 24500 (1.79655 iter/s, 55.6624s/100 iters), loss = 0.394083
I1012 14:29:29.158725 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.426337 (* 1 = 0.426337 loss)
I1012 14:29:29.158736 31118 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I1012 14:30:02.490362 31118 solver.cpp:357] Iteration 24600 (3.00038 iter/s, 33.3292s/100 iters), loss = 0.308323
I1012 14:30:02.490705 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.308537 (* 1 = 0.308537 loss)
I1012 14:30:02.490770 31118 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I1012 14:30:04.565671 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:30:35.507844 31118 solver.cpp:357] Iteration 24700 (3.02893 iter/s, 33.015s/100 iters), loss = 0.258352
I1012 14:30:35.508002 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.241594 (* 1 = 0.241594 loss)
I1012 14:30:35.508014 31118 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I1012 14:31:08.616215 31118 solver.cpp:357] Iteration 24800 (3.02061 iter/s, 33.1059s/100 iters), loss = 0.279652
I1012 14:31:08.616385 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.223074 (* 1 = 0.223074 loss)
I1012 14:31:08.616397 31118 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I1012 14:31:41.652896 31118 solver.cpp:357] Iteration 24900 (3.02716 iter/s, 33.0343s/100 iters), loss = 0.585503
I1012 14:31:41.653002 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.70288 (* 1 = 0.70288 loss)
I1012 14:31:41.653013 31118 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I1012 14:32:14.083282 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:32:14.705422 31118 solver.cpp:514] Iteration 25000, Testing net (#0)
I1012 14:32:36.884377 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:32:36.942581 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.871589 (* 1 = 0.871589 loss)
I1012 14:32:36.942612 31118 solver.cpp:580]     Test net output #1: prob = 0.704
I1012 14:32:36.942618 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:32:37.258215 31118 solver.cpp:357] Iteration 25000 (1.79847 iter/s, 55.6029s/100 iters), loss = 0.439549
I1012 14:32:37.258265 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.488862 (* 1 = 0.488862 loss)
I1012 14:32:37.258278 31118 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I1012 14:33:10.503073 31118 solver.cpp:357] Iteration 25100 (3.00819 iter/s, 33.2425s/100 iters), loss = 0.425472
I1012 14:33:10.503245 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.379169 (* 1 = 0.379169 loss)
I1012 14:33:10.503257 31118 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I1012 14:33:43.746928 31118 solver.cpp:357] Iteration 25200 (3.0081 iter/s, 33.2436s/100 iters), loss = 0.51435
I1012 14:33:43.747054 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.48882 (* 1 = 0.48882 loss)
I1012 14:33:43.747066 31118 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I1012 14:34:16.963781 31118 solver.cpp:357] Iteration 25300 (3.01073 iter/s, 33.2146s/100 iters), loss = 0.439391
I1012 14:34:16.963913 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.559309 (* 1 = 0.559309 loss)
I1012 14:34:16.963927 31118 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I1012 14:34:46.209573 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:34:50.048058 31118 solver.cpp:357] Iteration 25400 (3.02279 iter/s, 33.082s/100 iters), loss = 0.448999
I1012 14:34:50.048184 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.20331 (* 1 = 0.20331 loss)
I1012 14:34:50.048197 31118 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I1012 14:35:23.110677 31118 solver.cpp:514] Iteration 25500, Testing net (#0)
I1012 14:35:45.296255 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:35:45.311699 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.692018 (* 1 = 0.692018 loss)
I1012 14:35:45.311730 31118 solver.cpp:580]     Test net output #1: prob = 0.764101
I1012 14:35:45.311736 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:35:45.608454 31118 solver.cpp:357] Iteration 25500 (1.79991 iter/s, 55.5582s/100 iters), loss = 0.424805
I1012 14:35:45.608515 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.30913 (* 1 = 0.30913 loss)
I1012 14:35:45.608526 31118 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I1012 14:36:18.934550 31118 solver.cpp:357] Iteration 25600 (3.00085 iter/s, 33.3239s/100 iters), loss = 0.361692
I1012 14:36:18.934725 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.363767 (* 1 = 0.363767 loss)
I1012 14:36:18.934739 31118 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I1012 14:36:52.161689 31118 solver.cpp:357] Iteration 25700 (3.00978 iter/s, 33.225s/100 iters), loss = 0.321837
I1012 14:36:52.161856 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.281251 (* 1 = 0.281251 loss)
I1012 14:36:52.161869 31118 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I1012 14:37:18.295567 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:37:25.403318 31118 solver.cpp:357] Iteration 25800 (3.00847 iter/s, 33.2395s/100 iters), loss = 0.428254
I1012 14:37:25.403419 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.369774 (* 1 = 0.369774 loss)
I1012 14:37:25.403430 31118 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I1012 14:37:58.717314 31118 solver.cpp:357] Iteration 25900 (3.00193 iter/s, 33.3119s/100 iters), loss = 0.491733
I1012 14:37:58.717409 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.581637 (* 1 = 0.581637 loss)
I1012 14:37:58.717420 31118 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I1012 14:38:31.546540 31118 solver.cpp:514] Iteration 26000, Testing net (#0)
I1012 14:38:53.899730 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:38:54.022739 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.674691 (* 1 = 0.674691 loss)
I1012 14:38:54.022768 31118 solver.cpp:580]     Test net output #1: prob = 0.7713
I1012 14:38:54.022774 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:38:54.332859 31118 solver.cpp:357] Iteration 26000 (1.79806 iter/s, 55.6155s/100 iters), loss = 0.375045
I1012 14:38:54.332947 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.352609 (* 1 = 0.352609 loss)
I1012 14:38:54.332959 31118 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I1012 14:39:27.432677 31118 solver.cpp:357] Iteration 26100 (3.02136 iter/s, 33.0977s/100 iters), loss = 0.316618
I1012 14:39:27.432947 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.280186 (* 1 = 0.280186 loss)
I1012 14:39:27.432961 31118 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I1012 14:39:50.496632 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:40:00.810920 31118 solver.cpp:357] Iteration 26200 (2.99615 iter/s, 33.3762s/100 iters), loss = 0.375969
I1012 14:40:00.811095 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.333549 (* 1 = 0.333549 loss)
I1012 14:40:00.811110 31118 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I1012 14:40:34.165354 31118 solver.cpp:357] Iteration 26300 (2.99829 iter/s, 33.3524s/100 iters), loss = 0.337772
I1012 14:40:34.165463 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.387717 (* 1 = 0.387717 loss)
I1012 14:40:34.165474 31118 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I1012 14:41:07.372844 31118 solver.cpp:357] Iteration 26400 (3.01155 iter/s, 33.2054s/100 iters), loss = 0.274603
I1012 14:41:07.373179 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.262252 (* 1 = 0.262252 loss)
I1012 14:41:07.373193 31118 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I1012 14:41:40.303349 31118 solver.cpp:514] Iteration 26500, Testing net (#0)
I1012 14:42:02.740131 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:42:02.863138 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.85899 (* 1 = 0.85899 loss)
I1012 14:42:02.863167 31118 solver.cpp:580]     Test net output #1: prob = 0.7324
I1012 14:42:02.863173 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:42:03.174476 31118 solver.cpp:357] Iteration 26500 (1.79212 iter/s, 55.7997s/100 iters), loss = 0.332627
I1012 14:42:03.174532 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.311985 (* 1 = 0.311985 loss)
I1012 14:42:03.174542 31118 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I1012 14:42:23.054600 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:42:36.406561 31118 solver.cpp:357] Iteration 26600 (3.00933 iter/s, 33.23s/100 iters), loss = 0.33605
I1012 14:42:36.406637 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.323432 (* 1 = 0.323432 loss)
I1012 14:42:36.406649 31118 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I1012 14:43:09.701844 31118 solver.cpp:357] Iteration 26700 (3.00361 iter/s, 33.2933s/100 iters), loss = 0.438148
I1012 14:43:09.702023 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.373515 (* 1 = 0.373515 loss)
I1012 14:43:09.702036 31118 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I1012 14:43:42.915791 31118 solver.cpp:357] Iteration 26800 (3.01096 iter/s, 33.2119s/100 iters), loss = 0.467698
I1012 14:43:42.915962 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.420385 (* 1 = 0.420385 loss)
I1012 14:43:42.915976 31118 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I1012 14:44:16.165685 31118 solver.cpp:357] Iteration 26900 (3.00771 iter/s, 33.2479s/100 iters), loss = 0.354933
I1012 14:44:16.165846 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.315804 (* 1 = 0.315804 loss)
I1012 14:44:16.165860 31118 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I1012 14:44:33.033716 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:44:49.099570 31118 solver.cpp:514] Iteration 27000, Testing net (#0)
I1012 14:45:11.641894 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:45:11.663933 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.739035 (* 1 = 0.739035 loss)
I1012 14:45:11.663964 31118 solver.cpp:580]     Test net output #1: prob = 0.7542
I1012 14:45:11.663970 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:45:11.925467 31118 solver.cpp:357] Iteration 27000 (1.79347 iter/s, 55.758s/100 iters), loss = 0.283729
I1012 14:45:11.925534 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.210132 (* 1 = 0.210132 loss)
I1012 14:45:11.925552 31118 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I1012 14:45:45.122123 31118 solver.cpp:357] Iteration 27100 (3.01253 iter/s, 33.1947s/100 iters), loss = 0.424427
I1012 14:45:45.122403 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.439579 (* 1 = 0.439579 loss)
I1012 14:45:45.122431 31118 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I1012 14:46:18.322576 31118 solver.cpp:357] Iteration 27200 (3.01219 iter/s, 33.1985s/100 iters), loss = 0.44059
I1012 14:46:18.322722 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.533328 (* 1 = 0.533328 loss)
I1012 14:46:18.322736 31118 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I1012 14:46:51.535743 31118 solver.cpp:357] Iteration 27300 (3.01103 iter/s, 33.2112s/100 iters), loss = 0.373636
I1012 14:46:51.535905 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.335915 (* 1 = 0.335915 loss)
I1012 14:46:51.535917 31118 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I1012 14:47:05.281939 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:47:24.798970 31118 solver.cpp:357] Iteration 27400 (3.0065 iter/s, 33.2613s/100 iters), loss = 0.317362
I1012 14:47:24.799155 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.266249 (* 1 = 0.266249 loss)
I1012 14:47:24.799168 31118 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I1012 14:47:57.742569 31118 solver.cpp:514] Iteration 27500, Testing net (#0)
I1012 14:48:20.267205 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:48:20.392267 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.877879 (* 1 = 0.877879 loss)
I1012 14:48:20.392295 31118 solver.cpp:580]     Test net output #1: prob = 0.7191
I1012 14:48:20.392302 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:48:20.652739 31118 solver.cpp:357] Iteration 27500 (1.79045 iter/s, 55.852s/100 iters), loss = 0.460515
I1012 14:48:20.652786 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.406672 (* 1 = 0.406672 loss)
I1012 14:48:20.652799 31118 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I1012 14:48:53.763820 31118 solver.cpp:357] Iteration 27600 (3.02012 iter/s, 33.1113s/100 iters), loss = 0.359411
I1012 14:48:53.763929 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.284435 (* 1 = 0.284435 loss)
I1012 14:48:53.763942 31118 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I1012 14:49:26.823544 31118 solver.cpp:357] Iteration 27700 (3.02481 iter/s, 33.0599s/100 iters), loss = 0.530319
I1012 14:49:26.823710 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.5781 (* 1 = 0.5781 loss)
I1012 14:49:26.823724 31118 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I1012 14:49:37.301095 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:49:59.969240 31118 solver.cpp:357] Iteration 27800 (3.01716 iter/s, 33.1438s/100 iters), loss = 0.436568
I1012 14:49:59.969357 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.462802 (* 1 = 0.462802 loss)
I1012 14:49:59.969369 31118 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I1012 14:50:33.197103 31118 solver.cpp:357] Iteration 27900 (3.0097 iter/s, 33.2259s/100 iters), loss = 0.472747
I1012 14:50:33.197238 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.417421 (* 1 = 0.417421 loss)
I1012 14:50:33.197252 31118 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I1012 14:51:06.011634 31118 solver.cpp:514] Iteration 28000, Testing net (#0)
I1012 14:51:28.597420 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:51:28.646324 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.829565 (* 1 = 0.829565 loss)
I1012 14:51:28.646358 31118 solver.cpp:580]     Test net output #1: prob = 0.7178
I1012 14:51:28.646364 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:51:28.899592 31118 solver.cpp:357] Iteration 28000 (1.79531 iter/s, 55.7008s/100 iters), loss = 0.454455
I1012 14:51:28.899646 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.487315 (* 1 = 0.487315 loss)
I1012 14:51:28.899658 31118 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I1012 14:52:02.288223 31118 solver.cpp:357] Iteration 28100 (2.9952 iter/s, 33.3867s/100 iters), loss = 0.367486
I1012 14:52:02.288432 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.451561 (* 1 = 0.451561 loss)
I1012 14:52:02.288445 31118 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I1012 14:52:09.767596 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:52:35.737922 31118 solver.cpp:357] Iteration 28200 (2.98973 iter/s, 33.4478s/100 iters), loss = 0.43334
I1012 14:52:35.738044 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.283962 (* 1 = 0.283962 loss)
I1012 14:52:35.738055 31118 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I1012 14:53:08.842671 31118 solver.cpp:357] Iteration 28300 (3.0207 iter/s, 33.1049s/100 iters), loss = 0.375564
I1012 14:53:08.842804 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.433332 (* 1 = 0.433332 loss)
I1012 14:53:08.842818 31118 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I1012 14:53:42.190682 31118 solver.cpp:357] Iteration 28400 (2.99885 iter/s, 33.3461s/100 iters), loss = 0.253578
I1012 14:53:42.190861 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.249854 (* 1 = 0.249854 loss)
I1012 14:53:42.190874 31118 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I1012 14:54:14.906253 31118 solver.cpp:514] Iteration 28500, Testing net (#0)
I1012 14:54:37.029304 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:54:37.045892 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.964923 (* 1 = 0.964923 loss)
I1012 14:54:37.045919 31118 solver.cpp:580]     Test net output #1: prob = 0.6929
I1012 14:54:37.045927 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:54:37.324059 31118 solver.cpp:357] Iteration 28500 (1.81384 iter/s, 55.1317s/100 iters), loss = 0.439157
I1012 14:54:37.324124 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.45109 (* 1 = 0.45109 loss)
I1012 14:54:37.324136 31118 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I1012 14:54:41.730973 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:55:10.531270 31118 solver.cpp:357] Iteration 28600 (3.01135 iter/s, 33.2077s/100 iters), loss = 0.402163
I1012 14:55:10.531411 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.501346 (* 1 = 0.501346 loss)
I1012 14:55:10.531422 31118 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I1012 14:55:43.570747 31118 solver.cpp:357] Iteration 28700 (3.02641 iter/s, 33.0425s/100 iters), loss = 0.329719
I1012 14:55:43.570982 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.359249 (* 1 = 0.359249 loss)
I1012 14:55:43.571015 31118 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I1012 14:56:16.768221 31118 solver.cpp:357] Iteration 28800 (3.01217 iter/s, 33.1986s/100 iters), loss = 0.326956
I1012 14:56:16.768350 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.394442 (* 1 = 0.394442 loss)
I1012 14:56:16.768362 31118 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I1012 14:56:49.993659 31118 solver.cpp:357] Iteration 28900 (3.00965 iter/s, 33.2265s/100 iters), loss = 0.400759
I1012 14:56:49.993809 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.365011 (* 1 = 0.365011 loss)
I1012 14:56:49.993824 31118 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I1012 14:56:51.147730 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:57:22.898066 31118 solver.cpp:514] Iteration 29000, Testing net (#0)
I1012 14:57:45.438684 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:57:45.569746 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.13774 (* 1 = 1.13774 loss)
I1012 14:57:45.569792 31118 solver.cpp:580]     Test net output #1: prob = 0.6366
I1012 14:57:45.569799 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 14:57:45.811800 31118 solver.cpp:357] Iteration 29000 (1.79144 iter/s, 55.821s/100 iters), loss = 0.338492
I1012 14:57:45.811872 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.232416 (* 1 = 0.232416 loss)
I1012 14:57:45.811884 31118 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I1012 14:58:19.060195 31118 solver.cpp:357] Iteration 29100 (3.00761 iter/s, 33.249s/100 iters), loss = 0.431773
I1012 14:58:19.060472 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.517261 (* 1 = 0.517261 loss)
I1012 14:58:19.060487 31118 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I1012 14:58:52.310839 31118 solver.cpp:357] Iteration 29200 (3.00742 iter/s, 33.2511s/100 iters), loss = 0.42442
I1012 14:58:52.311015 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.360825 (* 1 = 0.360825 loss)
I1012 14:58:52.311028 31118 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I1012 14:59:23.698079 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 14:59:25.643803 31118 solver.cpp:357] Iteration 29300 (3 iter/s, 33.3333s/100 iters), loss = 0.354838
I1012 14:59:25.643872 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.308247 (* 1 = 0.308247 loss)
I1012 14:59:25.643883 31118 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I1012 14:59:58.911219 31118 solver.cpp:357] Iteration 29400 (3.00573 iter/s, 33.2698s/100 iters), loss = 0.407571
I1012 14:59:58.911382 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.412141 (* 1 = 0.412141 loss)
I1012 14:59:58.911393 31118 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I1012 15:00:31.904870 31118 solver.cpp:514] Iteration 29500, Testing net (#0)
I1012 15:00:54.049007 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:00:54.172127 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.21655 (* 1 = 1.21655 loss)
I1012 15:00:54.172154 31118 solver.cpp:580]     Test net output #1: prob = 0.6646
I1012 15:00:54.172163 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:00:54.456303 31118 solver.cpp:357] Iteration 29500 (1.80028 iter/s, 55.5468s/100 iters), loss = 0.407845
I1012 15:00:54.456352 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.333938 (* 1 = 0.333938 loss)
I1012 15:00:54.456363 31118 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I1012 15:01:27.666041 31118 solver.cpp:357] Iteration 29600 (3.01098 iter/s, 33.2118s/100 iters), loss = 0.455649
I1012 15:01:27.666409 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.377032 (* 1 = 0.377032 loss)
I1012 15:01:27.666420 31118 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I1012 15:01:55.922911 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:02:00.945504 31118 solver.cpp:357] Iteration 29700 (3.00486 iter/s, 33.2794s/100 iters), loss = 0.30451
I1012 15:02:00.945736 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.284824 (* 1 = 0.284824 loss)
I1012 15:02:00.945751 31118 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I1012 15:02:34.067801 31118 solver.cpp:357] Iteration 29800 (3.01913 iter/s, 33.1221s/100 iters), loss = 0.357276
I1012 15:02:34.067981 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.44312 (* 1 = 0.44312 loss)
I1012 15:02:34.067994 31118 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I1012 15:03:07.281440 31118 solver.cpp:357] Iteration 29900 (3.01083 iter/s, 33.2134s/100 iters), loss = 0.384712
I1012 15:03:07.281560 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.323691 (* 1 = 0.323691 loss)
I1012 15:03:07.281572 31118 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I1012 15:03:40.182489 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_30000.caffemodel
I1012 15:03:40.194299 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_30000.solverstate
I1012 15:03:40.197196 31118 solver.cpp:514] Iteration 30000, Testing net (#0)
I1012 15:04:02.402046 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:04:02.524940 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.743589 (* 1 = 0.743589 loss)
I1012 15:04:02.524966 31118 solver.cpp:580]     Test net output #1: prob = 0.7456
I1012 15:04:02.524972 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:04:02.839018 31118 solver.cpp:357] Iteration 30000 (1.7999 iter/s, 55.5585s/100 iters), loss = 0.366606
I1012 15:04:02.839072 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.445291 (* 1 = 0.445291 loss)
I1012 15:04:02.839085 31118 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I1012 15:04:27.748198 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:04:35.782202 31118 solver.cpp:357] Iteration 30100 (3.03557 iter/s, 32.9428s/100 iters), loss = 0.427068
I1012 15:04:35.782282 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.314091 (* 1 = 0.314091 loss)
I1012 15:04:35.782294 31118 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I1012 15:05:09.002799 31118 solver.cpp:357] Iteration 30200 (3.01022 iter/s, 33.2202s/100 iters), loss = 0.504475
I1012 15:05:09.002993 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.55076 (* 1 = 0.55076 loss)
I1012 15:05:09.003006 31118 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I1012 15:05:42.247611 31118 solver.cpp:357] Iteration 30300 (3.00803 iter/s, 33.2443s/100 iters), loss = 0.464895
I1012 15:05:42.247728 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.480586 (* 1 = 0.480586 loss)
I1012 15:05:42.247740 31118 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I1012 15:06:15.457306 31118 solver.cpp:357] Iteration 30400 (3.01122 iter/s, 33.2091s/100 iters), loss = 0.271851
I1012 15:06:15.457418 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.281554 (* 1 = 0.281554 loss)
I1012 15:06:15.457429 31118 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I1012 15:06:37.381605 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:06:48.304179 31118 solver.cpp:514] Iteration 30500, Testing net (#0)
I1012 15:07:10.813315 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:07:10.870779 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.23308 (* 1 = 1.23308 loss)
I1012 15:07:10.870811 31118 solver.cpp:580]     Test net output #1: prob = 0.6754
I1012 15:07:10.870818 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:07:11.138145 31118 solver.cpp:357] Iteration 30500 (1.79587 iter/s, 55.6834s/100 iters), loss = 0.278523
I1012 15:07:11.138196 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.344759 (* 1 = 0.344759 loss)
I1012 15:07:11.138208 31118 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I1012 15:07:44.361059 31118 solver.cpp:357] Iteration 30600 (3.01003 iter/s, 33.2222s/100 iters), loss = 0.37532
I1012 15:07:44.361148 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.247828 (* 1 = 0.247828 loss)
I1012 15:07:44.361158 31118 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I1012 15:08:17.388643 31118 solver.cpp:357] Iteration 30700 (3.02765 iter/s, 33.0289s/100 iters), loss = 0.451385
I1012 15:08:17.388834 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.382625 (* 1 = 0.382625 loss)
I1012 15:08:17.388864 31118 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I1012 15:08:50.634208 31118 solver.cpp:357] Iteration 30800 (3.00799 iter/s, 33.2448s/100 iters), loss = 0.286331
I1012 15:08:50.634335 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.292469 (* 1 = 0.292469 loss)
I1012 15:08:50.634349 31118 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I1012 15:09:09.594740 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:09:24.006660 31118 solver.cpp:357] Iteration 30900 (2.99655 iter/s, 33.3717s/100 iters), loss = 0.533474
I1012 15:09:24.006793 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.492626 (* 1 = 0.492626 loss)
I1012 15:09:24.006806 31118 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I1012 15:09:56.880311 31118 solver.cpp:514] Iteration 31000, Testing net (#0)
I1012 15:10:19.382148 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:10:19.504799 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.01356 (* 1 = 1.01356 loss)
I1012 15:10:19.504827 31118 solver.cpp:580]     Test net output #1: prob = 0.659801
I1012 15:10:19.504833 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:10:19.819099 31118 solver.cpp:357] Iteration 31000 (1.79171 iter/s, 55.8125s/100 iters), loss = 0.403352
I1012 15:10:19.819150 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.331252 (* 1 = 0.331252 loss)
I1012 15:10:19.819164 31118 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I1012 15:10:52.964728 31118 solver.cpp:357] Iteration 31100 (3.01707 iter/s, 33.1447s/100 iters), loss = 0.443261
I1012 15:10:52.964993 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.366587 (* 1 = 0.366587 loss)
I1012 15:10:52.965006 31118 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I1012 15:11:26.053452 31118 solver.cpp:357] Iteration 31200 (3.02226 iter/s, 33.0878s/100 iters), loss = 0.280007
I1012 15:11:26.053622 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.280346 (* 1 = 0.280346 loss)
I1012 15:11:26.053633 31118 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I1012 15:11:41.748647 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:11:59.398226 31118 solver.cpp:357] Iteration 31300 (2.99906 iter/s, 33.3438s/100 iters), loss = 0.265571
I1012 15:11:59.398352 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.293819 (* 1 = 0.293819 loss)
I1012 15:11:59.398366 31118 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I1012 15:12:32.496008 31118 solver.cpp:357] Iteration 31400 (3.02144 iter/s, 33.0968s/100 iters), loss = 0.434407
I1012 15:12:32.496140 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.480454 (* 1 = 0.480454 loss)
I1012 15:12:32.496153 31118 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I1012 15:13:05.539129 31118 solver.cpp:514] Iteration 31500, Testing net (#0)
I1012 15:13:27.713922 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:13:27.778556 31118 solver.cpp:580]     Test net output #0: Softmax1 = 1.71604 (* 1 = 1.71604 loss)
I1012 15:13:27.778586 31118 solver.cpp:580]     Test net output #1: prob = 0.5516
I1012 15:13:27.778594 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:13:28.090193 31118 solver.cpp:357] Iteration 31500 (1.79875 iter/s, 55.594s/100 iters), loss = 0.349245
I1012 15:13:28.090245 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.430907 (* 1 = 0.430907 loss)
I1012 15:13:28.090261 31118 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I1012 15:14:01.273699 31118 solver.cpp:357] Iteration 31600 (3.01364 iter/s, 33.1825s/100 iters), loss = 0.351789
I1012 15:14:01.273883 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.347249 (* 1 = 0.347249 loss)
I1012 15:14:01.273900 31118 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I1012 15:14:13.993216 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:14:34.474104 31118 solver.cpp:357] Iteration 31700 (3.01211 iter/s, 33.1994s/100 iters), loss = 0.464401
I1012 15:14:34.474267 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.511035 (* 1 = 0.511035 loss)
I1012 15:14:34.474280 31118 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I1012 15:15:07.679580 31118 solver.cpp:357] Iteration 31800 (3.01165 iter/s, 33.2044s/100 iters), loss = 0.384228
I1012 15:15:07.679744 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.371675 (* 1 = 0.371675 loss)
I1012 15:15:07.679761 31118 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I1012 15:15:40.713593 31118 solver.cpp:357] Iteration 31900 (3.02728 iter/s, 33.0329s/100 iters), loss = 0.399839
I1012 15:15:40.713773 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.37079 (* 1 = 0.37079 loss)
I1012 15:15:40.713788 31118 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I1012 15:16:13.504616 31118 solver.cpp:514] Iteration 32000, Testing net (#0)
I1012 15:16:35.622819 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:16:35.652725 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.81075 (* 1 = 0.81075 loss)
I1012 15:16:35.652758 31118 solver.cpp:580]     Test net output #1: prob = 0.727
I1012 15:16:35.652765 31118 solver.cpp:593]     Max_acc: 0.775501  with iter: 13500
I1012 15:16:35.968572 31118 solver.cpp:357] Iteration 32000 (1.8098 iter/s, 55.2546s/100 iters), loss = 0.382174
I1012 15:16:35.968623 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.492973 (* 1 = 0.492973 loss)
I1012 15:16:35.968633 31118 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I1012 15:16:35.968641 31118 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I1012 15:16:45.502434 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:17:09.259330 31118 solver.cpp:357] Iteration 32100 (3.00394 iter/s, 33.2896s/100 iters), loss = 0.316963
I1012 15:17:09.259416 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.363049 (* 1 = 0.363049 loss)
I1012 15:17:09.259429 31118 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I1012 15:17:42.548383 31118 solver.cpp:357] Iteration 32200 (3.00409 iter/s, 33.2879s/100 iters), loss = 0.360254
I1012 15:17:42.548528 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.306965 (* 1 = 0.306965 loss)
I1012 15:17:42.548539 31118 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I1012 15:18:15.719511 31118 solver.cpp:357] Iteration 32300 (3.01479 iter/s, 33.1699s/100 iters), loss = 0.219482
I1012 15:18:15.719686 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.15897 (* 1 = 0.15897 loss)
I1012 15:18:15.719698 31118 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I1012 15:18:49.029001 31118 solver.cpp:357] Iteration 32400 (3.00225 iter/s, 33.3083s/100 iters), loss = 0.204222
I1012 15:18:49.029168 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.177626 (* 1 = 0.177626 loss)
I1012 15:18:49.029181 31118 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I1012 15:18:55.373337 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:19:21.969096 31118 solver.cpp:514] Iteration 32500, Testing net (#0)
I1012 15:19:44.216538 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:19:44.338651 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.347516 (* 1 = 0.347516 loss)
I1012 15:19:44.338677 31118 solver.cpp:580]     Test net output #1: prob = 0.883702
I1012 15:19:44.338691 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_32500.caffemodel
I1012 15:19:44.349591 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_32500.solverstate
I1012 15:19:44.352002 31118 solver.cpp:593]     Max_acc: 0.883702  with iter: 32500
I1012 15:19:44.665601 31118 solver.cpp:357] Iteration 32500 (1.79739 iter/s, 55.6361s/100 iters), loss = 0.183251
I1012 15:19:44.665680 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.191254 (* 1 = 0.191254 loss)
I1012 15:19:44.665693 31118 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I1012 15:20:17.667358 31118 solver.cpp:357] Iteration 32600 (3.03025 iter/s, 33.0006s/100 iters), loss = 0.239079
I1012 15:20:17.667508 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.209931 (* 1 = 0.209931 loss)
I1012 15:20:17.667522 31118 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I1012 15:20:50.967931 31118 solver.cpp:357] Iteration 32700 (3.00288 iter/s, 33.3014s/100 iters), loss = 0.196012
I1012 15:20:50.968031 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.25685 (* 1 = 0.25685 loss)
I1012 15:20:50.968044 31118 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I1012 15:21:24.222065 31118 solver.cpp:357] Iteration 32800 (3.00707 iter/s, 33.255s/100 iters), loss = 0.19416
I1012 15:21:24.222187 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.161507 (* 1 = 0.161507 loss)
I1012 15:21:24.222199 31118 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I1012 15:21:27.494172 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:21:57.586324 31118 solver.cpp:357] Iteration 32900 (2.99733 iter/s, 33.3631s/100 iters), loss = 0.216493
I1012 15:21:57.586591 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.211886 (* 1 = 0.211886 loss)
I1012 15:21:57.586606 31118 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I1012 15:22:30.712714 31118 solver.cpp:514] Iteration 33000, Testing net (#0)
I1012 15:22:52.932734 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:22:52.965886 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.364243 (* 1 = 0.364243 loss)
I1012 15:22:52.965915 31118 solver.cpp:580]     Test net output #1: prob = 0.876602
I1012 15:22:52.965921 31118 solver.cpp:593]     Max_acc: 0.883702  with iter: 32500
I1012 15:22:53.278592 31118 solver.cpp:357] Iteration 33000 (1.7956 iter/s, 55.6917s/100 iters), loss = 0.152453
I1012 15:22:53.278640 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.116181 (* 1 = 0.116181 loss)
I1012 15:22:53.278652 31118 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I1012 15:23:26.350347 31118 solver.cpp:357] Iteration 33100 (3.02384 iter/s, 33.0705s/100 iters), loss = 0.173016
I1012 15:23:26.350445 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.157714 (* 1 = 0.157714 loss)
I1012 15:23:26.350457 31118 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I1012 15:23:59.536898 31118 solver.cpp:357] Iteration 33200 (3.01338 iter/s, 33.1853s/100 iters), loss = 0.204575
I1012 15:23:59.537003 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.141996 (* 1 = 0.141996 loss)
I1012 15:23:59.537014 31118 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I1012 15:23:59.653547 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:24:32.655922 31118 solver.cpp:357] Iteration 33300 (3.01933 iter/s, 33.1199s/100 iters), loss = 0.242044
I1012 15:24:32.656060 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.182412 (* 1 = 0.182412 loss)
I1012 15:24:32.656074 31118 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I1012 15:25:05.901479 31118 solver.cpp:357] Iteration 33400 (3.00803 iter/s, 33.2443s/100 iters), loss = 0.154577
I1012 15:25:05.901628 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.17181 (* 1 = 0.17181 loss)
I1012 15:25:05.901641 31118 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I1012 15:25:39.027184 31118 solver.cpp:514] Iteration 33500, Testing net (#0)
I1012 15:26:01.473846 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:26:01.597343 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.350967 (* 1 = 0.350967 loss)
I1012 15:26:01.597371 31118 solver.cpp:580]     Test net output #1: prob = 0.883002
I1012 15:26:01.597378 31118 solver.cpp:593]     Max_acc: 0.883702  with iter: 32500
I1012 15:26:01.908247 31118 solver.cpp:357] Iteration 33500 (1.78552 iter/s, 56.0062s/100 iters), loss = 0.195713
I1012 15:26:01.908298 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.299548 (* 1 = 0.299548 loss)
I1012 15:26:01.908308 31118 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I1012 15:26:32.029927 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:26:35.007211 31118 solver.cpp:357] Iteration 33600 (3.02129 iter/s, 33.0984s/100 iters), loss = 0.213091
I1012 15:26:35.007290 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.239699 (* 1 = 0.239699 loss)
I1012 15:26:35.007303 31118 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I1012 15:27:08.342125 31118 solver.cpp:357] Iteration 33700 (2.99997 iter/s, 33.3337s/100 iters), loss = 0.192812
I1012 15:27:08.342399 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.263481 (* 1 = 0.263481 loss)
I1012 15:27:08.342455 31118 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I1012 15:27:41.556468 31118 solver.cpp:357] Iteration 33800 (3.01087 iter/s, 33.213s/100 iters), loss = 0.172812
I1012 15:27:41.556604 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.154591 (* 1 = 0.154591 loss)
I1012 15:27:41.556617 31118 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I1012 15:28:14.819814 31118 solver.cpp:357] Iteration 33900 (3.00643 iter/s, 33.2621s/100 iters), loss = 0.208387
I1012 15:28:14.820063 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.289178 (* 1 = 0.289178 loss)
I1012 15:28:14.820077 31118 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I1012 15:28:42.020548 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:28:47.801499 31118 solver.cpp:514] Iteration 34000, Testing net (#0)
I1012 15:29:10.494629 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:29:10.617687 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.337748 (* 1 = 0.337748 loss)
I1012 15:29:10.617714 31118 solver.cpp:580]     Test net output #1: prob = 0.883102
I1012 15:29:10.617727 31118 solver.cpp:593]     Max_acc: 0.883702  with iter: 32500
I1012 15:29:10.927702 31118 solver.cpp:357] Iteration 34000 (1.78234 iter/s, 56.1059s/100 iters), loss = 0.10251
I1012 15:29:10.927784 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0882559 (* 1 = 0.0882559 loss)
I1012 15:29:10.927798 31118 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I1012 15:29:44.179163 31118 solver.cpp:357] Iteration 34100 (3.00775 iter/s, 33.2474s/100 iters), loss = 0.175431
I1012 15:29:44.179332 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.224762 (* 1 = 0.224762 loss)
I1012 15:29:44.179344 31118 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I1012 15:30:17.423411 31118 solver.cpp:357] Iteration 34200 (3.00839 iter/s, 33.2404s/100 iters), loss = 0.130944
I1012 15:30:17.423578 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.171397 (* 1 = 0.171397 loss)
I1012 15:30:17.423593 31118 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I1012 15:30:50.712270 31118 solver.cpp:357] Iteration 34300 (3.00434 iter/s, 33.2851s/100 iters), loss = 0.104173
I1012 15:30:50.712585 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.127377 (* 1 = 0.127377 loss)
I1012 15:30:50.712647 31118 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I1012 15:31:14.716234 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:31:23.947186 31118 solver.cpp:357] Iteration 34400 (3.00921 iter/s, 33.2313s/100 iters), loss = 0.28869
I1012 15:31:23.947327 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.237867 (* 1 = 0.237867 loss)
I1012 15:31:23.947340 31118 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I1012 15:31:56.849026 31118 solver.cpp:514] Iteration 34500, Testing net (#0)
I1012 15:32:19.104166 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:32:19.224823 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.316176 (* 1 = 0.316176 loss)
I1012 15:32:19.224853 31118 solver.cpp:580]     Test net output #1: prob = 0.892701
I1012 15:32:19.224866 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_34500.caffemodel
I1012 15:32:19.236474 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_34500.solverstate
I1012 15:32:19.238973 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:32:19.506510 31118 solver.cpp:357] Iteration 34500 (1.80002 iter/s, 55.5551s/100 iters), loss = 0.160523
I1012 15:32:19.506580 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.108556 (* 1 = 0.108556 loss)
I1012 15:32:19.506592 31118 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I1012 15:32:52.531116 31118 solver.cpp:357] Iteration 34600 (3.02815 iter/s, 33.0235s/100 iters), loss = 0.219144
I1012 15:32:52.531293 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.132093 (* 1 = 0.132093 loss)
I1012 15:32:52.531306 31118 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I1012 15:33:25.560096 31118 solver.cpp:357] Iteration 34700 (3.02794 iter/s, 33.0258s/100 iters), loss = 0.147145
I1012 15:33:25.560271 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.134784 (* 1 = 0.134784 loss)
I1012 15:33:25.560284 31118 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I1012 15:33:46.642326 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:33:58.957841 31118 solver.cpp:357] Iteration 34800 (2.99449 iter/s, 33.3947s/100 iters), loss = 0.152515
I1012 15:33:58.958040 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.214154 (* 1 = 0.214154 loss)
I1012 15:33:58.958051 31118 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I1012 15:34:32.079645 31118 solver.cpp:357] Iteration 34900 (3.01925 iter/s, 33.1208s/100 iters), loss = 0.149692
I1012 15:34:32.079825 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.131183 (* 1 = 0.131183 loss)
I1012 15:34:32.079838 31118 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I1012 15:35:05.000725 31118 solver.cpp:514] Iteration 35000, Testing net (#0)
I1012 15:35:27.335855 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:35:27.351637 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.336043 (* 1 = 0.336043 loss)
I1012 15:35:27.351666 31118 solver.cpp:580]     Test net output #1: prob = 0.884202
I1012 15:35:27.351673 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:35:27.646173 31118 solver.cpp:357] Iteration 35000 (1.79975 iter/s, 55.5632s/100 iters), loss = 0.129764
I1012 15:35:27.646232 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.136418 (* 1 = 0.136418 loss)
I1012 15:35:27.646245 31118 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I1012 15:36:00.963639 31118 solver.cpp:357] Iteration 35100 (3.00168 iter/s, 33.3147s/100 iters), loss = 0.166198
I1012 15:36:00.963814 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.155493 (* 1 = 0.155493 loss)
I1012 15:36:00.963825 31118 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I1012 15:36:18.734732 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:36:34.089694 31118 solver.cpp:357] Iteration 35200 (3.01902 iter/s, 33.1233s/100 iters), loss = 0.148471
I1012 15:36:34.089864 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.108217 (* 1 = 0.108217 loss)
I1012 15:36:34.089879 31118 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I1012 15:37:07.317867 31118 solver.cpp:357] Iteration 35300 (3.00973 iter/s, 33.2255s/100 iters), loss = 0.225818
I1012 15:37:07.318064 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.311792 (* 1 = 0.311792 loss)
I1012 15:37:07.318094 31118 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I1012 15:37:40.645625 31118 solver.cpp:357] Iteration 35400 (3.00073 iter/s, 33.3252s/100 iters), loss = 0.0987544
I1012 15:37:40.645789 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.114036 (* 1 = 0.114036 loss)
I1012 15:37:40.645802 31118 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I1012 15:38:13.426414 31118 solver.cpp:514] Iteration 35500, Testing net (#0)
I1012 15:38:35.719172 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:38:35.795004 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.348653 (* 1 = 0.348653 loss)
I1012 15:38:35.795043 31118 solver.cpp:580]     Test net output #1: prob = 0.883602
I1012 15:38:35.795049 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:38:36.050776 31118 solver.cpp:357] Iteration 35500 (1.80497 iter/s, 55.4025s/100 iters), loss = 0.160215
I1012 15:38:36.050844 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.161168 (* 1 = 0.161168 loss)
I1012 15:38:36.050858 31118 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I1012 15:38:50.660115 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:39:09.245009 31118 solver.cpp:357] Iteration 35600 (3.01279 iter/s, 33.1918s/100 iters), loss = 0.155384
I1012 15:39:09.245081 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.11334 (* 1 = 0.11334 loss)
I1012 15:39:09.245091 31118 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I1012 15:39:42.378878 31118 solver.cpp:357] Iteration 35700 (3.01827 iter/s, 33.1315s/100 iters), loss = 0.160618
I1012 15:39:42.379017 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.171137 (* 1 = 0.171137 loss)
I1012 15:39:42.379031 31118 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I1012 15:40:15.650038 31118 solver.cpp:357] Iteration 35800 (3.00582 iter/s, 33.2688s/100 iters), loss = 0.114274
I1012 15:40:15.650218 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.12864 (* 1 = 0.12864 loss)
I1012 15:40:15.650229 31118 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I1012 15:40:48.852483 31118 solver.cpp:357] Iteration 35900 (3.01203 iter/s, 33.2002s/100 iters), loss = 0.0935444
I1012 15:40:48.852596 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.126187 (* 1 = 0.126187 loss)
I1012 15:40:48.852607 31118 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I1012 15:41:00.361742 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:41:21.864030 31118 solver.cpp:514] Iteration 36000, Testing net (#0)
I1012 15:41:44.246816 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:41:44.369113 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.353033 (* 1 = 0.353033 loss)
I1012 15:41:44.369143 31118 solver.cpp:580]     Test net output #1: prob = 0.885102
I1012 15:41:44.369148 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:41:44.680954 31118 solver.cpp:357] Iteration 36000 (1.79121 iter/s, 55.8283s/100 iters), loss = 0.143285
I1012 15:41:44.681005 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.194753 (* 1 = 0.194753 loss)
I1012 15:41:44.681016 31118 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I1012 15:42:17.840406 31118 solver.cpp:357] Iteration 36100 (3.01593 iter/s, 33.1573s/100 iters), loss = 0.168156
I1012 15:42:17.840559 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.184375 (* 1 = 0.184375 loss)
I1012 15:42:17.840569 31118 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I1012 15:42:51.158104 31118 solver.cpp:357] Iteration 36200 (3.00142 iter/s, 33.3176s/100 iters), loss = 0.130878
I1012 15:42:51.158298 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0773855 (* 1 = 0.0773855 loss)
I1012 15:42:51.158311 31118 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I1012 15:43:24.280493 31118 solver.cpp:357] Iteration 36300 (3.0193 iter/s, 33.1203s/100 iters), loss = 0.213625
I1012 15:43:24.280627 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.233173 (* 1 = 0.233173 loss)
I1012 15:43:24.280639 31118 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I1012 15:43:32.782598 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:43:57.567378 31118 solver.cpp:357] Iteration 36400 (3.00438 iter/s, 33.2848s/100 iters), loss = 0.172895
I1012 15:43:57.567523 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.176907 (* 1 = 0.176907 loss)
I1012 15:43:57.567533 31118 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I1012 15:44:30.397363 31118 solver.cpp:514] Iteration 36500, Testing net (#0)
I1012 15:44:52.680979 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:44:52.804559 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.375139 (* 1 = 0.375139 loss)
I1012 15:44:52.804587 31118 solver.cpp:580]     Test net output #1: prob = 0.874001
I1012 15:44:52.804594 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:44:53.113962 31118 solver.cpp:357] Iteration 36500 (1.80035 iter/s, 55.5447s/100 iters), loss = 0.170052
I1012 15:44:53.114012 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.12743 (* 1 = 0.12743 loss)
I1012 15:44:53.114022 31118 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I1012 15:45:26.200794 31118 solver.cpp:357] Iteration 36600 (3.02254 iter/s, 33.0848s/100 iters), loss = 0.136548
I1012 15:45:26.200959 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.188199 (* 1 = 0.188199 loss)
I1012 15:45:26.200973 31118 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I1012 15:45:59.333415 31118 solver.cpp:357] Iteration 36700 (3.01836 iter/s, 33.1306s/100 iters), loss = 0.0851701
I1012 15:45:59.333588 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.048807 (* 1 = 0.048807 loss)
I1012 15:45:59.333601 31118 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I1012 15:46:04.725647 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:46:32.527649 31118 solver.cpp:357] Iteration 36800 (3.01275 iter/s, 33.1923s/100 iters), loss = 0.223155
I1012 15:46:32.527849 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.157296 (* 1 = 0.157296 loss)
I1012 15:46:32.527861 31118 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I1012 15:47:05.788779 31118 solver.cpp:357] Iteration 36900 (3.00669 iter/s, 33.2592s/100 iters), loss = 0.128777
I1012 15:47:05.788949 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.162535 (* 1 = 0.162535 loss)
I1012 15:47:05.788962 31118 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I1012 15:47:38.718305 31118 solver.cpp:514] Iteration 37000, Testing net (#0)
I1012 15:48:01.055877 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:48:01.070845 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.324399 (* 1 = 0.324399 loss)
I1012 15:48:01.070874 31118 solver.cpp:580]     Test net output #1: prob = 0.891702
I1012 15:48:01.070880 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:48:01.383842 31118 solver.cpp:357] Iteration 37000 (1.79878 iter/s, 55.5933s/100 iters), loss = 0.184034
I1012 15:48:01.383904 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.138275 (* 1 = 0.138275 loss)
I1012 15:48:01.383918 31118 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I1012 15:48:34.681694 31118 solver.cpp:357] Iteration 37100 (3.00337 iter/s, 33.2959s/100 iters), loss = 0.110873
I1012 15:48:34.681841 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.131697 (* 1 = 0.131697 loss)
I1012 15:48:34.681854 31118 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I1012 15:48:36.815349 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:49:07.779004 31118 solver.cpp:357] Iteration 37200 (3.02138 iter/s, 33.0975s/100 iters), loss = 0.142768
I1012 15:49:07.779131 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.159261 (* 1 = 0.159261 loss)
I1012 15:49:07.779145 31118 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I1012 15:49:41.144321 31118 solver.cpp:357] Iteration 37300 (2.99729 iter/s, 33.3634s/100 iters), loss = 0.0730664
I1012 15:49:41.144474 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0747956 (* 1 = 0.0747956 loss)
I1012 15:49:41.144486 31118 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I1012 15:50:14.370779 31118 solver.cpp:357] Iteration 37400 (3.00963 iter/s, 33.2266s/100 iters), loss = 0.160527
I1012 15:50:14.370937 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.138063 (* 1 = 0.138063 loss)
I1012 15:50:14.370950 31118 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I1012 15:50:46.516844 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:50:47.113891 31118 solver.cpp:514] Iteration 37500, Testing net (#0)
I1012 15:51:09.189767 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:51:09.314765 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.357983 (* 1 = 0.357983 loss)
I1012 15:51:09.314795 31118 solver.cpp:580]     Test net output #1: prob = 0.881502
I1012 15:51:09.314801 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:51:09.540747 31118 solver.cpp:357] Iteration 37500 (1.81263 iter/s, 55.1684s/100 iters), loss = 0.154654
I1012 15:51:09.540889 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.125152 (* 1 = 0.125152 loss)
I1012 15:51:09.540916 31118 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I1012 15:51:42.644528 31118 solver.cpp:357] Iteration 37600 (3.02097 iter/s, 33.102s/100 iters), loss = 0.107278
I1012 15:51:42.644714 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0723706 (* 1 = 0.0723706 loss)
I1012 15:51:42.644754 31118 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I1012 15:52:15.988386 31118 solver.cpp:357] Iteration 37700 (2.99903 iter/s, 33.3441s/100 iters), loss = 0.233134
I1012 15:52:15.988544 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.174828 (* 1 = 0.174828 loss)
I1012 15:52:15.988555 31118 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I1012 15:52:49.184792 31118 solver.cpp:357] Iteration 37800 (3.01254 iter/s, 33.1946s/100 iters), loss = 0.169568
I1012 15:52:49.185155 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.213023 (* 1 = 0.213023 loss)
I1012 15:52:49.185169 31118 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I1012 15:53:18.524950 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:53:22.368084 31118 solver.cpp:357] Iteration 37900 (3.01373 iter/s, 33.1815s/100 iters), loss = 0.12488
I1012 15:53:22.368232 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0709552 (* 1 = 0.0709552 loss)
I1012 15:53:22.368243 31118 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I1012 15:53:55.124305 31118 solver.cpp:514] Iteration 38000, Testing net (#0)
I1012 15:54:17.683285 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:54:17.808048 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.389597 (* 1 = 0.389597 loss)
I1012 15:54:17.808075 31118 solver.cpp:580]     Test net output #1: prob = 0.875901
I1012 15:54:17.808081 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:54:18.124636 31118 solver.cpp:357] Iteration 38000 (1.79356 iter/s, 55.7551s/100 iters), loss = 0.145183
I1012 15:54:18.124720 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.173858 (* 1 = 0.173858 loss)
I1012 15:54:18.124733 31118 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I1012 15:54:51.328577 31118 solver.cpp:357] Iteration 38100 (3.01185 iter/s, 33.2022s/100 iters), loss = 0.124422
I1012 15:54:51.328696 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.111602 (* 1 = 0.111602 loss)
I1012 15:54:51.328709 31118 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I1012 15:55:24.609881 31118 solver.cpp:357] Iteration 38200 (3.00485 iter/s, 33.2796s/100 iters), loss = 0.072861
I1012 15:55:24.609987 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0921359 (* 1 = 0.0921359 loss)
I1012 15:55:24.609998 31118 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I1012 15:55:50.788837 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:55:57.913137 31118 solver.cpp:357] Iteration 38300 (3.00268 iter/s, 33.3036s/100 iters), loss = 0.139367
I1012 15:55:57.913236 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.105872 (* 1 = 0.105872 loss)
I1012 15:55:57.913247 31118 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I1012 15:56:31.118283 31118 solver.cpp:357] Iteration 38400 (3.01175 iter/s, 33.2033s/100 iters), loss = 0.135847
I1012 15:56:31.118410 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.104265 (* 1 = 0.104265 loss)
I1012 15:56:31.118422 31118 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I1012 15:57:04.044899 31118 solver.cpp:514] Iteration 38500, Testing net (#0)
I1012 15:57:26.510095 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:57:26.635725 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.356847 (* 1 = 0.356847 loss)
I1012 15:57:26.635751 31118 solver.cpp:580]     Test net output #1: prob = 0.883101
I1012 15:57:26.635757 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 15:57:26.950105 31118 solver.cpp:357] Iteration 38500 (1.79114 iter/s, 55.8304s/100 iters), loss = 0.148079
I1012 15:57:26.950158 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.117405 (* 1 = 0.117405 loss)
I1012 15:57:26.950171 31118 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I1012 15:58:00.027273 31118 solver.cpp:357] Iteration 38600 (3.02339 iter/s, 33.0754s/100 iters), loss = 0.136773
I1012 15:58:00.027433 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.120545 (* 1 = 0.120545 loss)
I1012 15:58:00.027447 31118 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I1012 15:58:23.096163 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 15:58:33.391785 31118 solver.cpp:357] Iteration 38700 (2.99735 iter/s, 33.3628s/100 iters), loss = 0.139034
I1012 15:58:33.391911 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.129774 (* 1 = 0.129774 loss)
I1012 15:58:33.391922 31118 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I1012 15:59:06.618541 31118 solver.cpp:357] Iteration 38800 (3.00978 iter/s, 33.225s/100 iters), loss = 0.105611
I1012 15:59:06.618796 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.138781 (* 1 = 0.138781 loss)
I1012 15:59:06.618809 31118 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I1012 15:59:39.727413 31118 solver.cpp:357] Iteration 38900 (3.02047 iter/s, 33.1074s/100 iters), loss = 0.0830753
I1012 15:59:39.727546 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0651088 (* 1 = 0.0651088 loss)
I1012 15:59:39.727560 31118 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I1012 16:00:12.668560 31118 solver.cpp:514] Iteration 39000, Testing net (#0)
I1012 16:00:34.998204 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:00:35.012981 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.429754 (* 1 = 0.429754 loss)
I1012 16:00:35.013011 31118 solver.cpp:580]     Test net output #1: prob = 0.868001
I1012 16:00:35.013017 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:00:35.307184 31118 solver.cpp:357] Iteration 39000 (1.79926 iter/s, 55.5784s/100 iters), loss = 0.0471984
I1012 16:00:35.307250 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0601923 (* 1 = 0.0601923 loss)
I1012 16:00:35.307262 31118 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I1012 16:00:55.036571 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:01:08.597213 31118 solver.cpp:357] Iteration 39100 (3.00406 iter/s, 33.2883s/100 iters), loss = 0.158012
I1012 16:01:08.597296 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.175072 (* 1 = 0.175072 loss)
I1012 16:01:08.597308 31118 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I1012 16:01:41.761301 31118 solver.cpp:357] Iteration 39200 (3.01547 iter/s, 33.1624s/100 iters), loss = 0.210603
I1012 16:01:41.761451 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0955563 (* 1 = 0.0955563 loss)
I1012 16:01:41.761463 31118 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I1012 16:02:15.093169 31118 solver.cpp:357] Iteration 39300 (3.00028 iter/s, 33.3302s/100 iters), loss = 0.151139
I1012 16:02:15.093498 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.115817 (* 1 = 0.115817 loss)
I1012 16:02:15.093513 31118 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I1012 16:02:48.399297 31118 solver.cpp:357] Iteration 39400 (3.0026 iter/s, 33.3045s/100 iters), loss = 0.118358
I1012 16:02:48.399451 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0574459 (* 1 = 0.0574459 loss)
I1012 16:02:48.399461 31118 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I1012 16:03:05.127845 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:03:21.418577 31118 solver.cpp:514] Iteration 39500, Testing net (#0)
I1012 16:03:43.700717 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:03:43.823973 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.346502 (* 1 = 0.346502 loss)
I1012 16:03:43.824028 31118 solver.cpp:580]     Test net output #1: prob = 0.885302
I1012 16:03:43.824034 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:03:44.074621 31118 solver.cpp:357] Iteration 39500 (1.7961 iter/s, 55.6761s/100 iters), loss = 0.175687
I1012 16:03:44.074709 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.114888 (* 1 = 0.114888 loss)
I1012 16:03:44.074723 31118 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I1012 16:04:14.612767 31118 solver.cpp:357] Iteration 39600 (3.27461 iter/s, 30.538s/100 iters), loss = 0.144829
I1012 16:04:14.612908 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.139904 (* 1 = 0.139904 loss)
I1012 16:04:14.612920 31118 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I1012 16:04:47.889894 31118 solver.cpp:357] Iteration 39700 (3.00489 iter/s, 33.2791s/100 iters), loss = 0.141644
I1012 16:04:47.890005 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.127487 (* 1 = 0.127487 loss)
I1012 16:04:47.890017 31118 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I1012 16:05:21.148743 31118 solver.cpp:357] Iteration 39800 (3.00673 iter/s, 33.2587s/100 iters), loss = 0.103661
I1012 16:05:21.149049 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.122811 (* 1 = 0.122811 loss)
I1012 16:05:21.149065 31118 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I1012 16:05:34.883042 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:05:54.389300 31118 solver.cpp:357] Iteration 39900 (3.00839 iter/s, 33.2403s/100 iters), loss = 0.111099
I1012 16:05:54.389497 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.168855 (* 1 = 0.168855 loss)
I1012 16:05:54.389524 31118 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I1012 16:06:27.296128 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_40000.caffemodel
I1012 16:06:27.310214 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_40000.solverstate
I1012 16:06:27.313280 31118 solver.cpp:514] Iteration 40000, Testing net (#0)
I1012 16:06:49.652040 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:06:49.776757 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.351268 (* 1 = 0.351268 loss)
I1012 16:06:49.776784 31118 solver.cpp:580]     Test net output #1: prob = 0.886101
I1012 16:06:49.776790 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:06:50.017235 31118 solver.cpp:357] Iteration 40000 (1.79763 iter/s, 55.6289s/100 iters), loss = 0.155261
I1012 16:06:50.017325 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0977341 (* 1 = 0.0977341 loss)
I1012 16:06:50.017338 31118 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I1012 16:07:23.396113 31118 solver.cpp:357] Iteration 40100 (2.99594 iter/s, 33.3785s/100 iters), loss = 0.202694
I1012 16:07:23.396292 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.188939 (* 1 = 0.188939 loss)
I1012 16:07:23.396307 31118 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I1012 16:07:56.544756 31118 solver.cpp:357] Iteration 40200 (3.01676 iter/s, 33.1482s/100 iters), loss = 0.107984
I1012 16:07:56.544926 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0894469 (* 1 = 0.0894469 loss)
I1012 16:07:56.544939 31118 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I1012 16:08:07.069026 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:08:29.754658 31118 solver.cpp:357] Iteration 40300 (3.0112 iter/s, 33.2094s/100 iters), loss = 0.106205
I1012 16:08:29.754812 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.13302 (* 1 = 0.13302 loss)
I1012 16:08:29.754823 31118 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I1012 16:09:02.955983 31118 solver.cpp:357] Iteration 40400 (3.01198 iter/s, 33.2008s/100 iters), loss = 0.165301
I1012 16:09:02.956086 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.176966 (* 1 = 0.176966 loss)
I1012 16:09:02.956097 31118 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I1012 16:09:35.796181 31118 solver.cpp:514] Iteration 40500, Testing net (#0)
I1012 16:09:58.203970 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:09:58.334908 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.378292 (* 1 = 0.378292 loss)
I1012 16:09:58.334954 31118 solver.cpp:580]     Test net output #1: prob = 0.879402
I1012 16:09:58.334960 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:09:58.570611 31118 solver.cpp:357] Iteration 40500 (1.79801 iter/s, 55.6171s/100 iters), loss = 0.110413
I1012 16:09:58.570683 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0978649 (* 1 = 0.0978649 loss)
I1012 16:09:58.570695 31118 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I1012 16:10:31.817816 31118 solver.cpp:357] Iteration 40600 (3.00784 iter/s, 33.2465s/100 iters), loss = 0.127197
I1012 16:10:31.817970 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.17634 (* 1 = 0.17634 loss)
I1012 16:10:31.817983 31118 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I1012 16:10:39.200392 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:11:04.984519 31118 solver.cpp:357] Iteration 40700 (3.01514 iter/s, 33.166s/100 iters), loss = 0.122609
I1012 16:11:04.984714 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0793285 (* 1 = 0.0793285 loss)
I1012 16:11:04.984727 31118 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I1012 16:11:38.056927 31118 solver.cpp:357] Iteration 40800 (3.02374 iter/s, 33.0716s/100 iters), loss = 0.110497
I1012 16:11:38.057075 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.136041 (* 1 = 0.136041 loss)
I1012 16:11:38.057087 31118 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I1012 16:12:11.303596 31118 solver.cpp:357] Iteration 40900 (3.0079 iter/s, 33.2458s/100 iters), loss = 0.126012
I1012 16:12:11.303831 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.13573 (* 1 = 0.13573 loss)
I1012 16:12:11.303864 31118 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I1012 16:12:44.216275 31118 solver.cpp:514] Iteration 41000, Testing net (#0)
I1012 16:13:06.806780 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:13:06.822291 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.407773 (* 1 = 0.407773 loss)
I1012 16:13:06.822321 31118 solver.cpp:580]     Test net output #1: prob = 0.876102
I1012 16:13:06.822327 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:13:07.095754 31118 solver.cpp:357] Iteration 41000 (1.79237 iter/s, 55.7922s/100 iters), loss = 0.130799
I1012 16:13:07.095819 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.086323 (* 1 = 0.086323 loss)
I1012 16:13:07.095830 31118 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I1012 16:13:11.580641 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:13:40.304807 31118 solver.cpp:357] Iteration 41100 (3.01131 iter/s, 33.2081s/100 iters), loss = 0.0909549
I1012 16:13:40.304970 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0796257 (* 1 = 0.0796257 loss)
I1012 16:13:40.304983 31118 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I1012 16:14:13.546880 31118 solver.cpp:357] Iteration 41200 (3.00832 iter/s, 33.2411s/100 iters), loss = 0.0757933
I1012 16:14:13.547013 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0586571 (* 1 = 0.0586571 loss)
I1012 16:14:13.547025 31118 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I1012 16:14:46.960966 31118 solver.cpp:357] Iteration 41300 (2.99284 iter/s, 33.4131s/100 iters), loss = 0.0894456
I1012 16:14:46.961140 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.140363 (* 1 = 0.140363 loss)
I1012 16:14:46.961153 31118 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I1012 16:15:20.128819 31118 solver.cpp:357] Iteration 41400 (3.01506 iter/s, 33.1669s/100 iters), loss = 0.104808
I1012 16:15:20.128954 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.116244 (* 1 = 0.116244 loss)
I1012 16:15:20.128967 31118 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I1012 16:15:21.393941 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:15:53.047899 31118 solver.cpp:514] Iteration 41500, Testing net (#0)
I1012 16:16:15.436475 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:16:15.517688 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.380117 (* 1 = 0.380117 loss)
I1012 16:16:15.517719 31118 solver.cpp:580]     Test net output #1: prob = 0.878701
I1012 16:16:15.517733 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:16:15.834925 31118 solver.cpp:357] Iteration 41500 (1.79514 iter/s, 55.7059s/100 iters), loss = 0.106405
I1012 16:16:15.834981 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0494944 (* 1 = 0.0494944 loss)
I1012 16:16:15.834993 31118 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I1012 16:16:49.045135 31118 solver.cpp:357] Iteration 41600 (3.01122 iter/s, 33.2092s/100 iters), loss = 0.160267
I1012 16:16:49.045297 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.142416 (* 1 = 0.142416 loss)
I1012 16:16:49.045310 31118 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I1012 16:17:22.103238 31118 solver.cpp:357] Iteration 41700 (3.02507 iter/s, 33.057s/100 iters), loss = 0.141551
I1012 16:17:22.103457 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.099314 (* 1 = 0.099314 loss)
I1012 16:17:22.103471 31118 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I1012 16:17:53.464664 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:17:55.443209 31118 solver.cpp:357] Iteration 41800 (2.99932 iter/s, 33.3409s/100 iters), loss = 0.173964
I1012 16:17:55.443274 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0783485 (* 1 = 0.0783485 loss)
I1012 16:17:55.443284 31118 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I1012 16:18:28.679267 31118 solver.cpp:357] Iteration 41900 (3.00889 iter/s, 33.2348s/100 iters), loss = 0.0676936
I1012 16:18:28.679405 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0731832 (* 1 = 0.0731832 loss)
I1012 16:18:28.679417 31118 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I1012 16:19:01.591480 31118 solver.cpp:514] Iteration 42000, Testing net (#0)
I1012 16:19:23.801025 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:19:23.931787 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.406499 (* 1 = 0.406499 loss)
I1012 16:19:23.931885 31118 solver.cpp:580]     Test net output #1: prob = 0.870902
I1012 16:19:23.931905 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:19:24.203349 31118 solver.cpp:357] Iteration 42000 (1.80103 iter/s, 55.5237s/100 iters), loss = 0.0786704
I1012 16:19:24.203424 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0500906 (* 1 = 0.0500906 loss)
I1012 16:19:24.203438 31118 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I1012 16:19:57.482255 31118 solver.cpp:357] Iteration 42100 (3.00501 iter/s, 33.2778s/100 iters), loss = 0.104928
I1012 16:19:57.482389 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0851567 (* 1 = 0.0851567 loss)
I1012 16:19:57.482403 31118 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I1012 16:20:25.557386 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:20:30.670047 31118 solver.cpp:357] Iteration 42200 (3.01326 iter/s, 33.1866s/100 iters), loss = 0.169198
I1012 16:20:30.670363 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.139907 (* 1 = 0.139907 loss)
I1012 16:20:30.670377 31118 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I1012 16:21:04.077780 31118 solver.cpp:357] Iteration 42300 (2.99342 iter/s, 33.4066s/100 iters), loss = 0.0977746
I1012 16:21:04.077911 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0952509 (* 1 = 0.0952509 loss)
I1012 16:21:04.077924 31118 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I1012 16:21:37.156488 31118 solver.cpp:357] Iteration 42400 (3.0232 iter/s, 33.0775s/100 iters), loss = 0.1933
I1012 16:21:37.156636 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.180764 (* 1 = 0.180764 loss)
I1012 16:21:37.156649 31118 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I1012 16:22:10.280835 31118 solver.cpp:514] Iteration 42500, Testing net (#0)
I1012 16:22:32.728404 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:22:32.857206 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.406826 (* 1 = 0.406826 loss)
I1012 16:22:32.857255 31118 solver.cpp:580]     Test net output #1: prob = 0.868202
I1012 16:22:32.857262 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:22:33.106581 31118 solver.cpp:357] Iteration 42500 (1.78732 iter/s, 55.9496s/100 iters), loss = 0.0963337
I1012 16:22:33.106658 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.132004 (* 1 = 0.132004 loss)
I1012 16:22:33.106672 31118 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I1012 16:22:58.151816 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:23:06.179487 31118 solver.cpp:357] Iteration 42600 (3.02373 iter/s, 33.0717s/100 iters), loss = 0.120102
I1012 16:23:06.179565 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0897437 (* 1 = 0.0897437 loss)
I1012 16:23:06.179577 31118 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I1012 16:23:39.169872 31118 solver.cpp:357] Iteration 42700 (3.0313 iter/s, 32.9892s/100 iters), loss = 0.146873
I1012 16:23:39.170063 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.117349 (* 1 = 0.117349 loss)
I1012 16:23:39.170075 31118 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I1012 16:24:12.374372 31118 solver.cpp:357] Iteration 42800 (3.01175 iter/s, 33.2033s/100 iters), loss = 0.148598
I1012 16:24:12.374503 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.211145 (* 1 = 0.211145 loss)
I1012 16:24:12.374516 31118 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I1012 16:24:45.747472 31118 solver.cpp:357] Iteration 42900 (2.99654 iter/s, 33.3719s/100 iters), loss = 0.0828069
I1012 16:24:45.747596 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0744977 (* 1 = 0.0744977 loss)
I1012 16:24:45.747606 31118 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I1012 16:25:07.681679 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:25:18.668135 31118 solver.cpp:514] Iteration 43000, Testing net (#0)
I1012 16:25:40.887987 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:25:40.903870 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.332889 (* 1 = 0.332889 loss)
I1012 16:25:40.903903 31118 solver.cpp:580]     Test net output #1: prob = 0.889202
I1012 16:25:40.903910 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:25:41.217900 31118 solver.cpp:357] Iteration 43000 (1.80278 iter/s, 55.4698s/100 iters), loss = 0.1171
I1012 16:25:41.217954 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.198334 (* 1 = 0.198334 loss)
I1012 16:25:41.217969 31118 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I1012 16:26:14.545116 31118 solver.cpp:357] Iteration 43100 (3.00066 iter/s, 33.326s/100 iters), loss = 0.104246
I1012 16:26:14.545291 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.101116 (* 1 = 0.101116 loss)
I1012 16:26:14.545305 31118 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I1012 16:26:47.788108 31118 solver.cpp:357] Iteration 43200 (3.00827 iter/s, 33.2417s/100 iters), loss = 0.158473
I1012 16:26:47.788223 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.106884 (* 1 = 0.106884 loss)
I1012 16:26:47.788235 31118 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I1012 16:27:21.049479 31118 solver.cpp:357] Iteration 43300 (3.00661 iter/s, 33.2601s/100 iters), loss = 0.0649134
I1012 16:27:21.049607 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0958701 (* 1 = 0.0958701 loss)
I1012 16:27:21.049619 31118 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I1012 16:27:39.789299 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:27:54.256853 31118 solver.cpp:357] Iteration 43400 (3.0115 iter/s, 33.2061s/100 iters), loss = 0.163206
I1012 16:27:54.256995 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.115245 (* 1 = 0.115245 loss)
I1012 16:27:54.257009 31118 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I1012 16:28:27.197774 31118 solver.cpp:514] Iteration 43500, Testing net (#0)
I1012 16:28:49.433830 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:28:49.556772 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.452748 (* 1 = 0.452748 loss)
I1012 16:28:49.556799 31118 solver.cpp:580]     Test net output #1: prob = 0.861001
I1012 16:28:49.556805 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:28:49.867177 31118 solver.cpp:357] Iteration 43500 (1.79825 iter/s, 55.6097s/100 iters), loss = 0.0743498
I1012 16:28:49.867266 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0525802 (* 1 = 0.0525802 loss)
I1012 16:28:49.867278 31118 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I1012 16:29:23.004254 31118 solver.cpp:357] Iteration 43600 (3.01789 iter/s, 33.1358s/100 iters), loss = 0.159002
I1012 16:29:23.004417 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.134362 (* 1 = 0.134362 loss)
I1012 16:29:23.004431 31118 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I1012 16:29:56.057348 31118 solver.cpp:357] Iteration 43700 (3.02555 iter/s, 33.0518s/100 iters), loss = 0.152678
I1012 16:29:56.057694 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.118706 (* 1 = 0.118706 loss)
I1012 16:29:56.057708 31118 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I1012 16:30:11.701841 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:30:29.383393 31118 solver.cpp:357] Iteration 43800 (3.00077 iter/s, 33.3247s/100 iters), loss = 0.0852912
I1012 16:30:29.383533 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0770936 (* 1 = 0.0770936 loss)
I1012 16:30:29.383548 31118 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I1012 16:31:02.609511 31118 solver.cpp:357] Iteration 43900 (3.0098 iter/s, 33.2248s/100 iters), loss = 0.147514
I1012 16:31:02.609622 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0842692 (* 1 = 0.0842692 loss)
I1012 16:31:02.609633 31118 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I1012 16:31:35.517997 31118 solver.cpp:514] Iteration 44000, Testing net (#0)
I1012 16:31:57.953866 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:31:58.076213 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.402169 (* 1 = 0.402169 loss)
I1012 16:31:58.076243 31118 solver.cpp:580]     Test net output #1: prob = 0.872302
I1012 16:31:58.076251 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:31:58.387265 31118 solver.cpp:357] Iteration 44000 (1.79285 iter/s, 55.7771s/100 iters), loss = 0.147823
I1012 16:31:58.387354 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.130982 (* 1 = 0.130982 loss)
I1012 16:31:58.387367 31118 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I1012 16:32:31.536986 31118 solver.cpp:357] Iteration 44100 (3.01674 iter/s, 33.1484s/100 iters), loss = 0.176706
I1012 16:32:31.537160 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.106149 (* 1 = 0.106149 loss)
I1012 16:32:31.537173 31118 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I1012 16:32:44.281932 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:33:04.761059 31118 solver.cpp:357] Iteration 44200 (3.00999 iter/s, 33.2228s/100 iters), loss = 0.124661
I1012 16:33:04.761168 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.153633 (* 1 = 0.153633 loss)
I1012 16:33:04.761179 31118 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I1012 16:33:38.120163 31118 solver.cpp:357] Iteration 44300 (2.9978 iter/s, 33.3578s/100 iters), loss = 0.201117
I1012 16:33:38.120296 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.229762 (* 1 = 0.229762 loss)
I1012 16:33:38.120311 31118 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I1012 16:34:11.360626 31118 solver.cpp:357] Iteration 44400 (3.0085 iter/s, 33.2392s/100 iters), loss = 0.141533
I1012 16:34:11.360761 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.11379 (* 1 = 0.11379 loss)
I1012 16:34:11.360774 31118 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I1012 16:34:44.272029 31118 solver.cpp:514] Iteration 44500, Testing net (#0)
I1012 16:35:06.848513 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:35:06.971415 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.404374 (* 1 = 0.404374 loss)
I1012 16:35:06.971442 31118 solver.cpp:580]     Test net output #1: prob = 0.882502
I1012 16:35:06.971449 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:35:07.279579 31118 solver.cpp:357] Iteration 44500 (1.78833 iter/s, 55.9182s/100 iters), loss = 0.103519
I1012 16:35:07.279639 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.151329 (* 1 = 0.151329 loss)
I1012 16:35:07.279652 31118 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I1012 16:35:16.681295 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:35:40.434464 31118 solver.cpp:357] Iteration 44600 (3.01627 iter/s, 33.1536s/100 iters), loss = 0.122419
I1012 16:35:40.434551 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.150515 (* 1 = 0.150515 loss)
I1012 16:35:40.434563 31118 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I1012 16:36:13.691114 31118 solver.cpp:357] Iteration 44700 (3.00704 iter/s, 33.2553s/100 iters), loss = 0.184908
I1012 16:36:13.691334 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.244064 (* 1 = 0.244064 loss)
I1012 16:36:13.691345 31118 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I1012 16:36:46.883496 31118 solver.cpp:357] Iteration 44800 (3.01286 iter/s, 33.1911s/100 iters), loss = 0.118569
I1012 16:36:46.883641 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.117118 (* 1 = 0.117118 loss)
I1012 16:36:46.883654 31118 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I1012 16:37:20.178475 31118 solver.cpp:357] Iteration 44900 (3.00353 iter/s, 33.2942s/100 iters), loss = 0.16731
I1012 16:37:20.178619 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.166482 (* 1 = 0.166482 loss)
I1012 16:37:20.178628 31118 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I1012 16:37:26.534914 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:37:53.060199 31118 solver.cpp:514] Iteration 45000, Testing net (#0)
I1012 16:38:15.328630 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:38:15.450769 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.44807 (* 1 = 0.44807 loss)
I1012 16:38:15.450796 31118 solver.cpp:580]     Test net output #1: prob = 0.871801
I1012 16:38:15.450803 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:38:15.766561 31118 solver.cpp:357] Iteration 45000 (1.7988 iter/s, 55.5928s/100 iters), loss = 0.114528
I1012 16:38:15.766652 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.143055 (* 1 = 0.143055 loss)
I1012 16:38:15.766665 31118 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I1012 16:38:48.813571 31118 solver.cpp:357] Iteration 45100 (3.02594 iter/s, 33.0475s/100 iters), loss = 0.155322
I1012 16:38:48.813709 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0714281 (* 1 = 0.0714281 loss)
I1012 16:38:48.813729 31118 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I1012 16:39:22.117071 31118 solver.cpp:357] Iteration 45200 (3.00265 iter/s, 33.3039s/100 iters), loss = 0.117468
I1012 16:39:22.117260 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.137227 (* 1 = 0.137227 loss)
I1012 16:39:22.117290 31118 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I1012 16:39:55.297350 31118 solver.cpp:357] Iteration 45300 (3.01381 iter/s, 33.1806s/100 iters), loss = 0.133859
I1012 16:39:55.297477 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.159157 (* 1 = 0.159157 loss)
I1012 16:39:55.297488 31118 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I1012 16:39:58.448441 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:40:28.541020 31118 solver.cpp:357] Iteration 45400 (3.00807 iter/s, 33.2439s/100 iters), loss = 0.0979539
I1012 16:40:28.541232 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.100016 (* 1 = 0.100016 loss)
I1012 16:40:28.541260 31118 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I1012 16:41:01.536244 31118 solver.cpp:514] Iteration 45500, Testing net (#0)
I1012 16:41:23.786573 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:41:23.908746 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.349163 (* 1 = 0.349163 loss)
I1012 16:41:23.908773 31118 solver.cpp:580]     Test net output #1: prob = 0.884502
I1012 16:41:23.908780 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:41:24.208358 31118 solver.cpp:357] Iteration 45500 (1.79633 iter/s, 55.6691s/100 iters), loss = 0.104307
I1012 16:41:24.208428 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0755651 (* 1 = 0.0755651 loss)
I1012 16:41:24.208441 31118 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I1012 16:41:57.265494 31118 solver.cpp:357] Iteration 45600 (3.02487 iter/s, 33.0593s/100 iters), loss = 0.13349
I1012 16:41:57.265619 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0730394 (* 1 = 0.0730394 loss)
I1012 16:41:57.265633 31118 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I1012 16:42:30.389178 31118 solver.cpp:357] Iteration 45700 (3.01899 iter/s, 33.1236s/100 iters), loss = 0.17224
I1012 16:42:30.389410 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.202505 (* 1 = 0.202505 loss)
I1012 16:42:30.389420 31118 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I1012 16:42:30.614665 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:43:03.627537 31118 solver.cpp:357] Iteration 45800 (3.00858 iter/s, 33.2383s/100 iters), loss = 0.188198
I1012 16:43:03.627702 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.127012 (* 1 = 0.127012 loss)
I1012 16:43:03.627717 31118 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I1012 16:43:36.849128 31118 solver.cpp:357] Iteration 45900 (3.01011 iter/s, 33.2214s/100 iters), loss = 0.0923459
I1012 16:43:36.849236 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.12148 (* 1 = 0.12148 loss)
I1012 16:43:36.849247 31118 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I1012 16:44:09.750783 31118 solver.cpp:514] Iteration 46000, Testing net (#0)
I1012 16:44:31.921756 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:44:31.981314 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.582121 (* 1 = 0.582121 loss)
I1012 16:44:31.981345 31118 solver.cpp:580]     Test net output #1: prob = 0.823501
I1012 16:44:31.981351 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:44:32.296353 31118 solver.cpp:357] Iteration 46000 (1.80348 iter/s, 55.4483s/100 iters), loss = 0.105444
I1012 16:44:32.296406 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.142172 (* 1 = 0.142172 loss)
I1012 16:44:32.296418 31118 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I1012 16:45:02.730926 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:45:05.579916 31118 solver.cpp:357] Iteration 46100 (3.00452 iter/s, 33.2832s/100 iters), loss = 0.21381
I1012 16:45:05.580003 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.263199 (* 1 = 0.263199 loss)
I1012 16:45:05.580015 31118 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I1012 16:45:38.804903 31118 solver.cpp:357] Iteration 46200 (3.00982 iter/s, 33.2246s/100 iters), loss = 0.0741406
I1012 16:45:38.805127 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0796701 (* 1 = 0.0796701 loss)
I1012 16:45:38.805155 31118 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I1012 16:46:12.014457 31118 solver.cpp:357] Iteration 46300 (3.01122 iter/s, 33.2091s/100 iters), loss = 0.0858848
I1012 16:46:12.014622 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0950507 (* 1 = 0.0950507 loss)
I1012 16:46:12.014634 31118 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I1012 16:46:45.039207 31118 solver.cpp:357] Iteration 46400 (3.02789 iter/s, 33.0263s/100 iters), loss = 0.128221
I1012 16:46:45.039548 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.122747 (* 1 = 0.122747 loss)
I1012 16:46:45.039614 31118 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I1012 16:47:12.161317 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:47:17.989365 31118 solver.cpp:514] Iteration 46500, Testing net (#0)
I1012 16:47:40.320747 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:47:40.377516 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.409659 (* 1 = 0.409659 loss)
I1012 16:47:40.377547 31118 solver.cpp:580]     Test net output #1: prob = 0.869802
I1012 16:47:40.377554 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:47:40.692119 31118 solver.cpp:357] Iteration 46500 (1.79683 iter/s, 55.6535s/100 iters), loss = 0.0336068
I1012 16:47:40.692178 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0435466 (* 1 = 0.0435466 loss)
I1012 16:47:40.692189 31118 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I1012 16:48:13.979454 31118 solver.cpp:357] Iteration 46600 (3.0042 iter/s, 33.2868s/100 iters), loss = 0.144486
I1012 16:48:13.979629 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.181527 (* 1 = 0.181527 loss)
I1012 16:48:13.979643 31118 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I1012 16:48:47.061784 31118 solver.cpp:357] Iteration 46700 (3.02282 iter/s, 33.0817s/100 iters), loss = 0.0762705
I1012 16:48:47.062036 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0843673 (* 1 = 0.0843673 loss)
I1012 16:48:47.062049 31118 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I1012 16:49:20.256521 31118 solver.cpp:357] Iteration 46800 (3.01258 iter/s, 33.1941s/100 iters), loss = 0.0757131
I1012 16:49:20.256675 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0811662 (* 1 = 0.0811662 loss)
I1012 16:49:20.256690 31118 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I1012 16:49:44.198032 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:49:53.431149 31118 solver.cpp:357] Iteration 46900 (3.01441 iter/s, 33.174s/100 iters), loss = 0.206049
I1012 16:49:53.431318 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.127077 (* 1 = 0.127077 loss)
I1012 16:49:53.431331 31118 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I1012 16:50:26.430495 31118 solver.cpp:514] Iteration 47000, Testing net (#0)
I1012 16:50:48.823329 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:50:48.838299 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.596086 (* 1 = 0.596086 loss)
I1012 16:50:48.838328 31118 solver.cpp:580]     Test net output #1: prob = 0.826601
I1012 16:50:48.838335 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:50:49.133827 31118 solver.cpp:357] Iteration 47000 (1.79524 iter/s, 55.703s/100 iters), loss = 0.0716078
I1012 16:50:49.133888 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0404317 (* 1 = 0.0404317 loss)
I1012 16:50:49.133898 31118 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I1012 16:51:22.426434 31118 solver.cpp:357] Iteration 47100 (3.00374 iter/s, 33.2919s/100 iters), loss = 0.103305
I1012 16:51:22.426585 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0947106 (* 1 = 0.0947106 loss)
I1012 16:51:22.426597 31118 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I1012 16:51:55.533968 31118 solver.cpp:357] Iteration 47200 (3.02034 iter/s, 33.1088s/100 iters), loss = 0.0996526
I1012 16:51:55.534126 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.116891 (* 1 = 0.116891 loss)
I1012 16:51:55.534137 31118 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I1012 16:52:16.673934 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:52:28.998785 31118 solver.cpp:357] Iteration 47300 (2.98828 iter/s, 33.464s/100 iters), loss = 0.116186
I1012 16:52:28.998903 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.166222 (* 1 = 0.166222 loss)
I1012 16:52:28.998914 31118 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I1012 16:53:02.369179 31118 solver.cpp:357] Iteration 47400 (2.99674 iter/s, 33.3696s/100 iters), loss = 0.139616
I1012 16:53:02.369312 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.12165 (* 1 = 0.12165 loss)
I1012 16:53:02.369324 31118 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I1012 16:53:35.494117 31118 solver.cpp:514] Iteration 47500, Testing net (#0)
I1012 16:53:57.789217 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:53:57.912613 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.363023 (* 1 = 0.363023 loss)
I1012 16:53:57.912642 31118 solver.cpp:580]     Test net output #1: prob = 0.879702
I1012 16:53:57.912647 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:53:58.223608 31118 solver.cpp:357] Iteration 47500 (1.79037 iter/s, 55.8545s/100 iters), loss = 0.110092
I1012 16:53:58.223706 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0624463 (* 1 = 0.0624463 loss)
I1012 16:53:58.223719 31118 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I1012 16:54:31.446658 31118 solver.cpp:357] Iteration 47600 (3.01004 iter/s, 33.2222s/100 iters), loss = 0.156439
I1012 16:54:31.446784 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.14127 (* 1 = 0.14127 loss)
I1012 16:54:31.446796 31118 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I1012 16:54:49.155020 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:55:04.559406 31118 solver.cpp:357] Iteration 47700 (3.02007 iter/s, 33.1119s/100 iters), loss = 0.162634
I1012 16:55:04.559589 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.082603 (* 1 = 0.082603 loss)
I1012 16:55:04.559602 31118 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I1012 16:55:37.669363 31118 solver.cpp:357] Iteration 47800 (3.02032 iter/s, 33.1091s/100 iters), loss = 0.108652
I1012 16:55:37.669507 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.144017 (* 1 = 0.144017 loss)
I1012 16:55:37.669520 31118 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I1012 16:56:10.829672 31118 solver.cpp:357] Iteration 47900 (3.01574 iter/s, 33.1594s/100 iters), loss = 0.0874011
I1012 16:56:10.829807 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.100324 (* 1 = 0.100324 loss)
I1012 16:56:10.829819 31118 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I1012 16:56:43.899116 31118 solver.cpp:514] Iteration 48000, Testing net (#0)
I1012 16:57:06.330031 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:57:06.451906 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.524204 (* 1 = 0.524204 loss)
I1012 16:57:06.451934 31118 solver.cpp:580]     Test net output #1: prob = 0.853002
I1012 16:57:06.451941 31118 solver.cpp:593]     Max_acc: 0.892701  with iter: 34500
I1012 16:57:06.750186 31118 solver.cpp:357] Iteration 48000 (1.78825 iter/s, 55.9205s/100 iters), loss = 0.121224
I1012 16:57:06.750257 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.10443 (* 1 = 0.10443 loss)
I1012 16:57:06.750267 31118 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I1012 16:57:06.750272 31118 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I1012 16:57:21.451831 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:57:39.985666 31118 solver.cpp:357] Iteration 48100 (3.00872 iter/s, 33.2367s/100 iters), loss = 0.11651
I1012 16:57:39.985735 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.139843 (* 1 = 0.139843 loss)
I1012 16:57:39.985746 31118 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I1012 16:58:13.215023 31118 solver.cpp:357] Iteration 48200 (3.00947 iter/s, 33.2284s/100 iters), loss = 0.179216
I1012 16:58:13.215215 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.190019 (* 1 = 0.190019 loss)
I1012 16:58:13.215229 31118 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I1012 16:58:46.448087 31118 solver.cpp:357] Iteration 48300 (3.00914 iter/s, 33.2321s/100 iters), loss = 0.0976938
I1012 16:58:46.448251 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.167181 (* 1 = 0.167181 loss)
I1012 16:58:46.448262 31118 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I1012 16:59:19.439844 31118 solver.cpp:357] Iteration 48400 (3.03115 iter/s, 32.9908s/100 iters), loss = 0.0778419
I1012 16:59:19.439971 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0908831 (* 1 = 0.0908831 loss)
I1012 16:59:19.439982 31118 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I1012 16:59:30.787892 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 16:59:52.307696 31118 solver.cpp:514] Iteration 48500, Testing net (#0)
I1012 17:00:14.730587 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:00:14.855612 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.266061 (* 1 = 0.266061 loss)
I1012 17:00:14.855638 31118 solver.cpp:580]     Test net output #1: prob = 0.917902
I1012 17:00:14.855651 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_48500.caffemodel
I1012 17:00:14.866323 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_48500.solverstate
I1012 17:00:14.868755 31118 solver.cpp:593]     Max_acc: 0.917902  with iter: 48500
I1012 17:00:15.183432 31118 solver.cpp:357] Iteration 48500 (1.79393 iter/s, 55.7434s/100 iters), loss = 0.0564899
I1012 17:00:15.183521 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0695249 (* 1 = 0.0695249 loss)
I1012 17:00:15.183533 31118 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I1012 17:00:48.509337 31118 solver.cpp:357] Iteration 48600 (3.00076 iter/s, 33.3249s/100 iters), loss = 0.0624404
I1012 17:00:48.509538 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.072787 (* 1 = 0.072787 loss)
I1012 17:00:48.509549 31118 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I1012 17:01:21.669667 31118 solver.cpp:357] Iteration 48700 (3.01557 iter/s, 33.1612s/100 iters), loss = 0.0567933
I1012 17:01:21.669792 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0619515 (* 1 = 0.0619515 loss)
I1012 17:01:21.669804 31118 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I1012 17:01:54.969831 31118 solver.cpp:357] Iteration 48800 (3.00289 iter/s, 33.3012s/100 iters), loss = 0.129415
I1012 17:01:54.969969 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.136225 (* 1 = 0.136225 loss)
I1012 17:01:54.969981 31118 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I1012 17:02:03.454326 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:02:28.136968 31118 solver.cpp:357] Iteration 48900 (3.01514 iter/s, 33.166s/100 iters), loss = 0.0722987
I1012 17:02:28.137104 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0353071 (* 1 = 0.0353071 loss)
I1012 17:02:28.137117 31118 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I1012 17:03:01.046818 31118 solver.cpp:514] Iteration 49000, Testing net (#0)
I1012 17:03:23.315541 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:03:23.448997 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.252886 (* 1 = 0.252886 loss)
I1012 17:03:23.449041 31118 solver.cpp:580]     Test net output #1: prob = 0.920702
I1012 17:03:23.449057 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_49000.caffemodel
I1012 17:03:23.459905 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_49000.solverstate
I1012 17:03:23.462554 31118 solver.cpp:593]     Max_acc: 0.920702  with iter: 49000
I1012 17:03:23.702020 31118 solver.cpp:357] Iteration 49000 (1.7997 iter/s, 55.5648s/100 iters), loss = 0.0493888
I1012 17:03:23.702088 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.064026 (* 1 = 0.064026 loss)
I1012 17:03:23.702100 31118 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I1012 17:03:57.069979 31118 solver.cpp:357] Iteration 49100 (2.99698 iter/s, 33.3669s/100 iters), loss = 0.0782681
I1012 17:03:57.070129 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.11792 (* 1 = 0.11792 loss)
I1012 17:03:57.070142 31118 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I1012 17:04:30.276396 31118 solver.cpp:357] Iteration 49200 (3.01137 iter/s, 33.2074s/100 iters), loss = 0.0161136
I1012 17:04:30.276563 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0135799 (* 1 = 0.0135799 loss)
I1012 17:04:30.276574 31118 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I1012 17:04:35.605265 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:05:03.543407 31118 solver.cpp:357] Iteration 49300 (3.00607 iter/s, 33.266s/100 iters), loss = 0.0874665
I1012 17:05:03.543552 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0920596 (* 1 = 0.0920596 loss)
I1012 17:05:03.543565 31118 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I1012 17:05:36.645267 31118 solver.cpp:357] Iteration 49400 (3.02089 iter/s, 33.1029s/100 iters), loss = 0.0693863
I1012 17:05:36.645443 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0457693 (* 1 = 0.0457693 loss)
I1012 17:05:36.645454 31118 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I1012 17:06:09.535651 31118 solver.cpp:514] Iteration 49500, Testing net (#0)
I1012 17:06:31.868893 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:06:31.889834 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.249197 (* 1 = 0.249197 loss)
I1012 17:06:31.889863 31118 solver.cpp:580]     Test net output #1: prob = 0.921602
I1012 17:06:31.889876 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_49500.caffemodel
I1012 17:06:31.898888 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_49500.solverstate
I1012 17:06:31.901214 31118 solver.cpp:593]     Max_acc: 0.921602  with iter: 49500
I1012 17:06:32.200537 31118 solver.cpp:357] Iteration 49500 (1.80002 iter/s, 55.555s/100 iters), loss = 0.0670143
I1012 17:06:32.200600 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0518657 (* 1 = 0.0518657 loss)
I1012 17:06:32.200613 31118 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I1012 17:07:05.371944 31118 solver.cpp:357] Iteration 49600 (3.01474 iter/s, 33.1703s/100 iters), loss = 0.032534
I1012 17:07:05.372081 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0563768 (* 1 = 0.0563768 loss)
I1012 17:07:05.372092 31118 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I1012 17:07:07.560667 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:07:38.567593 31118 solver.cpp:357] Iteration 49700 (3.01236 iter/s, 33.1966s/100 iters), loss = 0.0442274
I1012 17:07:38.567777 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0277624 (* 1 = 0.0277624 loss)
I1012 17:07:38.567790 31118 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I1012 17:08:11.852457 31118 solver.cpp:357] Iteration 49800 (3.00447 iter/s, 33.2838s/100 iters), loss = 0.0371426
I1012 17:08:11.852630 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0212245 (* 1 = 0.0212245 loss)
I1012 17:08:11.852644 31118 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I1012 17:08:45.007144 31118 solver.cpp:357] Iteration 49900 (3.01626 iter/s, 33.1537s/100 iters), loss = 0.043087
I1012 17:08:45.007321 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.042318 (* 1 = 0.042318 loss)
I1012 17:08:45.007334 31118 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I1012 17:09:17.239719 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:09:17.881247 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_50000.caffemodel
I1012 17:09:17.894740 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_50000.solverstate
I1012 17:09:17.897835 31118 solver.cpp:514] Iteration 50000, Testing net (#0)
I1012 17:09:40.420792 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:09:40.542881 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.253021 (* 1 = 0.253021 loss)
I1012 17:09:40.542908 31118 solver.cpp:580]     Test net output #1: prob = 0.920102
I1012 17:09:40.542914 31118 solver.cpp:593]     Max_acc: 0.921602  with iter: 49500
I1012 17:09:40.852649 31118 solver.cpp:357] Iteration 50000 (1.79066 iter/s, 55.8452s/100 iters), loss = 0.0435872
I1012 17:09:40.852727 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.048791 (* 1 = 0.048791 loss)
I1012 17:09:40.852741 31118 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I1012 17:10:13.991123 31118 solver.cpp:357] Iteration 50100 (3.01774 iter/s, 33.1374s/100 iters), loss = 0.0758469
I1012 17:10:13.991242 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0973236 (* 1 = 0.0973236 loss)
I1012 17:10:13.991255 31118 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I1012 17:10:47.192395 31118 solver.cpp:357] Iteration 50200 (3.01184 iter/s, 33.2023s/100 iters), loss = 0.0355236
I1012 17:10:47.192514 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.026536 (* 1 = 0.026536 loss)
I1012 17:10:47.192526 31118 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I1012 17:11:20.453568 31118 solver.cpp:357] Iteration 50300 (3.00661 iter/s, 33.2601s/100 iters), loss = 0.0435897
I1012 17:11:20.453677 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.045352 (* 1 = 0.045352 loss)
I1012 17:11:20.453688 31118 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I1012 17:11:49.626776 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:11:53.577644 31118 solver.cpp:357] Iteration 50400 (3.01943 iter/s, 33.1188s/100 iters), loss = 0.0994391
I1012 17:11:53.578001 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0818851 (* 1 = 0.0818851 loss)
I1012 17:11:53.578014 31118 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I1012 17:12:26.647007 31118 solver.cpp:514] Iteration 50500, Testing net (#0)
I1012 17:12:49.047863 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:12:49.168833 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.255861 (* 1 = 0.255861 loss)
I1012 17:12:49.168859 31118 solver.cpp:580]     Test net output #1: prob = 0.921802
I1012 17:12:49.168871 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_50500.caffemodel
I1012 17:12:49.178845 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_50500.solverstate
I1012 17:12:49.181313 31118 solver.cpp:593]     Max_acc: 0.921802  with iter: 50500
I1012 17:12:49.463891 31118 solver.cpp:357] Iteration 50500 (1.78968 iter/s, 55.8759s/100 iters), loss = 0.0592105
I1012 17:12:49.463959 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0632075 (* 1 = 0.0632075 loss)
I1012 17:12:49.463970 31118 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I1012 17:13:22.807082 31118 solver.cpp:357] Iteration 50600 (2.99952 iter/s, 33.3387s/100 iters), loss = 0.0362239
I1012 17:13:22.807245 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0292007 (* 1 = 0.0292007 loss)
I1012 17:13:22.807260 31118 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I1012 17:13:55.908589 31118 solver.cpp:357] Iteration 50700 (3.02159 iter/s, 33.0952s/100 iters), loss = 0.0621552
I1012 17:13:55.908771 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0587313 (* 1 = 0.0587313 loss)
I1012 17:13:55.908784 31118 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I1012 17:14:22.084367 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:14:29.173411 31118 solver.cpp:357] Iteration 50800 (3.00673 iter/s, 33.2587s/100 iters), loss = 0.0784787
I1012 17:14:29.173576 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.033544 (* 1 = 0.033544 loss)
I1012 17:14:29.173588 31118 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I1012 17:15:02.601161 31118 solver.cpp:357] Iteration 50900 (2.99205 iter/s, 33.4219s/100 iters), loss = 0.0549412
I1012 17:15:02.601301 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0306617 (* 1 = 0.0306617 loss)
I1012 17:15:02.601313 31118 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I1012 17:15:35.424471 31118 solver.cpp:514] Iteration 51000, Testing net (#0)
I1012 17:15:57.742877 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:15:57.865063 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.256956 (* 1 = 0.256956 loss)
I1012 17:15:57.865092 31118 solver.cpp:580]     Test net output #1: prob = 0.920802
I1012 17:15:57.865097 31118 solver.cpp:593]     Max_acc: 0.921802  with iter: 50500
I1012 17:15:58.179611 31118 solver.cpp:357] Iteration 51000 (1.79951 iter/s, 55.5707s/100 iters), loss = 0.0572435
I1012 17:15:58.179666 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0694619 (* 1 = 0.0694619 loss)
I1012 17:15:58.179677 31118 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I1012 17:16:31.353770 31118 solver.cpp:357] Iteration 51100 (3.01487 iter/s, 33.1689s/100 iters), loss = 0.0359864
I1012 17:16:31.353881 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0150312 (* 1 = 0.0150312 loss)
I1012 17:16:31.353893 31118 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I1012 17:16:54.249420 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:17:04.572876 31118 solver.cpp:357] Iteration 51200 (3.01058 iter/s, 33.2161s/100 iters), loss = 0.0950602
I1012 17:17:04.572996 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.121933 (* 1 = 0.121933 loss)
I1012 17:17:04.573009 31118 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I1012 17:17:37.792824 31118 solver.cpp:357] Iteration 51300 (3.01068 iter/s, 33.2151s/100 iters), loss = 0.0701552
I1012 17:17:37.793036 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0821651 (* 1 = 0.0821651 loss)
I1012 17:17:37.793049 31118 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I1012 17:18:11.052666 31118 solver.cpp:357] Iteration 51400 (3.00705 iter/s, 33.2551s/100 iters), loss = 0.020149
I1012 17:18:11.052853 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.00903879 (* 1 = 0.00903879 loss)
I1012 17:18:11.052865 31118 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I1012 17:18:43.996716 31118 solver.cpp:514] Iteration 51500, Testing net (#0)
I1012 17:19:06.222362 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:19:06.346031 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.251141 (* 1 = 0.251141 loss)
I1012 17:19:06.346056 31118 solver.cpp:580]     Test net output #1: prob = 0.924502
I1012 17:19:06.346071 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_51500.caffemodel
I1012 17:19:06.356546 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_51500.solverstate
I1012 17:19:06.358960 31118 solver.cpp:593]     Max_acc: 0.924502  with iter: 51500
I1012 17:19:06.665539 31118 solver.cpp:357] Iteration 51500 (1.79834 iter/s, 55.6069s/100 iters), loss = 0.0287382
I1012 17:19:06.665637 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0298016 (* 1 = 0.0298016 loss)
I1012 17:19:06.665649 31118 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I1012 17:19:26.502506 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:19:39.873129 31118 solver.cpp:357] Iteration 51600 (3.01175 iter/s, 33.2033s/100 iters), loss = 0.0457745
I1012 17:19:39.873208 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0544606 (* 1 = 0.0544606 loss)
I1012 17:19:39.873220 31118 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I1012 17:20:13.242347 31118 solver.cpp:357] Iteration 51700 (2.99715 iter/s, 33.3651s/100 iters), loss = 0.0943899
I1012 17:20:13.242457 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0653474 (* 1 = 0.0653474 loss)
I1012 17:20:13.242468 31118 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I1012 17:20:46.618367 31118 solver.cpp:357] Iteration 51800 (2.99634 iter/s, 33.3741s/100 iters), loss = 0.0335632
I1012 17:20:46.618502 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0473021 (* 1 = 0.0473021 loss)
I1012 17:20:46.618515 31118 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I1012 17:21:19.870532 31118 solver.cpp:357] Iteration 51900 (3.00768 iter/s, 33.2483s/100 iters), loss = 0.0425367
I1012 17:21:19.870718 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0229941 (* 1 = 0.0229941 loss)
I1012 17:21:19.870730 31118 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I1012 17:21:36.767585 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:21:52.961617 31118 solver.cpp:514] Iteration 52000, Testing net (#0)
I1012 17:22:15.174886 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:22:15.296087 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.252185 (* 1 = 0.252185 loss)
I1012 17:22:15.296113 31118 solver.cpp:580]     Test net output #1: prob = 0.924002
I1012 17:22:15.296119 31118 solver.cpp:593]     Max_acc: 0.924502  with iter: 51500
I1012 17:22:15.605360 31118 solver.cpp:357] Iteration 52000 (1.79437 iter/s, 55.73s/100 iters), loss = 0.0397102
I1012 17:22:15.605415 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0543056 (* 1 = 0.0543056 loss)
I1012 17:22:15.605429 31118 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I1012 17:22:48.641839 31118 solver.cpp:357] Iteration 52100 (3.02729 iter/s, 33.0329s/100 iters), loss = 0.0939443
I1012 17:22:48.641942 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0986885 (* 1 = 0.0986885 loss)
I1012 17:22:48.641952 31118 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I1012 17:23:21.837432 31118 solver.cpp:357] Iteration 52200 (3.01278 iter/s, 33.192s/100 iters), loss = 0.0560959
I1012 17:23:21.837702 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0703997 (* 1 = 0.0703997 loss)
I1012 17:23:21.837718 31118 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I1012 17:23:55.126801 31118 solver.cpp:357] Iteration 52300 (3.00427 iter/s, 33.2859s/100 iters), loss = 0.0801972
I1012 17:23:55.126922 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.081296 (* 1 = 0.081296 loss)
I1012 17:23:55.126935 31118 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I1012 17:24:08.733870 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:24:28.336521 31118 solver.cpp:357] Iteration 52400 (3.01147 iter/s, 33.2064s/100 iters), loss = 0.0281846
I1012 17:24:28.336663 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0373361 (* 1 = 0.0373361 loss)
I1012 17:24:28.336678 31118 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I1012 17:25:01.367168 31118 solver.cpp:514] Iteration 52500, Testing net (#0)
I1012 17:25:23.684427 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:25:23.795358 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.260939 (* 1 = 0.260939 loss)
I1012 17:25:23.795401 31118 solver.cpp:580]     Test net output #1: prob = 0.923302
I1012 17:25:23.795408 31118 solver.cpp:593]     Max_acc: 0.924502  with iter: 51500
I1012 17:25:24.038713 31118 solver.cpp:357] Iteration 52500 (1.79539 iter/s, 55.6982s/100 iters), loss = 0.0524692
I1012 17:25:24.038784 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0246224 (* 1 = 0.0246224 loss)
I1012 17:25:24.038796 31118 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I1012 17:25:57.423977 31118 solver.cpp:357] Iteration 52600 (2.99562 iter/s, 33.3821s/100 iters), loss = 0.0786727
I1012 17:25:57.424114 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0417267 (* 1 = 0.0417267 loss)
I1012 17:25:57.424127 31118 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I1012 17:26:30.548732 31118 solver.cpp:357] Iteration 52700 (3.01899 iter/s, 33.1237s/100 iters), loss = 0.0815366
I1012 17:26:30.548900 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0659312 (* 1 = 0.0659312 loss)
I1012 17:26:30.548913 31118 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I1012 17:26:41.068045 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:27:03.747841 31118 solver.cpp:357] Iteration 52800 (3.01241 iter/s, 33.1961s/100 iters), loss = 0.0413753
I1012 17:27:03.748003 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0317783 (* 1 = 0.0317783 loss)
I1012 17:27:03.748014 31118 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I1012 17:27:36.994379 31118 solver.cpp:357] Iteration 52900 (3.0081 iter/s, 33.2435s/100 iters), loss = 0.0316307
I1012 17:27:36.994565 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0232708 (* 1 = 0.0232708 loss)
I1012 17:27:36.994580 31118 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I1012 17:28:09.816295 31118 solver.cpp:514] Iteration 53000, Testing net (#0)
I1012 17:28:32.212568 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:28:32.332711 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.253295 (* 1 = 0.253295 loss)
I1012 17:28:32.332741 31118 solver.cpp:580]     Test net output #1: prob = 0.925103
I1012 17:28:32.332754 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_53000.caffemodel
I1012 17:28:32.344394 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_53000.solverstate
I1012 17:28:32.346902 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:28:32.614272 31118 solver.cpp:357] Iteration 53000 (1.79803 iter/s, 55.6165s/100 iters), loss = 0.0500709
I1012 17:28:32.614339 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0804404 (* 1 = 0.0804404 loss)
I1012 17:28:32.614351 31118 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I1012 17:29:05.936197 31118 solver.cpp:357] Iteration 53100 (3.00109 iter/s, 33.3212s/100 iters), loss = 0.0509563
I1012 17:29:05.936379 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0328432 (* 1 = 0.0328432 loss)
I1012 17:29:05.936391 31118 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I1012 17:29:13.318022 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:29:39.145781 31118 solver.cpp:357] Iteration 53200 (3.01143 iter/s, 33.2068s/100 iters), loss = 0.0560391
I1012 17:29:39.145943 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0623295 (* 1 = 0.0623295 loss)
I1012 17:29:39.145957 31118 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I1012 17:30:12.417835 31118 solver.cpp:357] Iteration 53300 (3.00578 iter/s, 33.2693s/100 iters), loss = 0.0378176
I1012 17:30:12.417958 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0530567 (* 1 = 0.0530567 loss)
I1012 17:30:12.417969 31118 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I1012 17:30:45.546828 31118 solver.cpp:357] Iteration 53400 (3.01875 iter/s, 33.1263s/100 iters), loss = 0.0255221
I1012 17:30:45.547070 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0169156 (* 1 = 0.0169156 loss)
I1012 17:30:45.547116 31118 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I1012 17:31:18.455816 31118 solver.cpp:514] Iteration 53500, Testing net (#0)
I1012 17:31:40.632053 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:31:40.755502 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.257784 (* 1 = 0.257784 loss)
I1012 17:31:40.755529 31118 solver.cpp:580]     Test net output #1: prob = 0.923802
I1012 17:31:40.755535 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:31:41.068063 31118 solver.cpp:357] Iteration 53500 (1.80121 iter/s, 55.5182s/100 iters), loss = 0.038385
I1012 17:31:41.068114 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0182467 (* 1 = 0.0182467 loss)
I1012 17:31:41.068127 31118 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I1012 17:31:45.518016 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:32:14.515429 31118 solver.cpp:357] Iteration 53600 (2.99001 iter/s, 33.4447s/100 iters), loss = 0.0176369
I1012 17:32:14.515550 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0166274 (* 1 = 0.0166274 loss)
I1012 17:32:14.515561 31118 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I1012 17:32:47.524544 31118 solver.cpp:357] Iteration 53700 (3.02971 iter/s, 33.0065s/100 iters), loss = 0.0184647
I1012 17:32:47.524706 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.00945774 (* 1 = 0.00945774 loss)
I1012 17:32:47.524718 31118 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I1012 17:33:20.774371 31118 solver.cpp:357] Iteration 53800 (3.00777 iter/s, 33.2472s/100 iters), loss = 0.037599
I1012 17:33:20.774509 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0534441 (* 1 = 0.0534441 loss)
I1012 17:33:20.774523 31118 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I1012 17:33:54.109380 31118 solver.cpp:357] Iteration 53900 (3.00008 iter/s, 33.3324s/100 iters), loss = 0.042938
I1012 17:33:54.109532 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0297734 (* 1 = 0.0297734 loss)
I1012 17:33:54.109545 31118 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I1012 17:33:55.278743 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:34:26.853775 31118 solver.cpp:514] Iteration 54000, Testing net (#0)
I1012 17:34:49.275635 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:34:49.398619 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.261672 (* 1 = 0.261672 loss)
I1012 17:34:49.398648 31118 solver.cpp:580]     Test net output #1: prob = 0.922502
I1012 17:34:49.398653 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:34:49.712963 31118 solver.cpp:357] Iteration 54000 (1.79853 iter/s, 55.6009s/100 iters), loss = 0.0385835
I1012 17:34:49.713017 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0303928 (* 1 = 0.0303928 loss)
I1012 17:34:49.713029 31118 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I1012 17:35:22.841606 31118 solver.cpp:357] Iteration 54100 (3.01876 iter/s, 33.1261s/100 iters), loss = 0.0607679
I1012 17:35:22.841855 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.045484 (* 1 = 0.045484 loss)
I1012 17:35:22.841867 31118 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I1012 17:35:55.982926 31118 solver.cpp:357] Iteration 54200 (3.01761 iter/s, 33.1388s/100 iters), loss = 0.0338467
I1012 17:35:55.983090 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0330804 (* 1 = 0.0330804 loss)
I1012 17:35:55.983103 31118 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I1012 17:36:27.081048 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:36:28.993826 31118 solver.cpp:357] Iteration 54300 (3.02953 iter/s, 33.0085s/100 iters), loss = 0.086971
I1012 17:36:28.993899 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0647388 (* 1 = 0.0647388 loss)
I1012 17:36:28.993912 31118 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I1012 17:37:02.228507 31118 solver.cpp:357] Iteration 54400 (3.00912 iter/s, 33.2323s/100 iters), loss = 0.0515273
I1012 17:37:02.228678 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0271485 (* 1 = 0.0271485 loss)
I1012 17:37:02.228690 31118 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I1012 17:37:35.161358 31118 solver.cpp:514] Iteration 54500, Testing net (#0)
I1012 17:37:57.494200 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:37:57.625769 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.257152 (* 1 = 0.257152 loss)
I1012 17:37:57.625798 31118 solver.cpp:580]     Test net output #1: prob = 0.924302
I1012 17:37:57.625804 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:37:57.847508 31118 solver.cpp:357] Iteration 54500 (1.79803 iter/s, 55.6165s/100 iters), loss = 0.0300584
I1012 17:37:57.847625 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0358367 (* 1 = 0.0358367 loss)
I1012 17:37:57.847653 31118 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I1012 17:38:31.011114 31118 solver.cpp:357] Iteration 54600 (3.01557 iter/s, 33.1612s/100 iters), loss = 0.0402608
I1012 17:38:31.011307 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0304524 (* 1 = 0.0304524 loss)
I1012 17:38:31.011319 31118 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I1012 17:38:59.021337 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:39:04.114711 31118 solver.cpp:357] Iteration 54700 (3.02104 iter/s, 33.1012s/100 iters), loss = 0.0961871
I1012 17:39:04.114835 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0573534 (* 1 = 0.0573534 loss)
I1012 17:39:04.114847 31118 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I1012 17:39:37.327746 31118 solver.cpp:357] Iteration 54800 (3.01108 iter/s, 33.2107s/100 iters), loss = 0.0261011
I1012 17:39:37.327867 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0339675 (* 1 = 0.0339675 loss)
I1012 17:39:37.327877 31118 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I1012 17:40:10.553637 31118 solver.cpp:357] Iteration 54900 (3.00991 iter/s, 33.2236s/100 iters), loss = 0.0478789
I1012 17:40:10.553776 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0397677 (* 1 = 0.0397677 loss)
I1012 17:40:10.553788 31118 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I1012 17:40:43.458397 31118 solver.cpp:514] Iteration 55000, Testing net (#0)
I1012 17:41:05.831023 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:41:05.962946 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.258238 (* 1 = 0.258238 loss)
I1012 17:41:05.962991 31118 solver.cpp:580]     Test net output #1: prob = 0.922602
I1012 17:41:05.962998 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:41:06.201799 31118 solver.cpp:357] Iteration 55000 (1.79708 iter/s, 55.6458s/100 iters), loss = 0.0298234
I1012 17:41:06.201872 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0500993 (* 1 = 0.0500993 loss)
I1012 17:41:06.201884 31118 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I1012 17:41:31.326297 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:41:39.411856 31118 solver.cpp:357] Iteration 55100 (3.01135 iter/s, 33.2077s/100 iters), loss = 0.0452192
I1012 17:41:39.411931 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0231804 (* 1 = 0.0231804 loss)
I1012 17:41:39.411942 31118 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I1012 17:42:12.605911 31118 solver.cpp:357] Iteration 55200 (3.0128 iter/s, 33.1917s/100 iters), loss = 0.0434723
I1012 17:42:12.606053 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0350687 (* 1 = 0.0350687 loss)
I1012 17:42:12.606067 31118 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I1012 17:42:45.834661 31118 solver.cpp:357] Iteration 55300 (3.00965 iter/s, 33.2265s/100 iters), loss = 0.0354785
I1012 17:42:45.834779 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0353659 (* 1 = 0.0353659 loss)
I1012 17:42:45.834789 31118 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I1012 17:43:19.073204 31118 solver.cpp:357] Iteration 55400 (3.00876 iter/s, 33.2363s/100 iters), loss = 0.0230749
I1012 17:43:19.073350 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.00973209 (* 1 = 0.00973209 loss)
I1012 17:43:19.073364 31118 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I1012 17:43:41.016476 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:43:51.959848 31118 solver.cpp:514] Iteration 55500, Testing net (#0)
I1012 17:44:14.420604 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:44:14.549187 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.257661 (* 1 = 0.257661 loss)
I1012 17:44:14.549237 31118 solver.cpp:580]     Test net output #1: prob = 0.922802
I1012 17:44:14.549243 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:44:14.793138 31118 solver.cpp:357] Iteration 55500 (1.7947 iter/s, 55.7197s/100 iters), loss = 0.0265713
I1012 17:44:14.793193 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0392378 (* 1 = 0.0392378 loss)
I1012 17:44:14.793207 31118 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I1012 17:44:48.038738 31118 solver.cpp:357] Iteration 55600 (3.00812 iter/s, 33.2433s/100 iters), loss = 0.0319728
I1012 17:44:48.038836 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0186876 (* 1 = 0.0186876 loss)
I1012 17:44:48.038848 31118 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I1012 17:45:21.242596 31118 solver.cpp:357] Iteration 55700 (3.01172 iter/s, 33.2037s/100 iters), loss = 0.0506687
I1012 17:45:21.242703 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0211731 (* 1 = 0.0211731 loss)
I1012 17:45:21.242714 31118 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I1012 17:45:54.377614 31118 solver.cpp:357] Iteration 55800 (3.01763 iter/s, 33.1386s/100 iters), loss = 0.0244098
I1012 17:45:54.377740 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0296334 (* 1 = 0.0296334 loss)
I1012 17:45:54.377753 31118 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I1012 17:46:13.306185 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:46:27.615239 31118 solver.cpp:357] Iteration 55900 (3.00843 iter/s, 33.24s/100 iters), loss = 0.0408856
I1012 17:46:27.615386 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0407101 (* 1 = 0.0407101 loss)
I1012 17:46:27.615399 31118 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I1012 17:47:00.556839 31118 solver.cpp:514] Iteration 56000, Testing net (#0)
I1012 17:47:23.113600 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:47:23.226574 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.26294 (* 1 = 0.26294 loss)
I1012 17:47:23.226622 31118 solver.cpp:580]     Test net output #1: prob = 0.923702
I1012 17:47:23.226632 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:47:23.476778 31118 solver.cpp:357] Iteration 56000 (1.78998 iter/s, 55.8664s/100 iters), loss = 0.0354142
I1012 17:47:23.476846 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0127634 (* 1 = 0.0127634 loss)
I1012 17:47:23.476858 31118 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I1012 17:47:56.648545 31118 solver.cpp:357] Iteration 56100 (3.01446 iter/s, 33.1735s/100 iters), loss = 0.0606416
I1012 17:47:56.648742 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0526203 (* 1 = 0.0526203 loss)
I1012 17:47:56.648754 31118 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I1012 17:48:29.751344 31118 solver.cpp:357] Iteration 56200 (3.02075 iter/s, 33.1043s/100 iters), loss = 0.0367938
I1012 17:48:29.751502 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0252602 (* 1 = 0.0252602 loss)
I1012 17:48:29.751513 31118 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I1012 17:48:45.504739 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:49:02.943357 31118 solver.cpp:357] Iteration 56300 (3.01265 iter/s, 33.1934s/100 iters), loss = 0.0196951
I1012 17:49:02.943496 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0239507 (* 1 = 0.0239507 loss)
I1012 17:49:02.943511 31118 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I1012 17:49:36.204746 31118 solver.cpp:357] Iteration 56400 (3.00639 iter/s, 33.2625s/100 iters), loss = 0.0405281
I1012 17:49:36.204861 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0483783 (* 1 = 0.0483783 loss)
I1012 17:49:36.204872 31118 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I1012 17:50:09.158337 31118 solver.cpp:514] Iteration 56500, Testing net (#0)
I1012 17:50:31.392045 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:50:31.459084 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.262502 (* 1 = 0.262502 loss)
I1012 17:50:31.459117 31118 solver.cpp:580]     Test net output #1: prob = 0.923102
I1012 17:50:31.459123 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:50:31.771327 31118 solver.cpp:357] Iteration 56500 (1.79955 iter/s, 55.5696s/100 iters), loss = 0.0193033
I1012 17:50:31.771383 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0246552 (* 1 = 0.0246552 loss)
I1012 17:50:31.771396 31118 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I1012 17:51:02.157893 31118 solver.cpp:357] Iteration 56600 (3.29088 iter/s, 30.387s/100 iters), loss = 0.0235569
I1012 17:51:02.158042 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0241046 (* 1 = 0.0241046 loss)
I1012 17:51:02.158054 31118 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I1012 17:51:14.788292 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:51:35.237707 31118 solver.cpp:357] Iteration 56700 (3.02275 iter/s, 33.0824s/100 iters), loss = 0.0309338
I1012 17:51:35.237862 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0390734 (* 1 = 0.0390734 loss)
I1012 17:51:35.237876 31118 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I1012 17:52:08.654016 31118 solver.cpp:357] Iteration 56800 (2.99251 iter/s, 33.4167s/100 iters), loss = 0.0333248
I1012 17:52:08.654170 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0406495 (* 1 = 0.0406495 loss)
I1012 17:52:08.654182 31118 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I1012 17:52:41.806772 31118 solver.cpp:357] Iteration 56900 (3.01613 iter/s, 33.1551s/100 iters), loss = 0.0451875
I1012 17:52:41.806890 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.045452 (* 1 = 0.045452 loss)
I1012 17:52:41.806902 31118 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I1012 17:53:14.728981 31118 solver.cpp:514] Iteration 57000, Testing net (#0)
I1012 17:53:37.080595 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:53:37.102571 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.262678 (* 1 = 0.262678 loss)
I1012 17:53:37.102599 31118 solver.cpp:580]     Test net output #1: prob = 0.921202
I1012 17:53:37.102605 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:53:37.407915 31118 solver.cpp:357] Iteration 57000 (1.79847 iter/s, 55.6029s/100 iters), loss = 0.0438823
I1012 17:53:37.407975 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0583798 (* 1 = 0.0583798 loss)
I1012 17:53:37.407987 31118 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I1012 17:53:46.892082 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:54:10.736173 31118 solver.cpp:357] Iteration 57100 (3.00046 iter/s, 33.3282s/100 iters), loss = 0.0201253
I1012 17:54:10.736244 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0157762 (* 1 = 0.0157762 loss)
I1012 17:54:10.736255 31118 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I1012 17:54:43.833186 31118 solver.cpp:357] Iteration 57200 (3.02124 iter/s, 33.099s/100 iters), loss = 0.0391631
I1012 17:54:43.833329 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0316231 (* 1 = 0.0316231 loss)
I1012 17:54:43.833341 31118 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I1012 17:55:16.899623 31118 solver.cpp:357] Iteration 57300 (3.02423 iter/s, 33.0662s/100 iters), loss = 0.0382716
I1012 17:55:16.899749 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0402903 (* 1 = 0.0402903 loss)
I1012 17:55:16.899760 31118 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I1012 17:55:50.148689 31118 solver.cpp:357] Iteration 57400 (3.00763 iter/s, 33.2488s/100 iters), loss = 0.0438521
I1012 17:55:50.148892 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0485714 (* 1 = 0.0485714 loss)
I1012 17:55:50.148907 31118 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I1012 17:55:56.565539 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:56:23.108899 31118 solver.cpp:514] Iteration 57500, Testing net (#0)
I1012 17:56:45.515765 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:56:45.627363 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.26634 (* 1 = 0.26634 loss)
I1012 17:56:45.627414 31118 solver.cpp:580]     Test net output #1: prob = 0.921602
I1012 17:56:45.627421 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:56:45.865408 31118 solver.cpp:357] Iteration 57500 (1.79477 iter/s, 55.7175s/100 iters), loss = 0.048637
I1012 17:56:45.865489 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0201193 (* 1 = 0.0201193 loss)
I1012 17:56:45.865504 31118 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I1012 17:57:19.184710 31118 solver.cpp:357] Iteration 57600 (3.00131 iter/s, 33.3188s/100 iters), loss = 0.0214129
I1012 17:57:19.189838 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0123481 (* 1 = 0.0123481 loss)
I1012 17:57:19.189858 31118 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I1012 17:57:52.396816 31118 solver.cpp:357] Iteration 57700 (3.01139 iter/s, 33.2072s/100 iters), loss = 0.0205394
I1012 17:57:52.396986 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0151765 (* 1 = 0.0151765 loss)
I1012 17:57:52.396999 31118 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I1012 17:58:25.653400 31118 solver.cpp:357] Iteration 57800 (3.00698 iter/s, 33.2559s/100 iters), loss = 0.0381137
I1012 17:58:25.653695 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0236365 (* 1 = 0.0236365 loss)
I1012 17:58:25.653786 31118 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I1012 17:58:28.895287 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:58:58.872020 31118 solver.cpp:357] Iteration 57900 (3.01042 iter/s, 33.2179s/100 iters), loss = 0.0254106
I1012 17:58:58.872200 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0286837 (* 1 = 0.0286837 loss)
I1012 17:58:58.872215 31118 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I1012 17:59:31.730770 31118 solver.cpp:514] Iteration 58000, Testing net (#0)
I1012 17:59:54.158612 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 17:59:54.288775 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.266597 (* 1 = 0.266597 loss)
I1012 17:59:54.288874 31118 solver.cpp:580]     Test net output #1: prob = 0.919502
I1012 17:59:54.288895 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 17:59:54.549360 31118 solver.cpp:357] Iteration 58000 (1.79606 iter/s, 55.6775s/100 iters), loss = 0.0195713
I1012 17:59:54.549437 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0183711 (* 1 = 0.0183711 loss)
I1012 17:59:54.549449 31118 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I1012 18:00:27.734572 31118 solver.cpp:357] Iteration 58100 (3.01347 iter/s, 33.1844s/100 iters), loss = 0.0624842
I1012 18:00:27.734746 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0648669 (* 1 = 0.0648669 loss)
I1012 18:00:27.734757 31118 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I1012 18:01:01.013074 31118 solver.cpp:357] Iteration 58200 (3.00502 iter/s, 33.2776s/100 iters), loss = 0.0629525
I1012 18:01:01.013262 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0905184 (* 1 = 0.0905184 loss)
I1012 18:01:01.013274 31118 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I1012 18:01:01.236021 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:01:34.065141 31118 solver.cpp:357] Iteration 58300 (3.02562 iter/s, 33.0511s/100 iters), loss = 0.0196405
I1012 18:01:34.067240 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0206793 (* 1 = 0.0206793 loss)
I1012 18:01:34.067260 31118 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I1012 18:02:07.297627 31118 solver.cpp:357] Iteration 58400 (3.00919 iter/s, 33.2315s/100 iters), loss = 0.0339504
I1012 18:02:07.297885 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0504887 (* 1 = 0.0504887 loss)
I1012 18:02:07.297915 31118 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I1012 18:02:40.204387 31118 solver.cpp:514] Iteration 58500, Testing net (#0)
I1012 18:03:02.338505 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:03:02.452378 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.270573 (* 1 = 0.270573 loss)
I1012 18:03:02.452409 31118 solver.cpp:580]     Test net output #1: prob = 0.920802
I1012 18:03:02.452416 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:03:02.766726 31118 solver.cpp:357] Iteration 58500 (1.80281 iter/s, 55.4689s/100 iters), loss = 0.0370394
I1012 18:03:02.766780 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0517146 (* 1 = 0.0517146 loss)
I1012 18:03:02.766795 31118 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I1012 18:03:33.149303 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:03:36.011624 31118 solver.cpp:357] Iteration 58600 (3.00808 iter/s, 33.2438s/100 iters), loss = 0.0559423
I1012 18:03:36.011693 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.054407 (* 1 = 0.054407 loss)
I1012 18:03:36.011703 31118 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I1012 18:04:09.173336 31118 solver.cpp:357] Iteration 58700 (3.01543 iter/s, 33.1627s/100 iters), loss = 0.0531268
I1012 18:04:09.173502 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.054603 (* 1 = 0.054603 loss)
I1012 18:04:09.173514 31118 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I1012 18:04:42.334647 31118 solver.cpp:357] Iteration 58800 (3.01567 iter/s, 33.1602s/100 iters), loss = 0.0364842
I1012 18:04:42.334780 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.03617 (* 1 = 0.03617 loss)
I1012 18:04:42.334794 31118 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I1012 18:05:15.535933 31118 solver.cpp:357] Iteration 58900 (3.01204 iter/s, 33.2001s/100 iters), loss = 0.0344438
I1012 18:05:15.536113 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0383282 (* 1 = 0.0383282 loss)
I1012 18:05:15.536125 31118 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I1012 18:05:42.634796 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:05:48.444042 31118 solver.cpp:514] Iteration 59000, Testing net (#0)
I1012 18:06:10.911504 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:06:10.946810 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.271831 (* 1 = 0.271831 loss)
I1012 18:06:10.946846 31118 solver.cpp:580]     Test net output #1: prob = 0.920902
I1012 18:06:10.946852 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:06:11.259281 31118 solver.cpp:357] Iteration 59000 (1.7946 iter/s, 55.7228s/100 iters), loss = 0.0277473
I1012 18:06:11.259335 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0336903 (* 1 = 0.0336903 loss)
I1012 18:06:11.259348 31118 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I1012 18:06:44.394194 31118 solver.cpp:357] Iteration 59100 (3.01808 iter/s, 33.1337s/100 iters), loss = 0.0426285
I1012 18:06:44.394423 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.06413 (* 1 = 0.06413 loss)
I1012 18:06:44.394433 31118 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I1012 18:07:17.590579 31118 solver.cpp:357] Iteration 59200 (3.0125 iter/s, 33.1951s/100 iters), loss = 0.0550544
I1012 18:07:17.590765 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0679834 (* 1 = 0.0679834 loss)
I1012 18:07:17.590780 31118 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I1012 18:07:50.971103 31118 solver.cpp:357] Iteration 59300 (2.99587 iter/s, 33.3792s/100 iters), loss = 0.0346454
I1012 18:07:50.971323 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0251401 (* 1 = 0.0251401 loss)
I1012 18:07:50.971350 31118 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I1012 18:08:14.958420 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:08:24.211292 31118 solver.cpp:357] Iteration 59400 (3.00853 iter/s, 33.2389s/100 iters), loss = 0.0415616
I1012 18:08:24.211442 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.051011 (* 1 = 0.051011 loss)
I1012 18:08:24.211460 31118 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I1012 18:08:57.138090 31118 solver.cpp:514] Iteration 59500, Testing net (#0)
I1012 18:09:19.709902 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:09:19.839608 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.266729 (* 1 = 0.266729 loss)
I1012 18:09:19.839634 31118 solver.cpp:580]     Test net output #1: prob = 0.921302
I1012 18:09:19.839640 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:09:20.080224 31118 solver.cpp:357] Iteration 59500 (1.78993 iter/s, 55.8682s/100 iters), loss = 0.0371297
I1012 18:09:20.080271 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0323725 (* 1 = 0.0323725 loss)
I1012 18:09:20.080286 31118 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I1012 18:09:53.358402 31118 solver.cpp:357] Iteration 59600 (3.0049 iter/s, 33.2789s/100 iters), loss = 0.0301244
I1012 18:09:53.358528 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0374773 (* 1 = 0.0374773 loss)
I1012 18:09:53.358541 31118 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I1012 18:10:26.356539 31118 solver.cpp:357] Iteration 59700 (3.0306 iter/s, 32.9967s/100 iters), loss = 0.0403695
I1012 18:10:26.356659 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0551273 (* 1 = 0.0551273 loss)
I1012 18:10:26.356672 31118 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I1012 18:10:47.386335 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:10:59.577877 31118 solver.cpp:357] Iteration 59800 (3.01024 iter/s, 33.2199s/100 iters), loss = 0.0151333
I1012 18:10:59.578017 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.020239 (* 1 = 0.020239 loss)
I1012 18:10:59.578030 31118 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I1012 18:11:32.662850 31118 solver.cpp:357] Iteration 59900 (3.02265 iter/s, 33.0836s/100 iters), loss = 0.0484532
I1012 18:11:32.662983 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0193946 (* 1 = 0.0193946 loss)
I1012 18:11:32.662997 31118 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I1012 18:12:05.664582 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_60000.caffemodel
I1012 18:12:05.679013 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_60000.solverstate
I1012 18:12:05.682111 31118 solver.cpp:514] Iteration 60000, Testing net (#0)
I1012 18:12:28.090271 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:12:28.164876 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.272151 (* 1 = 0.272151 loss)
I1012 18:12:28.164917 31118 solver.cpp:580]     Test net output #1: prob = 0.921703
I1012 18:12:28.164923 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:12:28.418000 31118 solver.cpp:357] Iteration 60000 (1.79358 iter/s, 55.7543s/100 iters), loss = 0.0167048
I1012 18:12:28.418072 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0119196 (* 1 = 0.0119196 loss)
I1012 18:12:28.418087 31118 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I1012 18:13:01.808661 31118 solver.cpp:357] Iteration 60100 (2.99498 iter/s, 33.3892s/100 iters), loss = 0.0234798
I1012 18:13:01.808842 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0339337 (* 1 = 0.0339337 loss)
I1012 18:13:01.808854 31118 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I1012 18:13:19.686748 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:13:35.034556 31118 solver.cpp:357] Iteration 60200 (3.00965 iter/s, 33.2265s/100 iters), loss = 0.0356458
I1012 18:13:35.034734 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0450196 (* 1 = 0.0450196 loss)
I1012 18:13:35.034745 31118 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I1012 18:14:08.429657 31118 solver.cpp:357] Iteration 60300 (2.99458 iter/s, 33.3936s/100 iters), loss = 0.0191904
I1012 18:14:08.429792 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0204994 (* 1 = 0.0204994 loss)
I1012 18:14:08.429806 31118 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I1012 18:14:41.572119 31118 solver.cpp:357] Iteration 60400 (3.01741 iter/s, 33.141s/100 iters), loss = 0.0202
I1012 18:14:41.572288 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0186707 (* 1 = 0.0186707 loss)
I1012 18:14:41.572299 31118 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I1012 18:15:14.633942 31118 solver.cpp:514] Iteration 60500, Testing net (#0)
I1012 18:15:36.809813 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:15:36.931375 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.272111 (* 1 = 0.272111 loss)
I1012 18:15:36.931403 31118 solver.cpp:580]     Test net output #1: prob = 0.922202
I1012 18:15:36.931409 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:15:37.226449 31118 solver.cpp:357] Iteration 60500 (1.79684 iter/s, 55.6534s/100 iters), loss = 0.0297331
I1012 18:15:37.226495 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0448299 (* 1 = 0.0448299 loss)
I1012 18:15:37.226507 31118 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I1012 18:15:51.947063 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:16:10.504366 31118 solver.cpp:357] Iteration 60600 (3.00495 iter/s, 33.2785s/100 iters), loss = 0.0461893
I1012 18:16:10.504447 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0299161 (* 1 = 0.0299161 loss)
I1012 18:16:10.504460 31118 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I1012 18:16:43.886504 31118 solver.cpp:357] Iteration 60700 (2.99575 iter/s, 33.3807s/100 iters), loss = 0.0158391
I1012 18:16:43.886693 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0121693 (* 1 = 0.0121693 loss)
I1012 18:16:43.886704 31118 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I1012 18:17:16.976152 31118 solver.cpp:357] Iteration 60800 (3.02223 iter/s, 33.0882s/100 iters), loss = 0.0354749
I1012 18:17:16.976338 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0257311 (* 1 = 0.0257311 loss)
I1012 18:17:16.976351 31118 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I1012 18:17:50.256357 31118 solver.cpp:357] Iteration 60900 (3.00493 iter/s, 33.2787s/100 iters), loss = 0.0135797
I1012 18:17:50.256559 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0148085 (* 1 = 0.0148085 loss)
I1012 18:17:50.256588 31118 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I1012 18:18:01.752694 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:18:23.262058 31118 solver.cpp:514] Iteration 61000, Testing net (#0)
I1012 18:18:45.359148 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:18:45.483214 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.271426 (* 1 = 0.271426 loss)
I1012 18:18:45.483242 31118 solver.cpp:580]     Test net output #1: prob = 0.919702
I1012 18:18:45.483248 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:18:45.795323 31118 solver.cpp:357] Iteration 61000 (1.80057 iter/s, 55.5379s/100 iters), loss = 0.0420239
I1012 18:18:45.795411 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0599218 (* 1 = 0.0599218 loss)
I1012 18:18:45.795425 31118 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I1012 18:19:18.985821 31118 solver.cpp:357] Iteration 61100 (3.01305 iter/s, 33.189s/100 iters), loss = 0.019635
I1012 18:19:18.986009 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0180917 (* 1 = 0.0180917 loss)
I1012 18:19:18.986022 31118 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I1012 18:19:52.413161 31118 solver.cpp:357] Iteration 61200 (2.99154 iter/s, 33.4276s/100 iters), loss = 0.0335128
I1012 18:19:52.413337 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0207138 (* 1 = 0.0207138 loss)
I1012 18:19:52.413350 31118 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I1012 18:20:25.540452 31118 solver.cpp:357] Iteration 61300 (3.01843 iter/s, 33.1298s/100 iters), loss = 0.0465469
I1012 18:20:25.540630 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0459182 (* 1 = 0.0459182 loss)
I1012 18:20:25.540643 31118 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I1012 18:20:34.110154 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:20:58.796201 31118 solver.cpp:357] Iteration 61400 (3.00679 iter/s, 33.258s/100 iters), loss = 0.030868
I1012 18:20:58.796346 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0172922 (* 1 = 0.0172922 loss)
I1012 18:20:58.796360 31118 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I1012 18:21:31.730417 31118 solver.cpp:514] Iteration 61500, Testing net (#0)
I1012 18:21:53.760507 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:21:53.780236 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.272357 (* 1 = 0.272357 loss)
I1012 18:21:53.780262 31118 solver.cpp:580]     Test net output #1: prob = 0.920602
I1012 18:21:53.780268 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:21:54.054342 31118 solver.cpp:357] Iteration 61500 (1.80953 iter/s, 55.263s/100 iters), loss = 0.0238319
I1012 18:21:54.054414 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.025167 (* 1 = 0.025167 loss)
I1012 18:21:54.054426 31118 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I1012 18:22:27.316188 31118 solver.cpp:357] Iteration 61600 (3.00629 iter/s, 33.2636s/100 iters), loss = 0.0385687
I1012 18:22:27.316481 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0512528 (* 1 = 0.0512528 loss)
I1012 18:22:27.316496 31118 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I1012 18:23:00.471757 31118 solver.cpp:357] Iteration 61700 (3.01594 iter/s, 33.1571s/100 iters), loss = 0.0106049
I1012 18:23:00.471907 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.015838 (* 1 = 0.015838 loss)
I1012 18:23:00.471920 31118 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I1012 18:23:05.933887 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:23:33.647322 31118 solver.cpp:357] Iteration 61800 (3.01395 iter/s, 33.179s/100 iters), loss = 0.0340576
I1012 18:23:33.647490 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0305391 (* 1 = 0.0305391 loss)
I1012 18:23:33.647500 31118 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I1012 18:24:06.947999 31118 solver.cpp:357] Iteration 61900 (3.00283 iter/s, 33.302s/100 iters), loss = 0.0148358
I1012 18:24:06.948184 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.00893556 (* 1 = 0.00893556 loss)
I1012 18:24:06.948199 31118 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I1012 18:24:39.937753 31118 solver.cpp:514] Iteration 62000, Testing net (#0)
I1012 18:25:02.343907 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:25:02.370640 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.268931 (* 1 = 0.268931 loss)
I1012 18:25:02.370673 31118 solver.cpp:580]     Test net output #1: prob = 0.922802
I1012 18:25:02.370679 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:25:02.629659 31118 solver.cpp:357] Iteration 62000 (1.79582 iter/s, 55.685s/100 iters), loss = 0.0268251
I1012 18:25:02.629716 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0110162 (* 1 = 0.0110162 loss)
I1012 18:25:02.629736 31118 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I1012 18:25:35.861335 31118 solver.cpp:357] Iteration 62100 (3.00909 iter/s, 33.2326s/100 iters), loss = 0.0199604
I1012 18:25:35.861481 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0200042 (* 1 = 0.0200042 loss)
I1012 18:25:35.861495 31118 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I1012 18:25:38.092634 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:26:09.054420 31118 solver.cpp:357] Iteration 62200 (3.0126 iter/s, 33.1939s/100 iters), loss = 0.0239768
I1012 18:26:09.054558 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0275433 (* 1 = 0.0275433 loss)
I1012 18:26:09.054570 31118 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I1012 18:26:42.356380 31118 solver.cpp:357] Iteration 62300 (3.00276 iter/s, 33.3027s/100 iters), loss = 0.0363867
I1012 18:26:42.356499 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0339677 (* 1 = 0.0339677 loss)
I1012 18:26:42.356509 31118 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I1012 18:27:15.616834 31118 solver.cpp:357] Iteration 62400 (3.00652 iter/s, 33.2611s/100 iters), loss = 0.0250428
I1012 18:27:15.616966 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0222311 (* 1 = 0.0222311 loss)
I1012 18:27:15.616979 31118 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I1012 18:27:47.898599 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:27:48.549141 31118 solver.cpp:514] Iteration 62500, Testing net (#0)
I1012 18:28:11.142292 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:28:11.269706 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.267564 (* 1 = 0.267564 loss)
I1012 18:28:11.269747 31118 solver.cpp:580]     Test net output #1: prob = 0.923802
I1012 18:28:11.269754 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:28:11.509707 31118 solver.cpp:357] Iteration 62500 (1.78906 iter/s, 55.8952s/100 iters), loss = 0.0430229
I1012 18:28:11.509766 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0183391 (* 1 = 0.0183391 loss)
I1012 18:28:11.509778 31118 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I1012 18:28:44.741418 31118 solver.cpp:357] Iteration 62600 (3.00896 iter/s, 33.2341s/100 iters), loss = 0.0257407
I1012 18:28:44.741582 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0220382 (* 1 = 0.0220382 loss)
I1012 18:28:44.741595 31118 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I1012 18:29:18.014433 31118 solver.cpp:357] Iteration 62700 (3.00541 iter/s, 33.2733s/100 iters), loss = 0.0440055
I1012 18:29:18.014585 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0422783 (* 1 = 0.0422783 loss)
I1012 18:29:18.014597 31118 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I1012 18:29:51.204838 31118 solver.cpp:357] Iteration 62800 (3.0129 iter/s, 33.1906s/100 iters), loss = 0.0788141
I1012 18:29:51.204967 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.101523 (* 1 = 0.101523 loss)
I1012 18:29:51.204980 31118 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I1012 18:30:20.380061 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:30:24.208741 31118 solver.cpp:357] Iteration 62900 (3.02993 iter/s, 33.004s/100 iters), loss = 0.0288929
I1012 18:30:24.208930 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0204406 (* 1 = 0.0204406 loss)
I1012 18:30:24.208943 31118 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I1012 18:30:57.134109 31118 solver.cpp:514] Iteration 63000, Testing net (#0)
I1012 18:31:19.369845 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:31:19.496481 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.26661 (* 1 = 0.26661 loss)
I1012 18:31:19.496578 31118 solver.cpp:580]     Test net output #1: prob = 0.923902
I1012 18:31:19.496598 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:31:19.745888 31118 solver.cpp:357] Iteration 63000 (1.80055 iter/s, 55.5387s/100 iters), loss = 0.0192429
I1012 18:31:19.745971 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0198717 (* 1 = 0.0198717 loss)
I1012 18:31:19.745985 31118 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I1012 18:31:52.995265 31118 solver.cpp:357] Iteration 63100 (3.00758 iter/s, 33.2493s/100 iters), loss = 0.020652
I1012 18:31:52.995451 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.020164 (* 1 = 0.020164 loss)
I1012 18:31:52.995462 31118 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I1012 18:32:26.248051 31118 solver.cpp:357] Iteration 63200 (3.00728 iter/s, 33.2527s/100 iters), loss = 0.0173602
I1012 18:32:26.248205 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0165839 (* 1 = 0.0165839 loss)
I1012 18:32:26.248217 31118 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I1012 18:32:52.366287 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:32:59.476692 31118 solver.cpp:357] Iteration 63300 (3.00947 iter/s, 33.2285s/100 iters), loss = 0.0548926
I1012 18:32:59.476855 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0277045 (* 1 = 0.0277045 loss)
I1012 18:32:59.476866 31118 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I1012 18:33:32.663029 31118 solver.cpp:357] Iteration 63400 (3.01331 iter/s, 33.1861s/100 iters), loss = 0.0578183
I1012 18:33:32.663233 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0395378 (* 1 = 0.0395378 loss)
I1012 18:33:32.663249 31118 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I1012 18:34:05.514439 31118 solver.cpp:514] Iteration 63500, Testing net (#0)
I1012 18:34:27.806268 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:34:27.928742 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.280119 (* 1 = 0.280119 loss)
I1012 18:34:27.928769 31118 solver.cpp:580]     Test net output #1: prob = 0.922102
I1012 18:34:27.928776 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:34:28.242130 31118 solver.cpp:357] Iteration 63500 (1.7992 iter/s, 55.5802s/100 iters), loss = 0.0333565
I1012 18:34:28.242213 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0267127 (* 1 = 0.0267127 loss)
I1012 18:34:28.242224 31118 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I1012 18:35:01.249088 31118 solver.cpp:357] Iteration 63600 (3.02969 iter/s, 33.0066s/100 iters), loss = 0.0229033
I1012 18:35:01.249208 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0222268 (* 1 = 0.0222268 loss)
I1012 18:35:01.249225 31118 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I1012 18:35:24.224537 31123 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:35:34.575119 31118 solver.cpp:357] Iteration 63700 (3.00069 iter/s, 33.3257s/100 iters), loss = 0.0232519
I1012 18:35:34.575222 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0232049 (* 1 = 0.0232049 loss)
I1012 18:35:34.575230 31118 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I1012 18:36:07.616017 31118 solver.cpp:357] Iteration 63800 (3.0266 iter/s, 33.0404s/100 iters), loss = 0.030346
I1012 18:36:07.616161 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0229588 (* 1 = 0.0229588 loss)
I1012 18:36:07.616171 31118 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I1012 18:36:40.798282 31118 solver.cpp:357] Iteration 63900 (3.01351 iter/s, 33.1839s/100 iters), loss = 0.023679
I1012 18:36:40.798409 31118 solver.cpp:376]     Train net output #0: Softmax1 = 0.0204957 (* 1 = 0.0204957 loss)
I1012 18:36:40.798421 31118 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I1012 18:37:13.515802 31118 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_64000.caffemodel
I1012 18:37:13.526643 31118 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_SE-ResNet_20_iter_64000.solverstate
I1012 18:37:13.580654 31118 solver.cpp:472] Iteration 64000, loss = 0.0277398
I1012 18:37:13.580718 31118 solver.cpp:514] Iteration 64000, Testing net (#0)
I1012 18:37:36.034004 31124 data_layer.cpp:73] Restarting data prefetching from start.
I1012 18:37:36.155839 31118 solver.cpp:580]     Test net output #0: Softmax1 = 0.274361 (* 1 = 0.274361 loss)
I1012 18:37:36.155894 31118 solver.cpp:580]     Test net output #1: prob = 0.923202
I1012 18:37:36.155900 31118 solver.cpp:593]     Max_acc: 0.925103  with iter: 53000
I1012 18:37:36.155908 31118 solver.cpp:479] Optimization Done.
I1012 18:37:36.155913 31118 caffe.cpp:326] Optimization Done.
