WARNING: Logging before InitGoogleLogging() is written to STDERR
I1002 12:07:04.505115 30882 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I1002 12:07:04.505225 30882 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I1002 12:07:04.505230 30882 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I1002 12:07:04.505234 30882 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I1002 12:07:04.505237 30882 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I1002 12:07:04.505241 30882 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I1002 12:07:04.505296 30882 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I1002 12:07:04.505461 30882 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I1002 12:07:04.507175 30882 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I1002 12:07:04.507215 30882 caffe.cpp:269] Using GPUs 0
I1002 12:07:04.532630 30882 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I1002 12:07:04.933049 30882 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I1002 12:07:04.933091 30882 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I1002 12:07:04.952644 30882 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_WRN_20.prototxt"
test_net: "./test_WRN_20.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_WRN_20"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 2
type: "Nesterov"
I1002 12:07:04.952905 30882 solver.cpp:167] Creating training net from train_net file: ./train_WRN_20.prototxt
I1002 12:07:04.953878 30882 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_WRN_20.prototxt
I1002 12:07:04.953896 30882 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I1002 12:07:04.954893 30882 net.cpp:82] Initializing net from parameters: 
name: "WRN-20"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_Drop1"
  type: "Dropout"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_down"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn_down"
  type: "BatchNorm"
  bottom: "conv2_1_down"
  top: "conv2_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale_down"
  type: "Scale"
  bottom: "conv2_1_down"
  top: "conv2_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_1"
  type: "Eltwise"
  bottom: "conv2_1_1"
  bottom: "conv2_1_down"
  top: "conv2_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_1"
  top: "conv2_Eltwise_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_Drop2"
  type: "Dropout"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_2"
  type: "Eltwise"
  bottom: "conv2_Eltwise_1"
  bottom: "conv2_2_1"
  top: "conv2_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_2"
  top: "conv2_Eltwise_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_Drop3"
  type: "Dropout"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_3"
  type: "Eltwise"
  bottom: "conv2_Eltwise_2"
  bottom: "conv2_3_1"
  top: "conv2_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_3"
  top: "conv2_Eltwise_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_Drop1"
  type: "Dropout"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_1"
  type: "Eltwise"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_1"
  top: "conv3_Eltwise_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_Drop2"
  type: "Dropout"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_2"
  type: "Eltwise"
  bottom: "conv3_Eltwise_1"
  bottom: "conv3_2_1"
  top: "conv3_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_2"
  top: "conv3_Eltwise_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_Drop3"
  type: "Dropout"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_3"
  type: "Eltwise"
  bottom: "conv3_Eltwise_2"
  bottom: "conv3_3_1"
  top: "conv3_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_3"
  top: "conv3_Eltwise_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_Drop1"
  type: "Dropout"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_1"
  type: "Eltwise"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_1"
  top: "conv4_Eltwise_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_Drop2"
  type: "Dropout"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_2"
  type: "Eltwise"
  bottom: "conv4_Eltwise_1"
  bottom: "conv4_2_1"
  top: "conv4_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_2"
  top: "conv4_Eltwise_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_Drop3"
  type: "Dropout"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_3"
  type: "Eltwise"
  bottom: "conv4_Eltwise_2"
  bottom: "conv4_3_1"
  top: "conv4_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_3"
  top: "conv4_Eltwise_3"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "conv4_Eltwise_3"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax1"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label"
  top: "Softmax1"
}
I1002 12:07:04.955406 30882 layer_factory.hpp:77] Creating layer Data1
I1002 12:07:04.955606 30882 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I1002 12:07:04.955649 30882 net.cpp:128] Creating Layer Data1
I1002 12:07:04.955659 30882 net.cpp:522] Data1 -> data
I1002 12:07:04.955688 30882 net.cpp:522] Data1 -> label
I1002 12:07:04.957252 30882 data_layer.cpp:45] output data size: 64,3,32,32
I1002 12:07:04.961894 30882 net.cpp:172] Setting up Data1
I1002 12:07:04.961956 30882 net.cpp:186] Top shape: 64 3 32 32 (196608)
I1002 12:07:04.961963 30882 net.cpp:186] Top shape: 64 (64)
I1002 12:07:04.961967 30882 net.cpp:194] Memory required for data: 786688
I1002 12:07:04.961980 30882 layer_factory.hpp:77] Creating layer conv1
I1002 12:07:04.962011 30882 net.cpp:128] Creating Layer conv1
I1002 12:07:04.962021 30882 net.cpp:558] conv1 <- data
I1002 12:07:04.962039 30882 net.cpp:522] conv1 -> conv1
I1002 12:07:05.707237 30882 net.cpp:172] Setting up conv1
I1002 12:07:05.707294 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.707299 30882 net.cpp:194] Memory required for data: 4980992
I1002 12:07:05.707336 30882 layer_factory.hpp:77] Creating layer conv1/bn
I1002 12:07:05.707360 30882 net.cpp:128] Creating Layer conv1/bn
I1002 12:07:05.707365 30882 net.cpp:558] conv1/bn <- conv1
I1002 12:07:05.707373 30882 net.cpp:509] conv1/bn -> conv1 (in-place)
I1002 12:07:05.707597 30882 net.cpp:172] Setting up conv1/bn
I1002 12:07:05.707608 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.707613 30882 net.cpp:194] Memory required for data: 9175296
I1002 12:07:05.707626 30882 layer_factory.hpp:77] Creating layer conv1/scale
I1002 12:07:05.707636 30882 net.cpp:128] Creating Layer conv1/scale
I1002 12:07:05.707640 30882 net.cpp:558] conv1/scale <- conv1
I1002 12:07:05.707646 30882 net.cpp:509] conv1/scale -> conv1 (in-place)
I1002 12:07:05.707693 30882 layer_factory.hpp:77] Creating layer conv1/scale
I1002 12:07:05.707815 30882 net.cpp:172] Setting up conv1/scale
I1002 12:07:05.707826 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.707830 30882 net.cpp:194] Memory required for data: 13369600
I1002 12:07:05.707839 30882 layer_factory.hpp:77] Creating layer conv1/ReLU
I1002 12:07:05.707845 30882 net.cpp:128] Creating Layer conv1/ReLU
I1002 12:07:05.707850 30882 net.cpp:558] conv1/ReLU <- conv1
I1002 12:07:05.707855 30882 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I1002 12:07:05.708067 30882 net.cpp:172] Setting up conv1/ReLU
I1002 12:07:05.708081 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.708086 30882 net.cpp:194] Memory required for data: 17563904
I1002 12:07:05.708089 30882 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I1002 12:07:05.708096 30882 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I1002 12:07:05.708101 30882 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I1002 12:07:05.708107 30882 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I1002 12:07:05.708115 30882 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I1002 12:07:05.708156 30882 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I1002 12:07:05.708163 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.708169 30882 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I1002 12:07:05.708173 30882 net.cpp:194] Memory required for data: 25952512
I1002 12:07:05.708178 30882 layer_factory.hpp:77] Creating layer conv2_1_0
I1002 12:07:05.708230 30882 net.cpp:128] Creating Layer conv2_1_0
I1002 12:07:05.708235 30882 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I1002 12:07:05.708241 30882 net.cpp:522] conv2_1_0 -> conv2_1_0
I1002 12:07:05.711165 30882 net.cpp:172] Setting up conv2_1_0
I1002 12:07:05.711194 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.711199 30882 net.cpp:194] Memory required for data: 59506944
I1002 12:07:05.711212 30882 layer_factory.hpp:77] Creating layer conv2_1_bn0
I1002 12:07:05.711225 30882 net.cpp:128] Creating Layer conv2_1_bn0
I1002 12:07:05.711230 30882 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I1002 12:07:05.711238 30882 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I1002 12:07:05.711454 30882 net.cpp:172] Setting up conv2_1_bn0
I1002 12:07:05.711465 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.711469 30882 net.cpp:194] Memory required for data: 93061376
I1002 12:07:05.711479 30882 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1002 12:07:05.711489 30882 net.cpp:128] Creating Layer conv2_1_scale0
I1002 12:07:05.711494 30882 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I1002 12:07:05.711500 30882 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I1002 12:07:05.711540 30882 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1002 12:07:05.711660 30882 net.cpp:172] Setting up conv2_1_scale0
I1002 12:07:05.711671 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.711675 30882 net.cpp:194] Memory required for data: 126615808
I1002 12:07:05.711683 30882 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I1002 12:07:05.711689 30882 net.cpp:128] Creating Layer conv2_1_ReLU0
I1002 12:07:05.711694 30882 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I1002 12:07:05.711701 30882 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I1002 12:07:05.711917 30882 net.cpp:172] Setting up conv2_1_ReLU0
I1002 12:07:05.711930 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.711935 30882 net.cpp:194] Memory required for data: 160170240
I1002 12:07:05.711939 30882 layer_factory.hpp:77] Creating layer conv2_Drop1
I1002 12:07:05.711947 30882 net.cpp:128] Creating Layer conv2_Drop1
I1002 12:07:05.711951 30882 net.cpp:558] conv2_Drop1 <- conv2_1_0
I1002 12:07:05.711959 30882 net.cpp:509] conv2_Drop1 -> conv2_1_0 (in-place)
I1002 12:07:05.711992 30882 net.cpp:172] Setting up conv2_Drop1
I1002 12:07:05.711998 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.712004 30882 net.cpp:194] Memory required for data: 193724672
I1002 12:07:05.712008 30882 layer_factory.hpp:77] Creating layer conv2_1_1
I1002 12:07:05.712020 30882 net.cpp:128] Creating Layer conv2_1_1
I1002 12:07:05.712025 30882 net.cpp:558] conv2_1_1 <- conv2_1_0
I1002 12:07:05.712034 30882 net.cpp:522] conv2_1_1 -> conv2_1_1
I1002 12:07:05.716828 30882 net.cpp:172] Setting up conv2_1_1
I1002 12:07:05.716861 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.716866 30882 net.cpp:194] Memory required for data: 227279104
I1002 12:07:05.716877 30882 layer_factory.hpp:77] Creating layer conv2_1bn1
I1002 12:07:05.716886 30882 net.cpp:128] Creating Layer conv2_1bn1
I1002 12:07:05.716892 30882 net.cpp:558] conv2_1bn1 <- conv2_1_1
I1002 12:07:05.716902 30882 net.cpp:509] conv2_1bn1 -> conv2_1_1 (in-place)
I1002 12:07:05.717109 30882 net.cpp:172] Setting up conv2_1bn1
I1002 12:07:05.717121 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.717125 30882 net.cpp:194] Memory required for data: 260833536
I1002 12:07:05.717139 30882 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1002 12:07:05.717146 30882 net.cpp:128] Creating Layer conv2_1_scale1
I1002 12:07:05.717150 30882 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I1002 12:07:05.717156 30882 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I1002 12:07:05.717195 30882 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1002 12:07:05.717300 30882 net.cpp:172] Setting up conv2_1_scale1
I1002 12:07:05.717311 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.717315 30882 net.cpp:194] Memory required for data: 294387968
I1002 12:07:05.717344 30882 layer_factory.hpp:77] Creating layer conv2_1_down
I1002 12:07:05.717355 30882 net.cpp:128] Creating Layer conv2_1_down
I1002 12:07:05.717358 30882 net.cpp:558] conv2_1_down <- conv1_conv1/ReLU_0_split_1
I1002 12:07:05.717365 30882 net.cpp:522] conv2_1_down -> conv2_1_down
I1002 12:07:05.719786 30882 net.cpp:172] Setting up conv2_1_down
I1002 12:07:05.719813 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.719817 30882 net.cpp:194] Memory required for data: 327942400
I1002 12:07:05.719827 30882 layer_factory.hpp:77] Creating layer conv2_1_bn_down
I1002 12:07:05.719836 30882 net.cpp:128] Creating Layer conv2_1_bn_down
I1002 12:07:05.719841 30882 net.cpp:558] conv2_1_bn_down <- conv2_1_down
I1002 12:07:05.719848 30882 net.cpp:509] conv2_1_bn_down -> conv2_1_down (in-place)
I1002 12:07:05.720060 30882 net.cpp:172] Setting up conv2_1_bn_down
I1002 12:07:05.720070 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720074 30882 net.cpp:194] Memory required for data: 361496832
I1002 12:07:05.720084 30882 layer_factory.hpp:77] Creating layer conv2_1_scale_down
I1002 12:07:05.720094 30882 net.cpp:128] Creating Layer conv2_1_scale_down
I1002 12:07:05.720098 30882 net.cpp:558] conv2_1_scale_down <- conv2_1_down
I1002 12:07:05.720104 30882 net.cpp:509] conv2_1_scale_down -> conv2_1_down (in-place)
I1002 12:07:05.720145 30882 layer_factory.hpp:77] Creating layer conv2_1_scale_down
I1002 12:07:05.720261 30882 net.cpp:172] Setting up conv2_1_scale_down
I1002 12:07:05.720271 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720275 30882 net.cpp:194] Memory required for data: 395051264
I1002 12:07:05.720283 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_1
I1002 12:07:05.720291 30882 net.cpp:128] Creating Layer conv2_Eltwise_1
I1002 12:07:05.720295 30882 net.cpp:558] conv2_Eltwise_1 <- conv2_1_1
I1002 12:07:05.720300 30882 net.cpp:558] conv2_Eltwise_1 <- conv2_1_down
I1002 12:07:05.720306 30882 net.cpp:522] conv2_Eltwise_1 -> conv2_Eltwise_1
I1002 12:07:05.720336 30882 net.cpp:172] Setting up conv2_Eltwise_1
I1002 12:07:05.720345 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720348 30882 net.cpp:194] Memory required for data: 428605696
I1002 12:07:05.720352 30882 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I1002 12:07:05.720360 30882 net.cpp:128] Creating Layer conv2_1ReLU_1
I1002 12:07:05.720363 30882 net.cpp:558] conv2_1ReLU_1 <- conv2_Eltwise_1
I1002 12:07:05.720368 30882 net.cpp:509] conv2_1ReLU_1 -> conv2_Eltwise_1 (in-place)
I1002 12:07:05.720782 30882 net.cpp:172] Setting up conv2_1ReLU_1
I1002 12:07:05.720803 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720806 30882 net.cpp:194] Memory required for data: 462160128
I1002 12:07:05.720811 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:05.720820 30882 net.cpp:128] Creating Layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:05.720825 30882 net.cpp:558] conv2_Eltwise_1_conv2_1ReLU_1_0_split <- conv2_Eltwise_1
I1002 12:07:05.720832 30882 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I1002 12:07:05.720841 30882 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I1002 12:07:05.720886 30882 net.cpp:172] Setting up conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:05.720893 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720899 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.720902 30882 net.cpp:194] Memory required for data: 529268992
I1002 12:07:05.720906 30882 layer_factory.hpp:77] Creating layer conv2_2_0
I1002 12:07:05.720917 30882 net.cpp:128] Creating Layer conv2_2_0
I1002 12:07:05.720921 30882 net.cpp:558] conv2_2_0 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I1002 12:07:05.720929 30882 net.cpp:522] conv2_2_0 -> conv2_2_0
I1002 12:07:05.724601 30882 net.cpp:172] Setting up conv2_2_0
I1002 12:07:05.724627 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.724649 30882 net.cpp:194] Memory required for data: 562823424
I1002 12:07:05.724660 30882 layer_factory.hpp:77] Creating layer conv2_2_bn0
I1002 12:07:05.724673 30882 net.cpp:128] Creating Layer conv2_2_bn0
I1002 12:07:05.724678 30882 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I1002 12:07:05.724684 30882 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I1002 12:07:05.724905 30882 net.cpp:172] Setting up conv2_2_bn0
I1002 12:07:05.724916 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.724920 30882 net.cpp:194] Memory required for data: 596377856
I1002 12:07:05.724933 30882 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1002 12:07:05.724941 30882 net.cpp:128] Creating Layer conv2_2_scale0
I1002 12:07:05.724946 30882 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I1002 12:07:05.724951 30882 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I1002 12:07:05.724992 30882 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1002 12:07:05.725102 30882 net.cpp:172] Setting up conv2_2_scale0
I1002 12:07:05.725112 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.725116 30882 net.cpp:194] Memory required for data: 629932288
I1002 12:07:05.725124 30882 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I1002 12:07:05.725131 30882 net.cpp:128] Creating Layer conv2_2_ReLU0
I1002 12:07:05.725134 30882 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I1002 12:07:05.725141 30882 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I1002 12:07:05.725360 30882 net.cpp:172] Setting up conv2_2_ReLU0
I1002 12:07:05.725373 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.725378 30882 net.cpp:194] Memory required for data: 663486720
I1002 12:07:05.725383 30882 layer_factory.hpp:77] Creating layer conv2_Drop2
I1002 12:07:05.725389 30882 net.cpp:128] Creating Layer conv2_Drop2
I1002 12:07:05.725395 30882 net.cpp:558] conv2_Drop2 <- conv2_2_0
I1002 12:07:05.725401 30882 net.cpp:509] conv2_Drop2 -> conv2_2_0 (in-place)
I1002 12:07:05.725428 30882 net.cpp:172] Setting up conv2_Drop2
I1002 12:07:05.725436 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.725440 30882 net.cpp:194] Memory required for data: 697041152
I1002 12:07:05.725445 30882 layer_factory.hpp:77] Creating layer conv2_2_1
I1002 12:07:05.725455 30882 net.cpp:128] Creating Layer conv2_2_1
I1002 12:07:05.725458 30882 net.cpp:558] conv2_2_1 <- conv2_2_0
I1002 12:07:05.725466 30882 net.cpp:522] conv2_2_1 -> conv2_2_1
I1002 12:07:05.729137 30882 net.cpp:172] Setting up conv2_2_1
I1002 12:07:05.729164 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.729168 30882 net.cpp:194] Memory required for data: 730595584
I1002 12:07:05.729178 30882 layer_factory.hpp:77] Creating layer conv2_2bn1
I1002 12:07:05.729187 30882 net.cpp:128] Creating Layer conv2_2bn1
I1002 12:07:05.729192 30882 net.cpp:558] conv2_2bn1 <- conv2_2_1
I1002 12:07:05.729198 30882 net.cpp:509] conv2_2bn1 -> conv2_2_1 (in-place)
I1002 12:07:05.729423 30882 net.cpp:172] Setting up conv2_2bn1
I1002 12:07:05.729434 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.729439 30882 net.cpp:194] Memory required for data: 764150016
I1002 12:07:05.729449 30882 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1002 12:07:05.729455 30882 net.cpp:128] Creating Layer conv2_2_scale1
I1002 12:07:05.729460 30882 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I1002 12:07:05.729466 30882 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I1002 12:07:05.729506 30882 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1002 12:07:05.729619 30882 net.cpp:172] Setting up conv2_2_scale1
I1002 12:07:05.729629 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.729634 30882 net.cpp:194] Memory required for data: 797704448
I1002 12:07:05.729641 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_2
I1002 12:07:05.729647 30882 net.cpp:128] Creating Layer conv2_Eltwise_2
I1002 12:07:05.729652 30882 net.cpp:558] conv2_Eltwise_2 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I1002 12:07:05.729657 30882 net.cpp:558] conv2_Eltwise_2 <- conv2_2_1
I1002 12:07:05.729679 30882 net.cpp:522] conv2_Eltwise_2 -> conv2_Eltwise_2
I1002 12:07:05.729707 30882 net.cpp:172] Setting up conv2_Eltwise_2
I1002 12:07:05.729719 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.729724 30882 net.cpp:194] Memory required for data: 831258880
I1002 12:07:05.729728 30882 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I1002 12:07:05.729734 30882 net.cpp:128] Creating Layer conv2_2ReLU_1
I1002 12:07:05.729739 30882 net.cpp:558] conv2_2ReLU_1 <- conv2_Eltwise_2
I1002 12:07:05.729745 30882 net.cpp:509] conv2_2ReLU_1 -> conv2_Eltwise_2 (in-place)
I1002 12:07:05.729979 30882 net.cpp:172] Setting up conv2_2ReLU_1
I1002 12:07:05.729995 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.729998 30882 net.cpp:194] Memory required for data: 864813312
I1002 12:07:05.730003 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:05.730010 30882 net.cpp:128] Creating Layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:05.730015 30882 net.cpp:558] conv2_Eltwise_2_conv2_2ReLU_1_0_split <- conv2_Eltwise_2
I1002 12:07:05.730020 30882 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I1002 12:07:05.730029 30882 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I1002 12:07:05.730072 30882 net.cpp:172] Setting up conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:05.730079 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.730084 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.730088 30882 net.cpp:194] Memory required for data: 931922176
I1002 12:07:05.730093 30882 layer_factory.hpp:77] Creating layer conv2_3_0
I1002 12:07:05.730103 30882 net.cpp:128] Creating Layer conv2_3_0
I1002 12:07:05.730108 30882 net.cpp:558] conv2_3_0 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I1002 12:07:05.730114 30882 net.cpp:522] conv2_3_0 -> conv2_3_0
I1002 12:07:05.735005 30882 net.cpp:172] Setting up conv2_3_0
I1002 12:07:05.735034 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.735038 30882 net.cpp:194] Memory required for data: 965476608
I1002 12:07:05.735049 30882 layer_factory.hpp:77] Creating layer conv2_3_bn0
I1002 12:07:05.735061 30882 net.cpp:128] Creating Layer conv2_3_bn0
I1002 12:07:05.735066 30882 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I1002 12:07:05.735077 30882 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I1002 12:07:05.735306 30882 net.cpp:172] Setting up conv2_3_bn0
I1002 12:07:05.735317 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.735321 30882 net.cpp:194] Memory required for data: 999031040
I1002 12:07:05.735332 30882 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1002 12:07:05.735338 30882 net.cpp:128] Creating Layer conv2_3_scale0
I1002 12:07:05.735342 30882 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I1002 12:07:05.735348 30882 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I1002 12:07:05.735390 30882 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1002 12:07:05.735505 30882 net.cpp:172] Setting up conv2_3_scale0
I1002 12:07:05.735517 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.735520 30882 net.cpp:194] Memory required for data: 1032585472
I1002 12:07:05.735528 30882 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I1002 12:07:05.735535 30882 net.cpp:128] Creating Layer conv2_3_ReLU0
I1002 12:07:05.735539 30882 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I1002 12:07:05.735545 30882 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I1002 12:07:05.735951 30882 net.cpp:172] Setting up conv2_3_ReLU0
I1002 12:07:05.735972 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.735977 30882 net.cpp:194] Memory required for data: 1066139904
I1002 12:07:05.735981 30882 layer_factory.hpp:77] Creating layer conv2_Drop3
I1002 12:07:05.735991 30882 net.cpp:128] Creating Layer conv2_Drop3
I1002 12:07:05.735996 30882 net.cpp:558] conv2_Drop3 <- conv2_3_0
I1002 12:07:05.736002 30882 net.cpp:509] conv2_Drop3 -> conv2_3_0 (in-place)
I1002 12:07:05.736052 30882 net.cpp:172] Setting up conv2_Drop3
I1002 12:07:05.736060 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.736064 30882 net.cpp:194] Memory required for data: 1099694336
I1002 12:07:05.736068 30882 layer_factory.hpp:77] Creating layer conv2_3_1
I1002 12:07:05.736078 30882 net.cpp:128] Creating Layer conv2_3_1
I1002 12:07:05.736083 30882 net.cpp:558] conv2_3_1 <- conv2_3_0
I1002 12:07:05.736090 30882 net.cpp:522] conv2_3_1 -> conv2_3_1
I1002 12:07:05.739727 30882 net.cpp:172] Setting up conv2_3_1
I1002 12:07:05.739753 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.739758 30882 net.cpp:194] Memory required for data: 1133248768
I1002 12:07:05.739768 30882 layer_factory.hpp:77] Creating layer conv2_3bn1
I1002 12:07:05.739776 30882 net.cpp:128] Creating Layer conv2_3bn1
I1002 12:07:05.739781 30882 net.cpp:558] conv2_3bn1 <- conv2_3_1
I1002 12:07:05.739789 30882 net.cpp:509] conv2_3bn1 -> conv2_3_1 (in-place)
I1002 12:07:05.740029 30882 net.cpp:172] Setting up conv2_3bn1
I1002 12:07:05.740041 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740044 30882 net.cpp:194] Memory required for data: 1166803200
I1002 12:07:05.740054 30882 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1002 12:07:05.740061 30882 net.cpp:128] Creating Layer conv2_3_scale1
I1002 12:07:05.740065 30882 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I1002 12:07:05.740072 30882 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I1002 12:07:05.740113 30882 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1002 12:07:05.740232 30882 net.cpp:172] Setting up conv2_3_scale1
I1002 12:07:05.740243 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740247 30882 net.cpp:194] Memory required for data: 1200357632
I1002 12:07:05.740255 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_3
I1002 12:07:05.740263 30882 net.cpp:128] Creating Layer conv2_Eltwise_3
I1002 12:07:05.740268 30882 net.cpp:558] conv2_Eltwise_3 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I1002 12:07:05.740273 30882 net.cpp:558] conv2_Eltwise_3 <- conv2_3_1
I1002 12:07:05.740279 30882 net.cpp:522] conv2_Eltwise_3 -> conv2_Eltwise_3
I1002 12:07:05.740306 30882 net.cpp:172] Setting up conv2_Eltwise_3
I1002 12:07:05.740314 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740317 30882 net.cpp:194] Memory required for data: 1233912064
I1002 12:07:05.740321 30882 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I1002 12:07:05.740327 30882 net.cpp:128] Creating Layer conv2_3ReLU_1
I1002 12:07:05.740331 30882 net.cpp:558] conv2_3ReLU_1 <- conv2_Eltwise_3
I1002 12:07:05.740337 30882 net.cpp:509] conv2_3ReLU_1 -> conv2_Eltwise_3 (in-place)
I1002 12:07:05.740564 30882 net.cpp:172] Setting up conv2_3ReLU_1
I1002 12:07:05.740577 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740582 30882 net.cpp:194] Memory required for data: 1267466496
I1002 12:07:05.740586 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:05.740593 30882 net.cpp:128] Creating Layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:05.740597 30882 net.cpp:558] conv2_Eltwise_3_conv2_3ReLU_1_0_split <- conv2_Eltwise_3
I1002 12:07:05.740603 30882 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I1002 12:07:05.740612 30882 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I1002 12:07:05.740658 30882 net.cpp:172] Setting up conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:05.740664 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740669 30882 net.cpp:186] Top shape: 64 128 32 32 (8388608)
I1002 12:07:05.740674 30882 net.cpp:194] Memory required for data: 1334575360
I1002 12:07:05.740677 30882 layer_factory.hpp:77] Creating layer conv3_1_0
I1002 12:07:05.740687 30882 net.cpp:128] Creating Layer conv3_1_0
I1002 12:07:05.740692 30882 net.cpp:558] conv3_1_0 <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I1002 12:07:05.740716 30882 net.cpp:522] conv3_1_0 -> conv3_1_0
I1002 12:07:05.747967 30882 net.cpp:172] Setting up conv3_1_0
I1002 12:07:05.747995 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.748000 30882 net.cpp:194] Memory required for data: 1351352576
I1002 12:07:05.748011 30882 layer_factory.hpp:77] Creating layer conv3_1_bn0
I1002 12:07:05.748021 30882 net.cpp:128] Creating Layer conv3_1_bn0
I1002 12:07:05.748026 30882 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I1002 12:07:05.748037 30882 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I1002 12:07:05.748265 30882 net.cpp:172] Setting up conv3_1_bn0
I1002 12:07:05.748275 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.748280 30882 net.cpp:194] Memory required for data: 1368129792
I1002 12:07:05.748289 30882 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1002 12:07:05.748297 30882 net.cpp:128] Creating Layer conv3_1_scale0
I1002 12:07:05.748301 30882 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I1002 12:07:05.748307 30882 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I1002 12:07:05.748350 30882 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1002 12:07:05.748471 30882 net.cpp:172] Setting up conv3_1_scale0
I1002 12:07:05.748481 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.748484 30882 net.cpp:194] Memory required for data: 1384907008
I1002 12:07:05.748492 30882 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I1002 12:07:05.748499 30882 net.cpp:128] Creating Layer conv3_1_ReLU0
I1002 12:07:05.748503 30882 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I1002 12:07:05.748509 30882 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I1002 12:07:05.748733 30882 net.cpp:172] Setting up conv3_1_ReLU0
I1002 12:07:05.748746 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.748751 30882 net.cpp:194] Memory required for data: 1401684224
I1002 12:07:05.748755 30882 layer_factory.hpp:77] Creating layer conv3_Drop1
I1002 12:07:05.748762 30882 net.cpp:128] Creating Layer conv3_Drop1
I1002 12:07:05.748766 30882 net.cpp:558] conv3_Drop1 <- conv3_1_0
I1002 12:07:05.748772 30882 net.cpp:509] conv3_Drop1 -> conv3_1_0 (in-place)
I1002 12:07:05.748796 30882 net.cpp:172] Setting up conv3_Drop1
I1002 12:07:05.748803 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.748807 30882 net.cpp:194] Memory required for data: 1418461440
I1002 12:07:05.748811 30882 layer_factory.hpp:77] Creating layer conv3_1_1
I1002 12:07:05.748821 30882 net.cpp:128] Creating Layer conv3_1_1
I1002 12:07:05.748826 30882 net.cpp:558] conv3_1_1 <- conv3_1_0
I1002 12:07:05.748832 30882 net.cpp:522] conv3_1_1 -> conv3_1_1
I1002 12:07:05.760803 30882 net.cpp:172] Setting up conv3_1_1
I1002 12:07:05.760833 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.760838 30882 net.cpp:194] Memory required for data: 1435238656
I1002 12:07:05.760855 30882 layer_factory.hpp:77] Creating layer conv3_1bn1
I1002 12:07:05.760869 30882 net.cpp:128] Creating Layer conv3_1bn1
I1002 12:07:05.760874 30882 net.cpp:558] conv3_1bn1 <- conv3_1_1
I1002 12:07:05.760881 30882 net.cpp:509] conv3_1bn1 -> conv3_1_1 (in-place)
I1002 12:07:05.761111 30882 net.cpp:172] Setting up conv3_1bn1
I1002 12:07:05.761122 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.761126 30882 net.cpp:194] Memory required for data: 1452015872
I1002 12:07:05.761135 30882 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1002 12:07:05.761143 30882 net.cpp:128] Creating Layer conv3_1_scale1
I1002 12:07:05.761148 30882 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I1002 12:07:05.761154 30882 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I1002 12:07:05.761198 30882 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1002 12:07:05.761318 30882 net.cpp:172] Setting up conv3_1_scale1
I1002 12:07:05.761329 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.761333 30882 net.cpp:194] Memory required for data: 1468793088
I1002 12:07:05.761340 30882 layer_factory.hpp:77] Creating layer conv3_1_down
I1002 12:07:05.761351 30882 net.cpp:128] Creating Layer conv3_1_down
I1002 12:07:05.761389 30882 net.cpp:558] conv3_1_down <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I1002 12:07:05.761396 30882 net.cpp:522] conv3_1_down -> conv3_1_down
I1002 12:07:05.763113 30882 net.cpp:172] Setting up conv3_1_down
I1002 12:07:05.763139 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.763144 30882 net.cpp:194] Memory required for data: 1485570304
I1002 12:07:05.763154 30882 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I1002 12:07:05.763164 30882 net.cpp:128] Creating Layer conv3_1_bn_down
I1002 12:07:05.763168 30882 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I1002 12:07:05.763175 30882 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I1002 12:07:05.763404 30882 net.cpp:172] Setting up conv3_1_bn_down
I1002 12:07:05.763414 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.763419 30882 net.cpp:194] Memory required for data: 1502347520
I1002 12:07:05.763428 30882 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1002 12:07:05.763437 30882 net.cpp:128] Creating Layer conv3_1_scale_down
I1002 12:07:05.763440 30882 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I1002 12:07:05.763447 30882 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I1002 12:07:05.763490 30882 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1002 12:07:05.763617 30882 net.cpp:172] Setting up conv3_1_scale_down
I1002 12:07:05.763626 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.763631 30882 net.cpp:194] Memory required for data: 1519124736
I1002 12:07:05.763638 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_1
I1002 12:07:05.763646 30882 net.cpp:128] Creating Layer conv3_Eltwise_1
I1002 12:07:05.763650 30882 net.cpp:558] conv3_Eltwise_1 <- conv3_1_1
I1002 12:07:05.763655 30882 net.cpp:558] conv3_Eltwise_1 <- conv3_1_down
I1002 12:07:05.763661 30882 net.cpp:522] conv3_Eltwise_1 -> conv3_Eltwise_1
I1002 12:07:05.763684 30882 net.cpp:172] Setting up conv3_Eltwise_1
I1002 12:07:05.763690 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.763695 30882 net.cpp:194] Memory required for data: 1535901952
I1002 12:07:05.763698 30882 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I1002 12:07:05.763705 30882 net.cpp:128] Creating Layer conv3_1ReLU_1
I1002 12:07:05.763708 30882 net.cpp:558] conv3_1ReLU_1 <- conv3_Eltwise_1
I1002 12:07:05.763715 30882 net.cpp:509] conv3_1ReLU_1 -> conv3_Eltwise_1 (in-place)
I1002 12:07:05.763937 30882 net.cpp:172] Setting up conv3_1ReLU_1
I1002 12:07:05.763950 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.763954 30882 net.cpp:194] Memory required for data: 1552679168
I1002 12:07:05.763958 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:05.763965 30882 net.cpp:128] Creating Layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:05.763972 30882 net.cpp:558] conv3_Eltwise_1_conv3_1ReLU_1_0_split <- conv3_Eltwise_1
I1002 12:07:05.763978 30882 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I1002 12:07:05.763985 30882 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I1002 12:07:05.764031 30882 net.cpp:172] Setting up conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:05.764039 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.764045 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.764048 30882 net.cpp:194] Memory required for data: 1586233600
I1002 12:07:05.764052 30882 layer_factory.hpp:77] Creating layer conv3_2_0
I1002 12:07:05.764062 30882 net.cpp:128] Creating Layer conv3_2_0
I1002 12:07:05.764067 30882 net.cpp:558] conv3_2_0 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I1002 12:07:05.764075 30882 net.cpp:522] conv3_2_0 -> conv3_2_0
I1002 12:07:05.776052 30882 net.cpp:172] Setting up conv3_2_0
I1002 12:07:05.776080 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.776085 30882 net.cpp:194] Memory required for data: 1603010816
I1002 12:07:05.776096 30882 layer_factory.hpp:77] Creating layer conv3_2_bn0
I1002 12:07:05.776129 30882 net.cpp:128] Creating Layer conv3_2_bn0
I1002 12:07:05.776134 30882 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I1002 12:07:05.776141 30882 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I1002 12:07:05.776377 30882 net.cpp:172] Setting up conv3_2_bn0
I1002 12:07:05.776388 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.776392 30882 net.cpp:194] Memory required for data: 1619788032
I1002 12:07:05.776402 30882 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1002 12:07:05.776410 30882 net.cpp:128] Creating Layer conv3_2_scale0
I1002 12:07:05.776415 30882 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I1002 12:07:05.776422 30882 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I1002 12:07:05.776466 30882 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1002 12:07:05.776592 30882 net.cpp:172] Setting up conv3_2_scale0
I1002 12:07:05.776602 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.776607 30882 net.cpp:194] Memory required for data: 1636565248
I1002 12:07:05.776614 30882 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I1002 12:07:05.776620 30882 net.cpp:128] Creating Layer conv3_2_ReLU0
I1002 12:07:05.776625 30882 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I1002 12:07:05.776630 30882 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I1002 12:07:05.777053 30882 net.cpp:172] Setting up conv3_2_ReLU0
I1002 12:07:05.777076 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.777079 30882 net.cpp:194] Memory required for data: 1653342464
I1002 12:07:05.777084 30882 layer_factory.hpp:77] Creating layer conv3_Drop2
I1002 12:07:05.777093 30882 net.cpp:128] Creating Layer conv3_Drop2
I1002 12:07:05.777098 30882 net.cpp:558] conv3_Drop2 <- conv3_2_0
I1002 12:07:05.777106 30882 net.cpp:509] conv3_Drop2 -> conv3_2_0 (in-place)
I1002 12:07:05.777134 30882 net.cpp:172] Setting up conv3_Drop2
I1002 12:07:05.777142 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.777145 30882 net.cpp:194] Memory required for data: 1670119680
I1002 12:07:05.777149 30882 layer_factory.hpp:77] Creating layer conv3_2_1
I1002 12:07:05.777159 30882 net.cpp:128] Creating Layer conv3_2_1
I1002 12:07:05.777163 30882 net.cpp:558] conv3_2_1 <- conv3_2_0
I1002 12:07:05.777170 30882 net.cpp:522] conv3_2_1 -> conv3_2_1
I1002 12:07:05.789163 30882 net.cpp:172] Setting up conv3_2_1
I1002 12:07:05.789192 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.789196 30882 net.cpp:194] Memory required for data: 1686896896
I1002 12:07:05.789208 30882 layer_factory.hpp:77] Creating layer conv3_2bn1
I1002 12:07:05.789218 30882 net.cpp:128] Creating Layer conv3_2bn1
I1002 12:07:05.789223 30882 net.cpp:558] conv3_2bn1 <- conv3_2_1
I1002 12:07:05.789235 30882 net.cpp:509] conv3_2bn1 -> conv3_2_1 (in-place)
I1002 12:07:05.789479 30882 net.cpp:172] Setting up conv3_2bn1
I1002 12:07:05.789490 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.789494 30882 net.cpp:194] Memory required for data: 1703674112
I1002 12:07:05.789503 30882 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1002 12:07:05.789511 30882 net.cpp:128] Creating Layer conv3_2_scale1
I1002 12:07:05.789515 30882 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I1002 12:07:05.789521 30882 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I1002 12:07:05.789566 30882 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1002 12:07:05.789700 30882 net.cpp:172] Setting up conv3_2_scale1
I1002 12:07:05.789710 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.789714 30882 net.cpp:194] Memory required for data: 1720451328
I1002 12:07:05.789722 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_2
I1002 12:07:05.789733 30882 net.cpp:128] Creating Layer conv3_Eltwise_2
I1002 12:07:05.789738 30882 net.cpp:558] conv3_Eltwise_2 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I1002 12:07:05.789744 30882 net.cpp:558] conv3_Eltwise_2 <- conv3_2_1
I1002 12:07:05.789749 30882 net.cpp:522] conv3_Eltwise_2 -> conv3_Eltwise_2
I1002 12:07:05.789793 30882 net.cpp:172] Setting up conv3_Eltwise_2
I1002 12:07:05.789801 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.789804 30882 net.cpp:194] Memory required for data: 1737228544
I1002 12:07:05.789809 30882 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I1002 12:07:05.789816 30882 net.cpp:128] Creating Layer conv3_2ReLU_1
I1002 12:07:05.789821 30882 net.cpp:558] conv3_2ReLU_1 <- conv3_Eltwise_2
I1002 12:07:05.789826 30882 net.cpp:509] conv3_2ReLU_1 -> conv3_Eltwise_2 (in-place)
I1002 12:07:05.790081 30882 net.cpp:172] Setting up conv3_2ReLU_1
I1002 12:07:05.790097 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.790102 30882 net.cpp:194] Memory required for data: 1754005760
I1002 12:07:05.790107 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:05.790113 30882 net.cpp:128] Creating Layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:05.790118 30882 net.cpp:558] conv3_Eltwise_2_conv3_2ReLU_1_0_split <- conv3_Eltwise_2
I1002 12:07:05.790125 30882 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I1002 12:07:05.790133 30882 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I1002 12:07:05.790184 30882 net.cpp:172] Setting up conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:05.790199 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.790205 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.790208 30882 net.cpp:194] Memory required for data: 1787560192
I1002 12:07:05.790212 30882 layer_factory.hpp:77] Creating layer conv3_3_0
I1002 12:07:05.790222 30882 net.cpp:128] Creating Layer conv3_3_0
I1002 12:07:05.790227 30882 net.cpp:558] conv3_3_0 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I1002 12:07:05.790235 30882 net.cpp:522] conv3_3_0 -> conv3_3_0
I1002 12:07:05.802268 30882 net.cpp:172] Setting up conv3_3_0
I1002 12:07:05.802296 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.802301 30882 net.cpp:194] Memory required for data: 1804337408
I1002 12:07:05.802312 30882 layer_factory.hpp:77] Creating layer conv3_3_bn0
I1002 12:07:05.802323 30882 net.cpp:128] Creating Layer conv3_3_bn0
I1002 12:07:05.802328 30882 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I1002 12:07:05.802340 30882 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I1002 12:07:05.802585 30882 net.cpp:172] Setting up conv3_3_bn0
I1002 12:07:05.802597 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.802600 30882 net.cpp:194] Memory required for data: 1821114624
I1002 12:07:05.802610 30882 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1002 12:07:05.802618 30882 net.cpp:128] Creating Layer conv3_3_scale0
I1002 12:07:05.802623 30882 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I1002 12:07:05.802628 30882 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I1002 12:07:05.802672 30882 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1002 12:07:05.802801 30882 net.cpp:172] Setting up conv3_3_scale0
I1002 12:07:05.802812 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.802816 30882 net.cpp:194] Memory required for data: 1837891840
I1002 12:07:05.802824 30882 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I1002 12:07:05.802830 30882 net.cpp:128] Creating Layer conv3_3_ReLU0
I1002 12:07:05.802835 30882 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I1002 12:07:05.802840 30882 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I1002 12:07:05.803067 30882 net.cpp:172] Setting up conv3_3_ReLU0
I1002 12:07:05.803079 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.803084 30882 net.cpp:194] Memory required for data: 1854669056
I1002 12:07:05.803088 30882 layer_factory.hpp:77] Creating layer conv3_Drop3
I1002 12:07:05.803097 30882 net.cpp:128] Creating Layer conv3_Drop3
I1002 12:07:05.803102 30882 net.cpp:558] conv3_Drop3 <- conv3_3_0
I1002 12:07:05.803107 30882 net.cpp:509] conv3_Drop3 -> conv3_3_0 (in-place)
I1002 12:07:05.803133 30882 net.cpp:172] Setting up conv3_Drop3
I1002 12:07:05.803139 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.803161 30882 net.cpp:194] Memory required for data: 1871446272
I1002 12:07:05.803166 30882 layer_factory.hpp:77] Creating layer conv3_3_1
I1002 12:07:05.803177 30882 net.cpp:128] Creating Layer conv3_3_1
I1002 12:07:05.803181 30882 net.cpp:558] conv3_3_1 <- conv3_3_0
I1002 12:07:05.803189 30882 net.cpp:522] conv3_3_1 -> conv3_3_1
I1002 12:07:05.815167 30882 net.cpp:172] Setting up conv3_3_1
I1002 12:07:05.815196 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.815201 30882 net.cpp:194] Memory required for data: 1888223488
I1002 12:07:05.815212 30882 layer_factory.hpp:77] Creating layer conv3_3bn1
I1002 12:07:05.815222 30882 net.cpp:128] Creating Layer conv3_3bn1
I1002 12:07:05.815227 30882 net.cpp:558] conv3_3bn1 <- conv3_3_1
I1002 12:07:05.815238 30882 net.cpp:509] conv3_3bn1 -> conv3_3_1 (in-place)
I1002 12:07:05.815481 30882 net.cpp:172] Setting up conv3_3bn1
I1002 12:07:05.815492 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.815496 30882 net.cpp:194] Memory required for data: 1905000704
I1002 12:07:05.815506 30882 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1002 12:07:05.815515 30882 net.cpp:128] Creating Layer conv3_3_scale1
I1002 12:07:05.815518 30882 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I1002 12:07:05.815524 30882 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I1002 12:07:05.815568 30882 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1002 12:07:05.815699 30882 net.cpp:172] Setting up conv3_3_scale1
I1002 12:07:05.815709 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.815713 30882 net.cpp:194] Memory required for data: 1921777920
I1002 12:07:05.815721 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_3
I1002 12:07:05.815729 30882 net.cpp:128] Creating Layer conv3_Eltwise_3
I1002 12:07:05.815734 30882 net.cpp:558] conv3_Eltwise_3 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I1002 12:07:05.815740 30882 net.cpp:558] conv3_Eltwise_3 <- conv3_3_1
I1002 12:07:05.815747 30882 net.cpp:522] conv3_Eltwise_3 -> conv3_Eltwise_3
I1002 12:07:05.815768 30882 net.cpp:172] Setting up conv3_Eltwise_3
I1002 12:07:05.815775 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.815779 30882 net.cpp:194] Memory required for data: 1938555136
I1002 12:07:05.815783 30882 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I1002 12:07:05.815789 30882 net.cpp:128] Creating Layer conv3_3ReLU_1
I1002 12:07:05.815793 30882 net.cpp:558] conv3_3ReLU_1 <- conv3_Eltwise_3
I1002 12:07:05.815800 30882 net.cpp:509] conv3_3ReLU_1 -> conv3_Eltwise_3 (in-place)
I1002 12:07:05.816224 30882 net.cpp:172] Setting up conv3_3ReLU_1
I1002 12:07:05.816246 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.816251 30882 net.cpp:194] Memory required for data: 1955332352
I1002 12:07:05.816256 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:05.816264 30882 net.cpp:128] Creating Layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:05.816269 30882 net.cpp:558] conv3_Eltwise_3_conv3_3ReLU_1_0_split <- conv3_Eltwise_3
I1002 12:07:05.816277 30882 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I1002 12:07:05.816287 30882 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I1002 12:07:05.816336 30882 net.cpp:172] Setting up conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:05.816344 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.816349 30882 net.cpp:186] Top shape: 64 256 16 16 (4194304)
I1002 12:07:05.816354 30882 net.cpp:194] Memory required for data: 1988886784
I1002 12:07:05.816357 30882 layer_factory.hpp:77] Creating layer conv4_1_0
I1002 12:07:05.816368 30882 net.cpp:128] Creating Layer conv4_1_0
I1002 12:07:05.816373 30882 net.cpp:558] conv4_1_0 <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I1002 12:07:05.816380 30882 net.cpp:522] conv4_1_0 -> conv4_1_0
I1002 12:07:05.838840 30882 net.cpp:172] Setting up conv4_1_0
I1002 12:07:05.838910 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.838915 30882 net.cpp:194] Memory required for data: 1997275392
I1002 12:07:05.838932 30882 layer_factory.hpp:77] Creating layer conv4_1_bn0
I1002 12:07:05.838946 30882 net.cpp:128] Creating Layer conv4_1_bn0
I1002 12:07:05.838958 30882 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I1002 12:07:05.838966 30882 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I1002 12:07:05.839221 30882 net.cpp:172] Setting up conv4_1_bn0
I1002 12:07:05.839231 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.839236 30882 net.cpp:194] Memory required for data: 2005664000
I1002 12:07:05.839246 30882 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1002 12:07:05.839256 30882 net.cpp:128] Creating Layer conv4_1_scale0
I1002 12:07:05.839259 30882 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I1002 12:07:05.839264 30882 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I1002 12:07:05.839309 30882 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1002 12:07:05.839452 30882 net.cpp:172] Setting up conv4_1_scale0
I1002 12:07:05.839462 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.839465 30882 net.cpp:194] Memory required for data: 2014052608
I1002 12:07:05.839473 30882 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I1002 12:07:05.839480 30882 net.cpp:128] Creating Layer conv4_1_ReLU0
I1002 12:07:05.839485 30882 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I1002 12:07:05.839490 30882 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I1002 12:07:05.839913 30882 net.cpp:172] Setting up conv4_1_ReLU0
I1002 12:07:05.839934 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.839938 30882 net.cpp:194] Memory required for data: 2022441216
I1002 12:07:05.839943 30882 layer_factory.hpp:77] Creating layer conv4_Drop1
I1002 12:07:05.839952 30882 net.cpp:128] Creating Layer conv4_Drop1
I1002 12:07:05.839957 30882 net.cpp:558] conv4_Drop1 <- conv4_1_0
I1002 12:07:05.839964 30882 net.cpp:509] conv4_Drop1 -> conv4_1_0 (in-place)
I1002 12:07:05.840003 30882 net.cpp:172] Setting up conv4_Drop1
I1002 12:07:05.840010 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.840014 30882 net.cpp:194] Memory required for data: 2030829824
I1002 12:07:05.840018 30882 layer_factory.hpp:77] Creating layer conv4_1_1
I1002 12:07:05.840030 30882 net.cpp:128] Creating Layer conv4_1_1
I1002 12:07:05.840034 30882 net.cpp:558] conv4_1_1 <- conv4_1_0
I1002 12:07:05.840041 30882 net.cpp:522] conv4_1_1 -> conv4_1_1
I1002 12:07:05.885280 30882 net.cpp:172] Setting up conv4_1_1
I1002 12:07:05.885340 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.885344 30882 net.cpp:194] Memory required for data: 2039218432
I1002 12:07:05.885365 30882 layer_factory.hpp:77] Creating layer conv4_1bn1
I1002 12:07:05.885380 30882 net.cpp:128] Creating Layer conv4_1bn1
I1002 12:07:05.885392 30882 net.cpp:558] conv4_1bn1 <- conv4_1_1
I1002 12:07:05.885401 30882 net.cpp:509] conv4_1bn1 -> conv4_1_1 (in-place)
I1002 12:07:05.885659 30882 net.cpp:172] Setting up conv4_1bn1
I1002 12:07:05.885670 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.885674 30882 net.cpp:194] Memory required for data: 2047607040
I1002 12:07:05.885685 30882 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1002 12:07:05.885694 30882 net.cpp:128] Creating Layer conv4_1_scale1
I1002 12:07:05.885699 30882 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I1002 12:07:05.885704 30882 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I1002 12:07:05.885749 30882 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1002 12:07:05.885893 30882 net.cpp:172] Setting up conv4_1_scale1
I1002 12:07:05.885921 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.885926 30882 net.cpp:194] Memory required for data: 2055995648
I1002 12:07:05.885933 30882 layer_factory.hpp:77] Creating layer conv4_1_down
I1002 12:07:05.885946 30882 net.cpp:128] Creating Layer conv4_1_down
I1002 12:07:05.885951 30882 net.cpp:558] conv4_1_down <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I1002 12:07:05.885958 30882 net.cpp:522] conv4_1_down -> conv4_1_down
I1002 12:07:05.889369 30882 net.cpp:172] Setting up conv4_1_down
I1002 12:07:05.889394 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.889400 30882 net.cpp:194] Memory required for data: 2064384256
I1002 12:07:05.889410 30882 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I1002 12:07:05.889418 30882 net.cpp:128] Creating Layer conv4_1_bn_down
I1002 12:07:05.889423 30882 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I1002 12:07:05.889430 30882 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I1002 12:07:05.889688 30882 net.cpp:172] Setting up conv4_1_bn_down
I1002 12:07:05.889698 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.889703 30882 net.cpp:194] Memory required for data: 2072772864
I1002 12:07:05.889713 30882 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1002 12:07:05.889719 30882 net.cpp:128] Creating Layer conv4_1_scale_down
I1002 12:07:05.889724 30882 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I1002 12:07:05.889729 30882 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I1002 12:07:05.889771 30882 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1002 12:07:05.889920 30882 net.cpp:172] Setting up conv4_1_scale_down
I1002 12:07:05.889931 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.889936 30882 net.cpp:194] Memory required for data: 2081161472
I1002 12:07:05.889945 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_1
I1002 12:07:05.889952 30882 net.cpp:128] Creating Layer conv4_Eltwise_1
I1002 12:07:05.889957 30882 net.cpp:558] conv4_Eltwise_1 <- conv4_1_1
I1002 12:07:05.889962 30882 net.cpp:558] conv4_Eltwise_1 <- conv4_1_down
I1002 12:07:05.889968 30882 net.cpp:522] conv4_Eltwise_1 -> conv4_Eltwise_1
I1002 12:07:05.889995 30882 net.cpp:172] Setting up conv4_Eltwise_1
I1002 12:07:05.890002 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.890007 30882 net.cpp:194] Memory required for data: 2089550080
I1002 12:07:05.890010 30882 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I1002 12:07:05.890017 30882 net.cpp:128] Creating Layer conv4_1ReLU_1
I1002 12:07:05.890022 30882 net.cpp:558] conv4_1ReLU_1 <- conv4_Eltwise_1
I1002 12:07:05.890027 30882 net.cpp:509] conv4_1ReLU_1 -> conv4_Eltwise_1 (in-place)
I1002 12:07:05.890255 30882 net.cpp:172] Setting up conv4_1ReLU_1
I1002 12:07:05.890270 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.890275 30882 net.cpp:194] Memory required for data: 2097938688
I1002 12:07:05.890280 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:05.890286 30882 net.cpp:128] Creating Layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:05.890292 30882 net.cpp:558] conv4_Eltwise_1_conv4_1ReLU_1_0_split <- conv4_Eltwise_1
I1002 12:07:05.890298 30882 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I1002 12:07:05.890306 30882 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I1002 12:07:05.890357 30882 net.cpp:172] Setting up conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:05.890367 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.890372 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.890377 30882 net.cpp:194] Memory required for data: 2114715904
I1002 12:07:05.890380 30882 layer_factory.hpp:77] Creating layer conv4_2_0
I1002 12:07:05.890390 30882 net.cpp:128] Creating Layer conv4_2_0
I1002 12:07:05.890395 30882 net.cpp:558] conv4_2_0 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I1002 12:07:05.890403 30882 net.cpp:522] conv4_2_0 -> conv4_2_0
I1002 12:07:05.935626 30882 net.cpp:172] Setting up conv4_2_0
I1002 12:07:05.935684 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.935689 30882 net.cpp:194] Memory required for data: 2123104512
I1002 12:07:05.935708 30882 layer_factory.hpp:77] Creating layer conv4_2_bn0
I1002 12:07:05.935724 30882 net.cpp:128] Creating Layer conv4_2_bn0
I1002 12:07:05.935735 30882 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I1002 12:07:05.935793 30882 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I1002 12:07:05.936054 30882 net.cpp:172] Setting up conv4_2_bn0
I1002 12:07:05.936065 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.936069 30882 net.cpp:194] Memory required for data: 2131493120
I1002 12:07:05.936089 30882 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1002 12:07:05.936100 30882 net.cpp:128] Creating Layer conv4_2_scale0
I1002 12:07:05.936103 30882 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I1002 12:07:05.936110 30882 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I1002 12:07:05.936154 30882 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1002 12:07:05.936305 30882 net.cpp:172] Setting up conv4_2_scale0
I1002 12:07:05.936316 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.936319 30882 net.cpp:194] Memory required for data: 2139881728
I1002 12:07:05.936327 30882 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I1002 12:07:05.936336 30882 net.cpp:128] Creating Layer conv4_2_ReLU0
I1002 12:07:05.936341 30882 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I1002 12:07:05.936345 30882 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I1002 12:07:05.936779 30882 net.cpp:172] Setting up conv4_2_ReLU0
I1002 12:07:05.936801 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.936805 30882 net.cpp:194] Memory required for data: 2148270336
I1002 12:07:05.936810 30882 layer_factory.hpp:77] Creating layer conv4_Drop2
I1002 12:07:05.936820 30882 net.cpp:128] Creating Layer conv4_Drop2
I1002 12:07:05.936825 30882 net.cpp:558] conv4_Drop2 <- conv4_2_0
I1002 12:07:05.936832 30882 net.cpp:509] conv4_Drop2 -> conv4_2_0 (in-place)
I1002 12:07:05.936867 30882 net.cpp:172] Setting up conv4_Drop2
I1002 12:07:05.936875 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.936878 30882 net.cpp:194] Memory required for data: 2156658944
I1002 12:07:05.936883 30882 layer_factory.hpp:77] Creating layer conv4_2_1
I1002 12:07:05.936895 30882 net.cpp:128] Creating Layer conv4_2_1
I1002 12:07:05.936899 30882 net.cpp:558] conv4_2_1 <- conv4_2_0
I1002 12:07:05.936906 30882 net.cpp:522] conv4_2_1 -> conv4_2_1
I1002 12:07:05.982172 30882 net.cpp:172] Setting up conv4_2_1
I1002 12:07:05.982229 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.982234 30882 net.cpp:194] Memory required for data: 2165047552
I1002 12:07:05.982254 30882 layer_factory.hpp:77] Creating layer conv4_2bn1
I1002 12:07:05.982271 30882 net.cpp:128] Creating Layer conv4_2bn1
I1002 12:07:05.982278 30882 net.cpp:558] conv4_2bn1 <- conv4_2_1
I1002 12:07:05.982290 30882 net.cpp:509] conv4_2bn1 -> conv4_2_1 (in-place)
I1002 12:07:05.982556 30882 net.cpp:172] Setting up conv4_2bn1
I1002 12:07:05.982568 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.982573 30882 net.cpp:194] Memory required for data: 2173436160
I1002 12:07:05.982583 30882 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1002 12:07:05.982592 30882 net.cpp:128] Creating Layer conv4_2_scale1
I1002 12:07:05.982597 30882 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I1002 12:07:05.982602 30882 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I1002 12:07:05.982650 30882 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1002 12:07:05.982800 30882 net.cpp:172] Setting up conv4_2_scale1
I1002 12:07:05.982810 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.982815 30882 net.cpp:194] Memory required for data: 2181824768
I1002 12:07:05.982822 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_2
I1002 12:07:05.982831 30882 net.cpp:128] Creating Layer conv4_Eltwise_2
I1002 12:07:05.982836 30882 net.cpp:558] conv4_Eltwise_2 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I1002 12:07:05.982841 30882 net.cpp:558] conv4_Eltwise_2 <- conv4_2_1
I1002 12:07:05.982848 30882 net.cpp:522] conv4_Eltwise_2 -> conv4_Eltwise_2
I1002 12:07:05.982877 30882 net.cpp:172] Setting up conv4_Eltwise_2
I1002 12:07:05.982884 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.982888 30882 net.cpp:194] Memory required for data: 2190213376
I1002 12:07:05.982935 30882 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I1002 12:07:05.982945 30882 net.cpp:128] Creating Layer conv4_2ReLU_1
I1002 12:07:05.982949 30882 net.cpp:558] conv4_2ReLU_1 <- conv4_Eltwise_2
I1002 12:07:05.982954 30882 net.cpp:509] conv4_2ReLU_1 -> conv4_Eltwise_2 (in-place)
I1002 12:07:05.983192 30882 net.cpp:172] Setting up conv4_2ReLU_1
I1002 12:07:05.983207 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.983211 30882 net.cpp:194] Memory required for data: 2198601984
I1002 12:07:05.983216 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:05.983223 30882 net.cpp:128] Creating Layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:05.983227 30882 net.cpp:558] conv4_Eltwise_2_conv4_2ReLU_1_0_split <- conv4_Eltwise_2
I1002 12:07:05.983234 30882 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I1002 12:07:05.983243 30882 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I1002 12:07:05.983294 30882 net.cpp:172] Setting up conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:05.983309 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.983314 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:05.983319 30882 net.cpp:194] Memory required for data: 2215379200
I1002 12:07:05.983322 30882 layer_factory.hpp:77] Creating layer conv4_3_0
I1002 12:07:05.983335 30882 net.cpp:128] Creating Layer conv4_3_0
I1002 12:07:05.983340 30882 net.cpp:558] conv4_3_0 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I1002 12:07:05.983346 30882 net.cpp:522] conv4_3_0 -> conv4_3_0
I1002 12:07:06.028596 30882 net.cpp:172] Setting up conv4_3_0
I1002 12:07:06.028654 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.028658 30882 net.cpp:194] Memory required for data: 2223767808
I1002 12:07:06.028678 30882 layer_factory.hpp:77] Creating layer conv4_3_bn0
I1002 12:07:06.028694 30882 net.cpp:128] Creating Layer conv4_3_bn0
I1002 12:07:06.028707 30882 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I1002 12:07:06.028715 30882 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I1002 12:07:06.028983 30882 net.cpp:172] Setting up conv4_3_bn0
I1002 12:07:06.028995 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.028998 30882 net.cpp:194] Memory required for data: 2232156416
I1002 12:07:06.029009 30882 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1002 12:07:06.029021 30882 net.cpp:128] Creating Layer conv4_3_scale0
I1002 12:07:06.029024 30882 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I1002 12:07:06.029031 30882 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I1002 12:07:06.029075 30882 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1002 12:07:06.029227 30882 net.cpp:172] Setting up conv4_3_scale0
I1002 12:07:06.029237 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.029242 30882 net.cpp:194] Memory required for data: 2240545024
I1002 12:07:06.029249 30882 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I1002 12:07:06.029256 30882 net.cpp:128] Creating Layer conv4_3_ReLU0
I1002 12:07:06.029261 30882 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I1002 12:07:06.029266 30882 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I1002 12:07:06.029494 30882 net.cpp:172] Setting up conv4_3_ReLU0
I1002 12:07:06.029506 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.029510 30882 net.cpp:194] Memory required for data: 2248933632
I1002 12:07:06.029515 30882 layer_factory.hpp:77] Creating layer conv4_Drop3
I1002 12:07:06.029525 30882 net.cpp:128] Creating Layer conv4_Drop3
I1002 12:07:06.029528 30882 net.cpp:558] conv4_Drop3 <- conv4_3_0
I1002 12:07:06.029536 30882 net.cpp:509] conv4_Drop3 -> conv4_3_0 (in-place)
I1002 12:07:06.029567 30882 net.cpp:172] Setting up conv4_Drop3
I1002 12:07:06.029575 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.029579 30882 net.cpp:194] Memory required for data: 2257322240
I1002 12:07:06.029583 30882 layer_factory.hpp:77] Creating layer conv4_3_1
I1002 12:07:06.029639 30882 net.cpp:128] Creating Layer conv4_3_1
I1002 12:07:06.029644 30882 net.cpp:558] conv4_3_1 <- conv4_3_0
I1002 12:07:06.029650 30882 net.cpp:522] conv4_3_1 -> conv4_3_1
I1002 12:07:06.074976 30882 net.cpp:172] Setting up conv4_3_1
I1002 12:07:06.075034 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.075039 30882 net.cpp:194] Memory required for data: 2265710848
I1002 12:07:06.075059 30882 layer_factory.hpp:77] Creating layer conv4_3bn1
I1002 12:07:06.075076 30882 net.cpp:128] Creating Layer conv4_3bn1
I1002 12:07:06.075083 30882 net.cpp:558] conv4_3bn1 <- conv4_3_1
I1002 12:07:06.075095 30882 net.cpp:509] conv4_3bn1 -> conv4_3_1 (in-place)
I1002 12:07:06.075363 30882 net.cpp:172] Setting up conv4_3bn1
I1002 12:07:06.075374 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.075378 30882 net.cpp:194] Memory required for data: 2274099456
I1002 12:07:06.075388 30882 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1002 12:07:06.075398 30882 net.cpp:128] Creating Layer conv4_3_scale1
I1002 12:07:06.075402 30882 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I1002 12:07:06.075408 30882 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I1002 12:07:06.075454 30882 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1002 12:07:06.075606 30882 net.cpp:172] Setting up conv4_3_scale1
I1002 12:07:06.075618 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.075621 30882 net.cpp:194] Memory required for data: 2282488064
I1002 12:07:06.075629 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_3
I1002 12:07:06.075639 30882 net.cpp:128] Creating Layer conv4_Eltwise_3
I1002 12:07:06.075644 30882 net.cpp:558] conv4_Eltwise_3 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I1002 12:07:06.075649 30882 net.cpp:558] conv4_Eltwise_3 <- conv4_3_1
I1002 12:07:06.075655 30882 net.cpp:522] conv4_Eltwise_3 -> conv4_Eltwise_3
I1002 12:07:06.075685 30882 net.cpp:172] Setting up conv4_Eltwise_3
I1002 12:07:06.075692 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.075695 30882 net.cpp:194] Memory required for data: 2290876672
I1002 12:07:06.075700 30882 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I1002 12:07:06.075707 30882 net.cpp:128] Creating Layer conv4_3ReLU_1
I1002 12:07:06.075712 30882 net.cpp:558] conv4_3ReLU_1 <- conv4_Eltwise_3
I1002 12:07:06.075717 30882 net.cpp:509] conv4_3ReLU_1 -> conv4_Eltwise_3 (in-place)
I1002 12:07:06.075953 30882 net.cpp:172] Setting up conv4_3ReLU_1
I1002 12:07:06.075969 30882 net.cpp:186] Top shape: 64 512 8 8 (2097152)
I1002 12:07:06.075973 30882 net.cpp:194] Memory required for data: 2299265280
I1002 12:07:06.075978 30882 layer_factory.hpp:77] Creating layer Pooling1
I1002 12:07:06.075986 30882 net.cpp:128] Creating Layer Pooling1
I1002 12:07:06.075991 30882 net.cpp:558] Pooling1 <- conv4_Eltwise_3
I1002 12:07:06.075999 30882 net.cpp:522] Pooling1 -> Pooling1
I1002 12:07:06.076495 30882 net.cpp:172] Setting up Pooling1
I1002 12:07:06.076517 30882 net.cpp:186] Top shape: 64 512 1 1 (32768)
I1002 12:07:06.076521 30882 net.cpp:194] Memory required for data: 2299396352
I1002 12:07:06.076526 30882 layer_factory.hpp:77] Creating layer fc1
I1002 12:07:06.076539 30882 net.cpp:128] Creating Layer fc1
I1002 12:07:06.076545 30882 net.cpp:558] fc1 <- Pooling1
I1002 12:07:06.076552 30882 net.cpp:522] fc1 -> fc1
I1002 12:07:06.078161 30882 net.cpp:172] Setting up fc1
I1002 12:07:06.078181 30882 net.cpp:186] Top shape: 64 10 (640)
I1002 12:07:06.078186 30882 net.cpp:194] Memory required for data: 2299398912
I1002 12:07:06.078197 30882 layer_factory.hpp:77] Creating layer Softmax1
I1002 12:07:06.078204 30882 net.cpp:128] Creating Layer Softmax1
I1002 12:07:06.078214 30882 net.cpp:558] Softmax1 <- fc1
I1002 12:07:06.078219 30882 net.cpp:558] Softmax1 <- label
I1002 12:07:06.078228 30882 net.cpp:522] Softmax1 -> Softmax1
I1002 12:07:06.078239 30882 layer_factory.hpp:77] Creating layer Softmax1
I1002 12:07:06.078603 30882 net.cpp:172] Setting up Softmax1
I1002 12:07:06.078619 30882 net.cpp:186] Top shape: (1)
I1002 12:07:06.078658 30882 net.cpp:189]     with loss weight 1
I1002 12:07:06.078692 30882 net.cpp:194] Memory required for data: 2299398916
I1002 12:07:06.078698 30882 net.cpp:301] Softmax1 needs backward computation.
I1002 12:07:06.078703 30882 net.cpp:301] fc1 needs backward computation.
I1002 12:07:06.078707 30882 net.cpp:301] Pooling1 needs backward computation.
I1002 12:07:06.078712 30882 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I1002 12:07:06.078716 30882 net.cpp:301] conv4_Eltwise_3 needs backward computation.
I1002 12:07:06.078722 30882 net.cpp:301] conv4_3_scale1 needs backward computation.
I1002 12:07:06.078725 30882 net.cpp:301] conv4_3bn1 needs backward computation.
I1002 12:07:06.078729 30882 net.cpp:301] conv4_3_1 needs backward computation.
I1002 12:07:06.078733 30882 net.cpp:301] conv4_Drop3 needs backward computation.
I1002 12:07:06.078738 30882 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I1002 12:07:06.078742 30882 net.cpp:301] conv4_3_scale0 needs backward computation.
I1002 12:07:06.078747 30882 net.cpp:301] conv4_3_bn0 needs backward computation.
I1002 12:07:06.078752 30882 net.cpp:301] conv4_3_0 needs backward computation.
I1002 12:07:06.078757 30882 net.cpp:301] conv4_Eltwise_2_conv4_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.078761 30882 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I1002 12:07:06.078766 30882 net.cpp:301] conv4_Eltwise_2 needs backward computation.
I1002 12:07:06.078771 30882 net.cpp:301] conv4_2_scale1 needs backward computation.
I1002 12:07:06.078775 30882 net.cpp:301] conv4_2bn1 needs backward computation.
I1002 12:07:06.078780 30882 net.cpp:301] conv4_2_1 needs backward computation.
I1002 12:07:06.078784 30882 net.cpp:301] conv4_Drop2 needs backward computation.
I1002 12:07:06.078789 30882 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I1002 12:07:06.078794 30882 net.cpp:301] conv4_2_scale0 needs backward computation.
I1002 12:07:06.078797 30882 net.cpp:301] conv4_2_bn0 needs backward computation.
I1002 12:07:06.078801 30882 net.cpp:301] conv4_2_0 needs backward computation.
I1002 12:07:06.078806 30882 net.cpp:301] conv4_Eltwise_1_conv4_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.078811 30882 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I1002 12:07:06.078815 30882 net.cpp:301] conv4_Eltwise_1 needs backward computation.
I1002 12:07:06.078820 30882 net.cpp:301] conv4_1_scale_down needs backward computation.
I1002 12:07:06.078825 30882 net.cpp:301] conv4_1_bn_down needs backward computation.
I1002 12:07:06.078830 30882 net.cpp:301] conv4_1_down needs backward computation.
I1002 12:07:06.078835 30882 net.cpp:301] conv4_1_scale1 needs backward computation.
I1002 12:07:06.078838 30882 net.cpp:301] conv4_1bn1 needs backward computation.
I1002 12:07:06.078842 30882 net.cpp:301] conv4_1_1 needs backward computation.
I1002 12:07:06.078847 30882 net.cpp:301] conv4_Drop1 needs backward computation.
I1002 12:07:06.078851 30882 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I1002 12:07:06.078855 30882 net.cpp:301] conv4_1_scale0 needs backward computation.
I1002 12:07:06.078860 30882 net.cpp:301] conv4_1_bn0 needs backward computation.
I1002 12:07:06.078864 30882 net.cpp:301] conv4_1_0 needs backward computation.
I1002 12:07:06.078871 30882 net.cpp:301] conv3_Eltwise_3_conv3_3ReLU_1_0_split needs backward computation.
I1002 12:07:06.078876 30882 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I1002 12:07:06.078879 30882 net.cpp:301] conv3_Eltwise_3 needs backward computation.
I1002 12:07:06.078884 30882 net.cpp:301] conv3_3_scale1 needs backward computation.
I1002 12:07:06.078889 30882 net.cpp:301] conv3_3bn1 needs backward computation.
I1002 12:07:06.078893 30882 net.cpp:301] conv3_3_1 needs backward computation.
I1002 12:07:06.078898 30882 net.cpp:301] conv3_Drop3 needs backward computation.
I1002 12:07:06.078902 30882 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I1002 12:07:06.078907 30882 net.cpp:301] conv3_3_scale0 needs backward computation.
I1002 12:07:06.078910 30882 net.cpp:301] conv3_3_bn0 needs backward computation.
I1002 12:07:06.078922 30882 net.cpp:301] conv3_3_0 needs backward computation.
I1002 12:07:06.078927 30882 net.cpp:301] conv3_Eltwise_2_conv3_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.078933 30882 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I1002 12:07:06.078936 30882 net.cpp:301] conv3_Eltwise_2 needs backward computation.
I1002 12:07:06.078943 30882 net.cpp:301] conv3_2_scale1 needs backward computation.
I1002 12:07:06.078946 30882 net.cpp:301] conv3_2bn1 needs backward computation.
I1002 12:07:06.078951 30882 net.cpp:301] conv3_2_1 needs backward computation.
I1002 12:07:06.078955 30882 net.cpp:301] conv3_Drop2 needs backward computation.
I1002 12:07:06.078959 30882 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I1002 12:07:06.078963 30882 net.cpp:301] conv3_2_scale0 needs backward computation.
I1002 12:07:06.078968 30882 net.cpp:301] conv3_2_bn0 needs backward computation.
I1002 12:07:06.078971 30882 net.cpp:301] conv3_2_0 needs backward computation.
I1002 12:07:06.078976 30882 net.cpp:301] conv3_Eltwise_1_conv3_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.078981 30882 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I1002 12:07:06.078985 30882 net.cpp:301] conv3_Eltwise_1 needs backward computation.
I1002 12:07:06.078991 30882 net.cpp:301] conv3_1_scale_down needs backward computation.
I1002 12:07:06.078995 30882 net.cpp:301] conv3_1_bn_down needs backward computation.
I1002 12:07:06.078999 30882 net.cpp:301] conv3_1_down needs backward computation.
I1002 12:07:06.079005 30882 net.cpp:301] conv3_1_scale1 needs backward computation.
I1002 12:07:06.079008 30882 net.cpp:301] conv3_1bn1 needs backward computation.
I1002 12:07:06.079013 30882 net.cpp:301] conv3_1_1 needs backward computation.
I1002 12:07:06.079018 30882 net.cpp:301] conv3_Drop1 needs backward computation.
I1002 12:07:06.079022 30882 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I1002 12:07:06.079026 30882 net.cpp:301] conv3_1_scale0 needs backward computation.
I1002 12:07:06.079031 30882 net.cpp:301] conv3_1_bn0 needs backward computation.
I1002 12:07:06.079035 30882 net.cpp:301] conv3_1_0 needs backward computation.
I1002 12:07:06.079041 30882 net.cpp:301] conv2_Eltwise_3_conv2_3ReLU_1_0_split needs backward computation.
I1002 12:07:06.079046 30882 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I1002 12:07:06.079051 30882 net.cpp:301] conv2_Eltwise_3 needs backward computation.
I1002 12:07:06.079056 30882 net.cpp:301] conv2_3_scale1 needs backward computation.
I1002 12:07:06.079059 30882 net.cpp:301] conv2_3bn1 needs backward computation.
I1002 12:07:06.079063 30882 net.cpp:301] conv2_3_1 needs backward computation.
I1002 12:07:06.079068 30882 net.cpp:301] conv2_Drop3 needs backward computation.
I1002 12:07:06.079072 30882 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I1002 12:07:06.079077 30882 net.cpp:301] conv2_3_scale0 needs backward computation.
I1002 12:07:06.079080 30882 net.cpp:301] conv2_3_bn0 needs backward computation.
I1002 12:07:06.079085 30882 net.cpp:301] conv2_3_0 needs backward computation.
I1002 12:07:06.079090 30882 net.cpp:301] conv2_Eltwise_2_conv2_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.079094 30882 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I1002 12:07:06.079099 30882 net.cpp:301] conv2_Eltwise_2 needs backward computation.
I1002 12:07:06.079104 30882 net.cpp:301] conv2_2_scale1 needs backward computation.
I1002 12:07:06.079108 30882 net.cpp:301] conv2_2bn1 needs backward computation.
I1002 12:07:06.079113 30882 net.cpp:301] conv2_2_1 needs backward computation.
I1002 12:07:06.079118 30882 net.cpp:301] conv2_Drop2 needs backward computation.
I1002 12:07:06.079121 30882 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I1002 12:07:06.079125 30882 net.cpp:301] conv2_2_scale0 needs backward computation.
I1002 12:07:06.079129 30882 net.cpp:301] conv2_2_bn0 needs backward computation.
I1002 12:07:06.079133 30882 net.cpp:301] conv2_2_0 needs backward computation.
I1002 12:07:06.079139 30882 net.cpp:301] conv2_Eltwise_1_conv2_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.079150 30882 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I1002 12:07:06.079155 30882 net.cpp:301] conv2_Eltwise_1 needs backward computation.
I1002 12:07:06.079160 30882 net.cpp:301] conv2_1_scale_down needs backward computation.
I1002 12:07:06.079164 30882 net.cpp:301] conv2_1_bn_down needs backward computation.
I1002 12:07:06.079169 30882 net.cpp:301] conv2_1_down needs backward computation.
I1002 12:07:06.079174 30882 net.cpp:301] conv2_1_scale1 needs backward computation.
I1002 12:07:06.079179 30882 net.cpp:301] conv2_1bn1 needs backward computation.
I1002 12:07:06.079182 30882 net.cpp:301] conv2_1_1 needs backward computation.
I1002 12:07:06.079187 30882 net.cpp:301] conv2_Drop1 needs backward computation.
I1002 12:07:06.079191 30882 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I1002 12:07:06.079196 30882 net.cpp:301] conv2_1_scale0 needs backward computation.
I1002 12:07:06.079200 30882 net.cpp:301] conv2_1_bn0 needs backward computation.
I1002 12:07:06.079205 30882 net.cpp:301] conv2_1_0 needs backward computation.
I1002 12:07:06.079210 30882 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I1002 12:07:06.079214 30882 net.cpp:301] conv1/ReLU needs backward computation.
I1002 12:07:06.079218 30882 net.cpp:301] conv1/scale needs backward computation.
I1002 12:07:06.079223 30882 net.cpp:301] conv1/bn needs backward computation.
I1002 12:07:06.079227 30882 net.cpp:301] conv1 needs backward computation.
I1002 12:07:06.079232 30882 net.cpp:303] Data1 does not need backward computation.
I1002 12:07:06.079236 30882 net.cpp:348] This network produces output Softmax1
I1002 12:07:06.079290 30882 net.cpp:363] Network initialization done.
I1002 12:07:06.080516 30882 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_WRN_20.prototxt
I1002 12:07:06.080538 30882 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I1002 12:07:06.080548 30882 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_WRN_20.prototxt
I1002 12:07:06.081374 30882 net.cpp:82] Initializing net from parameters: 
name: "WRN-20"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "data"
  top: "label"
  transform_param {
    scale: 0.00390625
    mirror: false
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
  image_data_param {
    shuffle: false
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_Drop1"
  type: "Dropout"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_down"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn_down"
  type: "BatchNorm"
  bottom: "conv2_1_down"
  top: "conv2_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale_down"
  type: "Scale"
  bottom: "conv2_1_down"
  top: "conv2_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_1"
  type: "Eltwise"
  bottom: "conv2_1_1"
  bottom: "conv2_1_down"
  top: "conv2_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_1"
  top: "conv2_Eltwise_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_Drop2"
  type: "Dropout"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_2"
  type: "Eltwise"
  bottom: "conv2_Eltwise_1"
  bottom: "conv2_2_1"
  top: "conv2_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_2"
  top: "conv2_Eltwise_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_Drop3"
  type: "Dropout"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_3"
  type: "Eltwise"
  bottom: "conv2_Eltwise_2"
  bottom: "conv2_3_1"
  top: "conv2_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_3"
  top: "conv2_Eltwise_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_Drop1"
  type: "Dropout"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_1"
  type: "Eltwise"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_1"
  top: "conv3_Eltwise_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_Drop2"
  type: "Dropout"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_2"
  type: "Eltwise"
  bottom: "conv3_Eltwise_1"
  bottom: "conv3_2_1"
  top: "conv3_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_2"
  top: "conv3_Eltwise_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_Drop3"
  type: "Dropout"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_3"
  type: "Eltwise"
  bottom: "conv3_Eltwise_2"
  bottom: "conv3_3_1"
  top: "conv3_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_3"
  top: "conv3_Eltwise_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_Drop1"
  type: "Dropout"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_1"
  type: "Eltwise"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_1"
  top: "conv4_Eltwise_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_Drop2"
  type: "Dropout"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_2"
  type: "Eltwise"
  bottom: "conv4_Eltwise_1"
  bottom: "conv4_2_1"
  top: "conv4_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_2"
  top: "conv4_Eltwise_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_Drop3"
  type: "Dropout"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  dropout_param {
    dropout_ratio: 1
  }
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_3"
  type: "Eltwise"
  bottom: "conv4_Eltwise_2"
  bottom: "conv4_3_1"
  top: "conv4_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_3"
  top: "conv4_Eltwise_3"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "conv4_Eltwise_3"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax1"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label"
  top: "Softmax1"
}
layer {
  name: "prob"
  type: "Accuracy"
  bottom: "fc1"
  bottom: "label"
  top: "prob"
}
I1002 12:07:06.081781 30882 layer_factory.hpp:77] Creating layer Data1
I1002 12:07:06.081867 30882 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I1002 12:07:06.081885 30882 net.cpp:128] Creating Layer Data1
I1002 12:07:06.081892 30882 net.cpp:522] Data1 -> data
I1002 12:07:06.081912 30882 net.cpp:522] Data1 -> label
I1002 12:07:06.082067 30882 data_layer.cpp:45] output data size: 10,3,32,32
I1002 12:07:06.082819 30882 net.cpp:172] Setting up Data1
I1002 12:07:06.082834 30882 net.cpp:186] Top shape: 10 3 32 32 (30720)
I1002 12:07:06.082840 30882 net.cpp:186] Top shape: 10 (10)
I1002 12:07:06.082844 30882 net.cpp:194] Memory required for data: 122920
I1002 12:07:06.082849 30882 layer_factory.hpp:77] Creating layer label_Data1_1_split
I1002 12:07:06.082859 30882 net.cpp:128] Creating Layer label_Data1_1_split
I1002 12:07:06.082862 30882 net.cpp:558] label_Data1_1_split <- label
I1002 12:07:06.082870 30882 net.cpp:522] label_Data1_1_split -> label_Data1_1_split_0
I1002 12:07:06.082878 30882 net.cpp:522] label_Data1_1_split -> label_Data1_1_split_1
I1002 12:07:06.082933 30882 net.cpp:172] Setting up label_Data1_1_split
I1002 12:07:06.082940 30882 net.cpp:186] Top shape: 10 (10)
I1002 12:07:06.082947 30882 net.cpp:186] Top shape: 10 (10)
I1002 12:07:06.082949 30882 net.cpp:194] Memory required for data: 123000
I1002 12:07:06.082953 30882 layer_factory.hpp:77] Creating layer conv1
I1002 12:07:06.082964 30882 net.cpp:128] Creating Layer conv1
I1002 12:07:06.082969 30882 net.cpp:558] conv1 <- data
I1002 12:07:06.082976 30882 net.cpp:522] conv1 -> conv1
I1002 12:07:06.084314 30882 net.cpp:172] Setting up conv1
I1002 12:07:06.084332 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.084337 30882 net.cpp:194] Memory required for data: 778360
I1002 12:07:06.084349 30882 layer_factory.hpp:77] Creating layer conv1/bn
I1002 12:07:06.084360 30882 net.cpp:128] Creating Layer conv1/bn
I1002 12:07:06.084365 30882 net.cpp:558] conv1/bn <- conv1
I1002 12:07:06.084372 30882 net.cpp:509] conv1/bn -> conv1 (in-place)
I1002 12:07:06.084645 30882 net.cpp:172] Setting up conv1/bn
I1002 12:07:06.084653 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.084656 30882 net.cpp:194] Memory required for data: 1433720
I1002 12:07:06.084668 30882 layer_factory.hpp:77] Creating layer conv1/scale
I1002 12:07:06.084677 30882 net.cpp:128] Creating Layer conv1/scale
I1002 12:07:06.084681 30882 net.cpp:558] conv1/scale <- conv1
I1002 12:07:06.084687 30882 net.cpp:509] conv1/scale -> conv1 (in-place)
I1002 12:07:06.084738 30882 layer_factory.hpp:77] Creating layer conv1/scale
I1002 12:07:06.084908 30882 net.cpp:172] Setting up conv1/scale
I1002 12:07:06.084918 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.084920 30882 net.cpp:194] Memory required for data: 2089080
I1002 12:07:06.084928 30882 layer_factory.hpp:77] Creating layer conv1/ReLU
I1002 12:07:06.084935 30882 net.cpp:128] Creating Layer conv1/ReLU
I1002 12:07:06.084939 30882 net.cpp:558] conv1/ReLU <- conv1
I1002 12:07:06.084945 30882 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I1002 12:07:06.085182 30882 net.cpp:172] Setting up conv1/ReLU
I1002 12:07:06.085192 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.085196 30882 net.cpp:194] Memory required for data: 2744440
I1002 12:07:06.085201 30882 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I1002 12:07:06.085207 30882 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I1002 12:07:06.085211 30882 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I1002 12:07:06.085217 30882 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I1002 12:07:06.085225 30882 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I1002 12:07:06.085276 30882 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I1002 12:07:06.085283 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.085289 30882 net.cpp:186] Top shape: 10 16 32 32 (163840)
I1002 12:07:06.085292 30882 net.cpp:194] Memory required for data: 4055160
I1002 12:07:06.085296 30882 layer_factory.hpp:77] Creating layer conv2_1_0
I1002 12:07:06.085306 30882 net.cpp:128] Creating Layer conv2_1_0
I1002 12:07:06.085310 30882 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I1002 12:07:06.085317 30882 net.cpp:522] conv2_1_0 -> conv2_1_0
I1002 12:07:06.086891 30882 net.cpp:172] Setting up conv2_1_0
I1002 12:07:06.086910 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.086913 30882 net.cpp:194] Memory required for data: 9298040
I1002 12:07:06.086925 30882 layer_factory.hpp:77] Creating layer conv2_1_bn0
I1002 12:07:06.086935 30882 net.cpp:128] Creating Layer conv2_1_bn0
I1002 12:07:06.086941 30882 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I1002 12:07:06.086946 30882 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I1002 12:07:06.087205 30882 net.cpp:172] Setting up conv2_1_bn0
I1002 12:07:06.087219 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.087224 30882 net.cpp:194] Memory required for data: 14540920
I1002 12:07:06.087232 30882 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1002 12:07:06.087239 30882 net.cpp:128] Creating Layer conv2_1_scale0
I1002 12:07:06.087244 30882 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I1002 12:07:06.087249 30882 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I1002 12:07:06.087298 30882 layer_factory.hpp:77] Creating layer conv2_1_scale0
I1002 12:07:06.087450 30882 net.cpp:172] Setting up conv2_1_scale0
I1002 12:07:06.087462 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.087466 30882 net.cpp:194] Memory required for data: 19783800
I1002 12:07:06.087474 30882 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I1002 12:07:06.087481 30882 net.cpp:128] Creating Layer conv2_1_ReLU0
I1002 12:07:06.087484 30882 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I1002 12:07:06.087491 30882 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I1002 12:07:06.088277 30882 net.cpp:172] Setting up conv2_1_ReLU0
I1002 12:07:06.088300 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.088305 30882 net.cpp:194] Memory required for data: 25026680
I1002 12:07:06.088310 30882 layer_factory.hpp:77] Creating layer conv2_Drop1
I1002 12:07:06.088322 30882 net.cpp:128] Creating Layer conv2_Drop1
I1002 12:07:06.088327 30882 net.cpp:558] conv2_Drop1 <- conv2_1_0
I1002 12:07:06.088335 30882 net.cpp:509] conv2_Drop1 -> conv2_1_0 (in-place)
I1002 12:07:06.088402 30882 net.cpp:172] Setting up conv2_Drop1
I1002 12:07:06.088412 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.088415 30882 net.cpp:194] Memory required for data: 30269560
I1002 12:07:06.088419 30882 layer_factory.hpp:77] Creating layer conv2_1_1
I1002 12:07:06.088449 30882 net.cpp:128] Creating Layer conv2_1_1
I1002 12:07:06.088454 30882 net.cpp:558] conv2_1_1 <- conv2_1_0
I1002 12:07:06.088461 30882 net.cpp:522] conv2_1_1 -> conv2_1_1
I1002 12:07:06.092279 30882 net.cpp:172] Setting up conv2_1_1
I1002 12:07:06.092308 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.092311 30882 net.cpp:194] Memory required for data: 35512440
I1002 12:07:06.092321 30882 layer_factory.hpp:77] Creating layer conv2_1bn1
I1002 12:07:06.092331 30882 net.cpp:128] Creating Layer conv2_1bn1
I1002 12:07:06.092335 30882 net.cpp:558] conv2_1bn1 <- conv2_1_1
I1002 12:07:06.092342 30882 net.cpp:509] conv2_1bn1 -> conv2_1_1 (in-place)
I1002 12:07:06.092613 30882 net.cpp:172] Setting up conv2_1bn1
I1002 12:07:06.092623 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.092628 30882 net.cpp:194] Memory required for data: 40755320
I1002 12:07:06.092639 30882 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1002 12:07:06.092648 30882 net.cpp:128] Creating Layer conv2_1_scale1
I1002 12:07:06.092653 30882 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I1002 12:07:06.092658 30882 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I1002 12:07:06.092707 30882 layer_factory.hpp:77] Creating layer conv2_1_scale1
I1002 12:07:06.092844 30882 net.cpp:172] Setting up conv2_1_scale1
I1002 12:07:06.092854 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.092859 30882 net.cpp:194] Memory required for data: 45998200
I1002 12:07:06.092866 30882 layer_factory.hpp:77] Creating layer conv2_1_down
I1002 12:07:06.092876 30882 net.cpp:128] Creating Layer conv2_1_down
I1002 12:07:06.092882 30882 net.cpp:558] conv2_1_down <- conv1_conv1/ReLU_0_split_1
I1002 12:07:06.092890 30882 net.cpp:522] conv2_1_down -> conv2_1_down
I1002 12:07:06.094166 30882 net.cpp:172] Setting up conv2_1_down
I1002 12:07:06.094192 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.094195 30882 net.cpp:194] Memory required for data: 51241080
I1002 12:07:06.094205 30882 layer_factory.hpp:77] Creating layer conv2_1_bn_down
I1002 12:07:06.094220 30882 net.cpp:128] Creating Layer conv2_1_bn_down
I1002 12:07:06.094225 30882 net.cpp:558] conv2_1_bn_down <- conv2_1_down
I1002 12:07:06.094233 30882 net.cpp:509] conv2_1_bn_down -> conv2_1_down (in-place)
I1002 12:07:06.094488 30882 net.cpp:172] Setting up conv2_1_bn_down
I1002 12:07:06.094497 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.094501 30882 net.cpp:194] Memory required for data: 56483960
I1002 12:07:06.094511 30882 layer_factory.hpp:77] Creating layer conv2_1_scale_down
I1002 12:07:06.094518 30882 net.cpp:128] Creating Layer conv2_1_scale_down
I1002 12:07:06.094525 30882 net.cpp:558] conv2_1_scale_down <- conv2_1_down
I1002 12:07:06.094532 30882 net.cpp:509] conv2_1_scale_down -> conv2_1_down (in-place)
I1002 12:07:06.094580 30882 layer_factory.hpp:77] Creating layer conv2_1_scale_down
I1002 12:07:06.094718 30882 net.cpp:172] Setting up conv2_1_scale_down
I1002 12:07:06.094728 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.094732 30882 net.cpp:194] Memory required for data: 61726840
I1002 12:07:06.094740 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_1
I1002 12:07:06.094748 30882 net.cpp:128] Creating Layer conv2_Eltwise_1
I1002 12:07:06.094753 30882 net.cpp:558] conv2_Eltwise_1 <- conv2_1_1
I1002 12:07:06.094758 30882 net.cpp:558] conv2_Eltwise_1 <- conv2_1_down
I1002 12:07:06.094763 30882 net.cpp:522] conv2_Eltwise_1 -> conv2_Eltwise_1
I1002 12:07:06.094792 30882 net.cpp:172] Setting up conv2_Eltwise_1
I1002 12:07:06.094801 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.094805 30882 net.cpp:194] Memory required for data: 66969720
I1002 12:07:06.094810 30882 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I1002 12:07:06.094815 30882 net.cpp:128] Creating Layer conv2_1ReLU_1
I1002 12:07:06.094820 30882 net.cpp:558] conv2_1ReLU_1 <- conv2_Eltwise_1
I1002 12:07:06.094825 30882 net.cpp:509] conv2_1ReLU_1 -> conv2_Eltwise_1 (in-place)
I1002 12:07:06.095083 30882 net.cpp:172] Setting up conv2_1ReLU_1
I1002 12:07:06.095098 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.095101 30882 net.cpp:194] Memory required for data: 72212600
I1002 12:07:06.095106 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:06.095113 30882 net.cpp:128] Creating Layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:06.095118 30882 net.cpp:558] conv2_Eltwise_1_conv2_1ReLU_1_0_split <- conv2_Eltwise_1
I1002 12:07:06.095124 30882 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I1002 12:07:06.095132 30882 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I1002 12:07:06.095185 30882 net.cpp:172] Setting up conv2_Eltwise_1_conv2_1ReLU_1_0_split
I1002 12:07:06.095194 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.095199 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.095203 30882 net.cpp:194] Memory required for data: 82698360
I1002 12:07:06.095207 30882 layer_factory.hpp:77] Creating layer conv2_2_0
I1002 12:07:06.095218 30882 net.cpp:128] Creating Layer conv2_2_0
I1002 12:07:06.095226 30882 net.cpp:558] conv2_2_0 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I1002 12:07:06.095232 30882 net.cpp:522] conv2_2_0 -> conv2_2_0
I1002 12:07:06.100275 30882 net.cpp:172] Setting up conv2_2_0
I1002 12:07:06.100302 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.100307 30882 net.cpp:194] Memory required for data: 87941240
I1002 12:07:06.100322 30882 layer_factory.hpp:77] Creating layer conv2_2_bn0
I1002 12:07:06.100337 30882 net.cpp:128] Creating Layer conv2_2_bn0
I1002 12:07:06.100342 30882 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I1002 12:07:06.100353 30882 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I1002 12:07:06.100625 30882 net.cpp:172] Setting up conv2_2_bn0
I1002 12:07:06.100634 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.100638 30882 net.cpp:194] Memory required for data: 93184120
I1002 12:07:06.100653 30882 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1002 12:07:06.100661 30882 net.cpp:128] Creating Layer conv2_2_scale0
I1002 12:07:06.100666 30882 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I1002 12:07:06.100672 30882 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I1002 12:07:06.100723 30882 layer_factory.hpp:77] Creating layer conv2_2_scale0
I1002 12:07:06.100862 30882 net.cpp:172] Setting up conv2_2_scale0
I1002 12:07:06.100872 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.100877 30882 net.cpp:194] Memory required for data: 98427000
I1002 12:07:06.100885 30882 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I1002 12:07:06.100891 30882 net.cpp:128] Creating Layer conv2_2_ReLU0
I1002 12:07:06.100900 30882 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I1002 12:07:06.100906 30882 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I1002 12:07:06.101137 30882 net.cpp:172] Setting up conv2_2_ReLU0
I1002 12:07:06.101151 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.101155 30882 net.cpp:194] Memory required for data: 103669880
I1002 12:07:06.101160 30882 layer_factory.hpp:77] Creating layer conv2_Drop2
I1002 12:07:06.101167 30882 net.cpp:128] Creating Layer conv2_Drop2
I1002 12:07:06.101172 30882 net.cpp:558] conv2_Drop2 <- conv2_2_0
I1002 12:07:06.101178 30882 net.cpp:509] conv2_Drop2 -> conv2_2_0 (in-place)
I1002 12:07:06.101212 30882 net.cpp:172] Setting up conv2_Drop2
I1002 12:07:06.101220 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.101224 30882 net.cpp:194] Memory required for data: 108912760
I1002 12:07:06.101229 30882 layer_factory.hpp:77] Creating layer conv2_2_1
I1002 12:07:06.101239 30882 net.cpp:128] Creating Layer conv2_2_1
I1002 12:07:06.101244 30882 net.cpp:558] conv2_2_1 <- conv2_2_0
I1002 12:07:06.101250 30882 net.cpp:522] conv2_2_1 -> conv2_2_1
I1002 12:07:06.104979 30882 net.cpp:172] Setting up conv2_2_1
I1002 12:07:06.105005 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.105034 30882 net.cpp:194] Memory required for data: 114155640
I1002 12:07:06.105048 30882 layer_factory.hpp:77] Creating layer conv2_2bn1
I1002 12:07:06.105060 30882 net.cpp:128] Creating Layer conv2_2bn1
I1002 12:07:06.105065 30882 net.cpp:558] conv2_2bn1 <- conv2_2_1
I1002 12:07:06.105072 30882 net.cpp:509] conv2_2bn1 -> conv2_2_1 (in-place)
I1002 12:07:06.105353 30882 net.cpp:172] Setting up conv2_2bn1
I1002 12:07:06.105363 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.105368 30882 net.cpp:194] Memory required for data: 119398520
I1002 12:07:06.105378 30882 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1002 12:07:06.105392 30882 net.cpp:128] Creating Layer conv2_2_scale1
I1002 12:07:06.105396 30882 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I1002 12:07:06.105402 30882 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I1002 12:07:06.105451 30882 layer_factory.hpp:77] Creating layer conv2_2_scale1
I1002 12:07:06.105592 30882 net.cpp:172] Setting up conv2_2_scale1
I1002 12:07:06.105602 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.105605 30882 net.cpp:194] Memory required for data: 124641400
I1002 12:07:06.105613 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_2
I1002 12:07:06.105621 30882 net.cpp:128] Creating Layer conv2_Eltwise_2
I1002 12:07:06.105625 30882 net.cpp:558] conv2_Eltwise_2 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I1002 12:07:06.105630 30882 net.cpp:558] conv2_Eltwise_2 <- conv2_2_1
I1002 12:07:06.105638 30882 net.cpp:522] conv2_Eltwise_2 -> conv2_Eltwise_2
I1002 12:07:06.105665 30882 net.cpp:172] Setting up conv2_Eltwise_2
I1002 12:07:06.105674 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.105679 30882 net.cpp:194] Memory required for data: 129884280
I1002 12:07:06.105682 30882 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I1002 12:07:06.105690 30882 net.cpp:128] Creating Layer conv2_2ReLU_1
I1002 12:07:06.105695 30882 net.cpp:558] conv2_2ReLU_1 <- conv2_Eltwise_2
I1002 12:07:06.105700 30882 net.cpp:509] conv2_2ReLU_1 -> conv2_Eltwise_2 (in-place)
I1002 12:07:06.106140 30882 net.cpp:172] Setting up conv2_2ReLU_1
I1002 12:07:06.106161 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.106165 30882 net.cpp:194] Memory required for data: 135127160
I1002 12:07:06.106170 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:06.106178 30882 net.cpp:128] Creating Layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:06.106184 30882 net.cpp:558] conv2_Eltwise_2_conv2_2ReLU_1_0_split <- conv2_Eltwise_2
I1002 12:07:06.106190 30882 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I1002 12:07:06.106201 30882 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I1002 12:07:06.106261 30882 net.cpp:172] Setting up conv2_Eltwise_2_conv2_2ReLU_1_0_split
I1002 12:07:06.106272 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.106278 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.106282 30882 net.cpp:194] Memory required for data: 145612920
I1002 12:07:06.106287 30882 layer_factory.hpp:77] Creating layer conv2_3_0
I1002 12:07:06.106298 30882 net.cpp:128] Creating Layer conv2_3_0
I1002 12:07:06.106302 30882 net.cpp:558] conv2_3_0 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I1002 12:07:06.106309 30882 net.cpp:522] conv2_3_0 -> conv2_3_0
I1002 12:07:06.110069 30882 net.cpp:172] Setting up conv2_3_0
I1002 12:07:06.110095 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.110100 30882 net.cpp:194] Memory required for data: 150855800
I1002 12:07:06.110110 30882 layer_factory.hpp:77] Creating layer conv2_3_bn0
I1002 12:07:06.110119 30882 net.cpp:128] Creating Layer conv2_3_bn0
I1002 12:07:06.110124 30882 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I1002 12:07:06.110136 30882 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I1002 12:07:06.110414 30882 net.cpp:172] Setting up conv2_3_bn0
I1002 12:07:06.110424 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.110445 30882 net.cpp:194] Memory required for data: 156098680
I1002 12:07:06.110455 30882 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1002 12:07:06.110463 30882 net.cpp:128] Creating Layer conv2_3_scale0
I1002 12:07:06.110467 30882 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I1002 12:07:06.110474 30882 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I1002 12:07:06.110525 30882 layer_factory.hpp:77] Creating layer conv2_3_scale0
I1002 12:07:06.110671 30882 net.cpp:172] Setting up conv2_3_scale0
I1002 12:07:06.110680 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.110685 30882 net.cpp:194] Memory required for data: 161341560
I1002 12:07:06.110692 30882 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I1002 12:07:06.110699 30882 net.cpp:128] Creating Layer conv2_3_ReLU0
I1002 12:07:06.110703 30882 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I1002 12:07:06.110709 30882 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I1002 12:07:06.110944 30882 net.cpp:172] Setting up conv2_3_ReLU0
I1002 12:07:06.110957 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.110962 30882 net.cpp:194] Memory required for data: 166584440
I1002 12:07:06.110966 30882 layer_factory.hpp:77] Creating layer conv2_Drop3
I1002 12:07:06.110973 30882 net.cpp:128] Creating Layer conv2_Drop3
I1002 12:07:06.110978 30882 net.cpp:558] conv2_Drop3 <- conv2_3_0
I1002 12:07:06.110985 30882 net.cpp:509] conv2_Drop3 -> conv2_3_0 (in-place)
I1002 12:07:06.111016 30882 net.cpp:172] Setting up conv2_Drop3
I1002 12:07:06.111023 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.111027 30882 net.cpp:194] Memory required for data: 171827320
I1002 12:07:06.111032 30882 layer_factory.hpp:77] Creating layer conv2_3_1
I1002 12:07:06.111040 30882 net.cpp:128] Creating Layer conv2_3_1
I1002 12:07:06.111045 30882 net.cpp:558] conv2_3_1 <- conv2_3_0
I1002 12:07:06.111052 30882 net.cpp:522] conv2_3_1 -> conv2_3_1
I1002 12:07:06.116084 30882 net.cpp:172] Setting up conv2_3_1
I1002 12:07:06.116112 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.116117 30882 net.cpp:194] Memory required for data: 177070200
I1002 12:07:06.116127 30882 layer_factory.hpp:77] Creating layer conv2_3bn1
I1002 12:07:06.116138 30882 net.cpp:128] Creating Layer conv2_3bn1
I1002 12:07:06.116143 30882 net.cpp:558] conv2_3bn1 <- conv2_3_1
I1002 12:07:06.116154 30882 net.cpp:509] conv2_3bn1 -> conv2_3_1 (in-place)
I1002 12:07:06.116438 30882 net.cpp:172] Setting up conv2_3bn1
I1002 12:07:06.116449 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.116453 30882 net.cpp:194] Memory required for data: 182313080
I1002 12:07:06.116463 30882 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1002 12:07:06.116472 30882 net.cpp:128] Creating Layer conv2_3_scale1
I1002 12:07:06.116475 30882 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I1002 12:07:06.116482 30882 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I1002 12:07:06.116533 30882 layer_factory.hpp:77] Creating layer conv2_3_scale1
I1002 12:07:06.116678 30882 net.cpp:172] Setting up conv2_3_scale1
I1002 12:07:06.116689 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.116694 30882 net.cpp:194] Memory required for data: 187555960
I1002 12:07:06.116701 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_3
I1002 12:07:06.116709 30882 net.cpp:128] Creating Layer conv2_Eltwise_3
I1002 12:07:06.116714 30882 net.cpp:558] conv2_Eltwise_3 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I1002 12:07:06.116719 30882 net.cpp:558] conv2_Eltwise_3 <- conv2_3_1
I1002 12:07:06.116725 30882 net.cpp:522] conv2_Eltwise_3 -> conv2_Eltwise_3
I1002 12:07:06.116755 30882 net.cpp:172] Setting up conv2_Eltwise_3
I1002 12:07:06.116763 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.116766 30882 net.cpp:194] Memory required for data: 192798840
I1002 12:07:06.116770 30882 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I1002 12:07:06.116777 30882 net.cpp:128] Creating Layer conv2_3ReLU_1
I1002 12:07:06.116781 30882 net.cpp:558] conv2_3ReLU_1 <- conv2_Eltwise_3
I1002 12:07:06.116806 30882 net.cpp:509] conv2_3ReLU_1 -> conv2_Eltwise_3 (in-place)
I1002 12:07:06.117049 30882 net.cpp:172] Setting up conv2_3ReLU_1
I1002 12:07:06.117063 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.117069 30882 net.cpp:194] Memory required for data: 198041720
I1002 12:07:06.117072 30882 layer_factory.hpp:77] Creating layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:06.117080 30882 net.cpp:128] Creating Layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:06.117084 30882 net.cpp:558] conv2_Eltwise_3_conv2_3ReLU_1_0_split <- conv2_Eltwise_3
I1002 12:07:06.117091 30882 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I1002 12:07:06.117100 30882 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I1002 12:07:06.117154 30882 net.cpp:172] Setting up conv2_Eltwise_3_conv2_3ReLU_1_0_split
I1002 12:07:06.117164 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.117171 30882 net.cpp:186] Top shape: 10 128 32 32 (1310720)
I1002 12:07:06.117173 30882 net.cpp:194] Memory required for data: 208527480
I1002 12:07:06.117178 30882 layer_factory.hpp:77] Creating layer conv3_1_0
I1002 12:07:06.117188 30882 net.cpp:128] Creating Layer conv3_1_0
I1002 12:07:06.117193 30882 net.cpp:558] conv3_1_0 <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I1002 12:07:06.117200 30882 net.cpp:522] conv3_1_0 -> conv3_1_0
I1002 12:07:06.123100 30882 net.cpp:172] Setting up conv3_1_0
I1002 12:07:06.123126 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.123131 30882 net.cpp:194] Memory required for data: 211148920
I1002 12:07:06.123142 30882 layer_factory.hpp:77] Creating layer conv3_1_bn0
I1002 12:07:06.123150 30882 net.cpp:128] Creating Layer conv3_1_bn0
I1002 12:07:06.123155 30882 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I1002 12:07:06.123167 30882 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I1002 12:07:06.123430 30882 net.cpp:172] Setting up conv3_1_bn0
I1002 12:07:06.123441 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.123445 30882 net.cpp:194] Memory required for data: 213770360
I1002 12:07:06.123455 30882 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1002 12:07:06.123462 30882 net.cpp:128] Creating Layer conv3_1_scale0
I1002 12:07:06.123466 30882 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I1002 12:07:06.123472 30882 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I1002 12:07:06.123522 30882 layer_factory.hpp:77] Creating layer conv3_1_scale0
I1002 12:07:06.123669 30882 net.cpp:172] Setting up conv3_1_scale0
I1002 12:07:06.123679 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.123683 30882 net.cpp:194] Memory required for data: 216391800
I1002 12:07:06.123692 30882 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I1002 12:07:06.123698 30882 net.cpp:128] Creating Layer conv3_1_ReLU0
I1002 12:07:06.123703 30882 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I1002 12:07:06.123708 30882 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I1002 12:07:06.123942 30882 net.cpp:172] Setting up conv3_1_ReLU0
I1002 12:07:06.123956 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.123960 30882 net.cpp:194] Memory required for data: 219013240
I1002 12:07:06.123965 30882 layer_factory.hpp:77] Creating layer conv3_Drop1
I1002 12:07:06.123972 30882 net.cpp:128] Creating Layer conv3_Drop1
I1002 12:07:06.123976 30882 net.cpp:558] conv3_Drop1 <- conv3_1_0
I1002 12:07:06.123982 30882 net.cpp:509] conv3_Drop1 -> conv3_1_0 (in-place)
I1002 12:07:06.124011 30882 net.cpp:172] Setting up conv3_Drop1
I1002 12:07:06.124017 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.124022 30882 net.cpp:194] Memory required for data: 221634680
I1002 12:07:06.124025 30882 layer_factory.hpp:77] Creating layer conv3_1_1
I1002 12:07:06.124035 30882 net.cpp:128] Creating Layer conv3_1_1
I1002 12:07:06.124039 30882 net.cpp:558] conv3_1_1 <- conv3_1_0
I1002 12:07:06.124047 30882 net.cpp:522] conv3_1_1 -> conv3_1_1
I1002 12:07:06.136359 30882 net.cpp:172] Setting up conv3_1_1
I1002 12:07:06.136389 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.136395 30882 net.cpp:194] Memory required for data: 224256120
I1002 12:07:06.136416 30882 layer_factory.hpp:77] Creating layer conv3_1bn1
I1002 12:07:06.136433 30882 net.cpp:128] Creating Layer conv3_1bn1
I1002 12:07:06.136438 30882 net.cpp:558] conv3_1bn1 <- conv3_1_1
I1002 12:07:06.136447 30882 net.cpp:509] conv3_1bn1 -> conv3_1_1 (in-place)
I1002 12:07:06.136737 30882 net.cpp:172] Setting up conv3_1bn1
I1002 12:07:06.136750 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.136754 30882 net.cpp:194] Memory required for data: 226877560
I1002 12:07:06.136765 30882 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1002 12:07:06.136771 30882 net.cpp:128] Creating Layer conv3_1_scale1
I1002 12:07:06.136775 30882 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I1002 12:07:06.136781 30882 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I1002 12:07:06.136835 30882 layer_factory.hpp:77] Creating layer conv3_1_scale1
I1002 12:07:06.136991 30882 net.cpp:172] Setting up conv3_1_scale1
I1002 12:07:06.137002 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.137006 30882 net.cpp:194] Memory required for data: 229499000
I1002 12:07:06.137013 30882 layer_factory.hpp:77] Creating layer conv3_1_down
I1002 12:07:06.137027 30882 net.cpp:128] Creating Layer conv3_1_down
I1002 12:07:06.137032 30882 net.cpp:558] conv3_1_down <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I1002 12:07:06.137042 30882 net.cpp:522] conv3_1_down -> conv3_1_down
I1002 12:07:06.138897 30882 net.cpp:172] Setting up conv3_1_down
I1002 12:07:06.138919 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.138924 30882 net.cpp:194] Memory required for data: 232120440
I1002 12:07:06.138933 30882 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I1002 12:07:06.138949 30882 net.cpp:128] Creating Layer conv3_1_bn_down
I1002 12:07:06.138954 30882 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I1002 12:07:06.138964 30882 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I1002 12:07:06.139243 30882 net.cpp:172] Setting up conv3_1_bn_down
I1002 12:07:06.139253 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.139257 30882 net.cpp:194] Memory required for data: 234741880
I1002 12:07:06.139267 30882 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1002 12:07:06.139274 30882 net.cpp:128] Creating Layer conv3_1_scale_down
I1002 12:07:06.139278 30882 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I1002 12:07:06.139288 30882 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I1002 12:07:06.139338 30882 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I1002 12:07:06.139493 30882 net.cpp:172] Setting up conv3_1_scale_down
I1002 12:07:06.139504 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.139508 30882 net.cpp:194] Memory required for data: 237363320
I1002 12:07:06.139515 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_1
I1002 12:07:06.139525 30882 net.cpp:128] Creating Layer conv3_Eltwise_1
I1002 12:07:06.139530 30882 net.cpp:558] conv3_Eltwise_1 <- conv3_1_1
I1002 12:07:06.139535 30882 net.cpp:558] conv3_Eltwise_1 <- conv3_1_down
I1002 12:07:06.139541 30882 net.cpp:522] conv3_Eltwise_1 -> conv3_Eltwise_1
I1002 12:07:06.139565 30882 net.cpp:172] Setting up conv3_Eltwise_1
I1002 12:07:06.139572 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.139576 30882 net.cpp:194] Memory required for data: 239984760
I1002 12:07:06.139580 30882 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I1002 12:07:06.139590 30882 net.cpp:128] Creating Layer conv3_1ReLU_1
I1002 12:07:06.139593 30882 net.cpp:558] conv3_1ReLU_1 <- conv3_Eltwise_1
I1002 12:07:06.139600 30882 net.cpp:509] conv3_1ReLU_1 -> conv3_Eltwise_1 (in-place)
I1002 12:07:06.140046 30882 net.cpp:172] Setting up conv3_1ReLU_1
I1002 12:07:06.140070 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.140075 30882 net.cpp:194] Memory required for data: 242606200
I1002 12:07:06.140100 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:06.140110 30882 net.cpp:128] Creating Layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:06.140115 30882 net.cpp:558] conv3_Eltwise_1_conv3_1ReLU_1_0_split <- conv3_Eltwise_1
I1002 12:07:06.140123 30882 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I1002 12:07:06.140132 30882 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I1002 12:07:06.140194 30882 net.cpp:172] Setting up conv3_Eltwise_1_conv3_1ReLU_1_0_split
I1002 12:07:06.140205 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.140210 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.140214 30882 net.cpp:194] Memory required for data: 247849080
I1002 12:07:06.140218 30882 layer_factory.hpp:77] Creating layer conv3_2_0
I1002 12:07:06.140233 30882 net.cpp:128] Creating Layer conv3_2_0
I1002 12:07:06.140239 30882 net.cpp:558] conv3_2_0 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I1002 12:07:06.140245 30882 net.cpp:522] conv3_2_0 -> conv3_2_0
I1002 12:07:06.152405 30882 net.cpp:172] Setting up conv3_2_0
I1002 12:07:06.152433 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.152437 30882 net.cpp:194] Memory required for data: 250470520
I1002 12:07:06.152448 30882 layer_factory.hpp:77] Creating layer conv3_2_bn0
I1002 12:07:06.152462 30882 net.cpp:128] Creating Layer conv3_2_bn0
I1002 12:07:06.152467 30882 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I1002 12:07:06.152479 30882 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I1002 12:07:06.152775 30882 net.cpp:172] Setting up conv3_2_bn0
I1002 12:07:06.152786 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.152789 30882 net.cpp:194] Memory required for data: 253091960
I1002 12:07:06.152799 30882 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1002 12:07:06.152806 30882 net.cpp:128] Creating Layer conv3_2_scale0
I1002 12:07:06.152810 30882 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I1002 12:07:06.152818 30882 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I1002 12:07:06.152870 30882 layer_factory.hpp:77] Creating layer conv3_2_scale0
I1002 12:07:06.153029 30882 net.cpp:172] Setting up conv3_2_scale0
I1002 12:07:06.153040 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.153044 30882 net.cpp:194] Memory required for data: 255713400
I1002 12:07:06.153053 30882 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I1002 12:07:06.153060 30882 net.cpp:128] Creating Layer conv3_2_ReLU0
I1002 12:07:06.153065 30882 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I1002 12:07:06.153070 30882 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I1002 12:07:06.153317 30882 net.cpp:172] Setting up conv3_2_ReLU0
I1002 12:07:06.153331 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.153336 30882 net.cpp:194] Memory required for data: 258334840
I1002 12:07:06.153339 30882 layer_factory.hpp:77] Creating layer conv3_Drop2
I1002 12:07:06.153350 30882 net.cpp:128] Creating Layer conv3_Drop2
I1002 12:07:06.153355 30882 net.cpp:558] conv3_Drop2 <- conv3_2_0
I1002 12:07:06.153362 30882 net.cpp:509] conv3_Drop2 -> conv3_2_0 (in-place)
I1002 12:07:06.153389 30882 net.cpp:172] Setting up conv3_Drop2
I1002 12:07:06.153396 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.153400 30882 net.cpp:194] Memory required for data: 260956280
I1002 12:07:06.153404 30882 layer_factory.hpp:77] Creating layer conv3_2_1
I1002 12:07:06.153421 30882 net.cpp:128] Creating Layer conv3_2_1
I1002 12:07:06.153426 30882 net.cpp:558] conv3_2_1 <- conv3_2_0
I1002 12:07:06.153434 30882 net.cpp:522] conv3_2_1 -> conv3_2_1
I1002 12:07:06.165611 30882 net.cpp:172] Setting up conv3_2_1
I1002 12:07:06.165640 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.165645 30882 net.cpp:194] Memory required for data: 263577720
I1002 12:07:06.165655 30882 layer_factory.hpp:77] Creating layer conv3_2bn1
I1002 12:07:06.165668 30882 net.cpp:128] Creating Layer conv3_2bn1
I1002 12:07:06.165696 30882 net.cpp:558] conv3_2bn1 <- conv3_2_1
I1002 12:07:06.165706 30882 net.cpp:509] conv3_2bn1 -> conv3_2_1 (in-place)
I1002 12:07:06.166024 30882 net.cpp:172] Setting up conv3_2bn1
I1002 12:07:06.166038 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166043 30882 net.cpp:194] Memory required for data: 266199160
I1002 12:07:06.166052 30882 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1002 12:07:06.166065 30882 net.cpp:128] Creating Layer conv3_2_scale1
I1002 12:07:06.166070 30882 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I1002 12:07:06.166076 30882 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I1002 12:07:06.166133 30882 layer_factory.hpp:77] Creating layer conv3_2_scale1
I1002 12:07:06.166294 30882 net.cpp:172] Setting up conv3_2_scale1
I1002 12:07:06.166306 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166309 30882 net.cpp:194] Memory required for data: 268820600
I1002 12:07:06.166317 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_2
I1002 12:07:06.166328 30882 net.cpp:128] Creating Layer conv3_Eltwise_2
I1002 12:07:06.166333 30882 net.cpp:558] conv3_Eltwise_2 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I1002 12:07:06.166338 30882 net.cpp:558] conv3_Eltwise_2 <- conv3_2_1
I1002 12:07:06.166347 30882 net.cpp:522] conv3_Eltwise_2 -> conv3_Eltwise_2
I1002 12:07:06.166373 30882 net.cpp:172] Setting up conv3_Eltwise_2
I1002 12:07:06.166379 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166383 30882 net.cpp:194] Memory required for data: 271442040
I1002 12:07:06.166388 30882 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I1002 12:07:06.166394 30882 net.cpp:128] Creating Layer conv3_2ReLU_1
I1002 12:07:06.166399 30882 net.cpp:558] conv3_2ReLU_1 <- conv3_Eltwise_2
I1002 12:07:06.166406 30882 net.cpp:509] conv3_2ReLU_1 -> conv3_Eltwise_2 (in-place)
I1002 12:07:06.166658 30882 net.cpp:172] Setting up conv3_2ReLU_1
I1002 12:07:06.166672 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166676 30882 net.cpp:194] Memory required for data: 274063480
I1002 12:07:06.166682 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:06.166688 30882 net.cpp:128] Creating Layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:06.166693 30882 net.cpp:558] conv3_Eltwise_2_conv3_2ReLU_1_0_split <- conv3_Eltwise_2
I1002 12:07:06.166702 30882 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I1002 12:07:06.166723 30882 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I1002 12:07:06.166781 30882 net.cpp:172] Setting up conv3_Eltwise_2_conv3_2ReLU_1_0_split
I1002 12:07:06.166792 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166798 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.166802 30882 net.cpp:194] Memory required for data: 279306360
I1002 12:07:06.166806 30882 layer_factory.hpp:77] Creating layer conv3_3_0
I1002 12:07:06.166820 30882 net.cpp:128] Creating Layer conv3_3_0
I1002 12:07:06.166824 30882 net.cpp:558] conv3_3_0 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I1002 12:07:06.166833 30882 net.cpp:522] conv3_3_0 -> conv3_3_0
I1002 12:07:06.179014 30882 net.cpp:172] Setting up conv3_3_0
I1002 12:07:06.179045 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.179050 30882 net.cpp:194] Memory required for data: 281927800
I1002 12:07:06.179061 30882 layer_factory.hpp:77] Creating layer conv3_3_bn0
I1002 12:07:06.179070 30882 net.cpp:128] Creating Layer conv3_3_bn0
I1002 12:07:06.179075 30882 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I1002 12:07:06.179090 30882 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I1002 12:07:06.179390 30882 net.cpp:172] Setting up conv3_3_bn0
I1002 12:07:06.179400 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.179404 30882 net.cpp:194] Memory required for data: 284549240
I1002 12:07:06.179414 30882 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1002 12:07:06.179440 30882 net.cpp:128] Creating Layer conv3_3_scale0
I1002 12:07:06.179445 30882 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I1002 12:07:06.179450 30882 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I1002 12:07:06.179507 30882 layer_factory.hpp:77] Creating layer conv3_3_scale0
I1002 12:07:06.179669 30882 net.cpp:172] Setting up conv3_3_scale0
I1002 12:07:06.179680 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.179684 30882 net.cpp:194] Memory required for data: 287170680
I1002 12:07:06.179692 30882 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I1002 12:07:06.179702 30882 net.cpp:128] Creating Layer conv3_3_ReLU0
I1002 12:07:06.179706 30882 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I1002 12:07:06.179711 30882 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I1002 12:07:06.180168 30882 net.cpp:172] Setting up conv3_3_ReLU0
I1002 12:07:06.180191 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.180194 30882 net.cpp:194] Memory required for data: 289792120
I1002 12:07:06.180199 30882 layer_factory.hpp:77] Creating layer conv3_Drop3
I1002 12:07:06.180210 30882 net.cpp:128] Creating Layer conv3_Drop3
I1002 12:07:06.180215 30882 net.cpp:558] conv3_Drop3 <- conv3_3_0
I1002 12:07:06.180222 30882 net.cpp:509] conv3_Drop3 -> conv3_3_0 (in-place)
I1002 12:07:06.180255 30882 net.cpp:172] Setting up conv3_Drop3
I1002 12:07:06.180263 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.180266 30882 net.cpp:194] Memory required for data: 292413560
I1002 12:07:06.180271 30882 layer_factory.hpp:77] Creating layer conv3_3_1
I1002 12:07:06.180285 30882 net.cpp:128] Creating Layer conv3_3_1
I1002 12:07:06.180290 30882 net.cpp:558] conv3_3_1 <- conv3_3_0
I1002 12:07:06.180297 30882 net.cpp:522] conv3_3_1 -> conv3_3_1
I1002 12:07:06.192492 30882 net.cpp:172] Setting up conv3_3_1
I1002 12:07:06.192523 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.192528 30882 net.cpp:194] Memory required for data: 295035000
I1002 12:07:06.192538 30882 layer_factory.hpp:77] Creating layer conv3_3bn1
I1002 12:07:06.192548 30882 net.cpp:128] Creating Layer conv3_3bn1
I1002 12:07:06.192553 30882 net.cpp:558] conv3_3bn1 <- conv3_3_1
I1002 12:07:06.192566 30882 net.cpp:509] conv3_3bn1 -> conv3_3_1 (in-place)
I1002 12:07:06.192867 30882 net.cpp:172] Setting up conv3_3bn1
I1002 12:07:06.192878 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.192883 30882 net.cpp:194] Memory required for data: 297656440
I1002 12:07:06.192893 30882 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1002 12:07:06.192899 30882 net.cpp:128] Creating Layer conv3_3_scale1
I1002 12:07:06.192904 30882 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I1002 12:07:06.192912 30882 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I1002 12:07:06.192966 30882 layer_factory.hpp:77] Creating layer conv3_3_scale1
I1002 12:07:06.193128 30882 net.cpp:172] Setting up conv3_3_scale1
I1002 12:07:06.193138 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.193142 30882 net.cpp:194] Memory required for data: 300277880
I1002 12:07:06.193150 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_3
I1002 12:07:06.193161 30882 net.cpp:128] Creating Layer conv3_Eltwise_3
I1002 12:07:06.193166 30882 net.cpp:558] conv3_Eltwise_3 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I1002 12:07:06.193171 30882 net.cpp:558] conv3_Eltwise_3 <- conv3_3_1
I1002 12:07:06.193177 30882 net.cpp:522] conv3_Eltwise_3 -> conv3_Eltwise_3
I1002 12:07:06.193204 30882 net.cpp:172] Setting up conv3_Eltwise_3
I1002 12:07:06.193212 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.193215 30882 net.cpp:194] Memory required for data: 302899320
I1002 12:07:06.193219 30882 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I1002 12:07:06.193225 30882 net.cpp:128] Creating Layer conv3_3ReLU_1
I1002 12:07:06.193229 30882 net.cpp:558] conv3_3ReLU_1 <- conv3_Eltwise_3
I1002 12:07:06.193238 30882 net.cpp:509] conv3_3ReLU_1 -> conv3_Eltwise_3 (in-place)
I1002 12:07:06.193495 30882 net.cpp:172] Setting up conv3_3ReLU_1
I1002 12:07:06.193526 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.193531 30882 net.cpp:194] Memory required for data: 305520760
I1002 12:07:06.193536 30882 layer_factory.hpp:77] Creating layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:06.193545 30882 net.cpp:128] Creating Layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:06.193552 30882 net.cpp:558] conv3_Eltwise_3_conv3_3ReLU_1_0_split <- conv3_Eltwise_3
I1002 12:07:06.193562 30882 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I1002 12:07:06.193569 30882 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I1002 12:07:06.193632 30882 net.cpp:172] Setting up conv3_Eltwise_3_conv3_3ReLU_1_0_split
I1002 12:07:06.193642 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.193648 30882 net.cpp:186] Top shape: 10 256 16 16 (655360)
I1002 12:07:06.193652 30882 net.cpp:194] Memory required for data: 310763640
I1002 12:07:06.193656 30882 layer_factory.hpp:77] Creating layer conv4_1_0
I1002 12:07:06.193668 30882 net.cpp:128] Creating Layer conv4_1_0
I1002 12:07:06.193673 30882 net.cpp:558] conv4_1_0 <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I1002 12:07:06.193683 30882 net.cpp:522] conv4_1_0 -> conv4_1_0
I1002 12:07:06.216598 30882 net.cpp:172] Setting up conv4_1_0
I1002 12:07:06.216644 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.216648 30882 net.cpp:194] Memory required for data: 312074360
I1002 12:07:06.216666 30882 layer_factory.hpp:77] Creating layer conv4_1_bn0
I1002 12:07:06.216681 30882 net.cpp:128] Creating Layer conv4_1_bn0
I1002 12:07:06.216691 30882 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I1002 12:07:06.216698 30882 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I1002 12:07:06.217027 30882 net.cpp:172] Setting up conv4_1_bn0
I1002 12:07:06.217039 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.217043 30882 net.cpp:194] Memory required for data: 313385080
I1002 12:07:06.217053 30882 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1002 12:07:06.217063 30882 net.cpp:128] Creating Layer conv4_1_scale0
I1002 12:07:06.217067 30882 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I1002 12:07:06.217075 30882 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I1002 12:07:06.217133 30882 layer_factory.hpp:77] Creating layer conv4_1_scale0
I1002 12:07:06.217312 30882 net.cpp:172] Setting up conv4_1_scale0
I1002 12:07:06.217325 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.217329 30882 net.cpp:194] Memory required for data: 314695800
I1002 12:07:06.217337 30882 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I1002 12:07:06.217344 30882 net.cpp:128] Creating Layer conv4_1_ReLU0
I1002 12:07:06.217348 30882 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I1002 12:07:06.217355 30882 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I1002 12:07:06.217603 30882 net.cpp:172] Setting up conv4_1_ReLU0
I1002 12:07:06.217619 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.217624 30882 net.cpp:194] Memory required for data: 316006520
I1002 12:07:06.217628 30882 layer_factory.hpp:77] Creating layer conv4_Drop1
I1002 12:07:06.217638 30882 net.cpp:128] Creating Layer conv4_Drop1
I1002 12:07:06.217643 30882 net.cpp:558] conv4_Drop1 <- conv4_1_0
I1002 12:07:06.217648 30882 net.cpp:509] conv4_Drop1 -> conv4_1_0 (in-place)
I1002 12:07:06.217685 30882 net.cpp:172] Setting up conv4_Drop1
I1002 12:07:06.217692 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.217696 30882 net.cpp:194] Memory required for data: 317317240
I1002 12:07:06.217700 30882 layer_factory.hpp:77] Creating layer conv4_1_1
I1002 12:07:06.217713 30882 net.cpp:128] Creating Layer conv4_1_1
I1002 12:07:06.217718 30882 net.cpp:558] conv4_1_1 <- conv4_1_0
I1002 12:07:06.217728 30882 net.cpp:522] conv4_1_1 -> conv4_1_1
I1002 12:07:06.263242 30882 net.cpp:172] Setting up conv4_1_1
I1002 12:07:06.263300 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.263304 30882 net.cpp:194] Memory required for data: 318627960
I1002 12:07:06.263365 30882 layer_factory.hpp:77] Creating layer conv4_1bn1
I1002 12:07:06.263383 30882 net.cpp:128] Creating Layer conv4_1bn1
I1002 12:07:06.263396 30882 net.cpp:558] conv4_1bn1 <- conv4_1_1
I1002 12:07:06.263408 30882 net.cpp:509] conv4_1bn1 -> conv4_1_1 (in-place)
I1002 12:07:06.263736 30882 net.cpp:172] Setting up conv4_1bn1
I1002 12:07:06.263747 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.263751 30882 net.cpp:194] Memory required for data: 319938680
I1002 12:07:06.263762 30882 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1002 12:07:06.263770 30882 net.cpp:128] Creating Layer conv4_1_scale1
I1002 12:07:06.263775 30882 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I1002 12:07:06.263785 30882 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I1002 12:07:06.263844 30882 layer_factory.hpp:77] Creating layer conv4_1_scale1
I1002 12:07:06.264027 30882 net.cpp:172] Setting up conv4_1_scale1
I1002 12:07:06.264037 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.264042 30882 net.cpp:194] Memory required for data: 321249400
I1002 12:07:06.264050 30882 layer_factory.hpp:77] Creating layer conv4_1_down
I1002 12:07:06.264063 30882 net.cpp:128] Creating Layer conv4_1_down
I1002 12:07:06.264070 30882 net.cpp:558] conv4_1_down <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I1002 12:07:06.264081 30882 net.cpp:522] conv4_1_down -> conv4_1_down
I1002 12:07:06.267649 30882 net.cpp:172] Setting up conv4_1_down
I1002 12:07:06.267679 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.267683 30882 net.cpp:194] Memory required for data: 322560120
I1002 12:07:06.267693 30882 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I1002 12:07:06.267701 30882 net.cpp:128] Creating Layer conv4_1_bn_down
I1002 12:07:06.267706 30882 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I1002 12:07:06.267720 30882 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I1002 12:07:06.268033 30882 net.cpp:172] Setting up conv4_1_bn_down
I1002 12:07:06.268043 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.268048 30882 net.cpp:194] Memory required for data: 323870840
I1002 12:07:06.268057 30882 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1002 12:07:06.268064 30882 net.cpp:128] Creating Layer conv4_1_scale_down
I1002 12:07:06.268069 30882 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I1002 12:07:06.268074 30882 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I1002 12:07:06.268126 30882 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I1002 12:07:06.268304 30882 net.cpp:172] Setting up conv4_1_scale_down
I1002 12:07:06.268316 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.268319 30882 net.cpp:194] Memory required for data: 325181560
I1002 12:07:06.268327 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_1
I1002 12:07:06.268337 30882 net.cpp:128] Creating Layer conv4_Eltwise_1
I1002 12:07:06.268342 30882 net.cpp:558] conv4_Eltwise_1 <- conv4_1_1
I1002 12:07:06.268347 30882 net.cpp:558] conv4_Eltwise_1 <- conv4_1_down
I1002 12:07:06.268353 30882 net.cpp:522] conv4_Eltwise_1 -> conv4_Eltwise_1
I1002 12:07:06.268386 30882 net.cpp:172] Setting up conv4_Eltwise_1
I1002 12:07:06.268393 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.268398 30882 net.cpp:194] Memory required for data: 326492280
I1002 12:07:06.268401 30882 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I1002 12:07:06.268409 30882 net.cpp:128] Creating Layer conv4_1ReLU_1
I1002 12:07:06.268412 30882 net.cpp:558] conv4_1ReLU_1 <- conv4_Eltwise_1
I1002 12:07:06.268420 30882 net.cpp:509] conv4_1ReLU_1 -> conv4_Eltwise_1 (in-place)
I1002 12:07:06.268891 30882 net.cpp:172] Setting up conv4_1ReLU_1
I1002 12:07:06.268914 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.268918 30882 net.cpp:194] Memory required for data: 327803000
I1002 12:07:06.268923 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:06.268931 30882 net.cpp:128] Creating Layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:06.268952 30882 net.cpp:558] conv4_Eltwise_1_conv4_1ReLU_1_0_split <- conv4_Eltwise_1
I1002 12:07:06.268963 30882 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I1002 12:07:06.268973 30882 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I1002 12:07:06.269042 30882 net.cpp:172] Setting up conv4_Eltwise_1_conv4_1ReLU_1_0_split
I1002 12:07:06.269058 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.269064 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.269068 30882 net.cpp:194] Memory required for data: 330424440
I1002 12:07:06.269073 30882 layer_factory.hpp:77] Creating layer conv4_2_0
I1002 12:07:06.269084 30882 net.cpp:128] Creating Layer conv4_2_0
I1002 12:07:06.269089 30882 net.cpp:558] conv4_2_0 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I1002 12:07:06.269098 30882 net.cpp:522] conv4_2_0 -> conv4_2_0
I1002 12:07:06.314663 30882 net.cpp:172] Setting up conv4_2_0
I1002 12:07:06.314720 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.314724 30882 net.cpp:194] Memory required for data: 331735160
I1002 12:07:06.314746 30882 layer_factory.hpp:77] Creating layer conv4_2_bn0
I1002 12:07:06.314764 30882 net.cpp:128] Creating Layer conv4_2_bn0
I1002 12:07:06.314774 30882 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I1002 12:07:06.314783 30882 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I1002 12:07:06.315115 30882 net.cpp:172] Setting up conv4_2_bn0
I1002 12:07:06.315126 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.315131 30882 net.cpp:194] Memory required for data: 333045880
I1002 12:07:06.315156 30882 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1002 12:07:06.315167 30882 net.cpp:128] Creating Layer conv4_2_scale0
I1002 12:07:06.315171 30882 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I1002 12:07:06.315177 30882 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I1002 12:07:06.315239 30882 layer_factory.hpp:77] Creating layer conv4_2_scale0
I1002 12:07:06.315423 30882 net.cpp:172] Setting up conv4_2_scale0
I1002 12:07:06.315433 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.315438 30882 net.cpp:194] Memory required for data: 334356600
I1002 12:07:06.315444 30882 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I1002 12:07:06.315454 30882 net.cpp:128] Creating Layer conv4_2_ReLU0
I1002 12:07:06.315459 30882 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I1002 12:07:06.315467 30882 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I1002 12:07:06.315727 30882 net.cpp:172] Setting up conv4_2_ReLU0
I1002 12:07:06.315742 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.315747 30882 net.cpp:194] Memory required for data: 335667320
I1002 12:07:06.315752 30882 layer_factory.hpp:77] Creating layer conv4_Drop2
I1002 12:07:06.315762 30882 net.cpp:128] Creating Layer conv4_Drop2
I1002 12:07:06.315768 30882 net.cpp:558] conv4_Drop2 <- conv4_2_0
I1002 12:07:06.315776 30882 net.cpp:509] conv4_Drop2 -> conv4_2_0 (in-place)
I1002 12:07:06.315814 30882 net.cpp:172] Setting up conv4_Drop2
I1002 12:07:06.315821 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.315825 30882 net.cpp:194] Memory required for data: 336978040
I1002 12:07:06.315829 30882 layer_factory.hpp:77] Creating layer conv4_2_1
I1002 12:07:06.315845 30882 net.cpp:128] Creating Layer conv4_2_1
I1002 12:07:06.315850 30882 net.cpp:558] conv4_2_1 <- conv4_2_0
I1002 12:07:06.315857 30882 net.cpp:522] conv4_2_1 -> conv4_2_1
I1002 12:07:06.361389 30882 net.cpp:172] Setting up conv4_2_1
I1002 12:07:06.361449 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.361452 30882 net.cpp:194] Memory required for data: 338288760
I1002 12:07:06.361474 30882 layer_factory.hpp:77] Creating layer conv4_2bn1
I1002 12:07:06.361491 30882 net.cpp:128] Creating Layer conv4_2bn1
I1002 12:07:06.361503 30882 net.cpp:558] conv4_2bn1 <- conv4_2_1
I1002 12:07:06.361515 30882 net.cpp:509] conv4_2bn1 -> conv4_2_1 (in-place)
I1002 12:07:06.361846 30882 net.cpp:172] Setting up conv4_2bn1
I1002 12:07:06.361891 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.361896 30882 net.cpp:194] Memory required for data: 339599480
I1002 12:07:06.361925 30882 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1002 12:07:06.361935 30882 net.cpp:128] Creating Layer conv4_2_scale1
I1002 12:07:06.361939 30882 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I1002 12:07:06.361945 30882 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I1002 12:07:06.362013 30882 layer_factory.hpp:77] Creating layer conv4_2_scale1
I1002 12:07:06.362198 30882 net.cpp:172] Setting up conv4_2_scale1
I1002 12:07:06.362210 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.362215 30882 net.cpp:194] Memory required for data: 340910200
I1002 12:07:06.362222 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_2
I1002 12:07:06.362231 30882 net.cpp:128] Creating Layer conv4_Eltwise_2
I1002 12:07:06.362238 30882 net.cpp:558] conv4_Eltwise_2 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I1002 12:07:06.362244 30882 net.cpp:558] conv4_Eltwise_2 <- conv4_2_1
I1002 12:07:06.362251 30882 net.cpp:522] conv4_Eltwise_2 -> conv4_Eltwise_2
I1002 12:07:06.362285 30882 net.cpp:172] Setting up conv4_Eltwise_2
I1002 12:07:06.362293 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.362296 30882 net.cpp:194] Memory required for data: 342220920
I1002 12:07:06.362300 30882 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I1002 12:07:06.362308 30882 net.cpp:128] Creating Layer conv4_2ReLU_1
I1002 12:07:06.362311 30882 net.cpp:558] conv4_2ReLU_1 <- conv4_Eltwise_2
I1002 12:07:06.362319 30882 net.cpp:509] conv4_2ReLU_1 -> conv4_Eltwise_2 (in-place)
I1002 12:07:06.362581 30882 net.cpp:172] Setting up conv4_2ReLU_1
I1002 12:07:06.362596 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.362599 30882 net.cpp:194] Memory required for data: 343531640
I1002 12:07:06.362604 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:06.362617 30882 net.cpp:128] Creating Layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:06.362623 30882 net.cpp:558] conv4_Eltwise_2_conv4_2ReLU_1_0_split <- conv4_Eltwise_2
I1002 12:07:06.362632 30882 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I1002 12:07:06.362641 30882 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I1002 12:07:06.362709 30882 net.cpp:172] Setting up conv4_Eltwise_2_conv4_2ReLU_1_0_split
I1002 12:07:06.362718 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.362725 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.362728 30882 net.cpp:194] Memory required for data: 346153080
I1002 12:07:06.362732 30882 layer_factory.hpp:77] Creating layer conv4_3_0
I1002 12:07:06.362746 30882 net.cpp:128] Creating Layer conv4_3_0
I1002 12:07:06.362751 30882 net.cpp:558] conv4_3_0 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I1002 12:07:06.362759 30882 net.cpp:522] conv4_3_0 -> conv4_3_0
I1002 12:07:06.408284 30882 net.cpp:172] Setting up conv4_3_0
I1002 12:07:06.408344 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.408349 30882 net.cpp:194] Memory required for data: 347463800
I1002 12:07:06.408370 30882 layer_factory.hpp:77] Creating layer conv4_3_bn0
I1002 12:07:06.408388 30882 net.cpp:128] Creating Layer conv4_3_bn0
I1002 12:07:06.408399 30882 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I1002 12:07:06.408407 30882 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I1002 12:07:06.408735 30882 net.cpp:172] Setting up conv4_3_bn0
I1002 12:07:06.408746 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.408751 30882 net.cpp:194] Memory required for data: 348774520
I1002 12:07:06.408761 30882 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1002 12:07:06.408772 30882 net.cpp:128] Creating Layer conv4_3_scale0
I1002 12:07:06.408777 30882 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I1002 12:07:06.408783 30882 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I1002 12:07:06.408845 30882 layer_factory.hpp:77] Creating layer conv4_3_scale0
I1002 12:07:06.409075 30882 net.cpp:172] Setting up conv4_3_scale0
I1002 12:07:06.409086 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.409090 30882 net.cpp:194] Memory required for data: 350085240
I1002 12:07:06.409098 30882 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I1002 12:07:06.409108 30882 net.cpp:128] Creating Layer conv4_3_ReLU0
I1002 12:07:06.409113 30882 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I1002 12:07:06.409118 30882 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I1002 12:07:06.409375 30882 net.cpp:172] Setting up conv4_3_ReLU0
I1002 12:07:06.409391 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.409395 30882 net.cpp:194] Memory required for data: 351395960
I1002 12:07:06.409400 30882 layer_factory.hpp:77] Creating layer conv4_Drop3
I1002 12:07:06.409411 30882 net.cpp:128] Creating Layer conv4_Drop3
I1002 12:07:06.409416 30882 net.cpp:558] conv4_Drop3 <- conv4_3_0
I1002 12:07:06.409422 30882 net.cpp:509] conv4_Drop3 -> conv4_3_0 (in-place)
I1002 12:07:06.409462 30882 net.cpp:172] Setting up conv4_Drop3
I1002 12:07:06.409469 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.409473 30882 net.cpp:194] Memory required for data: 352706680
I1002 12:07:06.409477 30882 layer_factory.hpp:77] Creating layer conv4_3_1
I1002 12:07:06.409495 30882 net.cpp:128] Creating Layer conv4_3_1
I1002 12:07:06.409500 30882 net.cpp:558] conv4_3_1 <- conv4_3_0
I1002 12:07:06.409510 30882 net.cpp:522] conv4_3_1 -> conv4_3_1
I1002 12:07:06.455049 30882 net.cpp:172] Setting up conv4_3_1
I1002 12:07:06.455107 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.455112 30882 net.cpp:194] Memory required for data: 354017400
I1002 12:07:06.455132 30882 layer_factory.hpp:77] Creating layer conv4_3bn1
I1002 12:07:06.455152 30882 net.cpp:128] Creating Layer conv4_3bn1
I1002 12:07:06.455163 30882 net.cpp:558] conv4_3bn1 <- conv4_3_1
I1002 12:07:06.455174 30882 net.cpp:509] conv4_3bn1 -> conv4_3_1 (in-place)
I1002 12:07:06.455514 30882 net.cpp:172] Setting up conv4_3bn1
I1002 12:07:06.455526 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.455530 30882 net.cpp:194] Memory required for data: 355328120
I1002 12:07:06.455540 30882 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1002 12:07:06.455549 30882 net.cpp:128] Creating Layer conv4_3_scale1
I1002 12:07:06.455554 30882 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I1002 12:07:06.455559 30882 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I1002 12:07:06.455624 30882 layer_factory.hpp:77] Creating layer conv4_3_scale1
I1002 12:07:06.455807 30882 net.cpp:172] Setting up conv4_3_scale1
I1002 12:07:06.455818 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.455822 30882 net.cpp:194] Memory required for data: 356638840
I1002 12:07:06.455830 30882 layer_factory.hpp:77] Creating layer conv4_Eltwise_3
I1002 12:07:06.455839 30882 net.cpp:128] Creating Layer conv4_Eltwise_3
I1002 12:07:06.455845 30882 net.cpp:558] conv4_Eltwise_3 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I1002 12:07:06.455850 30882 net.cpp:558] conv4_Eltwise_3 <- conv4_3_1
I1002 12:07:06.455859 30882 net.cpp:522] conv4_Eltwise_3 -> conv4_Eltwise_3
I1002 12:07:06.455893 30882 net.cpp:172] Setting up conv4_Eltwise_3
I1002 12:07:06.455909 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.455914 30882 net.cpp:194] Memory required for data: 357949560
I1002 12:07:06.455919 30882 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I1002 12:07:06.455927 30882 net.cpp:128] Creating Layer conv4_3ReLU_1
I1002 12:07:06.455932 30882 net.cpp:558] conv4_3ReLU_1 <- conv4_Eltwise_3
I1002 12:07:06.455937 30882 net.cpp:509] conv4_3ReLU_1 -> conv4_Eltwise_3 (in-place)
I1002 12:07:06.456408 30882 net.cpp:172] Setting up conv4_3ReLU_1
I1002 12:07:06.456434 30882 net.cpp:186] Top shape: 10 512 8 8 (327680)
I1002 12:07:06.456439 30882 net.cpp:194] Memory required for data: 359260280
I1002 12:07:06.456444 30882 layer_factory.hpp:77] Creating layer Pooling1
I1002 12:07:06.456454 30882 net.cpp:128] Creating Layer Pooling1
I1002 12:07:06.456498 30882 net.cpp:558] Pooling1 <- conv4_Eltwise_3
I1002 12:07:06.456506 30882 net.cpp:522] Pooling1 -> Pooling1
I1002 12:07:06.456802 30882 net.cpp:172] Setting up Pooling1
I1002 12:07:06.456817 30882 net.cpp:186] Top shape: 10 512 1 1 (5120)
I1002 12:07:06.456822 30882 net.cpp:194] Memory required for data: 359280760
I1002 12:07:06.456826 30882 layer_factory.hpp:77] Creating layer fc1
I1002 12:07:06.456840 30882 net.cpp:128] Creating Layer fc1
I1002 12:07:06.456845 30882 net.cpp:558] fc1 <- Pooling1
I1002 12:07:06.456853 30882 net.cpp:522] fc1 -> fc1
I1002 12:07:06.457119 30882 net.cpp:172] Setting up fc1
I1002 12:07:06.457129 30882 net.cpp:186] Top shape: 10 10 (100)
I1002 12:07:06.457134 30882 net.cpp:194] Memory required for data: 359281160
I1002 12:07:06.457144 30882 layer_factory.hpp:77] Creating layer fc1_fc1_0_split
I1002 12:07:06.457150 30882 net.cpp:128] Creating Layer fc1_fc1_0_split
I1002 12:07:06.457155 30882 net.cpp:558] fc1_fc1_0_split <- fc1
I1002 12:07:06.457164 30882 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_0
I1002 12:07:06.457171 30882 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_1
I1002 12:07:06.457224 30882 net.cpp:172] Setting up fc1_fc1_0_split
I1002 12:07:06.457234 30882 net.cpp:186] Top shape: 10 10 (100)
I1002 12:07:06.457239 30882 net.cpp:186] Top shape: 10 10 (100)
I1002 12:07:06.457243 30882 net.cpp:194] Memory required for data: 359281960
I1002 12:07:06.457247 30882 layer_factory.hpp:77] Creating layer Softmax1
I1002 12:07:06.457257 30882 net.cpp:128] Creating Layer Softmax1
I1002 12:07:06.457262 30882 net.cpp:558] Softmax1 <- fc1_fc1_0_split_0
I1002 12:07:06.457267 30882 net.cpp:558] Softmax1 <- label_Data1_1_split_0
I1002 12:07:06.457273 30882 net.cpp:522] Softmax1 -> Softmax1
I1002 12:07:06.457285 30882 layer_factory.hpp:77] Creating layer Softmax1
I1002 12:07:06.457676 30882 net.cpp:172] Setting up Softmax1
I1002 12:07:06.457692 30882 net.cpp:186] Top shape: (1)
I1002 12:07:06.457696 30882 net.cpp:189]     with loss weight 1
I1002 12:07:06.457712 30882 net.cpp:194] Memory required for data: 359281964
I1002 12:07:06.457717 30882 layer_factory.hpp:77] Creating layer prob
I1002 12:07:06.457727 30882 net.cpp:128] Creating Layer prob
I1002 12:07:06.457732 30882 net.cpp:558] prob <- fc1_fc1_0_split_1
I1002 12:07:06.457738 30882 net.cpp:558] prob <- label_Data1_1_split_1
I1002 12:07:06.457746 30882 net.cpp:522] prob -> prob
I1002 12:07:06.457756 30882 net.cpp:172] Setting up prob
I1002 12:07:06.457762 30882 net.cpp:186] Top shape: (1)
I1002 12:07:06.457764 30882 net.cpp:194] Memory required for data: 359281968
I1002 12:07:06.457769 30882 net.cpp:303] prob does not need backward computation.
I1002 12:07:06.457774 30882 net.cpp:301] Softmax1 needs backward computation.
I1002 12:07:06.457779 30882 net.cpp:301] fc1_fc1_0_split needs backward computation.
I1002 12:07:06.457784 30882 net.cpp:301] fc1 needs backward computation.
I1002 12:07:06.457788 30882 net.cpp:301] Pooling1 needs backward computation.
I1002 12:07:06.457793 30882 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I1002 12:07:06.457798 30882 net.cpp:301] conv4_Eltwise_3 needs backward computation.
I1002 12:07:06.457803 30882 net.cpp:301] conv4_3_scale1 needs backward computation.
I1002 12:07:06.457806 30882 net.cpp:301] conv4_3bn1 needs backward computation.
I1002 12:07:06.457810 30882 net.cpp:301] conv4_3_1 needs backward computation.
I1002 12:07:06.457814 30882 net.cpp:301] conv4_Drop3 needs backward computation.
I1002 12:07:06.457818 30882 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I1002 12:07:06.457823 30882 net.cpp:301] conv4_3_scale0 needs backward computation.
I1002 12:07:06.457828 30882 net.cpp:301] conv4_3_bn0 needs backward computation.
I1002 12:07:06.457831 30882 net.cpp:301] conv4_3_0 needs backward computation.
I1002 12:07:06.457836 30882 net.cpp:301] conv4_Eltwise_2_conv4_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.457841 30882 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I1002 12:07:06.457845 30882 net.cpp:301] conv4_Eltwise_2 needs backward computation.
I1002 12:07:06.457866 30882 net.cpp:301] conv4_2_scale1 needs backward computation.
I1002 12:07:06.457871 30882 net.cpp:301] conv4_2bn1 needs backward computation.
I1002 12:07:06.457875 30882 net.cpp:301] conv4_2_1 needs backward computation.
I1002 12:07:06.457880 30882 net.cpp:301] conv4_Drop2 needs backward computation.
I1002 12:07:06.457883 30882 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I1002 12:07:06.457887 30882 net.cpp:301] conv4_2_scale0 needs backward computation.
I1002 12:07:06.457892 30882 net.cpp:301] conv4_2_bn0 needs backward computation.
I1002 12:07:06.457896 30882 net.cpp:301] conv4_2_0 needs backward computation.
I1002 12:07:06.457911 30882 net.cpp:301] conv4_Eltwise_1_conv4_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.457916 30882 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I1002 12:07:06.457919 30882 net.cpp:301] conv4_Eltwise_1 needs backward computation.
I1002 12:07:06.457924 30882 net.cpp:301] conv4_1_scale_down needs backward computation.
I1002 12:07:06.457928 30882 net.cpp:301] conv4_1_bn_down needs backward computation.
I1002 12:07:06.457933 30882 net.cpp:301] conv4_1_down needs backward computation.
I1002 12:07:06.457938 30882 net.cpp:301] conv4_1_scale1 needs backward computation.
I1002 12:07:06.457942 30882 net.cpp:301] conv4_1bn1 needs backward computation.
I1002 12:07:06.457947 30882 net.cpp:301] conv4_1_1 needs backward computation.
I1002 12:07:06.457952 30882 net.cpp:301] conv4_Drop1 needs backward computation.
I1002 12:07:06.457955 30882 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I1002 12:07:06.457959 30882 net.cpp:301] conv4_1_scale0 needs backward computation.
I1002 12:07:06.457963 30882 net.cpp:301] conv4_1_bn0 needs backward computation.
I1002 12:07:06.457967 30882 net.cpp:301] conv4_1_0 needs backward computation.
I1002 12:07:06.457973 30882 net.cpp:301] conv3_Eltwise_3_conv3_3ReLU_1_0_split needs backward computation.
I1002 12:07:06.457978 30882 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I1002 12:07:06.457981 30882 net.cpp:301] conv3_Eltwise_3 needs backward computation.
I1002 12:07:06.457988 30882 net.cpp:301] conv3_3_scale1 needs backward computation.
I1002 12:07:06.457993 30882 net.cpp:301] conv3_3bn1 needs backward computation.
I1002 12:07:06.457996 30882 net.cpp:301] conv3_3_1 needs backward computation.
I1002 12:07:06.458000 30882 net.cpp:301] conv3_Drop3 needs backward computation.
I1002 12:07:06.458004 30882 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I1002 12:07:06.458009 30882 net.cpp:301] conv3_3_scale0 needs backward computation.
I1002 12:07:06.458014 30882 net.cpp:301] conv3_3_bn0 needs backward computation.
I1002 12:07:06.458017 30882 net.cpp:301] conv3_3_0 needs backward computation.
I1002 12:07:06.458024 30882 net.cpp:301] conv3_Eltwise_2_conv3_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.458029 30882 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I1002 12:07:06.458034 30882 net.cpp:301] conv3_Eltwise_2 needs backward computation.
I1002 12:07:06.458039 30882 net.cpp:301] conv3_2_scale1 needs backward computation.
I1002 12:07:06.458042 30882 net.cpp:301] conv3_2bn1 needs backward computation.
I1002 12:07:06.458046 30882 net.cpp:301] conv3_2_1 needs backward computation.
I1002 12:07:06.458051 30882 net.cpp:301] conv3_Drop2 needs backward computation.
I1002 12:07:06.458055 30882 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I1002 12:07:06.458060 30882 net.cpp:301] conv3_2_scale0 needs backward computation.
I1002 12:07:06.458063 30882 net.cpp:301] conv3_2_bn0 needs backward computation.
I1002 12:07:06.458067 30882 net.cpp:301] conv3_2_0 needs backward computation.
I1002 12:07:06.458072 30882 net.cpp:301] conv3_Eltwise_1_conv3_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.458077 30882 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I1002 12:07:06.458082 30882 net.cpp:301] conv3_Eltwise_1 needs backward computation.
I1002 12:07:06.458086 30882 net.cpp:301] conv3_1_scale_down needs backward computation.
I1002 12:07:06.458091 30882 net.cpp:301] conv3_1_bn_down needs backward computation.
I1002 12:07:06.458103 30882 net.cpp:301] conv3_1_down needs backward computation.
I1002 12:07:06.458108 30882 net.cpp:301] conv3_1_scale1 needs backward computation.
I1002 12:07:06.458112 30882 net.cpp:301] conv3_1bn1 needs backward computation.
I1002 12:07:06.458117 30882 net.cpp:301] conv3_1_1 needs backward computation.
I1002 12:07:06.458122 30882 net.cpp:301] conv3_Drop1 needs backward computation.
I1002 12:07:06.458127 30882 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I1002 12:07:06.458130 30882 net.cpp:301] conv3_1_scale0 needs backward computation.
I1002 12:07:06.458134 30882 net.cpp:301] conv3_1_bn0 needs backward computation.
I1002 12:07:06.458138 30882 net.cpp:301] conv3_1_0 needs backward computation.
I1002 12:07:06.458142 30882 net.cpp:301] conv2_Eltwise_3_conv2_3ReLU_1_0_split needs backward computation.
I1002 12:07:06.458148 30882 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I1002 12:07:06.458151 30882 net.cpp:301] conv2_Eltwise_3 needs backward computation.
I1002 12:07:06.458158 30882 net.cpp:301] conv2_3_scale1 needs backward computation.
I1002 12:07:06.458161 30882 net.cpp:301] conv2_3bn1 needs backward computation.
I1002 12:07:06.458165 30882 net.cpp:301] conv2_3_1 needs backward computation.
I1002 12:07:06.458169 30882 net.cpp:301] conv2_Drop3 needs backward computation.
I1002 12:07:06.458174 30882 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I1002 12:07:06.458178 30882 net.cpp:301] conv2_3_scale0 needs backward computation.
I1002 12:07:06.458182 30882 net.cpp:301] conv2_3_bn0 needs backward computation.
I1002 12:07:06.458186 30882 net.cpp:301] conv2_3_0 needs backward computation.
I1002 12:07:06.458191 30882 net.cpp:301] conv2_Eltwise_2_conv2_2ReLU_1_0_split needs backward computation.
I1002 12:07:06.458196 30882 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I1002 12:07:06.458200 30882 net.cpp:301] conv2_Eltwise_2 needs backward computation.
I1002 12:07:06.458209 30882 net.cpp:301] conv2_2_scale1 needs backward computation.
I1002 12:07:06.458214 30882 net.cpp:301] conv2_2bn1 needs backward computation.
I1002 12:07:06.458217 30882 net.cpp:301] conv2_2_1 needs backward computation.
I1002 12:07:06.458222 30882 net.cpp:301] conv2_Drop2 needs backward computation.
I1002 12:07:06.458226 30882 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I1002 12:07:06.458230 30882 net.cpp:301] conv2_2_scale0 needs backward computation.
I1002 12:07:06.458235 30882 net.cpp:301] conv2_2_bn0 needs backward computation.
I1002 12:07:06.458240 30882 net.cpp:301] conv2_2_0 needs backward computation.
I1002 12:07:06.458245 30882 net.cpp:301] conv2_Eltwise_1_conv2_1ReLU_1_0_split needs backward computation.
I1002 12:07:06.458250 30882 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I1002 12:07:06.458253 30882 net.cpp:301] conv2_Eltwise_1 needs backward computation.
I1002 12:07:06.458258 30882 net.cpp:301] conv2_1_scale_down needs backward computation.
I1002 12:07:06.458262 30882 net.cpp:301] conv2_1_bn_down needs backward computation.
I1002 12:07:06.458266 30882 net.cpp:301] conv2_1_down needs backward computation.
I1002 12:07:06.458271 30882 net.cpp:301] conv2_1_scale1 needs backward computation.
I1002 12:07:06.458276 30882 net.cpp:301] conv2_1bn1 needs backward computation.
I1002 12:07:06.458281 30882 net.cpp:301] conv2_1_1 needs backward computation.
I1002 12:07:06.458284 30882 net.cpp:301] conv2_Drop1 needs backward computation.
I1002 12:07:06.458288 30882 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I1002 12:07:06.458292 30882 net.cpp:301] conv2_1_scale0 needs backward computation.
I1002 12:07:06.458297 30882 net.cpp:301] conv2_1_bn0 needs backward computation.
I1002 12:07:06.458302 30882 net.cpp:301] conv2_1_0 needs backward computation.
I1002 12:07:06.458307 30882 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I1002 12:07:06.458312 30882 net.cpp:301] conv1/ReLU needs backward computation.
I1002 12:07:06.458315 30882 net.cpp:301] conv1/scale needs backward computation.
I1002 12:07:06.458319 30882 net.cpp:301] conv1/bn needs backward computation.
I1002 12:07:06.458330 30882 net.cpp:301] conv1 needs backward computation.
I1002 12:07:06.458336 30882 net.cpp:303] label_Data1_1_split does not need backward computation.
I1002 12:07:06.458341 30882 net.cpp:303] Data1 does not need backward computation.
I1002 12:07:06.458345 30882 net.cpp:348] This network produces output Softmax1
I1002 12:07:06.458349 30882 net.cpp:348] This network produces output prob
I1002 12:07:06.458417 30882 net.cpp:363] Network initialization done.
I1002 12:07:06.458818 30882 solver.cpp:110] Solver scaffolding done.
I1002 12:07:06.468066 30882 caffe.cpp:313] Starting Optimization
I1002 12:07:06.468088 30882 solver.cpp:425] Solving WRN-20
I1002 12:07:06.468092 30882 solver.cpp:427] Learning Rate Policy: multistep
I1002 12:07:06.473034 30882 solver.cpp:514] Iteration 0, Testing net (#0)
I1002 12:07:28.616801 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:07:28.703616 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.30259 (* 1 = 2.30259 loss)
I1002 12:07:28.703642 30882 solver.cpp:580]     Test net output #1: prob = 0
I1002 12:07:29.531663 30882 solver.cpp:357] Iteration 0 (0 iter/s, 23.0643s/100 iters), loss = 3.05476
I1002 12:07:29.531723 30882 solver.cpp:376]     Train net output #0: Softmax1 = 3.20332 (* 1 = 3.20332 loss)
I1002 12:07:29.531754 30882 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I1002 12:08:50.948457 30882 solver.cpp:357] Iteration 100 (1.22821 iter/s, 81.4196s/100 iters), loss = 2.21961
I1002 12:08:50.948601 30882 solver.cpp:376]     Train net output #0: Softmax1 = 2.28012 (* 1 = 2.28012 loss)
I1002 12:08:50.948611 30882 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I1002 12:10:12.621795 30882 solver.cpp:357] Iteration 200 (1.22435 iter/s, 81.6761s/100 iters), loss = 2.11016
I1002 12:10:12.621965 30882 solver.cpp:376]     Train net output #0: Softmax1 = 2.16942 (* 1 = 2.16942 loss)
I1002 12:10:12.621975 30882 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I1002 12:11:34.259099 30882 solver.cpp:357] Iteration 300 (1.22489 iter/s, 81.64s/100 iters), loss = 1.98474
I1002 12:11:34.259234 30882 solver.cpp:376]     Train net output #0: Softmax1 = 2.0086 (* 1 = 2.0086 loss)
I1002 12:11:34.259245 30882 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I1002 12:12:46.340353 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:12:56.201143 30882 solver.cpp:357] Iteration 400 (1.22035 iter/s, 81.9437s/100 iters), loss = 1.9822
I1002 12:12:56.201201 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.85884 (* 1 = 1.85884 loss)
I1002 12:12:56.201210 30882 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I1002 12:14:17.629602 30882 solver.cpp:514] Iteration 500, Testing net (#0)
I1002 12:14:39.906759 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:14:39.994752 30882 solver.cpp:580]     Test net output #0: Softmax1 = 3.89784 (* 1 = 3.89784 loss)
I1002 12:14:39.994778 30882 solver.cpp:580]     Test net output #1: prob = 0.116599
I1002 12:14:40.804162 30882 solver.cpp:357] Iteration 500 (0.956074 iter/s, 104.594s/100 iters), loss = 1.81837
I1002 12:14:40.804211 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.82789 (* 1 = 1.82789 loss)
I1002 12:14:40.804222 30882 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I1002 12:16:02.939813 30882 solver.cpp:357] Iteration 600 (1.21758 iter/s, 82.1302s/100 iters), loss = 1.7539
I1002 12:16:02.939905 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.84176 (* 1 = 1.84176 loss)
I1002 12:16:02.939915 30882 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I1002 12:17:25.431839 30882 solver.cpp:357] Iteration 700 (1.21231 iter/s, 82.4875s/100 iters), loss = 1.63421
I1002 12:17:25.431936 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.59339 (* 1 = 1.59339 loss)
I1002 12:17:25.431946 30882 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I1002 12:18:30.198096 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:18:47.980554 30882 solver.cpp:357] Iteration 800 (1.21146 iter/s, 82.545s/100 iters), loss = 1.60757
I1002 12:18:47.980614 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.59983 (* 1 = 1.59983 loss)
I1002 12:18:47.980626 30882 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I1002 12:20:10.316545 30882 solver.cpp:357] Iteration 900 (1.21458 iter/s, 82.333s/100 iters), loss = 1.49414
I1002 12:20:10.316686 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.50292 (* 1 = 1.50292 loss)
I1002 12:20:10.316696 30882 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I1002 12:21:32.177445 30882 solver.cpp:514] Iteration 1000, Testing net (#0)
I1002 12:21:54.465999 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:21:54.552953 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.70385 (* 1 = 2.70385 loss)
I1002 12:21:54.552978 30882 solver.cpp:580]     Test net output #1: prob = 0.116599
I1002 12:21:55.366252 30882 solver.cpp:357] Iteration 1000 (0.951957 iter/s, 105.047s/100 iters), loss = 1.28629
I1002 12:21:55.366303 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.23017 (* 1 = 1.23017 loss)
I1002 12:21:55.366317 30882 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I1002 12:23:17.657168 30882 solver.cpp:357] Iteration 1100 (1.21523 iter/s, 82.2892s/100 iters), loss = 1.08386
I1002 12:23:17.657306 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.01259 (* 1 = 1.01259 loss)
I1002 12:23:17.657316 30882 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I1002 12:24:14.691725 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:24:40.313015 30882 solver.cpp:357] Iteration 1200 (1.20986 iter/s, 82.6544s/100 iters), loss = 1.23103
I1002 12:24:40.313076 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.23984 (* 1 = 1.23984 loss)
I1002 12:24:40.313086 30882 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I1002 12:26:02.836623 30882 solver.cpp:357] Iteration 1300 (1.21179 iter/s, 82.5226s/100 iters), loss = 0.970194
I1002 12:26:02.836714 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.878259 (* 1 = 0.878259 loss)
I1002 12:26:02.836726 30882 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I1002 12:27:25.454561 30882 solver.cpp:357] Iteration 1400 (1.2104 iter/s, 82.6172s/100 iters), loss = 0.966567
I1002 12:27:25.454701 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.964098 (* 1 = 0.964098 loss)
I1002 12:27:25.454711 30882 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I1002 12:28:47.314831 30882 solver.cpp:514] Iteration 1500, Testing net (#0)
I1002 12:29:09.617799 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:29:09.706281 30882 solver.cpp:580]     Test net output #0: Softmax1 = 3.34122 (* 1 = 3.34122 loss)
I1002 12:29:09.706307 30882 solver.cpp:580]     Test net output #1: prob = 0.100099
I1002 12:29:09.706326 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_1500.caffemodel
I1002 12:29:10.168094 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_1500.solverstate
I1002 12:29:10.305018 30882 solver.cpp:593]     Max_acc: 0.100099  with iter: 1500
I1002 12:29:11.117038 30882 solver.cpp:357] Iteration 1500 (0.946414 iter/s, 105.662s/100 iters), loss = 1.06122
I1002 12:29:11.117092 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.01391 (* 1 = 1.01391 loss)
I1002 12:29:11.117105 30882 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I1002 12:30:00.218152 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:30:33.765511 30882 solver.cpp:357] Iteration 1600 (1.20995 iter/s, 82.6484s/100 iters), loss = 1.02043
I1002 12:30:33.765651 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.930143 (* 1 = 0.930143 loss)
I1002 12:30:33.765662 30882 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I1002 12:31:56.528025 30882 solver.cpp:357] Iteration 1700 (1.20828 iter/s, 82.7625s/100 iters), loss = 1.25846
I1002 12:31:56.528220 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.21824 (* 1 = 1.21824 loss)
I1002 12:31:56.528231 30882 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I1002 12:33:19.269163 30882 solver.cpp:357] Iteration 1800 (1.20859 iter/s, 82.7412s/100 iters), loss = 1.04903
I1002 12:33:19.269310 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.0063 (* 1 = 1.0063 loss)
I1002 12:33:19.269321 30882 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I1002 12:34:41.967658 30882 solver.cpp:357] Iteration 1900 (1.20921 iter/s, 82.6987s/100 iters), loss = 0.935651
I1002 12:34:41.967805 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.859537 (* 1 = 0.859537 loss)
I1002 12:34:41.967816 30882 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I1002 12:35:23.768623 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:36:03.934017 30882 solver.cpp:514] Iteration 2000, Testing net (#0)
I1002 12:36:26.233963 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:36:26.321310 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.90814 (* 1 = 2.90814 loss)
I1002 12:36:26.321336 30882 solver.cpp:580]     Test net output #1: prob = 0.106299
I1002 12:36:26.321348 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_2000.caffemodel
I1002 12:36:26.723208 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_2000.solverstate
I1002 12:36:26.860821 30882 solver.cpp:593]     Max_acc: 0.106299  with iter: 2000
I1002 12:36:27.673997 30882 solver.cpp:357] Iteration 2000 (0.946013 iter/s, 105.707s/100 iters), loss = 0.798425
I1002 12:36:27.674149 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.701305 (* 1 = 0.701305 loss)
I1002 12:36:27.674178 30882 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I1002 12:37:50.302157 30882 solver.cpp:357] Iteration 2100 (1.21023 iter/s, 82.6286s/100 iters), loss = 0.870298
I1002 12:37:50.302281 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.861791 (* 1 = 0.861791 loss)
I1002 12:37:50.302291 30882 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I1002 12:39:13.110535 30882 solver.cpp:357] Iteration 2200 (1.2076 iter/s, 82.809s/100 iters), loss = 0.887461
I1002 12:39:13.110678 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.821899 (* 1 = 0.821899 loss)
I1002 12:39:13.110688 30882 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I1002 12:40:35.931530 30882 solver.cpp:357] Iteration 2300 (1.20741 iter/s, 82.8216s/100 iters), loss = 0.786041
I1002 12:40:35.931670 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.762865 (* 1 = 0.762865 loss)
I1002 12:40:35.931679 30882 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I1002 12:41:09.898479 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:41:58.717336 30882 solver.cpp:357] Iteration 2400 (1.20793 iter/s, 82.7865s/100 iters), loss = 0.728158
I1002 12:41:58.717474 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.792758 (* 1 = 0.792758 loss)
I1002 12:41:58.717485 30882 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I1002 12:43:20.718802 30882 solver.cpp:514] Iteration 2500, Testing net (#0)
I1002 12:43:43.030068 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:43:43.116735 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.52881 (* 1 = 2.52881 loss)
I1002 12:43:43.116760 30882 solver.cpp:580]     Test net output #1: prob = 0.2042
I1002 12:43:43.116773 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_2500.caffemodel
I1002 12:43:43.520144 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_2500.solverstate
I1002 12:43:43.658139 30882 solver.cpp:593]     Max_acc: 0.2042  with iter: 2500
I1002 12:43:44.470281 30882 solver.cpp:357] Iteration 2500 (0.945591 iter/s, 105.754s/100 iters), loss = 0.756337
I1002 12:43:44.470336 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.815834 (* 1 = 0.815834 loss)
I1002 12:43:44.470350 30882 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I1002 12:45:07.228607 30882 solver.cpp:357] Iteration 2600 (1.20833 iter/s, 82.7592s/100 iters), loss = 0.99312
I1002 12:45:07.228832 30882 solver.cpp:376]     Train net output #0: Softmax1 = 1.02079 (* 1 = 1.02079 loss)
I1002 12:45:07.228842 30882 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I1002 12:46:30.283478 30882 solver.cpp:357] Iteration 2700 (1.20401 iter/s, 83.0556s/100 iters), loss = 0.864235
I1002 12:46:30.283627 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.82651 (* 1 = 0.82651 loss)
I1002 12:46:30.283638 30882 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I1002 12:46:56.384529 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:47:53.128249 30882 solver.cpp:357] Iteration 2800 (1.20698 iter/s, 82.8512s/100 iters), loss = 0.550785
I1002 12:47:53.128346 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.513243 (* 1 = 0.513243 loss)
I1002 12:47:53.128357 30882 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I1002 12:49:15.967733 30882 solver.cpp:357] Iteration 2900 (1.20704 iter/s, 82.8476s/100 iters), loss = 0.65768
I1002 12:49:15.967873 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.629705 (* 1 = 0.629705 loss)
I1002 12:49:15.967883 30882 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I1002 12:50:38.120167 30882 solver.cpp:514] Iteration 3000, Testing net (#0)
I1002 12:51:00.426414 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:51:00.514266 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.60771 (* 1 = 2.60771 loss)
I1002 12:51:00.514292 30882 solver.cpp:580]     Test net output #1: prob = 0.1363
I1002 12:51:00.514299 30882 solver.cpp:593]     Max_acc: 0.2042  with iter: 2500
I1002 12:51:01.334542 30882 solver.cpp:357] Iteration 3000 (0.948985 iter/s, 105.376s/100 iters), loss = 0.705065
I1002 12:51:01.334591 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.817634 (* 1 = 0.817634 loss)
I1002 12:51:01.334604 30882 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I1002 12:52:24.316545 30882 solver.cpp:357] Iteration 3100 (1.20499 iter/s, 82.9883s/100 iters), loss = 0.626113
I1002 12:52:24.316687 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.750323 (* 1 = 0.750323 loss)
I1002 12:52:24.316699 30882 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I1002 12:52:42.528564 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:53:47.359480 30882 solver.cpp:357] Iteration 3200 (1.20412 iter/s, 83.0485s/100 iters), loss = 0.67825
I1002 12:53:47.359627 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.511846 (* 1 = 0.511846 loss)
I1002 12:53:47.359637 30882 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I1002 12:55:10.363313 30882 solver.cpp:357] Iteration 3300 (1.20469 iter/s, 83.0088s/100 iters), loss = 0.645812
I1002 12:55:10.363453 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.684678 (* 1 = 0.684678 loss)
I1002 12:55:10.363463 30882 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I1002 12:56:33.290532 30882 solver.cpp:357] Iteration 3400 (1.20581 iter/s, 82.9317s/100 iters), loss = 0.523471
I1002 12:56:33.290621 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.545579 (* 1 = 0.545579 loss)
I1002 12:56:33.290632 30882 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I1002 12:57:55.383687 30882 solver.cpp:514] Iteration 3500, Testing net (#0)
I1002 12:58:17.690665 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:58:17.778610 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.47896 (* 1 = 2.47896 loss)
I1002 12:58:17.778636 30882 solver.cpp:580]     Test net output #1: prob = 0.1395
I1002 12:58:17.778643 30882 solver.cpp:593]     Max_acc: 0.2042  with iter: 2500
I1002 12:58:18.598780 30882 solver.cpp:357] Iteration 3500 (0.949546 iter/s, 105.314s/100 iters), loss = 0.653147
I1002 12:58:18.598829 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.713541 (* 1 = 0.713541 loss)
I1002 12:58:18.598842 30882 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I1002 12:58:29.395014 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 12:59:41.482771 30882 solver.cpp:357] Iteration 3600 (1.20645 iter/s, 82.8878s/100 iters), loss = 0.612932
I1002 12:59:41.482892 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.657439 (* 1 = 0.657439 loss)
I1002 12:59:41.482906 30882 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I1002 13:01:04.321837 30882 solver.cpp:357] Iteration 3700 (1.20711 iter/s, 82.8425s/100 iters), loss = 0.485653
I1002 13:01:04.321941 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.439049 (* 1 = 0.439049 loss)
I1002 13:01:04.321954 30882 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I1002 13:02:27.234591 30882 solver.cpp:357] Iteration 3800 (1.20604 iter/s, 82.916s/100 iters), loss = 0.474904
I1002 13:02:27.237138 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.571534 (* 1 = 0.571534 loss)
I1002 13:02:27.237151 30882 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I1002 13:03:50.073922 30882 solver.cpp:357] Iteration 3900 (1.20715 iter/s, 82.84s/100 iters), loss = 0.632464
I1002 13:03:50.074045 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.68271 (* 1 = 0.68271 loss)
I1002 13:03:50.074056 30882 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I1002 13:03:52.983532 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:05:12.222153 30882 solver.cpp:514] Iteration 4000, Testing net (#0)
I1002 13:05:34.529322 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:05:34.617422 30882 solver.cpp:580]     Test net output #0: Softmax1 = 2.4526 (* 1 = 2.4526 loss)
I1002 13:05:34.617449 30882 solver.cpp:580]     Test net output #1: prob = 0.1854
I1002 13:05:34.617456 30882 solver.cpp:593]     Max_acc: 0.2042  with iter: 2500
I1002 13:05:35.438263 30882 solver.cpp:357] Iteration 4000 (0.949055 iter/s, 105.368s/100 iters), loss = 0.622332
I1002 13:05:35.438313 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.625166 (* 1 = 0.625166 loss)
I1002 13:05:35.438328 30882 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I1002 13:06:58.388886 30882 solver.cpp:357] Iteration 4100 (1.2055 iter/s, 82.9534s/100 iters), loss = 0.616345
I1002 13:06:58.389022 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.574502 (* 1 = 0.574502 loss)
I1002 13:06:58.389032 30882 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I1002 13:08:21.540416 30882 solver.cpp:357] Iteration 4200 (1.20259 iter/s, 83.1541s/100 iters), loss = 0.509457
I1002 13:08:21.540565 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.405194 (* 1 = 0.405194 loss)
I1002 13:08:21.540575 30882 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I1002 13:09:39.670361 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:09:44.640765 30882 solver.cpp:357] Iteration 4300 (1.20333 iter/s, 83.1028s/100 iters), loss = 0.461481
I1002 13:09:44.640827 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.432705 (* 1 = 0.432705 loss)
I1002 13:09:44.640838 30882 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I1002 13:11:07.646500 30882 solver.cpp:357] Iteration 4400 (1.2047 iter/s, 83.0082s/100 iters), loss = 0.677199
I1002 13:11:07.646641 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.693483 (* 1 = 0.693483 loss)
I1002 13:11:07.646653 30882 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I1002 13:12:29.833343 30882 solver.cpp:514] Iteration 4500, Testing net (#0)
I1002 13:12:52.141629 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:12:52.229849 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.73772 (* 1 = 1.73772 loss)
I1002 13:12:52.229874 30882 solver.cpp:580]     Test net output #1: prob = 0.3607
I1002 13:12:52.229887 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_4500.caffemodel
I1002 13:12:52.633266 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_4500.solverstate
I1002 13:12:52.771376 30882 solver.cpp:593]     Max_acc: 0.3607  with iter: 4500
I1002 13:12:53.583173 30882 solver.cpp:357] Iteration 4500 (0.943933 iter/s, 105.94s/100 iters), loss = 0.593126
I1002 13:12:53.583225 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.508631 (* 1 = 0.508631 loss)
I1002 13:12:53.583236 30882 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I1002 13:14:16.426265 30882 solver.cpp:357] Iteration 4600 (1.20707 iter/s, 82.8454s/100 iters), loss = 0.434943
I1002 13:14:16.426462 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.4791 (* 1 = 0.4791 loss)
I1002 13:14:16.426475 30882 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I1002 13:15:26.464429 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:15:39.310719 30882 solver.cpp:357] Iteration 4700 (1.20647 iter/s, 82.8866s/100 iters), loss = 0.419257
I1002 13:15:39.310780 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.381693 (* 1 = 0.381693 loss)
I1002 13:15:39.310789 30882 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I1002 13:17:02.412344 30882 solver.cpp:357] Iteration 4800 (1.20331 iter/s, 83.1039s/100 iters), loss = 0.525035
I1002 13:17:02.415725 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.633357 (* 1 = 0.633357 loss)
I1002 13:17:02.415740 30882 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I1002 13:18:25.522593 30882 solver.cpp:357] Iteration 4900 (1.20324 iter/s, 83.1091s/100 iters), loss = 0.522992
I1002 13:18:25.522737 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.463883 (* 1 = 0.463883 loss)
I1002 13:18:25.522747 30882 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I1002 13:19:47.671206 30882 solver.cpp:514] Iteration 5000, Testing net (#0)
I1002 13:20:10.068639 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:20:10.157735 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.933891 (* 1 = 0.933891 loss)
I1002 13:20:10.157763 30882 solver.cpp:580]     Test net output #1: prob = 0.6867
I1002 13:20:10.157774 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_5000.caffemodel
I1002 13:20:10.562618 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_5000.solverstate
I1002 13:20:10.700287 30882 solver.cpp:593]     Max_acc: 0.6867  with iter: 5000
I1002 13:20:11.519006 30882 solver.cpp:357] Iteration 5000 (0.943404 iter/s, 105.999s/100 iters), loss = 0.351356
I1002 13:20:11.519062 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.401317 (* 1 = 0.401317 loss)
I1002 13:20:11.519073 30882 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I1002 13:21:14.293476 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:21:34.644484 30882 solver.cpp:357] Iteration 5100 (1.20301 iter/s, 83.1249s/100 iters), loss = 0.481251
I1002 13:21:34.644546 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.377704 (* 1 = 0.377704 loss)
I1002 13:21:34.644554 30882 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I1002 13:22:57.760984 30882 solver.cpp:357] Iteration 5200 (1.2032 iter/s, 83.1119s/100 iters), loss = 0.689278
I1002 13:22:57.761122 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.802129 (* 1 = 0.802129 loss)
I1002 13:22:57.761132 30882 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I1002 13:24:20.716202 30882 solver.cpp:357] Iteration 5300 (1.20553 iter/s, 82.9514s/100 iters), loss = 0.506495
I1002 13:24:20.716344 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.574707 (* 1 = 0.574707 loss)
I1002 13:24:20.716356 30882 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I1002 13:25:43.682976 30882 solver.cpp:357] Iteration 5400 (1.20535 iter/s, 82.9637s/100 iters), loss = 0.335797
I1002 13:25:43.683116 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.288398 (* 1 = 0.288398 loss)
I1002 13:25:43.683127 30882 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I1002 13:26:38.724668 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:27:06.186075 30882 solver.cpp:514] Iteration 5500, Testing net (#0)
I1002 13:27:28.491025 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:27:28.578881 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.26992 (* 1 = 1.26992 loss)
I1002 13:27:28.578907 30882 solver.cpp:580]     Test net output #1: prob = 0.5677
I1002 13:27:28.578914 30882 solver.cpp:593]     Max_acc: 0.6867  with iter: 5000
I1002 13:27:29.403498 30882 solver.cpp:357] Iteration 5500 (0.945917 iter/s, 105.718s/100 iters), loss = 0.410591
I1002 13:27:29.403548 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.430948 (* 1 = 0.430948 loss)
I1002 13:27:29.403563 30882 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I1002 13:28:52.521822 30882 solver.cpp:357] Iteration 5600 (1.20313 iter/s, 83.1166s/100 iters), loss = 0.552579
I1002 13:28:52.522006 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.453292 (* 1 = 0.453292 loss)
I1002 13:28:52.522018 30882 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I1002 13:30:15.945001 30882 solver.cpp:357] Iteration 5700 (1.19873 iter/s, 83.4218s/100 iters), loss = 0.498099
I1002 13:30:15.945153 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.38665 (* 1 = 0.38665 loss)
I1002 13:30:15.945166 30882 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I1002 13:31:38.986975 30882 solver.cpp:357] Iteration 5800 (1.20422 iter/s, 83.041s/100 iters), loss = 0.311284
I1002 13:31:38.987120 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.389877 (* 1 = 0.389877 loss)
I1002 13:31:38.987133 30882 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I1002 13:32:25.964686 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:33:02.224423 30882 solver.cpp:357] Iteration 5900 (1.20139 iter/s, 83.2367s/100 iters), loss = 0.659751
I1002 13:33:02.224510 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.525565 (* 1 = 0.525565 loss)
I1002 13:33:02.224522 30882 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I1002 13:34:24.336881 30882 solver.cpp:514] Iteration 6000, Testing net (#0)
I1002 13:34:46.648773 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:34:46.737282 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.654267 (* 1 = 0.654267 loss)
I1002 13:34:46.737308 30882 solver.cpp:580]     Test net output #1: prob = 0.7788
I1002 13:34:46.737321 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_6000.caffemodel
I1002 13:34:47.139317 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_6000.solverstate
I1002 13:34:47.277348 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 13:34:48.089793 30882 solver.cpp:357] Iteration 6000 (0.9446 iter/s, 105.865s/100 iters), loss = 0.474064
I1002 13:34:48.089845 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.430573 (* 1 = 0.430573 loss)
I1002 13:34:48.089861 30882 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I1002 13:36:11.219493 30882 solver.cpp:357] Iteration 6100 (1.20294 iter/s, 83.1296s/100 iters), loss = 0.509201
I1002 13:36:11.219640 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.54549 (* 1 = 0.54549 loss)
I1002 13:36:11.219650 30882 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I1002 13:37:34.238404 30882 solver.cpp:357] Iteration 6200 (1.20454 iter/s, 83.0189s/100 iters), loss = 0.465108
I1002 13:37:34.238544 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.327547 (* 1 = 0.327547 loss)
I1002 13:37:34.238555 30882 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I1002 13:38:13.327437 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:38:57.233047 30882 solver.cpp:357] Iteration 6300 (1.20489 iter/s, 82.9948s/100 iters), loss = 0.407976
I1002 13:38:57.233162 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.380441 (* 1 = 0.380441 loss)
I1002 13:38:57.233177 30882 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I1002 13:40:20.207550 30882 solver.cpp:357] Iteration 6400 (1.20518 iter/s, 82.9749s/100 iters), loss = 0.564269
I1002 13:40:20.207686 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.606198 (* 1 = 0.606198 loss)
I1002 13:40:20.207697 30882 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I1002 13:41:42.383234 30882 solver.cpp:514] Iteration 6500, Testing net (#0)
I1002 13:42:04.690680 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:42:04.778355 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.801354 (* 1 = 0.801354 loss)
I1002 13:42:04.778381 30882 solver.cpp:580]     Test net output #1: prob = 0.7378
I1002 13:42:04.778388 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 13:42:05.602560 30882 solver.cpp:357] Iteration 6500 (0.948806 iter/s, 105.396s/100 iters), loss = 0.449872
I1002 13:42:05.602614 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.498599 (* 1 = 0.498599 loss)
I1002 13:42:05.602627 30882 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I1002 13:43:28.632346 30882 solver.cpp:357] Iteration 6600 (1.20438 iter/s, 83.0304s/100 iters), loss = 0.516264
I1002 13:43:28.632488 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.448473 (* 1 = 0.448473 loss)
I1002 13:43:28.632498 30882 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I1002 13:44:00.224756 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:44:51.721458 30882 solver.cpp:357] Iteration 6700 (1.20352 iter/s, 83.0898s/100 iters), loss = 0.418628
I1002 13:44:51.721599 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.402601 (* 1 = 0.402601 loss)
I1002 13:44:51.721609 30882 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I1002 13:46:14.748534 30882 solver.cpp:357] Iteration 6800 (1.20442 iter/s, 83.0278s/100 iters), loss = 0.40386
I1002 13:46:14.748674 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.335045 (* 1 = 0.335045 loss)
I1002 13:46:14.748685 30882 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I1002 13:47:37.523610 30882 solver.cpp:357] Iteration 6900 (1.20808 iter/s, 82.7758s/100 iters), loss = 0.531038
I1002 13:47:37.523861 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.424453 (* 1 = 0.424453 loss)
I1002 13:47:37.523871 30882 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I1002 13:48:59.730196 30882 solver.cpp:514] Iteration 7000, Testing net (#0)
I1002 13:49:22.046977 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:49:22.135738 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.672748 (* 1 = 0.672748 loss)
I1002 13:49:22.135764 30882 solver.cpp:580]     Test net output #1: prob = 0.7738
I1002 13:49:22.135771 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 13:49:22.954699 30882 solver.cpp:357] Iteration 7000 (0.948478 iter/s, 105.432s/100 iters), loss = 0.481042
I1002 13:49:22.954748 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.638947 (* 1 = 0.638947 loss)
I1002 13:49:22.954761 30882 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I1002 13:49:46.575361 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:50:45.789253 30882 solver.cpp:357] Iteration 7100 (1.20721 iter/s, 82.8355s/100 iters), loss = 0.43102
I1002 13:50:45.789960 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.479976 (* 1 = 0.479976 loss)
I1002 13:50:45.789974 30882 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I1002 13:52:08.578809 30882 solver.cpp:357] Iteration 7200 (1.20788 iter/s, 82.7899s/100 iters), loss = 0.535748
I1002 13:52:08.578943 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.495437 (* 1 = 0.495437 loss)
I1002 13:52:08.578954 30882 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I1002 13:53:31.414212 30882 solver.cpp:357] Iteration 7300 (1.2072 iter/s, 82.8363s/100 iters), loss = 0.408368
I1002 13:53:31.414312 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.344512 (* 1 = 0.344512 loss)
I1002 13:53:31.414322 30882 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I1002 13:54:54.666095 30882 solver.cpp:357] Iteration 7400 (1.20116 iter/s, 83.2529s/100 iters), loss = 0.516776
I1002 13:54:54.666213 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.514228 (* 1 = 0.514228 loss)
I1002 13:54:54.666223 30882 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I1002 13:55:10.436069 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:56:16.835865 30882 solver.cpp:514] Iteration 7500, Testing net (#0)
I1002 13:56:39.141196 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 13:56:39.229727 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.746255 (* 1 = 0.746255 loss)
I1002 13:56:39.229749 30882 solver.cpp:580]     Test net output #1: prob = 0.7538
I1002 13:56:39.229755 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 13:56:40.050724 30882 solver.cpp:357] Iteration 7500 (0.948833 iter/s, 105.393s/100 iters), loss = 0.357692
I1002 13:56:40.050773 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.42393 (* 1 = 0.42393 loss)
I1002 13:56:40.050784 30882 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I1002 13:58:03.338003 30882 solver.cpp:357] Iteration 7600 (1.20052 iter/s, 83.2974s/100 iters), loss = 0.396046
I1002 13:58:03.338100 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.268917 (* 1 = 0.268917 loss)
I1002 13:58:03.338111 30882 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I1002 13:59:26.250907 30882 solver.cpp:357] Iteration 7700 (1.20595 iter/s, 82.9219s/100 iters), loss = 0.435687
I1002 13:59:26.251050 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.539812 (* 1 = 0.539812 loss)
I1002 13:59:26.251061 30882 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I1002 14:00:49.285257 30882 solver.cpp:357] Iteration 7800 (1.2042 iter/s, 83.0423s/100 iters), loss = 0.433603
I1002 14:00:49.285375 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.427466 (* 1 = 0.427466 loss)
I1002 14:00:49.285385 30882 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I1002 14:00:57.180290 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:02:12.119227 30882 solver.cpp:357] Iteration 7900 (1.20713 iter/s, 82.8412s/100 iters), loss = 0.513748
I1002 14:02:12.121485 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.444721 (* 1 = 0.444721 loss)
I1002 14:02:12.121505 30882 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I1002 14:03:34.308475 30882 solver.cpp:514] Iteration 8000, Testing net (#0)
I1002 14:03:56.615069 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:03:56.703903 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.837661 (* 1 = 0.837661 loss)
I1002 14:03:56.703929 30882 solver.cpp:580]     Test net output #1: prob = 0.7167
I1002 14:03:56.703935 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:03:57.524236 30882 solver.cpp:357] Iteration 8000 (0.948666 iter/s, 105.411s/100 iters), loss = 0.418812
I1002 14:03:57.524287 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.417852 (* 1 = 0.417852 loss)
I1002 14:03:57.524300 30882 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I1002 14:05:20.638123 30882 solver.cpp:357] Iteration 8100 (1.20308 iter/s, 83.1198s/100 iters), loss = 0.407278
I1002 14:05:20.638257 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.473125 (* 1 = 0.473125 loss)
I1002 14:05:20.638267 30882 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I1002 14:06:43.803663 30882 solver.cpp:357] Iteration 8200 (1.20234 iter/s, 83.1709s/100 iters), loss = 0.438678
I1002 14:06:43.803807 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.494843 (* 1 = 0.494843 loss)
I1002 14:06:43.803817 30882 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I1002 14:06:44.223937 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:08:06.736598 30882 solver.cpp:357] Iteration 8300 (1.20572 iter/s, 82.9379s/100 iters), loss = 0.501759
I1002 14:08:06.736690 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.511547 (* 1 = 0.511547 loss)
I1002 14:08:06.736702 30882 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I1002 14:09:29.775308 30882 solver.cpp:357] Iteration 8400 (1.20419 iter/s, 83.0434s/100 iters), loss = 0.379456
I1002 14:09:29.775447 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.422987 (* 1 = 0.422987 loss)
I1002 14:09:29.775460 30882 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I1002 14:10:51.924278 30882 solver.cpp:514] Iteration 8500, Testing net (#0)
I1002 14:11:14.237530 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:11:14.326072 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.720387 (* 1 = 0.720387 loss)
I1002 14:11:14.326099 30882 solver.cpp:580]     Test net output #1: prob = 0.7568
I1002 14:11:14.326105 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:11:15.147966 30882 solver.cpp:357] Iteration 8500 (0.948963 iter/s, 105.378s/100 iters), loss = 0.371738
I1002 14:11:15.148020 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.429859 (* 1 = 0.429859 loss)
I1002 14:11:15.148035 30882 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I1002 14:12:30.568867 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:12:38.015478 30882 solver.cpp:357] Iteration 8600 (1.20669 iter/s, 82.8716s/100 iters), loss = 0.44591
I1002 14:12:38.015538 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.464914 (* 1 = 0.464914 loss)
I1002 14:12:38.015550 30882 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I1002 14:14:00.938060 30882 solver.cpp:357] Iteration 8700 (1.20589 iter/s, 82.9265s/100 iters), loss = 0.430613
I1002 14:14:00.938201 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.480848 (* 1 = 0.480848 loss)
I1002 14:14:00.938215 30882 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I1002 14:15:24.055280 30882 solver.cpp:357] Iteration 8800 (1.20307 iter/s, 83.1209s/100 iters), loss = 0.370433
I1002 14:15:24.055421 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.292716 (* 1 = 0.292716 loss)
I1002 14:15:24.055433 30882 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I1002 14:16:47.233500 30882 solver.cpp:357] Iteration 8900 (1.20219 iter/s, 83.1818s/100 iters), loss = 0.455521
I1002 14:16:47.233600 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.534202 (* 1 = 0.534202 loss)
I1002 14:16:47.233615 30882 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I1002 14:17:54.939636 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:18:09.407426 30882 solver.cpp:514] Iteration 9000, Testing net (#0)
I1002 14:18:31.707162 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:18:31.795007 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.926573 (* 1 = 0.926573 loss)
I1002 14:18:31.795032 30882 solver.cpp:580]     Test net output #1: prob = 0.7027
I1002 14:18:31.795039 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:18:32.612828 30882 solver.cpp:357] Iteration 9000 (0.948913 iter/s, 105.384s/100 iters), loss = 0.271885
I1002 14:18:32.612876 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.29864 (* 1 = 0.29864 loss)
I1002 14:18:32.612890 30882 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I1002 14:19:55.322474 30882 solver.cpp:357] Iteration 9100 (1.209 iter/s, 82.713s/100 iters), loss = 0.365319
I1002 14:19:55.322612 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.412093 (* 1 = 0.412093 loss)
I1002 14:19:55.322621 30882 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I1002 14:21:18.241633 30882 solver.cpp:357] Iteration 9200 (1.20595 iter/s, 82.9224s/100 iters), loss = 0.39651
I1002 14:21:18.241729 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.391345 (* 1 = 0.391345 loss)
I1002 14:21:18.241740 30882 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I1002 14:22:41.216507 30882 solver.cpp:357] Iteration 9300 (1.20514 iter/s, 82.9781s/100 iters), loss = 0.400958
I1002 14:22:41.216648 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.490888 (* 1 = 0.490888 loss)
I1002 14:22:41.216658 30882 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I1002 14:23:40.883438 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:24:04.129540 30882 solver.cpp:357] Iteration 9400 (1.20604 iter/s, 82.9161s/100 iters), loss = 0.51666
I1002 14:24:04.129601 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.35951 (* 1 = 0.35951 loss)
I1002 14:24:04.129612 30882 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I1002 14:25:26.323643 30882 solver.cpp:514] Iteration 9500, Testing net (#0)
I1002 14:25:48.627405 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:25:48.715116 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.0369 (* 1 = 1.0369 loss)
I1002 14:25:48.715142 30882 solver.cpp:580]     Test net output #1: prob = 0.6947
I1002 14:25:48.715149 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:25:49.540514 30882 solver.cpp:357] Iteration 9500 (0.948632 iter/s, 105.415s/100 iters), loss = 0.467317
I1002 14:25:49.540563 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.518857 (* 1 = 0.518857 loss)
I1002 14:25:49.540575 30882 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I1002 14:27:12.674226 30882 solver.cpp:357] Iteration 9600 (1.20284 iter/s, 83.1368s/100 iters), loss = 0.438041
I1002 14:27:12.674369 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.524028 (* 1 = 0.524028 loss)
I1002 14:27:12.674381 30882 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I1002 14:28:35.799664 30882 solver.cpp:357] Iteration 9700 (1.20296 iter/s, 83.1284s/100 iters), loss = 0.381683
I1002 14:28:35.799796 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.436079 (* 1 = 0.436079 loss)
I1002 14:28:35.799808 30882 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I1002 14:29:28.127039 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:29:58.892758 30882 solver.cpp:357] Iteration 9800 (1.20343 iter/s, 83.096s/100 iters), loss = 0.428253
I1002 14:29:58.892848 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.620999 (* 1 = 0.620999 loss)
I1002 14:29:58.892860 30882 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I1002 14:31:21.925585 30882 solver.cpp:357] Iteration 9900 (1.20434 iter/s, 83.0331s/100 iters), loss = 0.276085
I1002 14:31:21.925717 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.241086 (* 1 = 0.241086 loss)
I1002 14:31:21.925727 30882 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I1002 14:32:43.968592 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_10000.caffemodel
I1002 14:32:44.374079 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_10000.solverstate
I1002 14:32:44.512265 30882 solver.cpp:514] Iteration 10000, Testing net (#0)
I1002 14:33:06.815979 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:33:06.904333 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.727079 (* 1 = 0.727079 loss)
I1002 14:33:06.904359 30882 solver.cpp:580]     Test net output #1: prob = 0.744299
I1002 14:33:06.904366 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:33:07.726565 30882 solver.cpp:357] Iteration 10000 (0.945177 iter/s, 105.8s/100 iters), loss = 0.394433
I1002 14:33:07.726614 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.323696 (* 1 = 0.323696 loss)
I1002 14:33:07.726625 30882 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I1002 14:34:30.885432 30882 solver.cpp:357] Iteration 10100 (1.20252 iter/s, 83.1588s/100 iters), loss = 0.471641
I1002 14:34:30.885570 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.534953 (* 1 = 0.534953 loss)
I1002 14:34:30.885581 30882 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I1002 14:35:15.209086 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:35:53.694272 30882 solver.cpp:357] Iteration 10200 (1.2076 iter/s, 82.8091s/100 iters), loss = 0.409341
I1002 14:35:53.694377 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.412881 (* 1 = 0.412881 loss)
I1002 14:35:53.694391 30882 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I1002 14:37:16.687999 30882 solver.cpp:357] Iteration 10300 (1.2049 iter/s, 82.9943s/100 iters), loss = 0.343016
I1002 14:37:16.688138 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.418435 (* 1 = 0.418435 loss)
I1002 14:37:16.688149 30882 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I1002 14:38:39.668692 30882 solver.cpp:357] Iteration 10400 (1.20509 iter/s, 82.9814s/100 iters), loss = 0.437476
I1002 14:38:39.668828 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.572614 (* 1 = 0.572614 loss)
I1002 14:38:39.668838 30882 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I1002 14:40:01.568393 30882 solver.cpp:514] Iteration 10500, Testing net (#0)
I1002 14:40:23.869995 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:40:23.957556 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.13554 (* 1 = 1.13554 loss)
I1002 14:40:23.957582 30882 solver.cpp:580]     Test net output #1: prob = 0.6784
I1002 14:40:23.957589 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:40:24.778523 30882 solver.cpp:357] Iteration 10500 (0.951374 iter/s, 105.111s/100 iters), loss = 0.376784
I1002 14:40:24.778573 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.389041 (* 1 = 0.389041 loss)
I1002 14:40:24.778585 30882 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I1002 14:41:01.530686 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:41:47.931254 30882 solver.cpp:357] Iteration 10600 (1.20259 iter/s, 83.154s/100 iters), loss = 0.440166
I1002 14:41:47.931393 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.431673 (* 1 = 0.431673 loss)
I1002 14:41:47.931403 30882 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I1002 14:43:10.836525 30882 solver.cpp:357] Iteration 10700 (1.20618 iter/s, 82.9066s/100 iters), loss = 0.337647
I1002 14:43:10.836673 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.328996 (* 1 = 0.328996 loss)
I1002 14:43:10.836683 30882 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I1002 14:44:33.955341 30882 solver.cpp:357] Iteration 10800 (1.20308 iter/s, 83.1202s/100 iters), loss = 0.353192
I1002 14:44:33.955479 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.396923 (* 1 = 0.396923 loss)
I1002 14:44:33.955490 30882 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I1002 14:45:56.974285 30882 solver.cpp:357] Iteration 10900 (1.20452 iter/s, 83.0205s/100 iters), loss = 0.412587
I1002 14:45:56.974422 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.510982 (* 1 = 0.510982 loss)
I1002 14:45:56.974433 30882 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I1002 14:46:25.598762 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:47:19.094224 30882 solver.cpp:514] Iteration 11000, Testing net (#0)
I1002 14:47:41.407693 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:47:41.494143 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.749608 (* 1 = 0.749608 loss)
I1002 14:47:41.494168 30882 solver.cpp:580]     Test net output #1: prob = 0.7575
I1002 14:47:41.494174 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:47:42.317129 30882 solver.cpp:357] Iteration 11000 (0.949262 iter/s, 105.345s/100 iters), loss = 0.390223
I1002 14:47:42.317183 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.3066 (* 1 = 0.3066 loss)
I1002 14:47:42.317195 30882 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I1002 14:49:05.312568 30882 solver.cpp:357] Iteration 11100 (1.20486 iter/s, 82.9972s/100 iters), loss = 0.423437
I1002 14:49:05.312700 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.45607 (* 1 = 0.45607 loss)
I1002 14:49:05.312712 30882 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I1002 14:50:28.288321 30882 solver.cpp:357] Iteration 11200 (1.20515 iter/s, 82.9775s/100 iters), loss = 0.235194
I1002 14:50:28.288456 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.22308 (* 1 = 0.22308 loss)
I1002 14:50:28.288468 30882 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I1002 14:51:51.266672 30882 solver.cpp:357] Iteration 11300 (1.20511 iter/s, 82.9802s/100 iters), loss = 0.44693
I1002 14:51:51.266813 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.391473 (* 1 = 0.391473 loss)
I1002 14:51:51.266825 30882 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I1002 14:52:12.435896 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:53:14.386492 30882 solver.cpp:357] Iteration 11400 (1.20306 iter/s, 83.1217s/100 iters), loss = 0.427204
I1002 14:53:14.386633 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.365201 (* 1 = 0.365201 loss)
I1002 14:53:14.386644 30882 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I1002 14:54:36.616263 30882 solver.cpp:514] Iteration 11500, Testing net (#0)
I1002 14:54:58.921877 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:54:59.010090 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.818596 (* 1 = 0.818596 loss)
I1002 14:54:59.010116 30882 solver.cpp:580]     Test net output #1: prob = 0.749
I1002 14:54:59.010123 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 14:54:59.832834 30882 solver.cpp:357] Iteration 11500 (0.948327 iter/s, 105.449s/100 iters), loss = 0.417995
I1002 14:54:59.832883 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.348438 (* 1 = 0.348438 loss)
I1002 14:54:59.832896 30882 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I1002 14:56:22.785245 30882 solver.cpp:357] Iteration 11600 (1.20548 iter/s, 82.9544s/100 iters), loss = 0.463859
I1002 14:56:22.785388 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.548684 (* 1 = 0.548684 loss)
I1002 14:56:22.785399 30882 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I1002 14:57:45.673578 30882 solver.cpp:357] Iteration 11700 (1.20641 iter/s, 82.8903s/100 iters), loss = 0.415284
I1002 14:57:45.676007 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.303544 (* 1 = 0.303544 loss)
I1002 14:57:45.676021 30882 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I1002 14:57:58.952883 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 14:59:08.293431 30882 solver.cpp:357] Iteration 11800 (1.21037 iter/s, 82.6195s/100 iters), loss = 0.504057
I1002 14:59:08.293525 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.401497 (* 1 = 0.401497 loss)
I1002 14:59:08.293537 30882 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I1002 15:00:31.150216 30882 solver.cpp:357] Iteration 11900 (1.20687 iter/s, 82.8588s/100 iters), loss = 0.347922
I1002 15:00:31.150349 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.349465 (* 1 = 0.349465 loss)
I1002 15:00:31.150360 30882 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I1002 15:01:53.098794 30882 solver.cpp:514] Iteration 12000, Testing net (#0)
I1002 15:02:15.400132 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:02:15.488263 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.21358 (* 1 = 1.21358 loss)
I1002 15:02:15.488288 30882 solver.cpp:580]     Test net output #1: prob = 0.6225
I1002 15:02:15.488296 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 15:02:16.305758 30882 solver.cpp:357] Iteration 12000 (0.950949 iter/s, 105.158s/100 iters), loss = 0.445051
I1002 15:02:16.305807 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.394537 (* 1 = 0.394537 loss)
I1002 15:02:16.305817 30882 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I1002 15:03:39.068325 30882 solver.cpp:357] Iteration 12100 (1.20824 iter/s, 82.7647s/100 iters), loss = 0.503561
I1002 15:03:39.068469 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.624299 (* 1 = 0.624299 loss)
I1002 15:03:39.068480 30882 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I1002 15:03:44.460909 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:05:01.795560 30882 solver.cpp:357] Iteration 12200 (1.20877 iter/s, 82.7287s/100 iters), loss = 0.377865
I1002 15:05:01.795641 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.375236 (* 1 = 0.375236 loss)
I1002 15:05:01.795652 30882 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I1002 15:06:24.888869 30882 solver.cpp:357] Iteration 12300 (1.20346 iter/s, 83.0938s/100 iters), loss = 0.341107
I1002 15:06:24.889011 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.380665 (* 1 = 0.380665 loss)
I1002 15:06:24.889021 30882 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I1002 15:07:47.794965 30882 solver.cpp:357] Iteration 12400 (1.20617 iter/s, 82.9067s/100 iters), loss = 0.438432
I1002 15:07:47.795063 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.430755 (* 1 = 0.430755 loss)
I1002 15:07:47.795073 30882 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I1002 15:09:08.186641 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:09:09.839735 30882 solver.cpp:514] Iteration 12500, Testing net (#0)
I1002 15:09:32.144351 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:09:32.232751 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.20862 (* 1 = 1.20862 loss)
I1002 15:09:32.232779 30882 solver.cpp:580]     Test net output #1: prob = 0.6328
I1002 15:09:32.232785 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 15:09:33.057932 30882 solver.cpp:357] Iteration 12500 (0.949991 iter/s, 105.264s/100 iters), loss = 0.464397
I1002 15:09:33.057982 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.476671 (* 1 = 0.476671 loss)
I1002 15:09:33.057994 30882 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I1002 15:10:55.897063 30882 solver.cpp:357] Iteration 12600 (1.20714 iter/s, 82.8402s/100 iters), loss = 0.433163
I1002 15:10:55.897228 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.397828 (* 1 = 0.397828 loss)
I1002 15:10:55.897238 30882 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I1002 15:12:18.903570 30882 solver.cpp:357] Iteration 12700 (1.20471 iter/s, 83.0076s/100 iters), loss = 0.542137
I1002 15:12:18.903667 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.414676 (* 1 = 0.414676 loss)
I1002 15:12:18.903677 30882 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I1002 15:13:42.127995 30882 solver.cpp:357] Iteration 12800 (1.20155 iter/s, 83.2257s/100 iters), loss = 0.44328
I1002 15:13:42.130286 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.44039 (* 1 = 0.44039 loss)
I1002 15:13:42.130308 30882 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I1002 15:14:55.143160 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:15:05.104588 30882 solver.cpp:357] Iteration 12900 (1.20517 iter/s, 82.9758s/100 iters), loss = 0.431458
I1002 15:15:05.104648 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.33993 (* 1 = 0.33993 loss)
I1002 15:15:05.104658 30882 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I1002 15:16:27.307605 30882 solver.cpp:514] Iteration 13000, Testing net (#0)
I1002 15:16:49.631240 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:16:49.718175 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.804056 (* 1 = 0.804056 loss)
I1002 15:16:49.718202 30882 solver.cpp:580]     Test net output #1: prob = 0.7455
I1002 15:16:49.718209 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 15:16:50.541700 30882 solver.cpp:357] Iteration 13000 (0.948415 iter/s, 105.439s/100 iters), loss = 0.303113
I1002 15:16:50.541751 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.328866 (* 1 = 0.328866 loss)
I1002 15:16:50.541761 30882 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I1002 15:18:13.514649 30882 solver.cpp:357] Iteration 13100 (1.20519 iter/s, 82.9745s/100 iters), loss = 0.35466
I1002 15:18:13.514786 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.371312 (* 1 = 0.371312 loss)
I1002 15:18:13.514796 30882 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I1002 15:19:36.693770 30882 solver.cpp:357] Iteration 13200 (1.2022 iter/s, 83.1807s/100 iters), loss = 0.231927
I1002 15:19:36.693864 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.247216 (* 1 = 0.247216 loss)
I1002 15:19:36.693877 30882 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I1002 15:20:41.839931 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:20:59.665875 30882 solver.cpp:357] Iteration 13300 (1.2052 iter/s, 82.9737s/100 iters), loss = 0.434719
I1002 15:20:59.665949 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.35222 (* 1 = 0.35222 loss)
I1002 15:20:59.665958 30882 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I1002 15:22:22.679955 30882 solver.cpp:357] Iteration 13400 (1.20459 iter/s, 83.0157s/100 iters), loss = 0.40055
I1002 15:22:22.680071 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.36071 (* 1 = 0.36071 loss)
I1002 15:22:22.680085 30882 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I1002 15:23:44.947124 30882 solver.cpp:514] Iteration 13500, Testing net (#0)
I1002 15:24:07.336779 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:24:07.425271 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.768634 (* 1 = 0.768634 loss)
I1002 15:24:07.425297 30882 solver.cpp:580]     Test net output #1: prob = 0.7477
I1002 15:24:07.425303 30882 solver.cpp:593]     Max_acc: 0.7788  with iter: 6000
I1002 15:24:08.255259 30882 solver.cpp:357] Iteration 13500 (0.947172 iter/s, 105.577s/100 iters), loss = 0.431125
I1002 15:24:08.255309 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.463583 (* 1 = 0.463583 loss)
I1002 15:24:08.255321 30882 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I1002 15:25:31.225608 30882 solver.cpp:357] Iteration 13600 (1.20522 iter/s, 82.9721s/100 iters), loss = 0.274451
I1002 15:25:31.225749 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.187011 (* 1 = 0.187011 loss)
I1002 15:25:31.225759 30882 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I1002 15:26:28.246448 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:26:53.836796 30882 solver.cpp:357] Iteration 13700 (1.21047 iter/s, 82.6129s/100 iters), loss = 0.472618
I1002 15:26:53.836858 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.464917 (* 1 = 0.464917 loss)
I1002 15:26:53.836868 30882 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I1002 15:28:16.229219 30882 solver.cpp:357] Iteration 13800 (1.21368 iter/s, 82.3942s/100 iters), loss = 0.28522
I1002 15:28:16.229320 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.392706 (* 1 = 0.392706 loss)
I1002 15:28:16.229331 30882 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I1002 15:29:38.814457 30882 solver.cpp:357] Iteration 13900 (1.21084 iter/s, 82.587s/100 iters), loss = 0.316125
I1002 15:29:38.814590 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.300691 (* 1 = 0.300691 loss)
I1002 15:29:38.814600 30882 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I1002 15:31:00.539163 30882 solver.cpp:514] Iteration 14000, Testing net (#0)
I1002 15:31:22.848693 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:31:22.937273 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.664779 (* 1 = 0.664779 loss)
I1002 15:31:22.937299 30882 solver.cpp:580]     Test net output #1: prob = 0.7813
I1002 15:31:22.937311 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_14000.caffemodel
I1002 15:31:23.342535 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_14000.solverstate
I1002 15:31:23.480548 30882 solver.cpp:593]     Max_acc: 0.7813  with iter: 14000
I1002 15:31:24.291626 30882 solver.cpp:357] Iteration 14000 (0.948052 iter/s, 105.479s/100 iters), loss = 0.343431
I1002 15:31:24.291679 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.35051 (* 1 = 0.35051 loss)
I1002 15:31:24.291692 30882 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I1002 15:32:13.439879 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:32:46.915839 30882 solver.cpp:357] Iteration 14100 (1.21027 iter/s, 82.626s/100 iters), loss = 0.242973
I1002 15:32:46.915977 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.207713 (* 1 = 0.207713 loss)
I1002 15:32:46.915988 30882 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I1002 15:34:09.474154 30882 solver.cpp:357] Iteration 14200 (1.21124 iter/s, 82.5601s/100 iters), loss = 0.478683
I1002 15:34:09.474249 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.446719 (* 1 = 0.446719 loss)
I1002 15:34:09.474262 30882 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I1002 15:35:32.071069 30882 solver.cpp:357] Iteration 14300 (1.21067 iter/s, 82.5987s/100 iters), loss = 0.468456
I1002 15:35:32.071166 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.357312 (* 1 = 0.357312 loss)
I1002 15:35:32.071177 30882 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I1002 15:36:54.461771 30882 solver.cpp:357] Iteration 14400 (1.2137 iter/s, 82.3925s/100 iters), loss = 0.316833
I1002 15:36:54.461969 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.340917 (* 1 = 0.340917 loss)
I1002 15:36:54.461979 30882 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I1002 15:37:36.120079 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:38:16.045660 30882 solver.cpp:514] Iteration 14500, Testing net (#0)
I1002 15:38:38.359550 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:38:38.446413 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.640202 (* 1 = 0.640202 loss)
I1002 15:38:38.446440 30882 solver.cpp:580]     Test net output #1: prob = 0.7925
I1002 15:38:38.446452 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_14500.caffemodel
I1002 15:38:38.851610 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_14500.solverstate
I1002 15:38:38.990039 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 15:38:39.802656 30882 solver.cpp:357] Iteration 14500 (0.949279 iter/s, 105.343s/100 iters), loss = 0.305415
I1002 15:38:39.802711 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.26454 (* 1 = 0.26454 loss)
I1002 15:38:39.802724 30882 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I1002 15:40:02.091735 30882 solver.cpp:357] Iteration 14600 (1.21524 iter/s, 82.288s/100 iters), loss = 0.430145
I1002 15:40:02.091817 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.377162 (* 1 = 0.377162 loss)
I1002 15:40:02.091828 30882 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I1002 15:41:24.435292 30882 solver.cpp:357] Iteration 14700 (1.21444 iter/s, 82.3428s/100 iters), loss = 0.39119
I1002 15:41:24.435446 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.436827 (* 1 = 0.436827 loss)
I1002 15:41:24.435456 30882 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I1002 15:42:46.522790 30882 solver.cpp:357] Iteration 14800 (1.21822 iter/s, 82.087s/100 iters), loss = 0.333004
I1002 15:42:46.523392 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.282213 (* 1 = 0.282213 loss)
I1002 15:42:46.523406 30882 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I1002 15:43:20.318634 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:44:08.840869 30882 solver.cpp:357] Iteration 14900 (1.21481 iter/s, 82.3174s/100 iters), loss = 0.232842
I1002 15:44:08.841006 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.26898 (* 1 = 0.26898 loss)
I1002 15:44:08.841017 30882 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I1002 15:45:30.399395 30882 solver.cpp:514] Iteration 15000, Testing net (#0)
I1002 15:45:52.707864 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:45:52.795572 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.687844 (* 1 = 0.687844 loss)
I1002 15:45:52.795598 30882 solver.cpp:580]     Test net output #1: prob = 0.7712
I1002 15:45:52.795604 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 15:45:53.607774 30882 solver.cpp:357] Iteration 15000 (0.954499 iter/s, 104.767s/100 iters), loss = 0.39718
I1002 15:45:53.607823 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.372737 (* 1 = 0.372737 loss)
I1002 15:45:53.607836 30882 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I1002 15:47:15.832759 30882 solver.cpp:357] Iteration 15100 (1.21617 iter/s, 82.2253s/100 iters), loss = 0.375608
I1002 15:47:15.832880 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.309991 (* 1 = 0.309991 loss)
I1002 15:47:15.832890 30882 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I1002 15:48:38.167601 30882 solver.cpp:357] Iteration 15200 (1.21455 iter/s, 82.3353s/100 iters), loss = 0.555116
I1002 15:48:38.167747 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.489114 (* 1 = 0.489114 loss)
I1002 15:48:38.167757 30882 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I1002 15:49:04.056943 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:50:00.606248 30882 solver.cpp:357] Iteration 15300 (1.21301 iter/s, 82.4392s/100 iters), loss = 0.405871
I1002 15:50:00.606828 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.511598 (* 1 = 0.511598 loss)
I1002 15:50:00.606844 30882 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I1002 15:51:22.936022 30882 solver.cpp:357] Iteration 15400 (1.21462 iter/s, 82.3301s/100 iters), loss = 0.418589
I1002 15:51:22.936172 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.268991 (* 1 = 0.268991 loss)
I1002 15:51:22.936182 30882 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I1002 15:52:44.476330 30882 solver.cpp:514] Iteration 15500, Testing net (#0)
I1002 15:53:06.778254 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:53:06.864974 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.673704 (* 1 = 0.673704 loss)
I1002 15:53:06.865000 30882 solver.cpp:580]     Test net output #1: prob = 0.7793
I1002 15:53:06.865005 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 15:53:07.675264 30882 solver.cpp:357] Iteration 15500 (0.954742 iter/s, 104.74s/100 iters), loss = 0.374056
I1002 15:53:07.675313 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.424662 (* 1 = 0.424662 loss)
I1002 15:53:07.675326 30882 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I1002 15:54:30.007745 30882 solver.cpp:357] Iteration 15600 (1.21457 iter/s, 82.3335s/100 iters), loss = 0.327458
I1002 15:54:30.007886 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.357126 (* 1 = 0.357126 loss)
I1002 15:54:30.007897 30882 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I1002 15:54:48.107008 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 15:55:52.292407 30882 solver.cpp:357] Iteration 15700 (1.21528 iter/s, 82.2857s/100 iters), loss = 0.357546
I1002 15:55:52.292549 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.294558 (* 1 = 0.294558 loss)
I1002 15:55:52.292560 30882 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I1002 15:57:14.511716 30882 solver.cpp:357] Iteration 15800 (1.21624 iter/s, 82.2204s/100 iters), loss = 0.347844
I1002 15:57:14.511852 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.371356 (* 1 = 0.371356 loss)
I1002 15:57:14.511862 30882 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I1002 15:58:36.723898 30882 solver.cpp:357] Iteration 15900 (1.21635 iter/s, 82.2133s/100 iters), loss = 0.232656
I1002 15:58:36.724031 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.274404 (* 1 = 0.274404 loss)
I1002 15:58:36.724041 30882 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I1002 15:59:58.168694 30882 solver.cpp:514] Iteration 16000, Testing net (#0)
I1002 16:00:20.475625 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:00:20.562625 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.819928 (* 1 = 0.819928 loss)
I1002 16:00:20.562651 30882 solver.cpp:580]     Test net output #1: prob = 0.7547
I1002 16:00:20.562659 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:00:21.374155 30882 solver.cpp:357] Iteration 16000 (0.95555 iter/s, 104.652s/100 iters), loss = 0.395711
I1002 16:00:21.374204 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.376574 (* 1 = 0.376574 loss)
I1002 16:00:21.374217 30882 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I1002 16:00:32.018590 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:01:43.611541 30882 solver.cpp:357] Iteration 16100 (1.21597 iter/s, 82.2387s/100 iters), loss = 0.362073
I1002 16:01:43.611688 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.443307 (* 1 = 0.443307 loss)
I1002 16:01:43.611699 30882 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I1002 16:03:05.946022 30882 solver.cpp:357] Iteration 16200 (1.21454 iter/s, 82.3357s/100 iters), loss = 0.258713
I1002 16:03:05.946117 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.23736 (* 1 = 0.23736 loss)
I1002 16:03:05.946130 30882 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I1002 16:04:28.306838 30882 solver.cpp:357] Iteration 16300 (1.21415 iter/s, 82.3621s/100 iters), loss = 0.24073
I1002 16:04:28.307034 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.312051 (* 1 = 0.312051 loss)
I1002 16:04:28.307045 30882 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I1002 16:05:50.428886 30882 solver.cpp:357] Iteration 16400 (1.21768 iter/s, 82.1233s/100 iters), loss = 0.352669
I1002 16:05:50.428994 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.43033 (* 1 = 0.43033 loss)
I1002 16:05:50.429006 30882 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I1002 16:05:53.331493 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:07:12.055524 30882 solver.cpp:514] Iteration 16500, Testing net (#0)
I1002 16:07:34.360352 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:07:34.448776 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.891526 (* 1 = 0.891526 loss)
I1002 16:07:34.448801 30882 solver.cpp:580]     Test net output #1: prob = 0.727499
I1002 16:07:34.448807 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:07:35.261010 30882 solver.cpp:357] Iteration 16500 (0.95389 iter/s, 104.834s/100 iters), loss = 0.44072
I1002 16:07:35.261059 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.433047 (* 1 = 0.433047 loss)
I1002 16:07:35.261072 30882 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I1002 16:08:57.529026 30882 solver.cpp:357] Iteration 16600 (1.21552 iter/s, 82.2694s/100 iters), loss = 0.413098
I1002 16:08:57.529134 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.378833 (* 1 = 0.378833 loss)
I1002 16:08:57.529150 30882 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I1002 16:10:19.802858 30882 solver.cpp:357] Iteration 16700 (1.21543 iter/s, 82.2752s/100 iters), loss = 0.308841
I1002 16:10:19.803004 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.302757 (* 1 = 0.302757 loss)
I1002 16:10:19.803016 30882 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I1002 16:11:37.174427 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:11:42.104884 30882 solver.cpp:357] Iteration 16800 (1.21502 iter/s, 82.3034s/100 iters), loss = 0.326751
I1002 16:11:42.104945 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.195142 (* 1 = 0.195142 loss)
I1002 16:11:42.104957 30882 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I1002 16:13:04.410728 30882 solver.cpp:357] Iteration 16900 (1.21495 iter/s, 82.3081s/100 iters), loss = 0.392247
I1002 16:13:04.410821 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.395792 (* 1 = 0.395792 loss)
I1002 16:13:04.410833 30882 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I1002 16:14:25.803970 30882 solver.cpp:514] Iteration 17000, Testing net (#0)
I1002 16:14:48.112370 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:14:48.199404 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.796317 (* 1 = 0.796317 loss)
I1002 16:14:48.199429 30882 solver.cpp:580]     Test net output #1: prob = 0.7666
I1002 16:14:48.199436 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:14:49.009160 30882 solver.cpp:357] Iteration 17000 (0.955966 iter/s, 104.606s/100 iters), loss = 0.471331
I1002 16:14:49.009212 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.384836 (* 1 = 0.384836 loss)
I1002 16:14:49.009224 30882 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I1002 16:16:11.137842 30882 solver.cpp:357] Iteration 17100 (1.21752 iter/s, 82.1342s/100 iters), loss = 0.227492
I1002 16:16:11.137992 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.194931 (* 1 = 0.194931 loss)
I1002 16:16:11.138003 30882 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I1002 16:17:20.437450 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:17:33.146931 30882 solver.cpp:357] Iteration 17200 (1.2193 iter/s, 82.014s/100 iters), loss = 0.260755
I1002 16:17:33.146993 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.224562 (* 1 = 0.224562 loss)
I1002 16:17:33.147004 30882 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I1002 16:18:55.429548 30882 solver.cpp:357] Iteration 17300 (1.21526 iter/s, 82.2872s/100 iters), loss = 0.414051
I1002 16:18:55.431617 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.413114 (* 1 = 0.413114 loss)
I1002 16:18:55.431632 30882 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I1002 16:20:17.562400 30882 solver.cpp:357] Iteration 17400 (1.21751 iter/s, 82.1351s/100 iters), loss = 0.408798
I1002 16:20:17.562549 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.297757 (* 1 = 0.297757 loss)
I1002 16:20:17.562561 30882 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I1002 16:21:39.200863 30882 solver.cpp:514] Iteration 17500, Testing net (#0)
I1002 16:22:01.495877 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:22:01.584508 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.820894 (* 1 = 0.820894 loss)
I1002 16:22:01.584538 30882 solver.cpp:580]     Test net output #1: prob = 0.7614
I1002 16:22:01.584545 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:22:02.395830 30882 solver.cpp:357] Iteration 17500 (0.953849 iter/s, 104.838s/100 iters), loss = 0.184371
I1002 16:22:02.395884 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.237471 (* 1 = 0.237471 loss)
I1002 16:22:02.395897 30882 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I1002 16:23:04.424810 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:23:24.647070 30882 solver.cpp:357] Iteration 17600 (1.21573 iter/s, 82.2549s/100 iters), loss = 0.354541
I1002 16:23:24.647132 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.25752 (* 1 = 0.25752 loss)
I1002 16:23:24.647143 30882 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I1002 16:24:47.025950 30882 solver.cpp:357] Iteration 17700 (1.21385 iter/s, 82.3824s/100 iters), loss = 0.453753
I1002 16:24:47.026087 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.541519 (* 1 = 0.541519 loss)
I1002 16:24:47.026098 30882 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I1002 16:26:09.193044 30882 solver.cpp:357] Iteration 17800 (1.21698 iter/s, 82.1703s/100 iters), loss = 0.450196
I1002 16:26:09.193159 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.452438 (* 1 = 0.452438 loss)
I1002 16:26:09.193173 30882 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I1002 16:27:31.434336 30882 solver.cpp:357] Iteration 17900 (1.21589 iter/s, 82.2444s/100 iters), loss = 0.155046
I1002 16:27:31.434474 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.168335 (* 1 = 0.168335 loss)
I1002 16:27:31.434484 30882 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I1002 16:28:25.714594 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:28:52.825038 30882 solver.cpp:514] Iteration 18000, Testing net (#0)
I1002 16:29:15.121665 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:29:15.209648 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.797887 (* 1 = 0.797887 loss)
I1002 16:29:15.209674 30882 solver.cpp:580]     Test net output #1: prob = 0.761001
I1002 16:29:15.209681 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:29:16.019208 30882 solver.cpp:357] Iteration 18000 (0.956127 iter/s, 104.589s/100 iters), loss = 0.28052
I1002 16:29:16.019259 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.334765 (* 1 = 0.334765 loss)
I1002 16:29:16.019271 30882 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I1002 16:30:38.444227 30882 solver.cpp:357] Iteration 18100 (1.21318 iter/s, 82.4279s/100 iters), loss = 0.327556
I1002 16:30:38.444365 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.227988 (* 1 = 0.227988 loss)
I1002 16:30:38.444376 30882 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I1002 16:32:00.871150 30882 solver.cpp:357] Iteration 18200 (1.21316 iter/s, 82.4297s/100 iters), loss = 0.369902
I1002 16:32:00.871290 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.257107 (* 1 = 0.257107 loss)
I1002 16:32:00.871302 30882 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I1002 16:33:23.272889 30882 solver.cpp:357] Iteration 18300 (1.21353 iter/s, 82.4044s/100 iters), loss = 0.240944
I1002 16:33:23.273083 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.324049 (* 1 = 0.324049 loss)
I1002 16:33:23.273094 30882 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I1002 16:34:09.843533 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:34:45.518826 30882 solver.cpp:357] Iteration 18400 (1.21583 iter/s, 82.2485s/100 iters), loss = 0.529076
I1002 16:34:45.519073 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.471934 (* 1 = 0.471934 loss)
I1002 16:34:45.519083 30882 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I1002 16:36:06.778856 30882 solver.cpp:514] Iteration 18500, Testing net (#0)
I1002 16:36:29.097506 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:36:29.185569 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.723834 (* 1 = 0.723834 loss)
I1002 16:36:29.185595 30882 solver.cpp:580]     Test net output #1: prob = 0.7822
I1002 16:36:29.185601 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:36:29.995245 30882 solver.cpp:357] Iteration 18500 (0.957125 iter/s, 104.48s/100 iters), loss = 0.373503
I1002 16:36:29.995296 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.321944 (* 1 = 0.321944 loss)
I1002 16:36:29.995306 30882 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I1002 16:37:52.043857 30882 solver.cpp:357] Iteration 18600 (1.21875 iter/s, 82.0512s/100 iters), loss = 0.367669
I1002 16:37:52.044009 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.352011 (* 1 = 0.352011 loss)
I1002 16:37:52.044021 30882 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I1002 16:39:14.407137 30882 solver.cpp:357] Iteration 18700 (1.2141 iter/s, 82.3657s/100 iters), loss = 0.34374
I1002 16:39:14.407277 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.337302 (* 1 = 0.337302 loss)
I1002 16:39:14.407287 30882 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I1002 16:39:52.998512 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:40:36.530758 30882 solver.cpp:357] Iteration 18800 (1.21764 iter/s, 82.126s/100 iters), loss = 0.24841
I1002 16:40:36.530899 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.254987 (* 1 = 0.254987 loss)
I1002 16:40:36.530910 30882 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I1002 16:41:58.778264 30882 solver.cpp:357] Iteration 18900 (1.21581 iter/s, 82.2499s/100 iters), loss = 0.402277
I1002 16:41:58.778405 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.475271 (* 1 = 0.475271 loss)
I1002 16:41:58.778417 30882 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I1002 16:43:20.468621 30882 solver.cpp:514] Iteration 19000, Testing net (#0)
I1002 16:43:42.775825 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:43:42.863749 30882 solver.cpp:580]     Test net output #0: Softmax1 = 1.3776 (* 1 = 1.3776 loss)
I1002 16:43:42.863775 30882 solver.cpp:580]     Test net output #1: prob = 0.6552
I1002 16:43:42.863781 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:43:43.672835 30882 solver.cpp:357] Iteration 19000 (0.95331 iter/s, 104.898s/100 iters), loss = 0.351425
I1002 16:43:43.672885 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.264822 (* 1 = 0.264822 loss)
I1002 16:43:43.672897 30882 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I1002 16:45:06.176331 30882 solver.cpp:357] Iteration 19100 (1.21203 iter/s, 82.5059s/100 iters), loss = 0.360029
I1002 16:45:06.176425 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.315995 (* 1 = 0.315995 loss)
I1002 16:45:06.176436 30882 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I1002 16:45:37.466789 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:46:28.475747 30882 solver.cpp:357] Iteration 19200 (1.21504 iter/s, 82.3018s/100 iters), loss = 0.376825
I1002 16:46:28.475863 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.409347 (* 1 = 0.409347 loss)
I1002 16:46:28.475878 30882 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I1002 16:47:50.749660 30882 solver.cpp:357] Iteration 19300 (1.21546 iter/s, 82.2736s/100 iters), loss = 0.253338
I1002 16:47:50.749861 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.286965 (* 1 = 0.286965 loss)
I1002 16:47:50.749872 30882 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I1002 16:49:13.145231 30882 solver.cpp:357] Iteration 19400 (1.21379 iter/s, 82.3869s/100 iters), loss = 0.368833
I1002 16:49:13.145385 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.366727 (* 1 = 0.366727 loss)
I1002 16:49:13.145395 30882 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I1002 16:50:34.544302 30882 solver.cpp:514] Iteration 19500, Testing net (#0)
I1002 16:50:56.846520 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:50:56.935307 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.849704 (* 1 = 0.849704 loss)
I1002 16:50:56.935333 30882 solver.cpp:580]     Test net output #1: prob = 0.735499
I1002 16:50:56.935338 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:50:57.746801 30882 solver.cpp:357] Iteration 19500 (0.956091 iter/s, 104.593s/100 iters), loss = 0.333223
I1002 16:50:57.746850 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.452464 (* 1 = 0.452464 loss)
I1002 16:50:57.746861 30882 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I1002 16:51:21.096101 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:52:19.948354 30882 solver.cpp:357] Iteration 19600 (1.21661 iter/s, 82.1958s/100 iters), loss = 0.323503
I1002 16:52:19.948493 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.306774 (* 1 = 0.306774 loss)
I1002 16:52:19.948505 30882 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I1002 16:53:42.052858 30882 solver.cpp:357] Iteration 19700 (1.21803 iter/s, 82.0996s/100 iters), loss = 0.458844
I1002 16:53:42.052994 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.487498 (* 1 = 0.487498 loss)
I1002 16:53:42.053004 30882 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I1002 16:55:04.163998 30882 solver.cpp:357] Iteration 19800 (1.21792 iter/s, 82.107s/100 iters), loss = 0.389185
I1002 16:55:04.164139 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.483971 (* 1 = 0.483971 loss)
I1002 16:55:04.164149 30882 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I1002 16:56:26.354362 30882 solver.cpp:357] Iteration 19900 (1.21674 iter/s, 82.1869s/100 iters), loss = 0.423746
I1002 16:56:26.354497 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.40694 (* 1 = 0.40694 loss)
I1002 16:56:26.354507 30882 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I1002 16:56:41.970204 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:57:47.673475 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_20000.caffemodel
I1002 16:57:48.079411 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_20000.solverstate
I1002 16:57:48.218341 30882 solver.cpp:514] Iteration 20000, Testing net (#0)
I1002 16:58:10.526588 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 16:58:10.614321 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.631794 (* 1 = 0.631794 loss)
I1002 16:58:10.614347 30882 solver.cpp:580]     Test net output #1: prob = 0.7877
I1002 16:58:10.614353 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 16:58:11.425196 30882 solver.cpp:357] Iteration 20000 (0.951771 iter/s, 105.067s/100 iters), loss = 0.252311
I1002 16:58:11.425247 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.264723 (* 1 = 0.264723 loss)
I1002 16:58:11.425261 30882 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I1002 16:59:33.602712 30882 solver.cpp:357] Iteration 20100 (1.21691 iter/s, 82.1753s/100 iters), loss = 0.354288
I1002 16:59:33.602810 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.280248 (* 1 = 0.280248 loss)
I1002 16:59:33.602823 30882 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I1002 17:00:55.834084 30882 solver.cpp:357] Iteration 20200 (1.21611 iter/s, 82.2295s/100 iters), loss = 0.304838
I1002 17:00:55.834260 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.317731 (* 1 = 0.317731 loss)
I1002 17:00:55.834270 30882 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I1002 17:02:18.061461 30882 solver.cpp:357] Iteration 20300 (1.21616 iter/s, 82.2258s/100 iters), loss = 0.290305
I1002 17:02:18.061606 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.328141 (* 1 = 0.328141 loss)
I1002 17:02:18.061616 30882 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I1002 17:02:25.896322 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:03:40.151587 30882 solver.cpp:357] Iteration 20400 (1.21819 iter/s, 82.0888s/100 iters), loss = 0.312802
I1002 17:03:40.151733 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.354322 (* 1 = 0.354322 loss)
I1002 17:03:40.151743 30882 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I1002 17:05:01.696815 30882 solver.cpp:514] Iteration 20500, Testing net (#0)
I1002 17:05:24.000051 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:05:24.088027 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.682997 (* 1 = 0.682997 loss)
I1002 17:05:24.088052 30882 solver.cpp:580]     Test net output #1: prob = 0.7891
I1002 17:05:24.088058 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 17:05:24.898533 30882 solver.cpp:357] Iteration 20500 (0.954693 iter/s, 104.746s/100 iters), loss = 0.289415
I1002 17:05:24.898581 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.27781 (* 1 = 0.27781 loss)
I1002 17:05:24.898591 30882 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I1002 17:06:47.076063 30882 solver.cpp:357] Iteration 20600 (1.21689 iter/s, 82.1768s/100 iters), loss = 0.264358
I1002 17:06:47.076208 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.281941 (* 1 = 0.281941 loss)
I1002 17:06:47.076220 30882 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I1002 17:08:09.390308 30882 solver.cpp:357] Iteration 20700 (1.21487 iter/s, 82.3136s/100 iters), loss = 0.422118
I1002 17:08:09.390451 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.410051 (* 1 = 0.410051 loss)
I1002 17:08:09.390462 30882 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I1002 17:08:09.806483 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:09:31.727685 30882 solver.cpp:357] Iteration 20800 (1.21452 iter/s, 82.3369s/100 iters), loss = 0.389683
I1002 17:09:31.727823 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.389829 (* 1 = 0.389829 loss)
I1002 17:09:31.727833 30882 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I1002 17:10:54.079566 30882 solver.cpp:357] Iteration 20900 (1.21431 iter/s, 82.3515s/100 iters), loss = 0.341062
I1002 17:10:54.079684 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.363793 (* 1 = 0.363793 loss)
I1002 17:10:54.079699 30882 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I1002 17:12:15.408707 30882 solver.cpp:514] Iteration 21000, Testing net (#0)
I1002 17:12:37.706756 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:12:37.793846 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.955371 (* 1 = 0.955371 loss)
I1002 17:12:37.793872 30882 solver.cpp:580]     Test net output #1: prob = 0.698
I1002 17:12:37.793879 30882 solver.cpp:593]     Max_acc: 0.7925  with iter: 14500
I1002 17:12:38.605135 30882 solver.cpp:357] Iteration 21000 (0.956706 iter/s, 104.525s/100 iters), loss = 0.310067
I1002 17:12:38.605185 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.346412 (* 1 = 0.346412 loss)
I1002 17:12:38.605199 30882 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I1002 17:13:53.390404 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:14:00.796641 30882 solver.cpp:357] Iteration 21100 (1.21667 iter/s, 82.1915s/100 iters), loss = 0.3737
I1002 17:14:00.796705 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.410893 (* 1 = 0.410893 loss)
I1002 17:14:00.796715 30882 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I1002 17:15:23.070950 30882 solver.cpp:357] Iteration 21200 (1.21545 iter/s, 82.2743s/100 iters), loss = 0.339372
I1002 17:15:23.071139 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.358407 (* 1 = 0.358407 loss)
I1002 17:15:23.071151 30882 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I1002 17:16:45.206811 30882 solver.cpp:357] Iteration 21300 (1.2175 iter/s, 82.1358s/100 iters), loss = 0.391554
I1002 17:16:45.206964 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.201118 (* 1 = 0.201118 loss)
I1002 17:16:45.206979 30882 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I1002 17:18:07.545508 30882 solver.cpp:357] Iteration 21400 (1.21449 iter/s, 82.3388s/100 iters), loss = 0.388156
I1002 17:18:07.545640 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.40528 (* 1 = 0.40528 loss)
I1002 17:18:07.545650 30882 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I1002 17:19:14.663905 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:19:29.072198 30882 solver.cpp:514] Iteration 21500, Testing net (#0)
I1002 17:19:51.375640 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:19:51.463495 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.534454 (* 1 = 0.534454 loss)
I1002 17:19:51.463519 30882 solver.cpp:580]     Test net output #1: prob = 0.835901
I1002 17:19:51.463531 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_21500.caffemodel
I1002 17:19:51.866907 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_21500.solverstate
I1002 17:19:52.005205 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:19:52.816562 30882 solver.cpp:357] Iteration 21500 (0.949927 iter/s, 105.271s/100 iters), loss = 0.221228
I1002 17:19:52.816615 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.285404 (* 1 = 0.285404 loss)
I1002 17:19:52.816627 30882 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I1002 17:21:14.929929 30882 solver.cpp:357] Iteration 21600 (1.21782 iter/s, 82.1136s/100 iters), loss = 0.333946
I1002 17:21:14.930069 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.296837 (* 1 = 0.296837 loss)
I1002 17:21:14.930080 30882 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I1002 17:22:37.034595 30882 solver.cpp:357] Iteration 21700 (1.21788 iter/s, 82.1099s/100 iters), loss = 0.307905
I1002 17:22:37.034704 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.293809 (* 1 = 0.293809 loss)
I1002 17:22:37.034718 30882 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I1002 17:23:59.160105 30882 solver.cpp:357] Iteration 21800 (1.21755 iter/s, 82.1324s/100 iters), loss = 0.363512
I1002 17:23:59.160256 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.342093 (* 1 = 0.342093 loss)
I1002 17:23:59.160266 30882 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I1002 17:24:58.381640 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:25:21.417192 30882 solver.cpp:357] Iteration 21900 (1.21561 iter/s, 82.2632s/100 iters), loss = 0.514642
I1002 17:25:21.417251 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.343618 (* 1 = 0.343618 loss)
I1002 17:25:21.417261 30882 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I1002 17:26:42.851546 30882 solver.cpp:514] Iteration 22000, Testing net (#0)
I1002 17:27:05.159713 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:27:05.247655 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.627498 (* 1 = 0.627498 loss)
I1002 17:27:05.247681 30882 solver.cpp:580]     Test net output #1: prob = 0.7953
I1002 17:27:05.247687 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:27:06.058115 30882 solver.cpp:357] Iteration 22000 (0.955586 iter/s, 104.648s/100 iters), loss = 0.352376
I1002 17:27:06.058162 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.255936 (* 1 = 0.255936 loss)
I1002 17:27:06.058174 30882 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I1002 17:28:28.123970 30882 solver.cpp:357] Iteration 22100 (1.21846 iter/s, 82.0706s/100 iters), loss = 0.289603
I1002 17:28:28.124110 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.245253 (* 1 = 0.245253 loss)
I1002 17:28:28.124121 30882 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I1002 17:29:50.319152 30882 solver.cpp:357] Iteration 22200 (1.21655 iter/s, 82.1994s/100 iters), loss = 0.358669
I1002 17:29:50.319342 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.327812 (* 1 = 0.327812 loss)
I1002 17:29:50.319352 30882 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I1002 17:30:42.111524 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:31:12.496049 30882 solver.cpp:357] Iteration 22300 (1.21683 iter/s, 82.1807s/100 iters), loss = 0.32514
I1002 17:31:12.496191 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.41018 (* 1 = 0.41018 loss)
I1002 17:31:12.496201 30882 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I1002 17:32:34.672003 30882 solver.cpp:357] Iteration 22400 (1.21685 iter/s, 82.1794s/100 iters), loss = 0.287494
I1002 17:32:34.672142 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.340128 (* 1 = 0.340128 loss)
I1002 17:32:34.672152 30882 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I1002 17:33:56.086706 30882 solver.cpp:514] Iteration 22500, Testing net (#0)
I1002 17:34:18.401008 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:34:18.487606 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.473221 (* 1 = 0.473221 loss)
I1002 17:34:18.487632 30882 solver.cpp:580]     Test net output #1: prob = 0.8353
I1002 17:34:18.487637 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:34:19.300226 30882 solver.cpp:357] Iteration 22500 (0.955728 iter/s, 104.632s/100 iters), loss = 0.313852
I1002 17:34:19.300277 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.305637 (* 1 = 0.305637 loss)
I1002 17:34:19.300290 30882 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I1002 17:35:41.500339 30882 solver.cpp:357] Iteration 22600 (1.2165 iter/s, 82.2031s/100 iters), loss = 0.417814
I1002 17:35:41.500486 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.403133 (* 1 = 0.403133 loss)
I1002 17:35:41.500497 30882 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I1002 17:36:25.515033 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:37:03.789950 30882 solver.cpp:357] Iteration 22700 (1.21518 iter/s, 82.2923s/100 iters), loss = 0.339777
I1002 17:37:03.790048 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.346026 (* 1 = 0.346026 loss)
I1002 17:37:03.790060 30882 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I1002 17:38:26.090549 30882 solver.cpp:357] Iteration 22800 (1.21502 iter/s, 82.3032s/100 iters), loss = 0.255952
I1002 17:38:26.090683 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.280984 (* 1 = 0.280984 loss)
I1002 17:38:26.090694 30882 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I1002 17:39:48.096884 30882 solver.cpp:357] Iteration 22900 (1.21938 iter/s, 82.0087s/100 iters), loss = 0.416182
I1002 17:39:48.097026 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.465767 (* 1 = 0.465767 loss)
I1002 17:39:48.097036 30882 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I1002 17:41:09.462378 30882 solver.cpp:514] Iteration 23000, Testing net (#0)
I1002 17:41:31.766427 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:41:31.853286 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.851804 (* 1 = 0.851804 loss)
I1002 17:41:31.853310 30882 solver.cpp:580]     Test net output #1: prob = 0.7468
I1002 17:41:31.853317 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:41:32.665199 30882 solver.cpp:357] Iteration 23000 (0.956286 iter/s, 104.571s/100 iters), loss = 0.403444
I1002 17:41:32.665249 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.296656 (* 1 = 0.296656 loss)
I1002 17:41:32.665266 30882 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I1002 17:42:08.742025 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:42:54.808049 30882 solver.cpp:357] Iteration 23100 (1.21736 iter/s, 82.1451s/100 iters), loss = 0.399435
I1002 17:42:54.808239 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.32592 (* 1 = 0.32592 loss)
I1002 17:42:54.808250 30882 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I1002 17:44:16.905078 30882 solver.cpp:357] Iteration 23200 (1.21804 iter/s, 82.099s/100 iters), loss = 0.304982
I1002 17:44:16.905225 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.258952 (* 1 = 0.258952 loss)
I1002 17:44:16.905234 30882 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I1002 17:45:39.045806 30882 solver.cpp:357] Iteration 23300 (1.21739 iter/s, 82.1427s/100 iters), loss = 0.299564
I1002 17:45:39.045965 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.28706 (* 1 = 0.28706 loss)
I1002 17:45:39.045975 30882 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I1002 17:47:01.243835 30882 solver.cpp:357] Iteration 23400 (1.21655 iter/s, 82.1999s/100 iters), loss = 0.327171
I1002 17:47:01.243937 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.362954 (* 1 = 0.362954 loss)
I1002 17:47:01.243948 30882 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I1002 17:47:29.589226 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:48:22.498288 30882 solver.cpp:514] Iteration 23500, Testing net (#0)
I1002 17:48:44.807495 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:48:44.895375 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.974382 (* 1 = 0.974382 loss)
I1002 17:48:44.895401 30882 solver.cpp:580]     Test net output #1: prob = 0.718899
I1002 17:48:44.895407 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:48:45.706207 30882 solver.cpp:357] Iteration 23500 (0.95726 iter/s, 104.465s/100 iters), loss = 0.310066
I1002 17:48:45.706254 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.237168 (* 1 = 0.237168 loss)
I1002 17:48:45.706264 30882 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I1002 17:50:07.998312 30882 solver.cpp:357] Iteration 23600 (1.21516 iter/s, 82.294s/100 iters), loss = 0.343528
I1002 17:50:07.998452 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.392345 (* 1 = 0.392345 loss)
I1002 17:50:07.998463 30882 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I1002 17:51:30.051921 30882 solver.cpp:357] Iteration 23700 (1.21869 iter/s, 82.0554s/100 iters), loss = 0.185884
I1002 17:51:30.052069 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.109549 (* 1 = 0.109549 loss)
I1002 17:51:30.052080 30882 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I1002 17:52:52.148033 30882 solver.cpp:357] Iteration 23800 (1.21806 iter/s, 82.0978s/100 iters), loss = 0.474113
I1002 17:52:52.152140 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.398446 (* 1 = 0.398446 loss)
I1002 17:52:52.152155 30882 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I1002 17:53:13.082049 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:54:14.206038 30882 solver.cpp:357] Iteration 23900 (1.21868 iter/s, 82.0558s/100 iters), loss = 0.419569
I1002 17:54:14.206176 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.367408 (* 1 = 0.367408 loss)
I1002 17:54:14.206187 30882 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I1002 17:55:35.467900 30882 solver.cpp:514] Iteration 24000, Testing net (#0)
I1002 17:55:57.768810 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 17:55:57.856992 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.61767 (* 1 = 0.61767 loss)
I1002 17:55:57.857019 30882 solver.cpp:580]     Test net output #1: prob = 0.7868
I1002 17:55:57.857025 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 17:55:58.666468 30882 solver.cpp:357] Iteration 24000 (0.957272 iter/s, 104.463s/100 iters), loss = 0.323047
I1002 17:55:58.666517 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.288316 (* 1 = 0.288316 loss)
I1002 17:55:58.666530 30882 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I1002 17:57:20.698829 30882 solver.cpp:357] Iteration 24100 (1.21891 iter/s, 82.0407s/100 iters), loss = 0.330736
I1002 17:57:20.698971 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.374032 (* 1 = 0.374032 loss)
I1002 17:57:20.698982 30882 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I1002 17:58:42.789628 30882 solver.cpp:357] Iteration 24200 (1.21805 iter/s, 82.0982s/100 iters), loss = 0.312236
I1002 17:58:42.789820 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.205358 (* 1 = 0.205358 loss)
I1002 17:58:42.789831 30882 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I1002 17:58:55.935828 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:00:04.857165 30882 solver.cpp:357] Iteration 24300 (1.21841 iter/s, 82.0741s/100 iters), loss = 0.42607
I1002 18:00:04.857267 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.387766 (* 1 = 0.387766 loss)
I1002 18:00:04.857280 30882 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I1002 18:01:26.972901 30882 solver.cpp:357] Iteration 24400 (1.2177 iter/s, 82.1218s/100 iters), loss = 0.353612
I1002 18:01:26.973091 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.441169 (* 1 = 0.441169 loss)
I1002 18:01:26.973104 30882 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I1002 18:02:48.316512 30882 solver.cpp:514] Iteration 24500, Testing net (#0)
I1002 18:03:10.618172 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:03:10.706368 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.76097 (* 1 = 0.76097 loss)
I1002 18:03:10.706394 30882 solver.cpp:580]     Test net output #1: prob = 0.7504
I1002 18:03:10.706400 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:03:11.519299 30882 solver.cpp:357] Iteration 24500 (0.95645 iter/s, 104.553s/100 iters), loss = 0.314519
I1002 18:03:11.519348 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.26547 (* 1 = 0.26547 loss)
I1002 18:03:11.519361 30882 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I1002 18:04:33.576305 30882 solver.cpp:357] Iteration 24600 (1.21859 iter/s, 82.062s/100 iters), loss = 0.387166
I1002 18:04:33.576436 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.388935 (* 1 = 0.388935 loss)
I1002 18:04:33.576445 30882 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I1002 18:04:38.923830 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:05:55.717103 30882 solver.cpp:357] Iteration 24700 (1.21735 iter/s, 82.1453s/100 iters), loss = 0.320776
I1002 18:05:55.717274 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.346547 (* 1 = 0.346547 loss)
I1002 18:05:55.717285 30882 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I1002 18:07:17.660814 30882 solver.cpp:357] Iteration 24800 (1.22029 iter/s, 81.9479s/100 iters), loss = 0.313261
I1002 18:07:17.660954 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.309699 (* 1 = 0.309699 loss)
I1002 18:07:17.660966 30882 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I1002 18:08:39.950026 30882 solver.cpp:357] Iteration 24900 (1.21517 iter/s, 82.2932s/100 iters), loss = 0.471314
I1002 18:08:39.950148 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.472349 (* 1 = 0.472349 loss)
I1002 18:08:39.950158 30882 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I1002 18:09:59.501396 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:10:01.135164 30882 solver.cpp:514] Iteration 25000, Testing net (#0)
I1002 18:10:23.439977 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:10:23.527189 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.529134 (* 1 = 0.529134 loss)
I1002 18:10:23.527215 30882 solver.cpp:580]     Test net output #1: prob = 0.824501
I1002 18:10:23.527220 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:10:24.338618 30882 solver.cpp:357] Iteration 25000 (0.957916 iter/s, 104.393s/100 iters), loss = 0.358257
I1002 18:10:24.338667 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.325456 (* 1 = 0.325456 loss)
I1002 18:10:24.338678 30882 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I1002 18:11:46.341590 30882 solver.cpp:357] Iteration 25100 (1.21942 iter/s, 82.0065s/100 iters), loss = 0.396289
I1002 18:11:46.341791 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.36546 (* 1 = 0.36546 loss)
I1002 18:11:46.341801 30882 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I1002 18:13:08.316252 30882 solver.cpp:357] Iteration 25200 (1.21984 iter/s, 81.9779s/100 iters), loss = 0.440578
I1002 18:13:08.316375 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.297374 (* 1 = 0.297374 loss)
I1002 18:13:08.316390 30882 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I1002 18:14:30.503018 30882 solver.cpp:357] Iteration 25300 (1.21669 iter/s, 82.1899s/100 iters), loss = 0.385799
I1002 18:14:30.503114 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.437174 (* 1 = 0.437174 loss)
I1002 18:14:30.503124 30882 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I1002 18:15:42.820713 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:15:52.669024 30882 solver.cpp:357] Iteration 25400 (1.217 iter/s, 82.1691s/100 iters), loss = 0.43962
I1002 18:15:52.669086 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.304332 (* 1 = 0.304332 loss)
I1002 18:15:52.669097 30882 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I1002 18:17:13.929337 30882 solver.cpp:514] Iteration 25500, Testing net (#0)
I1002 18:17:36.238416 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:17:36.326076 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.673332 (* 1 = 0.673332 loss)
I1002 18:17:36.326102 30882 solver.cpp:580]     Test net output #1: prob = 0.7803
I1002 18:17:36.326108 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:17:37.137827 30882 solver.cpp:357] Iteration 25500 (0.957188 iter/s, 104.473s/100 iters), loss = 0.28822
I1002 18:17:37.137877 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.277861 (* 1 = 0.277861 loss)
I1002 18:17:37.137890 30882 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I1002 18:18:59.225795 30882 solver.cpp:357] Iteration 25600 (1.21816 iter/s, 82.0909s/100 iters), loss = 0.301977
I1002 18:18:59.225894 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.294732 (* 1 = 0.294732 loss)
I1002 18:18:59.225909 30882 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I1002 18:20:21.418426 30882 solver.cpp:357] Iteration 25700 (1.21661 iter/s, 82.1954s/100 iters), loss = 0.308452
I1002 18:20:21.418553 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.434808 (* 1 = 0.434808 loss)
I1002 18:20:21.418563 30882 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I1002 18:21:25.960158 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:21:43.630316 30882 solver.cpp:357] Iteration 25800 (1.21633 iter/s, 82.2146s/100 iters), loss = 0.367403
I1002 18:21:43.630376 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.289627 (* 1 = 0.289627 loss)
I1002 18:21:43.630388 30882 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I1002 18:23:05.745733 30882 solver.cpp:357] Iteration 25900 (1.21776 iter/s, 82.1182s/100 iters), loss = 0.53881
I1002 18:23:05.745827 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.453622 (* 1 = 0.453622 loss)
I1002 18:23:05.745839 30882 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I1002 18:24:27.302788 30882 solver.cpp:514] Iteration 26000, Testing net (#0)
I1002 18:24:49.605756 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:24:49.694881 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.570535 (* 1 = 0.570535 loss)
I1002 18:24:49.694907 30882 solver.cpp:580]     Test net output #1: prob = 0.8176
I1002 18:24:49.694913 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:24:50.506188 30882 solver.cpp:357] Iteration 26000 (0.954527 iter/s, 104.764s/100 iters), loss = 0.312837
I1002 18:24:50.506237 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.319009 (* 1 = 0.319009 loss)
I1002 18:24:50.506248 30882 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I1002 18:26:12.765453 30882 solver.cpp:357] Iteration 26100 (1.21563 iter/s, 82.2619s/100 iters), loss = 0.2666
I1002 18:26:12.765589 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.289645 (* 1 = 0.289645 loss)
I1002 18:26:12.765599 30882 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I1002 18:27:09.550598 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:27:35.038915 30882 solver.cpp:357] Iteration 26200 (1.21542 iter/s, 82.276s/100 iters), loss = 0.366412
I1002 18:27:35.038977 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.423218 (* 1 = 0.423218 loss)
I1002 18:27:35.038990 30882 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I1002 18:28:57.277698 30882 solver.cpp:357] Iteration 26300 (1.21593 iter/s, 82.2414s/100 iters), loss = 0.251677
I1002 18:28:57.277854 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.288008 (* 1 = 0.288008 loss)
I1002 18:28:57.277864 30882 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I1002 18:30:19.496330 30882 solver.cpp:357] Iteration 26400 (1.21627 iter/s, 82.2184s/100 iters), loss = 0.272915
I1002 18:30:19.496472 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.304152 (* 1 = 0.304152 loss)
I1002 18:30:19.496482 30882 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I1002 18:31:40.829465 30882 solver.cpp:514] Iteration 26500, Testing net (#0)
I1002 18:32:03.137257 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:32:03.225533 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.590791 (* 1 = 0.590791 loss)
I1002 18:32:03.225559 30882 solver.cpp:580]     Test net output #1: prob = 0.7996
I1002 18:32:03.225565 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:32:04.036881 30882 solver.cpp:357] Iteration 26500 (0.95664 iter/s, 104.533s/100 iters), loss = 0.311622
I1002 18:32:04.036931 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.359938 (* 1 = 0.359938 loss)
I1002 18:32:04.036942 30882 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I1002 18:32:52.868659 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:33:26.267659 30882 solver.cpp:357] Iteration 26600 (1.21616 iter/s, 82.2258s/100 iters), loss = 0.285349
I1002 18:33:26.267799 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.360693 (* 1 = 0.360693 loss)
I1002 18:33:26.267810 30882 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I1002 18:34:48.507763 30882 solver.cpp:357] Iteration 26700 (1.21601 iter/s, 82.236s/100 iters), loss = 0.436709
I1002 18:34:48.507861 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.479458 (* 1 = 0.479458 loss)
I1002 18:34:48.507874 30882 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I1002 18:36:10.550223 30882 solver.cpp:357] Iteration 26800 (1.21893 iter/s, 82.0392s/100 iters), loss = 0.429156
I1002 18:36:10.550320 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.408544 (* 1 = 0.408544 loss)
I1002 18:36:10.550330 30882 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I1002 18:37:32.566639 30882 solver.cpp:357] Iteration 26900 (1.21931 iter/s, 82.0138s/100 iters), loss = 0.271705
I1002 18:37:32.566771 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.192425 (* 1 = 0.192425 loss)
I1002 18:37:32.566781 30882 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I1002 18:38:14.157001 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:38:53.950029 30882 solver.cpp:514] Iteration 27000, Testing net (#0)
I1002 18:39:16.245836 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:39:16.334070 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.976471 (* 1 = 0.976471 loss)
I1002 18:39:16.334096 30882 solver.cpp:580]     Test net output #1: prob = 0.7311
I1002 18:39:16.334102 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:39:17.143079 30882 solver.cpp:357] Iteration 27000 (0.956261 iter/s, 104.574s/100 iters), loss = 0.26145
I1002 18:39:17.143127 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.199182 (* 1 = 0.199182 loss)
I1002 18:39:17.143139 30882 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I1002 18:40:39.174029 30882 solver.cpp:357] Iteration 27100 (1.21907 iter/s, 82.0296s/100 iters), loss = 0.41245
I1002 18:40:39.174170 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.357042 (* 1 = 0.357042 loss)
I1002 18:40:39.174180 30882 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I1002 18:42:01.362341 30882 solver.cpp:357] Iteration 27200 (1.21673 iter/s, 82.1873s/100 iters), loss = 0.413173
I1002 18:42:01.362498 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.518324 (* 1 = 0.518324 loss)
I1002 18:42:01.362509 30882 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I1002 18:43:23.554227 30882 solver.cpp:357] Iteration 27300 (1.21668 iter/s, 82.1912s/100 iters), loss = 0.348143
I1002 18:43:23.554330 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.289525 (* 1 = 0.289525 loss)
I1002 18:43:23.554342 30882 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I1002 18:43:57.313213 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:44:45.862988 30882 solver.cpp:357] Iteration 27400 (1.21494 iter/s, 82.3084s/100 iters), loss = 0.259273
I1002 18:44:45.863111 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.236979 (* 1 = 0.236979 loss)
I1002 18:44:45.863121 30882 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I1002 18:46:07.334856 30882 solver.cpp:514] Iteration 27500, Testing net (#0)
I1002 18:46:29.642029 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:46:29.729982 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.810725 (* 1 = 0.810725 loss)
I1002 18:46:29.730008 30882 solver.cpp:580]     Test net output #1: prob = 0.77
I1002 18:46:29.730015 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:46:30.540562 30882 solver.cpp:357] Iteration 27500 (0.955315 iter/s, 104.677s/100 iters), loss = 0.416099
I1002 18:46:30.540612 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.316736 (* 1 = 0.316736 loss)
I1002 18:46:30.540624 30882 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I1002 18:47:52.667250 30882 solver.cpp:357] Iteration 27600 (1.21763 iter/s, 82.1269s/100 iters), loss = 0.320702
I1002 18:47:52.667388 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.33461 (* 1 = 0.33461 loss)
I1002 18:47:52.667398 30882 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I1002 18:49:15.024721 30882 solver.cpp:357] Iteration 27700 (1.21421 iter/s, 82.3578s/100 iters), loss = 0.470396
I1002 18:49:15.024817 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.462672 (* 1 = 0.462672 loss)
I1002 18:49:15.024828 30882 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I1002 18:49:40.913542 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:50:37.196542 30882 solver.cpp:357] Iteration 27800 (1.21696 iter/s, 82.1723s/100 iters), loss = 0.312242
I1002 18:50:37.196687 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.434498 (* 1 = 0.434498 loss)
I1002 18:50:37.196697 30882 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I1002 18:51:59.380997 30882 solver.cpp:357] Iteration 27900 (1.21677 iter/s, 82.185s/100 iters), loss = 0.347313
I1002 18:51:59.381093 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.378947 (* 1 = 0.378947 loss)
I1002 18:51:59.381105 30882 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I1002 18:53:20.541941 30882 solver.cpp:514] Iteration 28000, Testing net (#0)
I1002 18:53:42.864672 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:53:42.951448 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.825501 (* 1 = 0.825501 loss)
I1002 18:53:42.951474 30882 solver.cpp:580]     Test net output #1: prob = 0.7463
I1002 18:53:42.951480 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 18:53:43.761051 30882 solver.cpp:357] Iteration 28000 (0.958029 iter/s, 104.381s/100 iters), loss = 0.260308
I1002 18:53:43.761101 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.322606 (* 1 = 0.322606 loss)
I1002 18:53:43.761112 30882 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I1002 18:55:05.806463 30882 solver.cpp:357] Iteration 28100 (1.21882 iter/s, 82.0463s/100 iters), loss = 0.293859
I1002 18:55:05.806556 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.334856 (* 1 = 0.334856 loss)
I1002 18:55:05.806567 30882 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I1002 18:55:23.899065 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 18:56:27.912084 30882 solver.cpp:357] Iteration 28200 (1.21793 iter/s, 82.1065s/100 iters), loss = 0.374707
I1002 18:56:27.912232 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.267545 (* 1 = 0.267545 loss)
I1002 18:56:27.912242 30882 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I1002 18:57:50.050839 30882 solver.cpp:357] Iteration 28300 (1.21744 iter/s, 82.1397s/100 iters), loss = 0.349915
I1002 18:57:50.050990 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.455536 (* 1 = 0.455536 loss)
I1002 18:57:50.051002 30882 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I1002 18:59:12.187429 30882 solver.cpp:357] Iteration 28400 (1.21747 iter/s, 82.1376s/100 iters), loss = 0.296557
I1002 18:59:12.187569 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.312608 (* 1 = 0.312608 loss)
I1002 18:59:12.187580 30882 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I1002 19:00:33.452335 30882 solver.cpp:514] Iteration 28500, Testing net (#0)
I1002 19:00:55.750171 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:00:55.838071 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.666664 (* 1 = 0.666664 loss)
I1002 19:00:55.838098 30882 solver.cpp:580]     Test net output #1: prob = 0.773501
I1002 19:00:55.838104 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:00:56.648926 30882 solver.cpp:357] Iteration 28500 (0.957278 iter/s, 104.463s/100 iters), loss = 0.245009
I1002 19:00:56.648975 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.264751 (* 1 = 0.264751 loss)
I1002 19:00:56.648988 30882 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I1002 19:01:07.292510 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:02:18.675063 30882 solver.cpp:357] Iteration 28600 (1.21911 iter/s, 82.0273s/100 iters), loss = 0.326355
I1002 19:02:18.675215 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.315287 (* 1 = 0.315287 loss)
I1002 19:02:18.675225 30882 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I1002 19:03:40.905613 30882 solver.cpp:357] Iteration 28700 (1.21608 iter/s, 82.2317s/100 iters), loss = 0.271217
I1002 19:03:40.905755 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.313544 (* 1 = 0.313544 loss)
I1002 19:03:40.905766 30882 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I1002 19:05:03.053287 30882 solver.cpp:357] Iteration 28800 (1.2172 iter/s, 82.1556s/100 iters), loss = 0.247459
I1002 19:05:03.053376 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.310675 (* 1 = 0.310675 loss)
I1002 19:05:03.053386 30882 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I1002 19:06:24.993605 30882 solver.cpp:357] Iteration 28900 (1.22025 iter/s, 81.9502s/100 iters), loss = 0.350739
I1002 19:06:24.993741 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.35635 (* 1 = 0.35635 loss)
I1002 19:06:24.993751 30882 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I1002 19:06:27.873852 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:07:46.138124 30882 solver.cpp:514] Iteration 29000, Testing net (#0)
I1002 19:08:08.446609 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:08:08.534209 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.682204 (* 1 = 0.682204 loss)
I1002 19:08:08.534235 30882 solver.cpp:580]     Test net output #1: prob = 0.7764
I1002 19:08:08.534241 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:08:09.344246 30882 solver.cpp:357] Iteration 29000 (0.958205 iter/s, 104.362s/100 iters), loss = 0.268193
I1002 19:08:09.344296 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.247719 (* 1 = 0.247719 loss)
I1002 19:08:09.344310 30882 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I1002 19:09:31.261880 30882 solver.cpp:357] Iteration 29100 (1.22062 iter/s, 81.9254s/100 iters), loss = 0.39548
I1002 19:09:31.262024 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.347007 (* 1 = 0.347007 loss)
I1002 19:09:31.262035 30882 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I1002 19:10:53.266101 30882 solver.cpp:357] Iteration 29200 (1.21935 iter/s, 82.0112s/100 iters), loss = 0.323465
I1002 19:10:53.266289 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.331134 (* 1 = 0.331134 loss)
I1002 19:10:53.266301 30882 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I1002 19:12:10.495048 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:12:15.433298 30882 solver.cpp:357] Iteration 29300 (1.21694 iter/s, 82.1736s/100 iters), loss = 0.322528
I1002 19:12:15.433359 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.307683 (* 1 = 0.307683 loss)
I1002 19:12:15.433370 30882 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I1002 19:13:37.586501 30882 solver.cpp:357] Iteration 29400 (1.21715 iter/s, 82.1592s/100 iters), loss = 0.442845
I1002 19:13:37.586659 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.499175 (* 1 = 0.499175 loss)
I1002 19:13:37.586669 30882 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I1002 19:14:58.803194 30882 solver.cpp:514] Iteration 29500, Testing net (#0)
I1002 19:15:21.111021 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:15:21.199297 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.523351 (* 1 = 0.523351 loss)
I1002 19:15:21.199323 30882 solver.cpp:580]     Test net output #1: prob = 0.824601
I1002 19:15:21.199329 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:15:22.011723 30882 solver.cpp:357] Iteration 29500 (0.95756 iter/s, 104.432s/100 iters), loss = 0.380459
I1002 19:15:22.011772 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.486602 (* 1 = 0.486602 loss)
I1002 19:15:22.011785 30882 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I1002 19:16:43.991508 30882 solver.cpp:357] Iteration 29600 (1.21974 iter/s, 81.9848s/100 iters), loss = 0.272348
I1002 19:16:43.991631 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.305145 (* 1 = 0.305145 loss)
I1002 19:16:43.991641 30882 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I1002 19:17:53.483856 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:18:06.195417 30882 solver.cpp:357] Iteration 29700 (1.21642 iter/s, 82.2086s/100 iters), loss = 0.340211
I1002 19:18:06.195478 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.279075 (* 1 = 0.279075 loss)
I1002 19:18:06.195490 30882 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I1002 19:19:28.372351 30882 solver.cpp:357] Iteration 29800 (1.21682 iter/s, 82.1814s/100 iters), loss = 0.278929
I1002 19:19:28.372495 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.295964 (* 1 = 0.295964 loss)
I1002 19:19:28.372506 30882 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I1002 19:20:50.558233 30882 solver.cpp:357] Iteration 29900 (1.21669 iter/s, 82.19s/100 iters), loss = 0.334926
I1002 19:20:50.558354 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.402393 (* 1 = 0.402393 loss)
I1002 19:20:50.558365 30882 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I1002 19:22:11.899005 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_30000.caffemodel
I1002 19:22:12.305454 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_30000.solverstate
I1002 19:22:12.444633 30882 solver.cpp:514] Iteration 30000, Testing net (#0)
I1002 19:22:34.746400 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:22:34.833786 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.915593 (* 1 = 0.915593 loss)
I1002 19:22:34.833813 30882 solver.cpp:580]     Test net output #1: prob = 0.7132
I1002 19:22:34.833819 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:22:35.648447 30882 solver.cpp:357] Iteration 30000 (0.951517 iter/s, 105.095s/100 iters), loss = 0.206817
I1002 19:22:35.648495 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.24417 (* 1 = 0.24417 loss)
I1002 19:22:35.648509 30882 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I1002 19:23:37.410955 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:23:57.562683 30882 solver.cpp:357] Iteration 30100 (1.22073 iter/s, 81.9181s/100 iters), loss = 0.370811
I1002 19:23:57.562739 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.224736 (* 1 = 0.224736 loss)
I1002 19:23:57.562754 30882 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I1002 19:25:19.606994 30882 solver.cpp:357] Iteration 30200 (1.2188 iter/s, 82.048s/100 iters), loss = 0.464155
I1002 19:25:19.607141 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.47448 (* 1 = 0.47448 loss)
I1002 19:25:19.607151 30882 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I1002 19:26:41.762701 30882 solver.cpp:357] Iteration 30300 (1.21715 iter/s, 82.1592s/100 iters), loss = 0.393953
I1002 19:26:41.762840 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.372488 (* 1 = 0.372488 loss)
I1002 19:26:41.762850 30882 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I1002 19:28:03.830767 30882 solver.cpp:357] Iteration 30400 (1.21845 iter/s, 82.0715s/100 iters), loss = 0.184938
I1002 19:28:03.830858 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.171929 (* 1 = 0.171929 loss)
I1002 19:28:03.830871 30882 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I1002 19:28:58.158140 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:29:25.218753 30882 solver.cpp:514] Iteration 30500, Testing net (#0)
I1002 19:29:47.522920 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:29:47.610901 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.926507 (* 1 = 0.926507 loss)
I1002 19:29:47.610927 30882 solver.cpp:580]     Test net output #1: prob = 0.720899
I1002 19:29:47.610934 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:29:48.422876 30882 solver.cpp:357] Iteration 30500 (0.956055 iter/s, 104.596s/100 iters), loss = 0.283031
I1002 19:29:48.422924 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.304842 (* 1 = 0.304842 loss)
I1002 19:29:48.422940 30882 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I1002 19:31:10.437188 30882 solver.cpp:357] Iteration 30600 (1.21925 iter/s, 82.0176s/100 iters), loss = 0.321505
I1002 19:31:10.437328 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.261286 (* 1 = 0.261286 loss)
I1002 19:31:10.437338 30882 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I1002 19:32:32.398041 30882 solver.cpp:357] Iteration 30700 (1.22005 iter/s, 81.9641s/100 iters), loss = 0.335698
I1002 19:32:32.398136 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.21054 (* 1 = 0.21054 loss)
I1002 19:32:32.398147 30882 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I1002 19:33:54.416462 30882 solver.cpp:357] Iteration 30800 (1.21919 iter/s, 82.0216s/100 iters), loss = 0.274477
I1002 19:33:54.416604 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.294297 (* 1 = 0.294297 loss)
I1002 19:33:54.416615 30882 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I1002 19:34:40.722815 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:35:16.389608 30882 solver.cpp:357] Iteration 30900 (1.21987 iter/s, 81.9762s/100 iters), loss = 0.491426
I1002 19:35:16.389729 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.340868 (* 1 = 0.340868 loss)
I1002 19:35:16.389739 30882 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I1002 19:36:37.753504 30882 solver.cpp:514] Iteration 31000, Testing net (#0)
I1002 19:37:00.064525 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:37:00.152417 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.591682 (* 1 = 0.591682 loss)
I1002 19:37:00.152442 30882 solver.cpp:580]     Test net output #1: prob = 0.805701
I1002 19:37:00.152449 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:37:00.962653 30882 solver.cpp:357] Iteration 31000 (0.956233 iter/s, 104.577s/100 iters), loss = 0.290889
I1002 19:37:00.962700 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.257501 (* 1 = 0.257501 loss)
I1002 19:37:00.962713 30882 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I1002 19:38:22.829563 30882 solver.cpp:357] Iteration 31100 (1.22146 iter/s, 81.869s/100 iters), loss = 0.273942
I1002 19:38:22.829751 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.25959 (* 1 = 0.25959 loss)
I1002 19:38:22.829762 30882 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I1002 19:39:44.854419 30882 solver.cpp:357] Iteration 31200 (1.21922 iter/s, 82.0194s/100 iters), loss = 0.289291
I1002 19:39:44.854560 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.252041 (* 1 = 0.252041 loss)
I1002 19:39:44.854570 30882 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I1002 19:40:23.347399 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:41:06.710727 30882 solver.cpp:357] Iteration 31300 (1.22172 iter/s, 81.852s/100 iters), loss = 0.302746
I1002 19:41:06.710866 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.340177 (* 1 = 0.340177 loss)
I1002 19:41:06.710880 30882 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I1002 19:42:28.754019 30882 solver.cpp:357] Iteration 31400 (1.21892 iter/s, 82.0398s/100 iters), loss = 0.371077
I1002 19:42:28.754161 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.318637 (* 1 = 0.318637 loss)
I1002 19:42:28.754173 30882 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I1002 19:43:49.918684 30882 solver.cpp:514] Iteration 31500, Testing net (#0)
I1002 19:44:12.224618 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:44:12.311941 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.811085 (* 1 = 0.811085 loss)
I1002 19:44:12.311967 30882 solver.cpp:580]     Test net output #1: prob = 0.7378
I1002 19:44:12.311974 30882 solver.cpp:593]     Max_acc: 0.835901  with iter: 21500
I1002 19:44:13.123616 30882 solver.cpp:357] Iteration 31500 (0.958163 iter/s, 104.366s/100 iters), loss = 0.257565
I1002 19:44:13.123667 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.291907 (* 1 = 0.291907 loss)
I1002 19:44:13.123677 30882 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I1002 19:45:34.986812 30882 solver.cpp:357] Iteration 31600 (1.22158 iter/s, 81.8615s/100 iters), loss = 0.337651
I1002 19:45:34.986951 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.381339 (* 1 = 0.381339 loss)
I1002 19:45:34.986963 30882 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I1002 19:46:06.157372 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:46:56.945803 30882 solver.cpp:357] Iteration 31700 (1.22014 iter/s, 81.9577s/100 iters), loss = 0.387571
I1002 19:46:56.945967 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.470739 (* 1 = 0.470739 loss)
I1002 19:46:56.945979 30882 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I1002 19:48:18.890933 30882 solver.cpp:357] Iteration 31800 (1.22034 iter/s, 81.9443s/100 iters), loss = 0.300446
I1002 19:48:18.891031 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.327882 (* 1 = 0.327882 loss)
I1002 19:48:18.891042 30882 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I1002 19:49:40.909730 30882 solver.cpp:357] Iteration 31900 (1.21924 iter/s, 82.0184s/100 iters), loss = 0.278651
I1002 19:49:40.909868 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.232642 (* 1 = 0.232642 loss)
I1002 19:49:40.909879 30882 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I1002 19:51:02.056538 30882 solver.cpp:514] Iteration 32000, Testing net (#0)
I1002 19:51:24.351589 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:51:24.440652 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.476075 (* 1 = 0.476075 loss)
I1002 19:51:24.440680 30882 solver.cpp:580]     Test net output #1: prob = 0.841801
I1002 19:51:24.440691 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_32000.caffemodel
I1002 19:51:24.842861 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_32000.solverstate
I1002 19:51:24.980958 30882 solver.cpp:593]     Max_acc: 0.841801  with iter: 32000
I1002 19:51:25.791710 30882 solver.cpp:357] Iteration 32000 (0.953453 iter/s, 104.882s/100 iters), loss = 0.237827
I1002 19:51:25.791764 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.303258 (* 1 = 0.303258 loss)
I1002 19:51:25.791772 30882 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I1002 19:51:25.791779 30882 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I1002 19:51:49.108201 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:52:47.848588 30882 solver.cpp:357] Iteration 32100 (1.21866 iter/s, 82.0572s/100 iters), loss = 0.248971
I1002 19:52:47.848752 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.311182 (* 1 = 0.311182 loss)
I1002 19:52:47.848762 30882 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I1002 19:54:09.923065 30882 solver.cpp:357] Iteration 32200 (1.2184 iter/s, 82.075s/100 iters), loss = 0.218719
I1002 19:54:09.923208 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.208757 (* 1 = 0.208757 loss)
I1002 19:54:09.923218 30882 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I1002 19:55:31.859163 30882 solver.cpp:357] Iteration 32300 (1.22045 iter/s, 81.9368s/100 iters), loss = 0.153243
I1002 19:55:31.859298 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.108681 (* 1 = 0.108681 loss)
I1002 19:55:31.859308 30882 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I1002 19:56:53.828367 30882 solver.cpp:357] Iteration 32400 (1.21996 iter/s, 81.9701s/100 iters), loss = 0.203778
I1002 19:56:53.828464 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.148044 (* 1 = 0.148044 loss)
I1002 19:56:53.828476 30882 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I1002 19:57:09.385340 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:58:14.870594 30882 solver.cpp:514] Iteration 32500, Testing net (#0)
I1002 19:58:37.168792 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 19:58:37.256991 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.241106 (* 1 = 0.241106 loss)
I1002 19:58:37.257017 30882 solver.cpp:580]     Test net output #1: prob = 0.916802
I1002 19:58:37.257030 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_32500.caffemodel
I1002 19:58:37.662046 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_32500.solverstate
I1002 19:58:37.799827 30882 solver.cpp:593]     Max_acc: 0.916802  with iter: 32500
I1002 19:58:38.610424 30882 solver.cpp:357] Iteration 32500 (0.954349 iter/s, 104.783s/100 iters), loss = 0.113591
I1002 19:58:38.610476 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.133213 (* 1 = 0.133213 loss)
I1002 19:58:38.610486 30882 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I1002 20:00:00.572389 30882 solver.cpp:357] Iteration 32600 (1.22006 iter/s, 81.9632s/100 iters), loss = 0.176974
I1002 20:00:00.572525 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.137832 (* 1 = 0.137832 loss)
I1002 20:00:00.572535 30882 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I1002 20:01:22.593396 30882 solver.cpp:357] Iteration 32700 (1.21918 iter/s, 82.0223s/100 iters), loss = 0.0994354
I1002 20:01:22.593494 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0868135 (* 1 = 0.0868135 loss)
I1002 20:01:22.593504 30882 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I1002 20:02:44.524304 30882 solver.cpp:357] Iteration 32800 (1.22052 iter/s, 81.9323s/100 iters), loss = 0.169196
I1002 20:02:44.524446 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.193576 (* 1 = 0.193576 loss)
I1002 20:02:44.524456 30882 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I1002 20:02:52.305390 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:04:06.545275 30882 solver.cpp:357] Iteration 32900 (1.21918 iter/s, 82.0224s/100 iters), loss = 0.135276
I1002 20:04:06.545377 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.158806 (* 1 = 0.158806 loss)
I1002 20:04:06.545390 30882 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I1002 20:05:27.783457 30882 solver.cpp:514] Iteration 33000, Testing net (#0)
I1002 20:05:50.098781 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:05:50.187378 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.223855 (* 1 = 0.223855 loss)
I1002 20:05:50.187404 30882 solver.cpp:580]     Test net output #1: prob = 0.920702
I1002 20:05:50.187422 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_33000.caffemodel
I1002 20:05:50.590391 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_33000.solverstate
I1002 20:05:50.728140 30882 solver.cpp:593]     Max_acc: 0.920702  with iter: 33000
I1002 20:05:51.539836 30882 solver.cpp:357] Iteration 33000 (0.952412 iter/s, 104.997s/100 iters), loss = 0.120427
I1002 20:05:51.539891 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0994007 (* 1 = 0.0994007 loss)
I1002 20:05:51.539906 30882 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I1002 20:07:13.447947 30882 solver.cpp:357] Iteration 33100 (1.22086 iter/s, 81.9098s/100 iters), loss = 0.128073
I1002 20:07:13.448494 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.170151 (* 1 = 0.170151 loss)
I1002 20:07:13.448505 30882 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I1002 20:08:35.427625 30882 solver.cpp:357] Iteration 33200 (1.2198 iter/s, 81.9809s/100 iters), loss = 0.118162
I1002 20:08:35.427721 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.105491 (* 1 = 0.105491 loss)
I1002 20:08:35.427732 30882 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I1002 20:08:35.844374 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:09:57.344106 30882 solver.cpp:357] Iteration 33300 (1.22073 iter/s, 81.9182s/100 iters), loss = 0.179273
I1002 20:09:57.344203 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.210561 (* 1 = 0.210561 loss)
I1002 20:09:57.344213 30882 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I1002 20:11:19.284099 30882 solver.cpp:357] Iteration 33400 (1.22038 iter/s, 81.9417s/100 iters), loss = 0.107109
I1002 20:11:19.284236 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.122113 (* 1 = 0.122113 loss)
I1002 20:11:19.284246 30882 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I1002 20:12:40.553158 30882 solver.cpp:514] Iteration 33500, Testing net (#0)
I1002 20:13:02.852077 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:13:02.938711 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.238117 (* 1 = 0.238117 loss)
I1002 20:13:02.938737 30882 solver.cpp:580]     Test net output #1: prob = 0.916102
I1002 20:13:02.938743 30882 solver.cpp:593]     Max_acc: 0.920702  with iter: 33000
I1002 20:13:03.749974 30882 solver.cpp:357] Iteration 33500 (0.957248 iter/s, 104.466s/100 iters), loss = 0.0961578
I1002 20:13:03.750023 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.115573 (* 1 = 0.115573 loss)
I1002 20:13:03.750036 30882 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I1002 20:14:18.210999 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:14:25.560999 30882 solver.cpp:357] Iteration 33600 (1.22236 iter/s, 81.8093s/100 iters), loss = 0.165062
I1002 20:14:25.561059 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.203646 (* 1 = 0.203646 loss)
I1002 20:14:25.561069 30882 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I1002 20:15:47.595919 30882 solver.cpp:357] Iteration 33700 (1.21901 iter/s, 82.0336s/100 iters), loss = 0.108638
I1002 20:15:47.596057 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.124588 (* 1 = 0.124588 loss)
I1002 20:15:47.596068 30882 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I1002 20:17:09.695952 30882 solver.cpp:357] Iteration 33800 (1.21804 iter/s, 82.099s/100 iters), loss = 0.0971946
I1002 20:17:09.696090 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.068862 (* 1 = 0.068862 loss)
I1002 20:17:09.696100 30882 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I1002 20:18:31.717523 30882 solver.cpp:357] Iteration 33900 (1.2192 iter/s, 82.0209s/100 iters), loss = 0.128047
I1002 20:18:31.717674 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.157984 (* 1 = 0.157984 loss)
I1002 20:18:31.717684 30882 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I1002 20:19:38.510381 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:19:52.832785 30882 solver.cpp:514] Iteration 34000, Testing net (#0)
I1002 20:20:15.135921 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:20:15.224046 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.256133 (* 1 = 0.256133 loss)
I1002 20:20:15.224071 30882 solver.cpp:580]     Test net output #1: prob = 0.914103
I1002 20:20:15.224078 30882 solver.cpp:593]     Max_acc: 0.920702  with iter: 33000
I1002 20:20:16.035374 30882 solver.cpp:357] Iteration 34000 (0.958613 iter/s, 104.317s/100 iters), loss = 0.0548128
I1002 20:20:16.035424 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0638833 (* 1 = 0.0638833 loss)
I1002 20:20:16.035436 30882 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I1002 20:21:37.933207 30882 solver.cpp:357] Iteration 34100 (1.22103 iter/s, 81.8978s/100 iters), loss = 0.114693
I1002 20:21:37.933348 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.107097 (* 1 = 0.107097 loss)
I1002 20:21:37.933358 30882 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I1002 20:22:59.869297 30882 solver.cpp:357] Iteration 34200 (1.22046 iter/s, 81.9362s/100 iters), loss = 0.060517
I1002 20:22:59.869395 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0870259 (* 1 = 0.0870259 loss)
I1002 20:22:59.869407 30882 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I1002 20:24:21.932505 30882 solver.cpp:357] Iteration 34300 (1.21857 iter/s, 82.0635s/100 iters), loss = 0.121848
I1002 20:24:21.932642 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.140752 (* 1 = 0.140752 loss)
I1002 20:24:21.932652 30882 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I1002 20:25:20.970603 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:25:43.885991 30882 solver.cpp:357] Iteration 34400 (1.2202 iter/s, 81.9539s/100 iters), loss = 0.141606
I1002 20:25:43.886051 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.12788 (* 1 = 0.12788 loss)
I1002 20:25:43.886062 30882 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I1002 20:27:05.010351 30882 solver.cpp:514] Iteration 34500, Testing net (#0)
I1002 20:27:27.317142 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:27:27.405763 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.234619 (* 1 = 0.234619 loss)
I1002 20:27:27.405789 30882 solver.cpp:580]     Test net output #1: prob = 0.922202
I1002 20:27:27.405802 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_34500.caffemodel
I1002 20:27:27.808750 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_34500.solverstate
I1002 20:27:27.946471 30882 solver.cpp:593]     Max_acc: 0.922202  with iter: 34500
I1002 20:27:28.756410 30882 solver.cpp:357] Iteration 34500 (0.95355 iter/s, 104.871s/100 iters), loss = 0.0700434
I1002 20:27:28.756465 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0564048 (* 1 = 0.0564048 loss)
I1002 20:27:28.756475 30882 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I1002 20:28:50.597415 30882 solver.cpp:357] Iteration 34600 (1.22187 iter/s, 81.8417s/100 iters), loss = 0.104372
I1002 20:28:50.597556 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0857018 (* 1 = 0.0857018 loss)
I1002 20:28:50.597568 30882 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I1002 20:30:12.478178 30882 solver.cpp:357] Iteration 34700 (1.22128 iter/s, 81.8815s/100 iters), loss = 0.0624011
I1002 20:30:12.478320 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0748488 (* 1 = 0.0748488 loss)
I1002 20:30:12.478332 30882 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I1002 20:31:04.102658 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:31:34.424304 30882 solver.cpp:357] Iteration 34800 (1.2203 iter/s, 81.9469s/100 iters), loss = 0.107024
I1002 20:31:34.424526 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.173653 (* 1 = 0.173653 loss)
I1002 20:31:34.424537 30882 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I1002 20:32:56.417547 30882 solver.cpp:357] Iteration 34900 (1.2196 iter/s, 81.994s/100 iters), loss = 0.0977791
I1002 20:32:56.417652 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0754199 (* 1 = 0.0754199 loss)
I1002 20:32:56.417663 30882 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I1002 20:34:17.591789 30882 solver.cpp:514] Iteration 35000, Testing net (#0)
I1002 20:34:39.890236 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:34:39.977665 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.20916 (* 1 = 0.20916 loss)
I1002 20:34:39.977690 30882 solver.cpp:580]     Test net output #1: prob = 0.930902
I1002 20:34:39.977702 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_35000.caffemodel
I1002 20:34:40.380643 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_35000.solverstate
I1002 20:34:40.518831 30882 solver.cpp:593]     Max_acc: 0.930902  with iter: 35000
I1002 20:34:41.330384 30882 solver.cpp:357] Iteration 35000 (0.953161 iter/s, 104.914s/100 iters), loss = 0.0456084
I1002 20:34:41.330437 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0314663 (* 1 = 0.0314663 loss)
I1002 20:34:41.330448 30882 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I1002 20:36:03.174067 30882 solver.cpp:357] Iteration 35100 (1.22183 iter/s, 81.8447s/100 iters), loss = 0.114854
I1002 20:36:03.174194 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.119473 (* 1 = 0.119473 loss)
I1002 20:36:03.174206 30882 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I1002 20:36:46.960626 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:37:25.056396 30882 solver.cpp:357] Iteration 35200 (1.22125 iter/s, 81.8833s/100 iters), loss = 0.0974819
I1002 20:37:25.056493 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0701387 (* 1 = 0.0701387 loss)
I1002 20:37:25.056504 30882 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I1002 20:38:47.076020 30882 solver.cpp:357] Iteration 35300 (1.2192 iter/s, 82.0207s/100 iters), loss = 0.114836
I1002 20:38:47.076118 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.122769 (* 1 = 0.122769 loss)
I1002 20:38:47.076128 30882 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I1002 20:40:08.967612 30882 solver.cpp:357] Iteration 35400 (1.22111 iter/s, 81.8927s/100 iters), loss = 0.0390444
I1002 20:40:08.967756 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0522277 (* 1 = 0.0522277 loss)
I1002 20:40:08.967766 30882 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I1002 20:41:30.217875 30882 solver.cpp:514] Iteration 35500, Testing net (#0)
I1002 20:41:52.524111 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:41:52.611867 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.253301 (* 1 = 0.253301 loss)
I1002 20:41:52.611893 30882 solver.cpp:580]     Test net output #1: prob = 0.920202
I1002 20:41:52.611899 30882 solver.cpp:593]     Max_acc: 0.930902  with iter: 35000
I1002 20:41:53.419652 30882 solver.cpp:357] Iteration 35500 (0.957364 iter/s, 104.453s/100 iters), loss = 0.0993858
I1002 20:41:53.419698 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.035181 (* 1 = 0.035181 loss)
I1002 20:41:53.419708 30882 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I1002 20:42:29.459811 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:43:15.314311 30882 solver.cpp:357] Iteration 35600 (1.22106 iter/s, 81.8959s/100 iters), loss = 0.0649635
I1002 20:43:15.314407 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0522356 (* 1 = 0.0522356 loss)
I1002 20:43:15.314419 30882 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I1002 20:44:37.515651 30882 solver.cpp:357] Iteration 35700 (1.21651 iter/s, 82.2025s/100 iters), loss = 0.0376141
I1002 20:44:37.515822 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.036426 (* 1 = 0.036426 loss)
I1002 20:44:37.515837 30882 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I1002 20:45:59.469074 30882 solver.cpp:357] Iteration 35800 (1.22019 iter/s, 81.9545s/100 iters), loss = 0.0645743
I1002 20:45:59.469198 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0883467 (* 1 = 0.0883467 loss)
I1002 20:45:59.469208 30882 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I1002 20:47:21.617909 30882 solver.cpp:357] Iteration 35900 (1.2172 iter/s, 82.1557s/100 iters), loss = 0.0489632
I1002 20:47:21.618050 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0670476 (* 1 = 0.0670476 loss)
I1002 20:47:21.618060 30882 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I1002 20:47:49.964840 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:48:42.945736 30882 solver.cpp:514] Iteration 36000, Testing net (#0)
I1002 20:49:05.232216 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:49:05.319247 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.246231 (* 1 = 0.246231 loss)
I1002 20:49:05.319272 30882 solver.cpp:580]     Test net output #1: prob = 0.924102
I1002 20:49:05.319278 30882 solver.cpp:593]     Max_acc: 0.930902  with iter: 35000
I1002 20:49:06.131268 30882 solver.cpp:357] Iteration 36000 (0.956707 iter/s, 104.525s/100 iters), loss = 0.0959756
I1002 20:49:06.131319 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.115609 (* 1 = 0.115609 loss)
I1002 20:49:06.131328 30882 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I1002 20:50:28.010663 30882 solver.cpp:357] Iteration 36100 (1.22119 iter/s, 81.8876s/100 iters), loss = 0.0434993
I1002 20:50:28.010788 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0443539 (* 1 = 0.0443539 loss)
I1002 20:50:28.010798 30882 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I1002 20:51:50.046586 30882 solver.cpp:357] Iteration 36200 (1.21887 iter/s, 82.0432s/100 iters), loss = 0.0228528
I1002 20:51:50.046727 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0258616 (* 1 = 0.0258616 loss)
I1002 20:51:50.046737 30882 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I1002 20:53:12.172852 30882 solver.cpp:357] Iteration 36300 (1.21754 iter/s, 82.1328s/100 iters), loss = 0.131103
I1002 20:53:12.172989 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.070746 (* 1 = 0.070746 loss)
I1002 20:53:12.172998 30882 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I1002 20:53:33.077697 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:54:34.077224 30882 solver.cpp:357] Iteration 36400 (1.22085 iter/s, 81.9102s/100 iters), loss = 0.104612
I1002 20:54:34.077323 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.098075 (* 1 = 0.098075 loss)
I1002 20:54:34.077332 30882 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I1002 20:55:55.258395 30882 solver.cpp:514] Iteration 36500, Testing net (#0)
I1002 20:56:17.567941 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 20:56:17.656659 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.235381 (* 1 = 0.235381 loss)
I1002 20:56:17.656685 30882 solver.cpp:580]     Test net output #1: prob = 0.928803
I1002 20:56:17.656692 30882 solver.cpp:593]     Max_acc: 0.930902  with iter: 35000
I1002 20:56:18.465411 30882 solver.cpp:357] Iteration 36500 (0.9579 iter/s, 104.395s/100 iters), loss = 0.0978726
I1002 20:56:18.465461 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0984093 (* 1 = 0.0984093 loss)
I1002 20:56:18.465474 30882 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I1002 20:57:40.513314 30882 solver.cpp:357] Iteration 36600 (1.21873 iter/s, 82.0528s/100 iters), loss = 0.0738184
I1002 20:57:40.513454 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0621262 (* 1 = 0.0621262 loss)
I1002 20:57:40.513464 30882 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I1002 20:59:02.465886 30882 solver.cpp:357] Iteration 36700 (1.22015 iter/s, 81.957s/100 iters), loss = 0.0250716
I1002 20:59:02.466084 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0230837 (* 1 = 0.0230837 loss)
I1002 20:59:02.466094 30882 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I1002 20:59:15.609290 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:00:24.484643 30882 solver.cpp:357] Iteration 36800 (1.21917 iter/s, 82.0228s/100 iters), loss = 0.0918906
I1002 21:00:24.484787 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.114689 (* 1 = 0.114689 loss)
I1002 21:00:24.484797 30882 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I1002 21:01:46.362349 30882 solver.cpp:357] Iteration 36900 (1.22128 iter/s, 81.8815s/100 iters), loss = 0.0459195
I1002 21:01:46.362483 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0418095 (* 1 = 0.0418095 loss)
I1002 21:01:46.362498 30882 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I1002 21:03:07.387091 30882 solver.cpp:514] Iteration 37000, Testing net (#0)
I1002 21:03:29.675810 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:03:29.763470 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.210439 (* 1 = 0.210439 loss)
I1002 21:03:29.763496 30882 solver.cpp:580]     Test net output #1: prob = 0.933502
I1002 21:03:29.763509 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_37000.caffemodel
I1002 21:03:30.166102 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_37000.solverstate
I1002 21:03:30.303972 30882 solver.cpp:593]     Max_acc: 0.933502  with iter: 37000
I1002 21:03:31.113854 30882 solver.cpp:357] Iteration 37000 (0.954598 iter/s, 104.756s/100 iters), loss = 0.0694203
I1002 21:03:31.113912 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0583963 (* 1 = 0.0583963 loss)
I1002 21:03:31.113922 30882 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I1002 21:04:52.894737 30882 solver.cpp:357] Iteration 37100 (1.22273 iter/s, 81.7843s/100 iters), loss = 0.0446738
I1002 21:04:52.894865 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0351369 (* 1 = 0.0351369 loss)
I1002 21:04:52.894876 30882 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I1002 21:04:58.224822 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:06:14.964601 30882 solver.cpp:357] Iteration 37200 (1.21843 iter/s, 82.0731s/100 iters), loss = 0.0767756
I1002 21:06:14.964738 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0990696 (* 1 = 0.0990696 loss)
I1002 21:06:14.964748 30882 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I1002 21:07:36.964421 30882 solver.cpp:357] Iteration 37300 (1.21947 iter/s, 82.0029s/100 iters), loss = 0.0866356
I1002 21:07:36.964551 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0491897 (* 1 = 0.0491897 loss)
I1002 21:07:36.964561 30882 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I1002 21:08:58.910421 30882 solver.cpp:357] Iteration 37400 (1.22027 iter/s, 81.9489s/100 iters), loss = 0.092793
I1002 21:08:58.910563 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0779652 (* 1 = 0.0779652 loss)
I1002 21:08:58.910575 30882 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I1002 21:10:18.369741 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:10:20.012910 30882 solver.cpp:514] Iteration 37500, Testing net (#0)
I1002 21:10:42.315490 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:10:42.403650 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.222284 (* 1 = 0.222284 loss)
I1002 21:10:42.403676 30882 solver.cpp:580]     Test net output #1: prob = 0.928902
I1002 21:10:42.403681 30882 solver.cpp:593]     Max_acc: 0.933502  with iter: 37000
I1002 21:10:43.213599 30882 solver.cpp:357] Iteration 37500 (0.95871 iter/s, 104.307s/100 iters), loss = 0.0403319
I1002 21:10:43.213649 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0523215 (* 1 = 0.0523215 loss)
I1002 21:10:43.213661 30882 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I1002 21:12:05.031146 30882 solver.cpp:357] Iteration 37600 (1.22219 iter/s, 81.8204s/100 iters), loss = 0.0637837
I1002 21:12:05.031293 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0248627 (* 1 = 0.0248627 loss)
I1002 21:12:05.031304 30882 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I1002 21:13:27.039319 30882 solver.cpp:357] Iteration 37700 (1.21935 iter/s, 82.0108s/100 iters), loss = 0.0858628
I1002 21:13:27.039474 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.117682 (* 1 = 0.117682 loss)
I1002 21:13:27.039484 30882 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I1002 21:14:49.099974 30882 solver.cpp:357] Iteration 37800 (1.21857 iter/s, 82.0633s/100 iters), loss = 0.0890738
I1002 21:14:49.100117 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.107462 (* 1 = 0.107462 loss)
I1002 21:14:49.100127 30882 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I1002 21:16:01.266705 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:16:11.097951 30882 solver.cpp:357] Iteration 37900 (1.2195 iter/s, 82.0005s/100 iters), loss = 0.077284
I1002 21:16:11.098012 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0471586 (* 1 = 0.0471586 loss)
I1002 21:16:11.098023 30882 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I1002 21:17:32.114370 30882 solver.cpp:514] Iteration 38000, Testing net (#0)
I1002 21:17:54.408330 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:17:54.496882 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.239327 (* 1 = 0.239327 loss)
I1002 21:17:54.496908 30882 solver.cpp:580]     Test net output #1: prob = 0.928802
I1002 21:17:54.496914 30882 solver.cpp:593]     Max_acc: 0.933502  with iter: 37000
I1002 21:17:55.309535 30882 solver.cpp:357] Iteration 38000 (0.959556 iter/s, 104.215s/100 iters), loss = 0.054528
I1002 21:17:55.309583 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0767999 (* 1 = 0.0767999 loss)
I1002 21:17:55.309594 30882 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I1002 21:19:17.134935 30882 solver.cpp:357] Iteration 38100 (1.22208 iter/s, 81.8279s/100 iters), loss = 0.0347199
I1002 21:19:17.135074 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0156053 (* 1 = 0.0156053 loss)
I1002 21:19:17.135085 30882 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I1002 21:20:39.293467 30882 solver.cpp:357] Iteration 38200 (1.21712 iter/s, 82.161s/100 iters), loss = 0.0261538
I1002 21:20:39.293560 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0407984 (* 1 = 0.0407984 loss)
I1002 21:20:39.293571 30882 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I1002 21:21:43.690023 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:22:01.281883 30882 solver.cpp:357] Iteration 38300 (1.21972 iter/s, 81.9862s/100 iters), loss = 0.0612185
I1002 21:22:01.281946 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0442564 (* 1 = 0.0442564 loss)
I1002 21:22:01.281958 30882 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I1002 21:23:23.154536 30882 solver.cpp:357] Iteration 38400 (1.22143 iter/s, 81.871s/100 iters), loss = 0.0549342
I1002 21:23:23.154681 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0350524 (* 1 = 0.0350524 loss)
I1002 21:23:23.154691 30882 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I1002 21:24:44.255297 30882 solver.cpp:514] Iteration 38500, Testing net (#0)
I1002 21:25:06.552887 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:25:06.639281 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.229938 (* 1 = 0.229938 loss)
I1002 21:25:06.639307 30882 solver.cpp:580]     Test net output #1: prob = 0.930602
I1002 21:25:06.639313 30882 solver.cpp:593]     Max_acc: 0.933502  with iter: 37000
I1002 21:25:07.448618 30882 solver.cpp:357] Iteration 38500 (0.958841 iter/s, 104.293s/100 iters), loss = 0.046265
I1002 21:25:07.448668 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0252681 (* 1 = 0.0252681 loss)
I1002 21:25:07.448681 30882 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I1002 21:26:29.209293 30882 solver.cpp:357] Iteration 38600 (1.22309 iter/s, 81.76s/100 iters), loss = 0.0521099
I1002 21:26:29.209486 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0232236 (* 1 = 0.0232236 loss)
I1002 21:26:29.209496 30882 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I1002 21:27:25.879950 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:27:51.306799 30882 solver.cpp:357] Iteration 38700 (1.21807 iter/s, 82.097s/100 iters), loss = 0.0556305
I1002 21:27:51.306862 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0901201 (* 1 = 0.0901201 loss)
I1002 21:27:51.306874 30882 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I1002 21:29:13.239977 30882 solver.cpp:357] Iteration 38800 (1.22051 iter/s, 81.9331s/100 iters), loss = 0.0514583
I1002 21:29:13.240119 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0218456 (* 1 = 0.0218456 loss)
I1002 21:29:13.240130 30882 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I1002 21:30:35.205900 30882 solver.cpp:357] Iteration 38900 (1.22002 iter/s, 81.966s/100 iters), loss = 0.067206
I1002 21:30:35.206035 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0260684 (* 1 = 0.0260684 loss)
I1002 21:30:35.206046 30882 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I1002 21:31:56.288703 30882 solver.cpp:514] Iteration 39000, Testing net (#0)
I1002 21:32:18.584779 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:32:18.672667 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.227598 (* 1 = 0.227598 loss)
I1002 21:32:18.672693 30882 solver.cpp:580]     Test net output #1: prob = 0.933602
I1002 21:32:18.672709 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_39000.caffemodel
I1002 21:32:19.081192 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_39000.solverstate
I1002 21:32:19.221674 30882 solver.cpp:593]     Max_acc: 0.933602  with iter: 39000
I1002 21:32:20.033293 30882 solver.cpp:357] Iteration 39000 (0.953945 iter/s, 104.828s/100 iters), loss = 0.0328491
I1002 21:32:20.033346 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0323317 (* 1 = 0.0323317 loss)
I1002 21:32:20.033358 30882 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I1002 21:33:08.634333 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:33:41.710543 30882 solver.cpp:357] Iteration 39100 (1.22432 iter/s, 81.6779s/100 iters), loss = 0.0512835
I1002 21:33:41.714488 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0438613 (* 1 = 0.0438613 loss)
I1002 21:33:41.714501 30882 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I1002 21:35:03.656525 30882 solver.cpp:357] Iteration 39200 (1.22036 iter/s, 81.9429s/100 iters), loss = 0.0377967
I1002 21:35:03.656617 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0402604 (* 1 = 0.0402604 loss)
I1002 21:35:03.656627 30882 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I1002 21:36:25.481572 30882 solver.cpp:357] Iteration 39300 (1.22211 iter/s, 81.8259s/100 iters), loss = 0.0765034
I1002 21:36:25.481673 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.108323 (* 1 = 0.108323 loss)
I1002 21:36:25.481683 30882 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I1002 21:37:47.231091 30882 solver.cpp:357] Iteration 39400 (1.22323 iter/s, 81.7505s/100 iters), loss = 0.0335739
I1002 21:37:47.231243 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.024585 (* 1 = 0.024585 loss)
I1002 21:37:47.231254 30882 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I1002 21:38:28.623046 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:39:08.326552 30882 solver.cpp:514] Iteration 39500, Testing net (#0)
I1002 21:39:30.635020 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:39:30.722826 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.22458 (* 1 = 0.22458 loss)
I1002 21:39:30.722851 30882 solver.cpp:580]     Test net output #1: prob = 0.932903
I1002 21:39:30.722857 30882 solver.cpp:593]     Max_acc: 0.933602  with iter: 39000
I1002 21:39:31.533697 30882 solver.cpp:357] Iteration 39500 (0.958737 iter/s, 104.304s/100 iters), loss = 0.0344492
I1002 21:39:31.533747 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0504419 (* 1 = 0.0504419 loss)
I1002 21:39:31.533759 30882 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I1002 21:40:53.462848 30882 solver.cpp:357] Iteration 39600 (1.22055 iter/s, 81.9303s/100 iters), loss = 0.0684372
I1002 21:40:53.463038 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0574464 (* 1 = 0.0574464 loss)
I1002 21:40:53.463049 30882 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I1002 21:42:15.318790 30882 solver.cpp:357] Iteration 39700 (1.22164 iter/s, 81.857s/100 iters), loss = 0.0302664
I1002 21:42:15.318949 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0305901 (* 1 = 0.0305901 loss)
I1002 21:42:15.318959 30882 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I1002 21:43:37.366153 30882 solver.cpp:357] Iteration 39800 (1.21879 iter/s, 82.0485s/100 iters), loss = 0.0692735
I1002 21:43:37.366298 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0353214 (* 1 = 0.0353214 loss)
I1002 21:43:37.366309 30882 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I1002 21:44:10.975138 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:44:59.344653 30882 solver.cpp:357] Iteration 39900 (1.21981 iter/s, 81.9797s/100 iters), loss = 0.0396983
I1002 21:44:59.344799 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0596314 (* 1 = 0.0596314 loss)
I1002 21:44:59.344810 30882 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I1002 21:46:20.585319 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_40000.caffemodel
I1002 21:46:20.992579 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_40000.solverstate
I1002 21:46:21.131923 30882 solver.cpp:514] Iteration 40000, Testing net (#0)
I1002 21:46:43.427137 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:46:43.514812 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.276622 (* 1 = 0.276622 loss)
I1002 21:46:43.514838 30882 solver.cpp:580]     Test net output #1: prob = 0.921602
I1002 21:46:43.514844 30882 solver.cpp:593]     Max_acc: 0.933602  with iter: 39000
I1002 21:46:44.325001 30882 solver.cpp:357] Iteration 40000 (0.952544 iter/s, 104.982s/100 iters), loss = 0.0762566
I1002 21:46:44.325049 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0276757 (* 1 = 0.0276757 loss)
I1002 21:46:44.325062 30882 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I1002 21:48:06.152441 30882 solver.cpp:357] Iteration 40100 (1.22206 iter/s, 81.8288s/100 iters), loss = 0.0870436
I1002 21:48:06.152539 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.115473 (* 1 = 0.115473 loss)
I1002 21:48:06.152549 30882 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I1002 21:49:28.206601 30882 solver.cpp:357] Iteration 40200 (1.21869 iter/s, 82.0556s/100 iters), loss = 0.146653
I1002 21:49:28.206745 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0603839 (* 1 = 0.0603839 loss)
I1002 21:49:28.206755 30882 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I1002 21:49:54.038492 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:50:50.210700 30882 solver.cpp:357] Iteration 40300 (1.21943 iter/s, 82.0055s/100 iters), loss = 0.0482066
I1002 21:50:50.210844 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0885643 (* 1 = 0.0885643 loss)
I1002 21:50:50.210855 30882 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I1002 21:52:12.163148 30882 solver.cpp:357] Iteration 40400 (1.2202 iter/s, 81.9538s/100 iters), loss = 0.0395503
I1002 21:52:12.163285 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0154843 (* 1 = 0.0154843 loss)
I1002 21:52:12.163296 30882 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I1002 21:53:33.444541 30882 solver.cpp:514] Iteration 40500, Testing net (#0)
I1002 21:53:55.753414 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:53:55.841658 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.290892 (* 1 = 0.290892 loss)
I1002 21:53:55.841684 30882 solver.cpp:580]     Test net output #1: prob = 0.918102
I1002 21:53:55.841691 30882 solver.cpp:593]     Max_acc: 0.933602  with iter: 39000
I1002 21:53:56.650774 30882 solver.cpp:357] Iteration 40500 (0.957034 iter/s, 104.489s/100 iters), loss = 0.0349405
I1002 21:53:56.650823 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0541457 (* 1 = 0.0541457 loss)
I1002 21:53:56.650835 30882 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I1002 21:55:18.474222 30882 solver.cpp:357] Iteration 40600 (1.22212 iter/s, 81.8252s/100 iters), loss = 0.0322567
I1002 21:55:18.474419 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0281441 (* 1 = 0.0281441 loss)
I1002 21:55:18.474429 30882 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I1002 21:55:36.456143 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 21:56:40.361171 30882 solver.cpp:357] Iteration 40700 (1.22117 iter/s, 81.8888s/100 iters), loss = 0.0233356
I1002 21:56:40.361275 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0310639 (* 1 = 0.0310639 loss)
I1002 21:56:40.361285 30882 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I1002 21:58:02.224982 30882 solver.cpp:357] Iteration 40800 (1.22151 iter/s, 81.8657s/100 iters), loss = 0.0289815
I1002 21:58:02.225118 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0242169 (* 1 = 0.0242169 loss)
I1002 21:58:02.225128 30882 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I1002 21:59:24.267057 30882 solver.cpp:357] Iteration 40900 (1.21886 iter/s, 82.0439s/100 iters), loss = 0.032584
I1002 21:59:24.267199 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0228561 (* 1 = 0.0228561 loss)
I1002 21:59:24.267210 30882 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I1002 22:00:45.327741 30882 solver.cpp:514] Iteration 41000, Testing net (#0)
I1002 22:01:07.624053 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:01:07.712322 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.20986 (* 1 = 0.20986 loss)
I1002 22:01:07.712348 30882 solver.cpp:580]     Test net output #1: prob = 0.936003
I1002 22:01:07.712360 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_41000.caffemodel
I1002 22:01:08.117684 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_41000.solverstate
I1002 22:01:08.256330 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:01:09.067914 30882 solver.cpp:357] Iteration 41000 (0.95417 iter/s, 104.803s/100 iters), loss = 0.059684
I1002 22:01:09.067966 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0463198 (* 1 = 0.0463198 loss)
I1002 22:01:09.067977 30882 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I1002 22:01:19.728611 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:02:30.830343 30882 solver.cpp:357] Iteration 41100 (1.22303 iter/s, 81.7643s/100 iters), loss = 0.0361063
I1002 22:02:30.830479 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.058947 (* 1 = 0.058947 loss)
I1002 22:02:30.830489 30882 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I1002 22:03:52.857090 30882 solver.cpp:357] Iteration 41200 (1.21909 iter/s, 82.0285s/100 iters), loss = 0.0890336
I1002 22:03:52.857234 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.131419 (* 1 = 0.131419 loss)
I1002 22:03:52.857245 30882 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I1002 22:05:14.725330 30882 solver.cpp:357] Iteration 41300 (1.22145 iter/s, 81.8699s/100 iters), loss = 0.0205722
I1002 22:05:14.725479 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0163305 (* 1 = 0.0163305 loss)
I1002 22:05:14.725491 30882 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I1002 22:06:36.615002 30882 solver.cpp:357] Iteration 41400 (1.22113 iter/s, 81.8913s/100 iters), loss = 0.0535146
I1002 22:06:36.615098 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0904751 (* 1 = 0.0904751 loss)
I1002 22:06:36.615109 30882 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I1002 22:06:39.491328 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:07:57.791899 30882 solver.cpp:514] Iteration 41500, Testing net (#0)
I1002 22:08:20.081689 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:08:20.169616 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.297891 (* 1 = 0.297891 loss)
I1002 22:08:20.169642 30882 solver.cpp:580]     Test net output #1: prob = 0.917002
I1002 22:08:20.169648 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:08:20.982015 30882 solver.cpp:357] Iteration 41500 (0.958137 iter/s, 104.369s/100 iters), loss = 0.0500112
I1002 22:08:20.982059 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0470429 (* 1 = 0.0470429 loss)
I1002 22:08:20.982070 30882 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I1002 22:09:42.842139 30882 solver.cpp:357] Iteration 41600 (1.22157 iter/s, 81.8619s/100 iters), loss = 0.0467479
I1002 22:09:42.842286 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0173822 (* 1 = 0.0173822 loss)
I1002 22:09:42.842296 30882 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I1002 22:11:04.855299 30882 solver.cpp:357] Iteration 41700 (1.21929 iter/s, 82.0148s/100 iters), loss = 0.072228
I1002 22:11:04.855393 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0156727 (* 1 = 0.0156727 loss)
I1002 22:11:04.855404 30882 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I1002 22:12:21.912147 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:12:26.812510 30882 solver.cpp:357] Iteration 41800 (1.22012 iter/s, 81.9589s/100 iters), loss = 0.0480456
I1002 22:12:26.812575 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.015412 (* 1 = 0.015412 loss)
I1002 22:12:26.812589 30882 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I1002 22:13:48.776196 30882 solver.cpp:357] Iteration 41900 (1.22003 iter/s, 81.9654s/100 iters), loss = 0.0409619
I1002 22:13:48.776317 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0652829 (* 1 = 0.0652829 loss)
I1002 22:13:48.776326 30882 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I1002 22:15:09.881414 30882 solver.cpp:514] Iteration 42000, Testing net (#0)
I1002 22:15:32.181504 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:15:32.269371 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.253087 (* 1 = 0.253087 loss)
I1002 22:15:32.269395 30882 solver.cpp:580]     Test net output #1: prob = 0.928902
I1002 22:15:32.269402 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:15:33.078403 30882 solver.cpp:357] Iteration 42000 (0.958733 iter/s, 104.304s/100 iters), loss = 0.0627664
I1002 22:15:33.078452 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0729717 (* 1 = 0.0729717 loss)
I1002 22:15:33.078464 30882 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I1002 22:16:54.948568 30882 solver.cpp:357] Iteration 42100 (1.22142 iter/s, 81.8718s/100 iters), loss = 0.0411917
I1002 22:16:54.948709 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0339654 (* 1 = 0.0339654 loss)
I1002 22:16:54.948719 30882 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I1002 22:18:04.357439 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:18:17.072474 30882 solver.cpp:357] Iteration 42200 (1.21765 iter/s, 82.1255s/100 iters), loss = 0.049379
I1002 22:18:17.072535 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0316906 (* 1 = 0.0316906 loss)
I1002 22:18:17.072546 30882 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I1002 22:19:38.950225 30882 solver.cpp:357] Iteration 42300 (1.22131 iter/s, 81.8794s/100 iters), loss = 0.0242039
I1002 22:19:38.950336 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0216802 (* 1 = 0.0216802 loss)
I1002 22:19:38.950347 30882 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I1002 22:21:00.925740 30882 solver.cpp:357] Iteration 42400 (1.21985 iter/s, 81.9771s/100 iters), loss = 0.0682402
I1002 22:21:00.925840 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0281958 (* 1 = 0.0281958 loss)
I1002 22:21:00.925853 30882 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I1002 22:22:22.075686 30882 solver.cpp:514] Iteration 42500, Testing net (#0)
I1002 22:22:44.380656 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:22:44.467864 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.249171 (* 1 = 0.249171 loss)
I1002 22:22:44.467890 30882 solver.cpp:580]     Test net output #1: prob = 0.928602
I1002 22:22:44.467896 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:22:45.280740 30882 solver.cpp:357] Iteration 42500 (0.958248 iter/s, 104.357s/100 iters), loss = 0.0298852
I1002 22:22:45.280787 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0487223 (* 1 = 0.0487223 loss)
I1002 22:22:45.280800 30882 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I1002 22:23:47.066743 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:24:07.136626 30882 solver.cpp:357] Iteration 42600 (1.22163 iter/s, 81.8575s/100 iters), loss = 0.0486504
I1002 22:24:07.136688 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0236772 (* 1 = 0.0236772 loss)
I1002 22:24:07.136699 30882 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I1002 22:25:28.822476 30882 solver.cpp:357] Iteration 42700 (1.22418 iter/s, 81.6875s/100 iters), loss = 0.0434081
I1002 22:25:28.822621 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0553848 (* 1 = 0.0553848 loss)
I1002 22:25:28.822631 30882 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I1002 22:26:50.722755 30882 solver.cpp:357] Iteration 42800 (1.22097 iter/s, 81.9018s/100 iters), loss = 0.0638016
I1002 22:26:50.722877 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0776425 (* 1 = 0.0776425 loss)
I1002 22:26:50.722887 30882 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I1002 22:28:12.615417 30882 solver.cpp:357] Iteration 42900 (1.22109 iter/s, 81.8942s/100 iters), loss = 0.0376597
I1002 22:28:12.615561 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0296441 (* 1 = 0.0296441 loss)
I1002 22:28:12.615573 30882 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I1002 22:29:06.527119 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:29:33.508913 30882 solver.cpp:514] Iteration 43000, Testing net (#0)
I1002 22:29:55.813004 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:29:55.901492 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.263732 (* 1 = 0.263732 loss)
I1002 22:29:55.901518 30882 solver.cpp:580]     Test net output #1: prob = 0.926802
I1002 22:29:55.901525 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:29:56.713486 30882 solver.cpp:357] Iteration 43000 (0.960602 iter/s, 104.101s/100 iters), loss = 0.0158406
I1002 22:29:56.713536 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0154988 (* 1 = 0.0154988 loss)
I1002 22:29:56.713548 30882 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I1002 22:31:18.571630 30882 solver.cpp:357] Iteration 43100 (1.22155 iter/s, 81.8633s/100 iters), loss = 0.0600048
I1002 22:31:18.571770 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0494851 (* 1 = 0.0494851 loss)
I1002 22:31:18.571782 30882 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I1002 22:32:40.436744 30882 solver.cpp:357] Iteration 43200 (1.22145 iter/s, 81.8698s/100 iters), loss = 0.0345765
I1002 22:32:40.436882 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0326391 (* 1 = 0.0326391 loss)
I1002 22:32:40.436892 30882 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I1002 22:34:02.308320 30882 solver.cpp:357] Iteration 43300 (1.22136 iter/s, 81.8759s/100 iters), loss = 0.026483
I1002 22:34:02.308406 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0247729 (* 1 = 0.0247729 loss)
I1002 22:34:02.308418 30882 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I1002 22:34:48.629099 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:35:24.284931 30882 solver.cpp:357] Iteration 43400 (1.2198 iter/s, 81.9807s/100 iters), loss = 0.0454301
I1002 22:35:24.285102 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0498152 (* 1 = 0.0498152 loss)
I1002 22:35:24.285116 30882 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I1002 22:36:45.407770 30882 solver.cpp:514] Iteration 43500, Testing net (#0)
I1002 22:37:07.712509 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:37:07.799535 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.300659 (* 1 = 0.300659 loss)
I1002 22:37:07.799561 30882 solver.cpp:580]     Test net output #1: prob = 0.915002
I1002 22:37:07.799566 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:37:08.612643 30882 solver.cpp:357] Iteration 43500 (0.958475 iter/s, 104.332s/100 iters), loss = 0.0244869
I1002 22:37:08.612692 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.012366 (* 1 = 0.012366 loss)
I1002 22:37:08.612704 30882 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I1002 22:38:30.416594 30882 solver.cpp:357] Iteration 43600 (1.22238 iter/s, 81.8075s/100 iters), loss = 0.0688562
I1002 22:38:30.416728 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0576318 (* 1 = 0.0576318 loss)
I1002 22:38:30.416739 30882 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I1002 22:39:52.230720 30882 solver.cpp:357] Iteration 43700 (1.22223 iter/s, 81.8174s/100 iters), loss = 0.024644
I1002 22:39:52.230857 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0199736 (* 1 = 0.0199736 loss)
I1002 22:39:52.230868 30882 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I1002 22:40:30.925524 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:41:14.447211 30882 solver.cpp:357] Iteration 43800 (1.21625 iter/s, 82.2196s/100 iters), loss = 0.038619
I1002 22:41:14.447310 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0272072 (* 1 = 0.0272072 loss)
I1002 22:41:14.447320 30882 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I1002 22:42:36.479240 30882 solver.cpp:357] Iteration 43900 (1.21899 iter/s, 82.035s/100 iters), loss = 0.0652356
I1002 22:42:36.479384 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.107282 (* 1 = 0.107282 loss)
I1002 22:42:36.479395 30882 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I1002 22:43:57.538149 30882 solver.cpp:514] Iteration 44000, Testing net (#0)
I1002 22:44:19.837736 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:44:19.926561 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.253631 (* 1 = 0.253631 loss)
I1002 22:44:19.926587 30882 solver.cpp:580]     Test net output #1: prob = 0.927903
I1002 22:44:19.926594 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:44:20.738818 30882 solver.cpp:357] Iteration 44000 (0.959111 iter/s, 104.263s/100 iters), loss = 0.0613855
I1002 22:44:20.738868 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0619684 (* 1 = 0.0619684 loss)
I1002 22:44:20.738878 30882 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I1002 22:45:42.618613 30882 solver.cpp:357] Iteration 44100 (1.22126 iter/s, 81.8826s/100 iters), loss = 0.0564099
I1002 22:45:42.618712 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0837019 (* 1 = 0.0837019 loss)
I1002 22:45:42.618723 30882 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I1002 22:46:13.766608 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:47:04.569056 30882 solver.cpp:357] Iteration 44200 (1.22021 iter/s, 81.9531s/100 iters), loss = 0.0222754
I1002 22:47:04.569146 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0273829 (* 1 = 0.0273829 loss)
I1002 22:47:04.569156 30882 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I1002 22:48:26.410660 30882 solver.cpp:357] Iteration 44300 (1.22183 iter/s, 81.8442s/100 iters), loss = 0.0383729
I1002 22:48:26.410758 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0414689 (* 1 = 0.0414689 loss)
I1002 22:48:26.410768 30882 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I1002 22:49:48.409567 30882 solver.cpp:357] Iteration 44400 (1.21949 iter/s, 82.0015s/100 iters), loss = 0.0703148
I1002 22:49:48.409768 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.114521 (* 1 = 0.114521 loss)
I1002 22:49:48.409778 30882 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I1002 22:51:09.398819 30882 solver.cpp:514] Iteration 44500, Testing net (#0)
I1002 22:51:31.700314 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:51:31.788250 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.249919 (* 1 = 0.249919 loss)
I1002 22:51:31.788276 30882 solver.cpp:580]     Test net output #1: prob = 0.930602
I1002 22:51:31.788282 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:51:32.599331 30882 solver.cpp:357] Iteration 44500 (0.959759 iter/s, 104.193s/100 iters), loss = 0.0275861
I1002 22:51:32.599381 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0263947 (* 1 = 0.0263947 loss)
I1002 22:51:32.599392 30882 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I1002 22:51:55.908372 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:52:54.371340 30882 solver.cpp:357] Iteration 44600 (1.22288 iter/s, 81.7745s/100 iters), loss = 0.0229503
I1002 22:52:54.371484 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0210971 (* 1 = 0.0210971 loss)
I1002 22:52:54.371495 30882 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I1002 22:54:16.461228 30882 solver.cpp:357] Iteration 44700 (1.21814 iter/s, 82.0923s/100 iters), loss = 0.0211281
I1002 22:54:16.461362 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0152588 (* 1 = 0.0152588 loss)
I1002 22:54:16.461375 30882 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I1002 22:55:38.281250 30882 solver.cpp:357] Iteration 44800 (1.22216 iter/s, 81.8224s/100 iters), loss = 0.0471657
I1002 22:55:38.281390 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0491935 (* 1 = 0.0491935 loss)
I1002 22:55:38.281400 30882 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I1002 22:57:00.236311 30882 solver.cpp:357] Iteration 44900 (1.22015 iter/s, 81.9574s/100 iters), loss = 0.0416251
I1002 22:57:00.239111 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0172389 (* 1 = 0.0172389 loss)
I1002 22:57:00.239125 30882 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I1002 22:57:15.838317 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:58:21.260550 30882 solver.cpp:514] Iteration 45000, Testing net (#0)
I1002 22:58:43.551738 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 22:58:43.639505 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.282151 (* 1 = 0.282151 loss)
I1002 22:58:43.639531 30882 solver.cpp:580]     Test net output #1: prob = 0.918303
I1002 22:58:43.639537 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 22:58:44.449527 30882 solver.cpp:357] Iteration 45000 (0.959568 iter/s, 104.214s/100 iters), loss = 0.0570746
I1002 22:58:44.449574 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0582942 (* 1 = 0.0582942 loss)
I1002 22:58:44.449586 30882 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I1002 23:00:06.219633 30882 solver.cpp:357] Iteration 45100 (1.22291 iter/s, 81.7725s/100 iters), loss = 0.0690228
I1002 23:00:06.219769 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0912307 (* 1 = 0.0912307 loss)
I1002 23:00:06.219779 30882 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I1002 23:01:28.084599 30882 solver.cpp:357] Iteration 45200 (1.22149 iter/s, 81.8672s/100 iters), loss = 0.0230336
I1002 23:01:28.084734 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0199362 (* 1 = 0.0199362 loss)
I1002 23:01:28.084744 30882 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I1002 23:02:49.906004 30882 solver.cpp:357] Iteration 45300 (1.22214 iter/s, 81.8237s/100 iters), loss = 0.0276056
I1002 23:02:49.906152 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0470288 (* 1 = 0.0470288 loss)
I1002 23:02:49.906163 30882 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I1002 23:02:57.677433 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:04:11.760270 30882 solver.cpp:357] Iteration 45400 (1.22167 iter/s, 81.8551s/100 iters), loss = 0.0515391
I1002 23:04:11.760462 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0733063 (* 1 = 0.0733063 loss)
I1002 23:04:11.760473 30882 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I1002 23:05:32.797451 30882 solver.cpp:514] Iteration 45500, Testing net (#0)
I1002 23:05:55.096864 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:05:55.184947 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.241809 (* 1 = 0.241809 loss)
I1002 23:05:55.184973 30882 solver.cpp:580]     Test net output #1: prob = 0.927702
I1002 23:05:55.184979 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:05:55.993762 30882 solver.cpp:357] Iteration 45500 (0.959394 iter/s, 104.232s/100 iters), loss = 0.0664446
I1002 23:05:55.993813 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0585385 (* 1 = 0.0585385 loss)
I1002 23:05:55.993822 30882 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I1002 23:07:17.814723 30882 solver.cpp:357] Iteration 45600 (1.22219 iter/s, 81.8206s/100 iters), loss = 0.0558242
I1002 23:07:17.814867 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0652651 (* 1 = 0.0652651 loss)
I1002 23:07:17.814879 30882 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I1002 23:08:39.729210 30882 solver.cpp:357] Iteration 45700 (1.22079 iter/s, 81.9144s/100 iters), loss = 0.0706984
I1002 23:08:39.729328 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.104961 (* 1 = 0.104961 loss)
I1002 23:08:39.729338 30882 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I1002 23:08:40.145925 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:10:01.495554 30882 solver.cpp:357] Iteration 45800 (1.22299 iter/s, 81.7665s/100 iters), loss = 0.0479081
I1002 23:10:01.495693 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0415397 (* 1 = 0.0415397 loss)
I1002 23:10:01.495703 30882 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I1002 23:11:23.292284 30882 solver.cpp:357] Iteration 45900 (1.22254 iter/s, 81.7971s/100 iters), loss = 0.055088
I1002 23:11:23.292423 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0599575 (* 1 = 0.0599575 loss)
I1002 23:11:23.292431 30882 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I1002 23:12:44.326407 30882 solver.cpp:514] Iteration 46000, Testing net (#0)
I1002 23:13:06.626309 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:13:06.714478 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.269584 (* 1 = 0.269584 loss)
I1002 23:13:06.714504 30882 solver.cpp:580]     Test net output #1: prob = 0.925202
I1002 23:13:06.714509 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:13:07.525696 30882 solver.cpp:357] Iteration 46000 (0.959378 iter/s, 104.234s/100 iters), loss = 0.0247125
I1002 23:13:07.525746 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0279861 (* 1 = 0.0279861 loss)
I1002 23:13:07.525763 30882 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I1002 23:14:21.994091 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:14:29.349411 30882 solver.cpp:357] Iteration 46100 (1.22213 iter/s, 81.8246s/100 iters), loss = 0.0420026
I1002 23:14:29.349472 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0336577 (* 1 = 0.0336577 loss)
I1002 23:14:29.349481 30882 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I1002 23:15:51.201860 30882 solver.cpp:357] Iteration 46200 (1.2217 iter/s, 81.8534s/100 iters), loss = 0.0383211
I1002 23:15:51.202013 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.048709 (* 1 = 0.048709 loss)
I1002 23:15:51.202023 30882 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I1002 23:17:13.096794 30882 solver.cpp:357] Iteration 46300 (1.22106 iter/s, 81.8959s/100 iters), loss = 0.0366567
I1002 23:17:13.096913 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0511779 (* 1 = 0.0511779 loss)
I1002 23:17:13.096925 30882 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I1002 23:18:34.980177 30882 solver.cpp:357] Iteration 46400 (1.22123 iter/s, 81.8845s/100 iters), loss = 0.0390706
I1002 23:18:34.980368 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0580779 (* 1 = 0.0580779 loss)
I1002 23:18:34.980379 30882 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I1002 23:19:41.711884 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:19:56.037223 30882 solver.cpp:514] Iteration 46500, Testing net (#0)
I1002 23:20:18.343420 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:20:18.431241 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.265042 (* 1 = 0.265042 loss)
I1002 23:20:18.431267 30882 solver.cpp:580]     Test net output #1: prob = 0.924602
I1002 23:20:18.431273 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:20:19.242051 30882 solver.cpp:357] Iteration 46500 (0.95911 iter/s, 104.263s/100 iters), loss = 0.0251441
I1002 23:20:19.242101 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0309595 (* 1 = 0.0309595 loss)
I1002 23:20:19.242116 30882 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I1002 23:21:41.086035 30882 solver.cpp:357] Iteration 46600 (1.22182 iter/s, 81.8453s/100 iters), loss = 0.049029
I1002 23:21:41.086176 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0315679 (* 1 = 0.0315679 loss)
I1002 23:21:41.086186 30882 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I1002 23:23:02.994699 30882 solver.cpp:357] Iteration 46700 (1.22085 iter/s, 81.91s/100 iters), loss = 0.0495657
I1002 23:23:02.994787 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0419902 (* 1 = 0.0419902 loss)
I1002 23:23:02.994801 30882 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I1002 23:24:24.922204 30882 solver.cpp:357] Iteration 46800 (1.22057 iter/s, 81.9289s/100 iters), loss = 0.0702716
I1002 23:24:24.922323 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0960193 (* 1 = 0.0960193 loss)
I1002 23:24:24.922336 30882 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I1002 23:25:23.836218 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:25:46.721932 30882 solver.cpp:357] Iteration 46900 (1.22248 iter/s, 81.8011s/100 iters), loss = 0.0544215
I1002 23:25:46.721995 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0723405 (* 1 = 0.0723405 loss)
I1002 23:25:46.722004 30882 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I1002 23:27:07.990994 30882 solver.cpp:514] Iteration 47000, Testing net (#0)
I1002 23:27:30.289300 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:27:30.377849 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.28178 (* 1 = 0.28178 loss)
I1002 23:27:30.377876 30882 solver.cpp:580]     Test net output #1: prob = 0.921802
I1002 23:27:30.377882 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:27:31.188069 30882 solver.cpp:357] Iteration 47000 (0.95723 iter/s, 104.468s/100 iters), loss = 0.0193224
I1002 23:27:31.188118 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0203041 (* 1 = 0.0203041 loss)
I1002 23:27:31.188132 30882 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I1002 23:28:53.039173 30882 solver.cpp:357] Iteration 47100 (1.22171 iter/s, 81.8527s/100 iters), loss = 0.101762
I1002 23:28:53.039314 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.11379 (* 1 = 0.11379 loss)
I1002 23:28:53.039324 30882 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I1002 23:30:14.772315 30882 solver.cpp:357] Iteration 47200 (1.22347 iter/s, 81.7346s/100 iters), loss = 0.0651129
I1002 23:30:14.772454 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.107676 (* 1 = 0.107676 loss)
I1002 23:30:14.772464 30882 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I1002 23:31:06.473001 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:31:36.710620 30882 solver.cpp:357] Iteration 47300 (1.22041 iter/s, 81.9398s/100 iters), loss = 0.0243447
I1002 23:31:36.710714 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0203931 (* 1 = 0.0203931 loss)
I1002 23:31:36.710726 30882 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I1002 23:32:58.447782 30882 solver.cpp:357] Iteration 47400 (1.22341 iter/s, 81.7387s/100 iters), loss = 0.0506454
I1002 23:32:58.450476 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0246396 (* 1 = 0.0246396 loss)
I1002 23:32:58.450491 30882 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I1002 23:34:19.545115 30882 solver.cpp:514] Iteration 47500, Testing net (#0)
I1002 23:34:41.846982 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:34:41.935159 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.254639 (* 1 = 0.254639 loss)
I1002 23:34:41.935185 30882 solver.cpp:580]     Test net output #1: prob = 0.928102
I1002 23:34:41.935191 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:34:42.747025 30882 solver.cpp:357] Iteration 47500 (0.958784 iter/s, 104.299s/100 iters), loss = 0.0401703
I1002 23:34:42.747073 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0115581 (* 1 = 0.0115581 loss)
I1002 23:34:42.747087 30882 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I1002 23:36:04.529120 30882 solver.cpp:357] Iteration 47600 (1.22274 iter/s, 81.7837s/100 iters), loss = 0.0441392
I1002 23:36:04.529214 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0380363 (* 1 = 0.0380363 loss)
I1002 23:36:04.529224 30882 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I1002 23:36:48.198557 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:37:26.263527 30882 solver.cpp:357] Iteration 47700 (1.22345 iter/s, 81.736s/100 iters), loss = 0.06415
I1002 23:37:26.263628 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0709769 (* 1 = 0.0709769 loss)
I1002 23:37:26.263638 30882 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I1002 23:38:48.283424 30882 solver.cpp:357] Iteration 47800 (1.21916 iter/s, 82.0235s/100 iters), loss = 0.0283567
I1002 23:38:48.283566 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0437465 (* 1 = 0.0437465 loss)
I1002 23:38:48.283576 30882 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I1002 23:40:09.996498 30882 solver.cpp:357] Iteration 47900 (1.22374 iter/s, 81.717s/100 iters), loss = 0.066885
I1002 23:40:09.996640 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0744576 (* 1 = 0.0744576 loss)
I1002 23:40:09.996650 30882 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I1002 23:41:31.131163 30882 solver.cpp:514] Iteration 48000, Testing net (#0)
I1002 23:41:53.427055 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:41:53.514878 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.264965 (* 1 = 0.264965 loss)
I1002 23:41:53.514904 30882 solver.cpp:580]     Test net output #1: prob = 0.926302
I1002 23:41:53.514910 30882 solver.cpp:593]     Max_acc: 0.936003  with iter: 41000
I1002 23:41:54.324860 30882 solver.cpp:357] Iteration 48000 (0.95847 iter/s, 104.333s/100 iters), loss = 0.104977
I1002 23:41:54.324910 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0737532 (* 1 = 0.0737532 loss)
I1002 23:41:54.324921 30882 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I1002 23:41:54.324928 30882 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I1002 23:42:30.294349 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:43:16.086581 30882 solver.cpp:357] Iteration 48100 (1.22302 iter/s, 81.7651s/100 iters), loss = 0.0541208
I1002 23:43:16.086678 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0133816 (* 1 = 0.0133816 loss)
I1002 23:43:16.086689 30882 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I1002 23:44:38.168154 30882 solver.cpp:357] Iteration 48200 (1.21825 iter/s, 82.0847s/100 iters), loss = 0.0156058
I1002 23:44:38.168262 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00703886 (* 1 = 0.00703886 loss)
I1002 23:44:38.168273 30882 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I1002 23:46:00.082998 30882 solver.cpp:357] Iteration 48300 (1.22074 iter/s, 81.9178s/100 iters), loss = 0.0223534
I1002 23:46:00.083143 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.029438 (* 1 = 0.029438 loss)
I1002 23:46:00.083153 30882 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I1002 23:47:22.110572 30882 solver.cpp:357] Iteration 48400 (1.21906 iter/s, 82.0303s/100 iters), loss = 0.0300141
I1002 23:47:22.110760 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.041538 (* 1 = 0.041538 loss)
I1002 23:47:22.110771 30882 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I1002 23:47:50.362534 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:48:43.183693 30882 solver.cpp:514] Iteration 48500, Testing net (#0)
I1002 23:49:05.482489 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:49:05.570770 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175248 (* 1 = 0.175248 loss)
I1002 23:49:05.570796 30882 solver.cpp:580]     Test net output #1: prob = 0.947203
I1002 23:49:05.570809 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_48500.caffemodel
I1002 23:49:05.974733 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_48500.solverstate
I1002 23:49:06.112838 30882 solver.cpp:593]     Max_acc: 0.947203  with iter: 48500
I1002 23:49:06.923599 30882 solver.cpp:357] Iteration 48500 (0.954049 iter/s, 104.816s/100 iters), loss = 0.0525834
I1002 23:49:06.923650 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0654779 (* 1 = 0.0654779 loss)
I1002 23:49:06.923662 30882 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I1002 23:50:28.687078 30882 solver.cpp:357] Iteration 48600 (1.223 iter/s, 81.7661s/100 iters), loss = 0.0200575
I1002 23:50:28.687175 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0175468 (* 1 = 0.0175468 loss)
I1002 23:50:28.687186 30882 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I1002 23:51:50.737108 30882 solver.cpp:357] Iteration 48700 (1.21873 iter/s, 82.0525s/100 iters), loss = 0.0407155
I1002 23:51:50.737246 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0477695 (* 1 = 0.0477695 loss)
I1002 23:51:50.737257 30882 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I1002 23:53:12.726150 30882 solver.cpp:357] Iteration 48800 (1.21964 iter/s, 81.9914s/100 iters), loss = 0.0206951
I1002 23:53:12.728911 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0141333 (* 1 = 0.0141333 loss)
I1002 23:53:12.728925 30882 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I1002 23:53:33.654553 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:54:34.740459 30882 solver.cpp:357] Iteration 48900 (1.2193 iter/s, 82.014s/100 iters), loss = 0.0207211
I1002 23:54:34.746448 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00851404 (* 1 = 0.00851404 loss)
I1002 23:54:34.746461 30882 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I1002 23:55:56.258649 30882 solver.cpp:514] Iteration 49000, Testing net (#0)
I1002 23:56:18.571199 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1002 23:56:18.659035 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.180009 (* 1 = 0.180009 loss)
I1002 23:56:18.659061 30882 solver.cpp:580]     Test net output #1: prob = 0.947103
I1002 23:56:18.659068 30882 solver.cpp:593]     Max_acc: 0.947203  with iter: 48500
I1002 23:56:19.473101 30882 solver.cpp:357] Iteration 49000 (0.954839 iter/s, 104.73s/100 iters), loss = 0.0429942
I1002 23:56:19.473152 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0593246 (* 1 = 0.0593246 loss)
I1002 23:56:19.473162 30882 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I1002 23:57:41.884522 30882 solver.cpp:357] Iteration 49100 (1.21339 iter/s, 82.4137s/100 iters), loss = 0.0117898
I1002 23:57:41.884620 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00798903 (* 1 = 0.00798903 loss)
I1002 23:57:41.884631 30882 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I1002 23:59:04.295998 30882 solver.cpp:357] Iteration 49200 (1.21339 iter/s, 82.4137s/100 iters), loss = 0.025166
I1002 23:59:04.296093 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0305111 (* 1 = 0.0305111 loss)
I1002 23:59:04.296105 30882 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I1002 23:59:17.482462 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:00:26.787473 30882 solver.cpp:357] Iteration 49300 (1.21221 iter/s, 82.4937s/100 iters), loss = 0.0181332
I1003 00:00:26.787675 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0146884 (* 1 = 0.0146884 loss)
I1003 00:00:26.787686 30882 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I1003 00:01:49.370329 30882 solver.cpp:357] Iteration 49400 (1.21087 iter/s, 82.5849s/100 iters), loss = 0.0106474
I1003 00:01:49.370479 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00392955 (* 1 = 0.00392955 loss)
I1003 00:01:49.370489 30882 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I1003 00:03:11.010836 30882 solver.cpp:514] Iteration 49500, Testing net (#0)
I1003 00:03:33.326083 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:03:33.414582 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174559 (* 1 = 0.174559 loss)
I1003 00:03:33.414604 30882 solver.cpp:580]     Test net output #1: prob = 0.949802
I1003 00:03:33.414618 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_49500.caffemodel
I1003 00:03:33.820323 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_49500.solverstate
I1003 00:03:33.958575 30882 solver.cpp:593]     Max_acc: 0.949802  with iter: 49500
I1003 00:03:34.769876 30882 solver.cpp:357] Iteration 49500 (0.948746 iter/s, 105.402s/100 iters), loss = 0.0290665
I1003 00:03:34.769932 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0358787 (* 1 = 0.0358787 loss)
I1003 00:03:34.769945 30882 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I1003 00:04:57.246403 30882 solver.cpp:357] Iteration 49600 (1.21243 iter/s, 82.4787s/100 iters), loss = 0.0126492
I1003 00:04:57.246500 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.013997 (* 1 = 0.013997 loss)
I1003 00:04:57.246513 30882 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I1003 00:05:02.621294 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:06:19.975929 30882 solver.cpp:357] Iteration 49700 (1.20873 iter/s, 82.7316s/100 iters), loss = 0.0176871
I1003 00:06:19.976073 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0181628 (* 1 = 0.0181628 loss)
I1003 00:06:19.976083 30882 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I1003 00:07:42.611399 30882 solver.cpp:357] Iteration 49800 (1.2101 iter/s, 82.6375s/100 iters), loss = 0.0286693
I1003 00:07:42.613452 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.022141 (* 1 = 0.022141 loss)
I1003 00:07:42.613471 30882 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I1003 00:09:05.091547 30882 solver.cpp:357] Iteration 49900 (1.21241 iter/s, 82.4803s/100 iters), loss = 0.025628
I1003 00:09:05.091640 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0054153 (* 1 = 0.0054153 loss)
I1003 00:09:05.091651 30882 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I1003 00:10:25.266235 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:10:26.904572 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_50000.caffemodel
I1003 00:10:27.309500 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_50000.solverstate
I1003 00:10:27.447832 30882 solver.cpp:514] Iteration 50000, Testing net (#0)
I1003 00:10:49.753783 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:10:49.841933 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172915 (* 1 = 0.172915 loss)
I1003 00:10:49.841959 30882 solver.cpp:580]     Test net output #1: prob = 0.951103
I1003 00:10:49.841971 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_50000.caffemodel
I1003 00:10:50.542277 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_50000.solverstate
I1003 00:10:50.702205 30882 solver.cpp:593]     Max_acc: 0.951103  with iter: 50000
I1003 00:10:51.514802 30882 solver.cpp:357] Iteration 50000 (0.939621 iter/s, 106.426s/100 iters), loss = 0.0116969
I1003 00:10:51.514874 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00644312 (* 1 = 0.00644312 loss)
I1003 00:10:51.514886 30882 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I1003 00:12:13.926865 30882 solver.cpp:357] Iteration 50100 (1.21342 iter/s, 82.4115s/100 iters), loss = 0.0195087
I1003 00:12:13.927018 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00953124 (* 1 = 0.00953124 loss)
I1003 00:12:13.927029 30882 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I1003 00:13:36.661116 30882 solver.cpp:357] Iteration 50200 (1.20881 iter/s, 82.7262s/100 iters), loss = 0.0320302
I1003 00:13:36.661221 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0552158 (* 1 = 0.0552158 loss)
I1003 00:13:36.661231 30882 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I1003 00:14:59.421605 30882 solver.cpp:357] Iteration 50300 (1.2084 iter/s, 82.7538s/100 iters), loss = 0.0101792
I1003 00:14:59.421744 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0135673 (* 1 = 0.0135673 loss)
I1003 00:14:59.421754 30882 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I1003 00:16:12.250437 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:16:22.163754 30882 solver.cpp:357] Iteration 50400 (1.20866 iter/s, 82.7366s/100 iters), loss = 0.0539392
I1003 00:16:22.163816 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0508104 (* 1 = 0.0508104 loss)
I1003 00:16:22.163826 30882 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I1003 00:17:44.091109 30882 solver.cpp:514] Iteration 50500, Testing net (#0)
I1003 00:18:06.415499 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:18:06.504174 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174375 (* 1 = 0.174375 loss)
I1003 00:18:06.504201 30882 solver.cpp:580]     Test net output #1: prob = 0.951002
I1003 00:18:06.504207 30882 solver.cpp:593]     Max_acc: 0.951103  with iter: 50000
I1003 00:18:07.322494 30882 solver.cpp:357] Iteration 50500 (0.950994 iter/s, 105.153s/100 iters), loss = 0.0044858
I1003 00:18:07.322544 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00524159 (* 1 = 0.00524159 loss)
I1003 00:18:07.322557 30882 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I1003 00:19:29.895403 30882 solver.cpp:357] Iteration 50600 (1.2111 iter/s, 82.5694s/100 iters), loss = 0.0311198
I1003 00:19:29.895545 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00436263 (* 1 = 0.00436263 loss)
I1003 00:19:29.895555 30882 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I1003 00:20:52.424252 30882 solver.cpp:357] Iteration 50700 (1.21174 iter/s, 82.5259s/100 iters), loss = 0.0128477
I1003 00:20:52.424394 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.021586 (* 1 = 0.021586 loss)
I1003 00:20:52.424405 30882 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I1003 00:21:57.326172 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:22:15.128777 30882 solver.cpp:357] Iteration 50800 (1.20916 iter/s, 82.7021s/100 iters), loss = 0.0109287
I1003 00:22:15.128844 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0136673 (* 1 = 0.0136673 loss)
I1003 00:22:15.128854 30882 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I1003 00:23:37.632755 30882 solver.cpp:357] Iteration 50900 (1.21209 iter/s, 82.5021s/100 iters), loss = 0.00838783
I1003 00:23:37.632894 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00683103 (* 1 = 0.00683103 loss)
I1003 00:23:37.632907 30882 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I1003 00:24:59.380971 30882 solver.cpp:514] Iteration 51000, Testing net (#0)
I1003 00:25:21.698134 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:25:21.786470 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174451 (* 1 = 0.174451 loss)
I1003 00:25:21.786497 30882 solver.cpp:580]     Test net output #1: prob = 0.952802
I1003 00:25:21.786509 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_51000.caffemodel
I1003 00:25:22.192785 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_51000.solverstate
I1003 00:25:22.332851 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 00:25:23.143674 30882 solver.cpp:357] Iteration 51000 (0.947786 iter/s, 105.509s/100 iters), loss = 0.0116277
I1003 00:25:23.143729 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00492073 (* 1 = 0.00492073 loss)
I1003 00:25:23.143741 30882 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I1003 00:26:45.918386 30882 solver.cpp:357] Iteration 51100 (1.20811 iter/s, 82.7737s/100 iters), loss = 0.0031814
I1003 00:26:45.918581 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00268083 (* 1 = 0.00268083 loss)
I1003 00:26:45.918591 30882 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I1003 00:27:43.001701 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:28:08.622553 30882 solver.cpp:357] Iteration 51200 (1.20914 iter/s, 82.7033s/100 iters), loss = 0.0093279
I1003 00:28:08.622617 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00472481 (* 1 = 0.00472481 loss)
I1003 00:28:08.622627 30882 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I1003 00:29:31.313113 30882 solver.cpp:357] Iteration 51300 (1.20934 iter/s, 82.69s/100 iters), loss = 0.017099
I1003 00:29:31.313257 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0269482 (* 1 = 0.0269482 loss)
I1003 00:29:31.313267 30882 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I1003 00:30:54.088063 30882 solver.cpp:357] Iteration 51400 (1.2081 iter/s, 82.7745s/100 iters), loss = 0.0057536
I1003 00:30:54.088202 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00374424 (* 1 = 0.00374424 loss)
I1003 00:30:54.088213 30882 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I1003 00:32:16.131855 30882 solver.cpp:514] Iteration 51500, Testing net (#0)
I1003 00:32:38.436439 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:32:38.524487 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174809 (* 1 = 0.174809 loss)
I1003 00:32:38.524513 30882 solver.cpp:580]     Test net output #1: prob = 0.951702
I1003 00:32:38.524519 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 00:32:39.343895 30882 solver.cpp:357] Iteration 51500 (0.950068 iter/s, 105.256s/100 iters), loss = 0.00533291
I1003 00:32:39.343945 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00632253 (* 1 = 0.00632253 loss)
I1003 00:32:39.343955 30882 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I1003 00:33:28.655138 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:34:02.249752 30882 solver.cpp:357] Iteration 51600 (1.20619 iter/s, 82.9059s/100 iters), loss = 0.00715915
I1003 00:34:02.249840 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00886175 (* 1 = 0.00886175 loss)
I1003 00:34:02.249850 30882 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I1003 00:35:25.192268 30882 solver.cpp:357] Iteration 51700 (1.20565 iter/s, 82.9426s/100 iters), loss = 0.00736919
I1003 00:35:25.192415 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0105301 (* 1 = 0.0105301 loss)
I1003 00:35:25.192425 30882 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I1003 00:36:48.019577 30882 solver.cpp:357] Iteration 51800 (1.20733 iter/s, 82.8275s/100 iters), loss = 0.0113811
I1003 00:36:48.019678 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00678936 (* 1 = 0.00678936 loss)
I1003 00:36:48.019690 30882 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I1003 00:38:10.679685 30882 solver.cpp:357] Iteration 51900 (1.20977 iter/s, 82.6604s/100 iters), loss = 0.00946377
I1003 00:38:10.679828 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00455078 (* 1 = 0.00455078 loss)
I1003 00:38:10.679837 30882 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I1003 00:38:52.417568 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:39:32.660660 30882 solver.cpp:514] Iteration 52000, Testing net (#0)
I1003 00:39:54.976668 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:39:55.064754 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174495 (* 1 = 0.174495 loss)
I1003 00:39:55.064779 30882 solver.cpp:580]     Test net output #1: prob = 0.950403
I1003 00:39:55.064785 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 00:39:55.886157 30882 solver.cpp:357] Iteration 52000 (0.950508 iter/s, 105.207s/100 iters), loss = 0.0148683
I1003 00:39:55.886206 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0228782 (* 1 = 0.0228782 loss)
I1003 00:39:55.886217 30882 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I1003 00:41:18.653412 30882 solver.cpp:357] Iteration 52100 (1.2082 iter/s, 82.7677s/100 iters), loss = 0.00734608
I1003 00:41:18.653511 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00681889 (* 1 = 0.00681889 loss)
I1003 00:41:18.653522 30882 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I1003 00:42:41.479894 30882 solver.cpp:357] Iteration 52200 (1.20734 iter/s, 82.827s/100 iters), loss = 0.00817427
I1003 00:42:41.479991 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00239422 (* 1 = 0.00239422 loss)
I1003 00:42:41.480001 30882 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I1003 00:44:04.208932 30882 solver.cpp:357] Iteration 52300 (1.20876 iter/s, 82.7296s/100 iters), loss = 0.0137257
I1003 00:44:04.209026 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00785479 (* 1 = 0.00785479 loss)
I1003 00:44:04.209036 30882 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I1003 00:44:38.135641 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:45:26.928519 30882 solver.cpp:357] Iteration 52400 (1.2089 iter/s, 82.7202s/100 iters), loss = 0.0033809
I1003 00:45:26.928654 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00432507 (* 1 = 0.00432507 loss)
I1003 00:45:26.928664 30882 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I1003 00:46:48.864653 30882 solver.cpp:514] Iteration 52500, Testing net (#0)
I1003 00:47:11.168156 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:47:11.256335 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.16987 (* 1 = 0.16987 loss)
I1003 00:47:11.256362 30882 solver.cpp:580]     Test net output #1: prob = 0.951802
I1003 00:47:11.256368 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 00:47:12.077512 30882 solver.cpp:357] Iteration 52500 (0.950985 iter/s, 105.154s/100 iters), loss = 0.0113483
I1003 00:47:12.077563 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00572973 (* 1 = 0.00572973 loss)
I1003 00:47:12.077572 30882 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I1003 00:48:34.802964 30882 solver.cpp:357] Iteration 52600 (1.20874 iter/s, 82.7308s/100 iters), loss = 0.0122094
I1003 00:48:34.803107 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0138819 (* 1 = 0.0138819 loss)
I1003 00:48:34.803117 30882 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I1003 00:49:57.528200 30882 solver.cpp:357] Iteration 52700 (1.20875 iter/s, 82.7299s/100 iters), loss = 0.0276001
I1003 00:49:57.528343 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00969936 (* 1 = 0.00969936 loss)
I1003 00:49:57.528358 30882 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I1003 00:50:23.571904 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:51:20.223614 30882 solver.cpp:357] Iteration 52800 (1.20919 iter/s, 82.6997s/100 iters), loss = 0.0107321
I1003 00:51:20.223754 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0133517 (* 1 = 0.0133517 loss)
I1003 00:51:20.223765 30882 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I1003 00:52:43.140719 30882 solver.cpp:357] Iteration 52900 (1.20597 iter/s, 82.921s/100 iters), loss = 0.00690585
I1003 00:52:43.142231 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00602007 (* 1 = 0.00602007 loss)
I1003 00:52:43.142248 30882 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I1003 00:54:05.282958 30882 solver.cpp:514] Iteration 53000, Testing net (#0)
I1003 00:54:27.600950 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:54:27.689043 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174293 (* 1 = 0.174293 loss)
I1003 00:54:27.689069 30882 solver.cpp:580]     Test net output #1: prob = 0.952202
I1003 00:54:27.689074 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 00:54:28.506636 30882 solver.cpp:357] Iteration 53000 (0.949045 iter/s, 105.369s/100 iters), loss = 0.00935017
I1003 00:54:28.506685 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0170953 (* 1 = 0.0170953 loss)
I1003 00:54:28.506695 30882 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I1003 00:55:51.215056 30882 solver.cpp:357] Iteration 53100 (1.20902 iter/s, 82.7117s/100 iters), loss = 0.00907326
I1003 00:55:51.215159 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.010452 (* 1 = 0.010452 loss)
I1003 00:55:51.215170 30882 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I1003 00:56:09.469976 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 00:57:13.970088 30882 solver.cpp:357] Iteration 53200 (1.20834 iter/s, 82.758s/100 iters), loss = 0.00729633
I1003 00:57:13.970228 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00516012 (* 1 = 0.00516012 loss)
I1003 00:57:13.970239 30882 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I1003 00:58:36.451030 30882 solver.cpp:357] Iteration 53300 (1.21236 iter/s, 82.4837s/100 iters), loss = 0.0208477
I1003 00:58:36.451169 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0335818 (* 1 = 0.0335818 loss)
I1003 00:58:36.451179 30882 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I1003 00:59:59.211877 30882 solver.cpp:357] Iteration 53400 (1.20826 iter/s, 82.7634s/100 iters), loss = 0.00276387
I1003 00:59:59.212013 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00340287 (* 1 = 0.00340287 loss)
I1003 00:59:59.212023 30882 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I1003 01:01:21.341369 30882 solver.cpp:514] Iteration 53500, Testing net (#0)
I1003 01:01:43.674091 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:01:43.761050 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.173856 (* 1 = 0.173856 loss)
I1003 01:01:43.761077 30882 solver.cpp:580]     Test net output #1: prob = 0.952703
I1003 01:01:43.761083 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:01:44.578959 30882 solver.cpp:357] Iteration 53500 (0.949035 iter/s, 105.37s/100 iters), loss = 0.0135218
I1003 01:01:44.579010 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00364711 (* 1 = 0.00364711 loss)
I1003 01:01:44.579025 30882 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I1003 01:01:55.323581 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:03:07.415848 30882 solver.cpp:357] Iteration 53600 (1.20716 iter/s, 82.8392s/100 iters), loss = 0.0152023
I1003 01:03:07.415980 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00197124 (* 1 = 0.00197124 loss)
I1003 01:03:07.415990 30882 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I1003 01:04:30.217310 30882 solver.cpp:357] Iteration 53700 (1.20768 iter/s, 82.8036s/100 iters), loss = 0.00639175
I1003 01:04:30.217433 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00604421 (* 1 = 0.00604421 loss)
I1003 01:04:30.217443 30882 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I1003 01:05:53.018563 30882 solver.cpp:357] Iteration 53800 (1.20768 iter/s, 82.8033s/100 iters), loss = 0.00324086
I1003 01:05:53.018700 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00468873 (* 1 = 0.00468873 loss)
I1003 01:05:53.018710 30882 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I1003 01:07:15.887979 30882 solver.cpp:357] Iteration 53900 (1.20669 iter/s, 82.8714s/100 iters), loss = 0.00360144
I1003 01:07:15.888118 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00465356 (* 1 = 0.00465356 loss)
I1003 01:07:15.888128 30882 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I1003 01:07:18.787578 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:08:37.865337 30882 solver.cpp:514] Iteration 54000, Testing net (#0)
I1003 01:09:00.171011 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:09:00.259876 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174796 (* 1 = 0.174796 loss)
I1003 01:09:00.259902 30882 solver.cpp:580]     Test net output #1: prob = 0.951302
I1003 01:09:00.259908 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:09:01.079300 30882 solver.cpp:357] Iteration 54000 (0.950626 iter/s, 105.194s/100 iters), loss = 0.00578522
I1003 01:09:01.079351 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00577003 (* 1 = 0.00577003 loss)
I1003 01:09:01.079365 30882 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I1003 01:10:23.797075 30882 solver.cpp:357] Iteration 54100 (1.2089 iter/s, 82.7197s/100 iters), loss = 0.0142816
I1003 01:10:23.797219 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0249898 (* 1 = 0.0249898 loss)
I1003 01:10:23.797230 30882 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I1003 01:11:46.539939 30882 solver.cpp:357] Iteration 54200 (1.20854 iter/s, 82.7447s/100 iters), loss = 0.00346015
I1003 01:11:46.540082 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00294023 (* 1 = 0.00294023 loss)
I1003 01:11:46.540094 30882 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I1003 01:13:04.342669 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:13:09.279109 30882 solver.cpp:357] Iteration 54300 (1.20859 iter/s, 82.741s/100 iters), loss = 0.00272509
I1003 01:13:09.279171 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00335314 (* 1 = 0.00335314 loss)
I1003 01:13:09.279181 30882 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I1003 01:14:32.124625 30882 solver.cpp:357] Iteration 54400 (1.20704 iter/s, 82.8474s/100 iters), loss = 0.0110344
I1003 01:14:32.124779 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0118302 (* 1 = 0.0118302 loss)
I1003 01:14:32.124789 30882 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I1003 01:15:54.206780 30882 solver.cpp:514] Iteration 54500, Testing net (#0)
I1003 01:16:16.518398 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:16:16.607168 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174083 (* 1 = 0.174083 loss)
I1003 01:16:16.607194 30882 solver.cpp:580]     Test net output #1: prob = 0.952302
I1003 01:16:16.607200 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:16:17.428515 30882 solver.cpp:357] Iteration 54500 (0.949612 iter/s, 105.306s/100 iters), loss = 0.00965243
I1003 01:16:17.428565 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00548118 (* 1 = 0.00548118 loss)
I1003 01:16:17.428578 30882 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I1003 01:17:40.112277 30882 solver.cpp:357] Iteration 54600 (1.2094 iter/s, 82.6856s/100 iters), loss = 0.0068546
I1003 01:17:40.113144 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00208768 (* 1 = 0.00208768 loss)
I1003 01:17:40.113158 30882 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I1003 01:18:50.119531 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:19:03.015882 30882 solver.cpp:357] Iteration 54700 (1.20621 iter/s, 82.9046s/100 iters), loss = 0.0102782
I1003 01:19:03.015941 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00749965 (* 1 = 0.00749965 loss)
I1003 01:19:03.015952 30882 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I1003 01:20:25.922335 30882 solver.cpp:357] Iteration 54800 (1.20615 iter/s, 82.9082s/100 iters), loss = 0.00459339
I1003 01:20:25.922477 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00381358 (* 1 = 0.00381358 loss)
I1003 01:20:25.922487 30882 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I1003 01:21:48.672157 30882 solver.cpp:357] Iteration 54900 (1.20844 iter/s, 82.7516s/100 iters), loss = 0.00597939
I1003 01:21:48.672303 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00506326 (* 1 = 0.00506326 loss)
I1003 01:21:48.672314 30882 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I1003 01:23:10.831766 30882 solver.cpp:514] Iteration 55000, Testing net (#0)
I1003 01:23:33.143002 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:23:33.231171 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175548 (* 1 = 0.175548 loss)
I1003 01:23:33.231197 30882 solver.cpp:580]     Test net output #1: prob = 0.950902
I1003 01:23:33.231204 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:23:34.054618 30882 solver.cpp:357] Iteration 55000 (0.9488 iter/s, 105.396s/100 iters), loss = 0.0123334
I1003 01:23:34.054668 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00269661 (* 1 = 0.00269661 loss)
I1003 01:23:34.054680 30882 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I1003 01:24:36.576633 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:24:56.901967 30882 solver.cpp:357] Iteration 55100 (1.2069 iter/s, 82.857s/100 iters), loss = 0.00812487
I1003 01:24:56.902027 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00153526 (* 1 = 0.00153526 loss)
I1003 01:24:56.902037 30882 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I1003 01:26:19.885757 30882 solver.cpp:357] Iteration 55200 (1.20493 iter/s, 82.9925s/100 iters), loss = 0.0058576
I1003 01:26:19.885897 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00348058 (* 1 = 0.00348058 loss)
I1003 01:26:19.885913 30882 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I1003 01:27:42.749552 30882 solver.cpp:357] Iteration 55300 (1.20669 iter/s, 82.8716s/100 iters), loss = 0.0164167
I1003 01:27:42.749691 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00395143 (* 1 = 0.00395143 loss)
I1003 01:27:42.749702 30882 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I1003 01:29:05.589550 30882 solver.cpp:357] Iteration 55400 (1.20704 iter/s, 82.8471s/100 iters), loss = 0.00326929
I1003 01:29:05.589646 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00252167 (* 1 = 0.00252167 loss)
I1003 01:29:05.589658 30882 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I1003 01:30:00.136991 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:30:27.444177 30882 solver.cpp:514] Iteration 55500, Testing net (#0)
I1003 01:30:49.751461 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:30:49.839862 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.176327 (* 1 = 0.176327 loss)
I1003 01:30:49.839889 30882 solver.cpp:580]     Test net output #1: prob = 0.951302
I1003 01:30:49.839895 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:30:50.657546 30882 solver.cpp:357] Iteration 55500 (0.95169 iter/s, 105.076s/100 iters), loss = 0.00362287
I1003 01:30:50.657595 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00327379 (* 1 = 0.00327379 loss)
I1003 01:30:50.657609 30882 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I1003 01:32:13.440466 30882 solver.cpp:357] Iteration 55600 (1.20789 iter/s, 82.7889s/100 iters), loss = 0.00468385
I1003 01:32:13.440611 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00175779 (* 1 = 0.00175779 loss)
I1003 01:32:13.440621 30882 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I1003 01:33:36.217350 30882 solver.cpp:357] Iteration 55700 (1.20799 iter/s, 82.7823s/100 iters), loss = 0.00649653
I1003 01:33:36.217486 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00636092 (* 1 = 0.00636092 loss)
I1003 01:33:36.217496 30882 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I1003 01:34:59.038324 30882 solver.cpp:357] Iteration 55800 (1.20735 iter/s, 82.8261s/100 iters), loss = 0.00697877
I1003 01:34:59.038419 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104076 (* 1 = 0.0104076 loss)
I1003 01:34:59.038430 30882 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I1003 01:35:45.852669 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:36:21.873996 30882 solver.cpp:357] Iteration 55900 (1.20714 iter/s, 82.8405s/100 iters), loss = 0.0087626
I1003 01:36:21.874203 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00395507 (* 1 = 0.00395507 loss)
I1003 01:36:21.874214 30882 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I1003 01:37:43.829386 30882 solver.cpp:514] Iteration 56000, Testing net (#0)
I1003 01:38:06.149665 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:38:06.237375 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175947 (* 1 = 0.175947 loss)
I1003 01:38:06.237401 30882 solver.cpp:580]     Test net output #1: prob = 0.951703
I1003 01:38:06.237407 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:38:07.057130 30882 solver.cpp:357] Iteration 56000 (0.950671 iter/s, 105.189s/100 iters), loss = 0.00427603
I1003 01:38:07.057176 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00328424 (* 1 = 0.00328424 loss)
I1003 01:38:07.057188 30882 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I1003 01:39:29.904666 30882 solver.cpp:357] Iteration 56100 (1.20697 iter/s, 82.8519s/100 iters), loss = 0.0189375
I1003 01:39:29.904810 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102538 (* 1 = 0.0102538 loss)
I1003 01:39:29.904820 30882 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I1003 01:40:52.782528 30882 solver.cpp:357] Iteration 56200 (1.20653 iter/s, 82.882s/100 iters), loss = 0.00496858
I1003 01:40:52.782683 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00713454 (* 1 = 0.00713454 loss)
I1003 01:40:52.782693 30882 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I1003 01:41:31.763238 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:42:15.627135 30882 solver.cpp:357] Iteration 56300 (1.20702 iter/s, 82.8486s/100 iters), loss = 0.00620519
I1003 01:42:15.627281 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00739727 (* 1 = 0.00739727 loss)
I1003 01:42:15.627291 30882 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I1003 01:43:38.464411 30882 solver.cpp:357] Iteration 56400 (1.20713 iter/s, 82.8411s/100 iters), loss = 0.0134311
I1003 01:43:38.464545 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0238205 (* 1 = 0.0238205 loss)
I1003 01:43:38.464555 30882 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I1003 01:45:00.506745 30882 solver.cpp:514] Iteration 56500, Testing net (#0)
I1003 01:45:22.823812 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:45:22.912055 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175811 (* 1 = 0.175811 loss)
I1003 01:45:22.912081 30882 solver.cpp:580]     Test net output #1: prob = 0.951902
I1003 01:45:22.912087 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:45:23.731881 30882 solver.cpp:357] Iteration 56500 (0.949918 iter/s, 105.272s/100 iters), loss = 0.0129422
I1003 01:45:23.731930 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0240778 (* 1 = 0.0240778 loss)
I1003 01:45:23.731940 30882 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I1003 01:46:46.489246 30882 solver.cpp:357] Iteration 56600 (1.2083 iter/s, 82.7611s/100 iters), loss = 0.00272434
I1003 01:46:46.491626 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00358105 (* 1 = 0.00358105 loss)
I1003 01:46:46.491641 30882 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I1003 01:47:17.933300 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:48:09.055887 30882 solver.cpp:357] Iteration 56700 (1.21112 iter/s, 82.568s/100 iters), loss = 0.0172432
I1003 01:48:09.056032 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0319262 (* 1 = 0.0319262 loss)
I1003 01:48:09.056042 30882 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I1003 01:49:31.939797 30882 solver.cpp:357] Iteration 56800 (1.20646 iter/s, 82.8874s/100 iters), loss = 0.00934476
I1003 01:49:31.939940 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.011423 (* 1 = 0.011423 loss)
I1003 01:49:31.939950 30882 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I1003 01:50:54.644492 30882 solver.cpp:357] Iteration 56900 (1.20907 iter/s, 82.7081s/100 iters), loss = 0.0104989
I1003 01:50:54.644685 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00728688 (* 1 = 0.00728688 loss)
I1003 01:50:54.644696 30882 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I1003 01:52:16.546512 30882 solver.cpp:514] Iteration 57000, Testing net (#0)
I1003 01:52:38.858383 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:52:38.946540 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174848 (* 1 = 0.174848 loss)
I1003 01:52:38.946568 30882 solver.cpp:580]     Test net output #1: prob = 0.951802
I1003 01:52:38.946573 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:52:39.768522 30882 solver.cpp:357] Iteration 57000 (0.951218 iter/s, 105.128s/100 iters), loss = 0.00405975
I1003 01:52:39.768571 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00431032 (* 1 = 0.00431032 loss)
I1003 01:52:39.768584 30882 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I1003 01:53:03.395027 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:54:02.605149 30882 solver.cpp:357] Iteration 57100 (1.20715 iter/s, 82.8401s/100 iters), loss = 0.0261852
I1003 01:54:02.605239 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00561307 (* 1 = 0.00561307 loss)
I1003 01:54:02.605252 30882 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I1003 01:55:25.448813 30882 solver.cpp:357] Iteration 57200 (1.20704 iter/s, 82.847s/100 iters), loss = 0.009816
I1003 01:55:25.448909 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00397179 (* 1 = 0.00397179 loss)
I1003 01:55:25.448921 30882 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I1003 01:56:48.436676 30882 solver.cpp:357] Iteration 57300 (1.20496 iter/s, 82.9904s/100 iters), loss = 0.0237411
I1003 01:56:48.436810 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0417749 (* 1 = 0.0417749 loss)
I1003 01:56:48.436821 30882 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I1003 01:58:11.517119 30882 solver.cpp:357] Iteration 57400 (1.20365 iter/s, 83.0806s/100 iters), loss = 0.0137318
I1003 01:58:11.517264 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0240424 (* 1 = 0.0240424 loss)
I1003 01:58:11.517276 30882 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I1003 01:58:27.276834 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:59:33.624215 30882 solver.cpp:514] Iteration 57500, Testing net (#0)
I1003 01:59:55.941651 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 01:59:56.029860 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175133 (* 1 = 0.175133 loss)
I1003 01:59:56.029886 30882 solver.cpp:580]     Test net output #1: prob = 0.951703
I1003 01:59:56.029893 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 01:59:56.851146 30882 solver.cpp:357] Iteration 57500 (0.949354 iter/s, 105.335s/100 iters), loss = 0.00246109
I1003 01:59:56.851197 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00343031 (* 1 = 0.00343031 loss)
I1003 01:59:56.851208 30882 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I1003 02:01:19.580868 30882 solver.cpp:357] Iteration 57600 (1.20874 iter/s, 82.7307s/100 iters), loss = 0.0064325
I1003 02:01:19.580967 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0013727 (* 1 = 0.0013727 loss)
I1003 02:01:19.580978 30882 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I1003 02:02:42.436329 30882 solver.cpp:357] Iteration 57700 (1.2069 iter/s, 82.8567s/100 iters), loss = 0.0129082
I1003 02:02:42.439206 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0111306 (* 1 = 0.0111306 loss)
I1003 02:02:42.439222 30882 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I1003 02:04:05.435046 30882 solver.cpp:357] Iteration 57800 (1.20486 iter/s, 82.9974s/100 iters), loss = 0.00573491
I1003 02:04:05.435139 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00773931 (* 1 = 0.00773931 loss)
I1003 02:04:05.435151 30882 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I1003 02:04:13.284147 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:05:27.989593 30882 solver.cpp:357] Iteration 57900 (1.2113 iter/s, 82.5562s/100 iters), loss = 0.00688527
I1003 02:05:27.989784 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00418322 (* 1 = 0.00418322 loss)
I1003 02:05:27.989795 30882 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I1003 02:06:49.999317 30882 solver.cpp:514] Iteration 58000, Testing net (#0)
I1003 02:07:12.311368 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:07:12.400301 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.178195 (* 1 = 0.178195 loss)
I1003 02:07:12.400327 30882 solver.cpp:580]     Test net output #1: prob = 0.952702
I1003 02:07:12.400334 30882 solver.cpp:593]     Max_acc: 0.952802  with iter: 51000
I1003 02:07:13.218281 30882 solver.cpp:357] Iteration 58000 (0.950291 iter/s, 105.231s/100 iters), loss = 0.0042139
I1003 02:07:13.218329 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00407868 (* 1 = 0.00407868 loss)
I1003 02:07:13.218339 30882 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I1003 02:08:35.973300 30882 solver.cpp:357] Iteration 58100 (1.20836 iter/s, 82.757s/100 iters), loss = 0.0272415
I1003 02:08:35.973398 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.047633 (* 1 = 0.047633 loss)
I1003 02:08:35.973408 30882 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I1003 02:09:58.736840 30882 solver.cpp:357] Iteration 58200 (1.20823 iter/s, 82.7656s/100 iters), loss = 0.00615975
I1003 02:09:58.736960 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00501655 (* 1 = 0.00501655 loss)
I1003 02:09:58.736971 30882 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I1003 02:09:59.154891 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:11:21.596287 30882 solver.cpp:357] Iteration 58300 (1.20683 iter/s, 82.8616s/100 iters), loss = 0.0189584
I1003 02:11:21.596426 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0151844 (* 1 = 0.0151844 loss)
I1003 02:11:21.596436 30882 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I1003 02:12:44.305032 30882 solver.cpp:357] Iteration 58400 (1.20903 iter/s, 82.7109s/100 iters), loss = 0.00478805
I1003 02:12:44.305135 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00488101 (* 1 = 0.00488101 loss)
I1003 02:12:44.305145 30882 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I1003 02:14:06.349742 30882 solver.cpp:514] Iteration 58500, Testing net (#0)
I1003 02:14:28.662431 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:14:28.750299 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.175111 (* 1 = 0.175111 loss)
I1003 02:14:28.750327 30882 solver.cpp:580]     Test net output #1: prob = 0.953802
I1003 02:14:28.750339 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_58500.caffemodel
I1003 02:14:29.159433 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_58500.solverstate
I1003 02:14:29.299713 30882 solver.cpp:593]     Max_acc: 0.953802  with iter: 58500
I1003 02:14:30.112416 30882 solver.cpp:357] Iteration 58500 (0.945087 iter/s, 105.81s/100 iters), loss = 0.00723314
I1003 02:14:30.112469 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0108028 (* 1 = 0.0108028 loss)
I1003 02:14:30.112480 30882 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I1003 02:15:45.570603 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:15:53.041654 30882 solver.cpp:357] Iteration 58600 (1.20581 iter/s, 82.9316s/100 iters), loss = 0.0110957
I1003 02:15:53.041716 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0148538 (* 1 = 0.0148538 loss)
I1003 02:15:53.041728 30882 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I1003 02:17:15.725186 30882 solver.cpp:357] Iteration 58700 (1.20939 iter/s, 82.686s/100 iters), loss = 0.00418322
I1003 02:17:15.725330 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00220086 (* 1 = 0.00220086 loss)
I1003 02:17:15.725340 30882 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I1003 02:18:38.449048 30882 solver.cpp:357] Iteration 58800 (1.20881 iter/s, 82.7263s/100 iters), loss = 0.00556724
I1003 02:18:38.449230 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00562426 (* 1 = 0.00562426 loss)
I1003 02:18:38.449246 30882 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I1003 02:20:01.144364 30882 solver.cpp:357] Iteration 58900 (1.20922 iter/s, 82.6977s/100 iters), loss = 0.00995229
I1003 02:20:01.144512 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0180581 (* 1 = 0.0180581 loss)
I1003 02:20:01.144526 30882 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I1003 02:21:08.404214 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:21:22.834394 30882 solver.cpp:514] Iteration 59000, Testing net (#0)
I1003 02:21:45.152458 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:21:45.240303 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.171921 (* 1 = 0.171921 loss)
I1003 02:21:45.240329 30882 solver.cpp:580]     Test net output #1: prob = 0.952702
I1003 02:21:45.240334 30882 solver.cpp:593]     Max_acc: 0.953802  with iter: 58500
I1003 02:21:46.059881 30882 solver.cpp:357] Iteration 59000 (0.953119 iter/s, 104.919s/100 iters), loss = 0.0205131
I1003 02:21:46.059931 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0383582 (* 1 = 0.0383582 loss)
I1003 02:21:46.059947 30882 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I1003 02:23:08.814285 30882 solver.cpp:357] Iteration 59100 (1.20836 iter/s, 82.757s/100 iters), loss = 0.00605719
I1003 02:23:08.814427 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00705206 (* 1 = 0.00705206 loss)
I1003 02:23:08.814437 30882 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I1003 02:24:31.643620 30882 solver.cpp:357] Iteration 59200 (1.20727 iter/s, 82.8318s/100 iters), loss = 0.0040049
I1003 02:24:31.643760 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00438249 (* 1 = 0.00438249 loss)
I1003 02:24:31.643770 30882 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I1003 02:25:54.383133 30882 solver.cpp:357] Iteration 59300 (1.20858 iter/s, 82.7421s/100 iters), loss = 0.00962439
I1003 02:25:54.383232 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0133285 (* 1 = 0.0133285 loss)
I1003 02:25:54.383242 30882 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I1003 02:26:53.823094 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:27:16.900928 30882 solver.cpp:357] Iteration 59400 (1.21182 iter/s, 82.5204s/100 iters), loss = 0.0124546
I1003 02:27:16.900991 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00927646 (* 1 = 0.00927646 loss)
I1003 02:27:16.901001 30882 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I1003 02:28:38.726013 30882 solver.cpp:514] Iteration 59500, Testing net (#0)
I1003 02:29:01.046764 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:29:01.135366 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.171814 (* 1 = 0.171814 loss)
I1003 02:29:01.135392 30882 solver.cpp:580]     Test net output #1: prob = 0.953302
I1003 02:29:01.135398 30882 solver.cpp:593]     Max_acc: 0.953802  with iter: 58500
I1003 02:29:01.957353 30882 solver.cpp:357] Iteration 59500 (0.951839 iter/s, 105.06s/100 iters), loss = 0.0040739
I1003 02:29:01.957412 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00476241 (* 1 = 0.00476241 loss)
I1003 02:29:01.957424 30882 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I1003 02:30:24.658959 30882 solver.cpp:357] Iteration 59600 (1.20913 iter/s, 82.7043s/100 iters), loss = 0.00831505
I1003 02:30:24.659107 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0105292 (* 1 = 0.0105292 loss)
I1003 02:30:24.659117 30882 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I1003 02:31:47.296015 30882 solver.cpp:357] Iteration 59700 (1.21019 iter/s, 82.6314s/100 iters), loss = 0.0031761
I1003 02:31:47.296133 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00150396 (* 1 = 0.00150396 loss)
I1003 02:31:47.296144 30882 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I1003 02:32:39.430596 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:33:10.022994 30882 solver.cpp:357] Iteration 59800 (1.20899 iter/s, 82.7134s/100 iters), loss = 0.0245467
I1003 02:33:10.023139 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00585923 (* 1 = 0.00585923 loss)
I1003 02:33:10.023150 30882 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I1003 02:34:32.769912 30882 solver.cpp:357] Iteration 59900 (1.20867 iter/s, 82.7353s/100 iters), loss = 0.00423256
I1003 02:34:32.770059 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00475181 (* 1 = 0.00475181 loss)
I1003 02:34:32.770071 30882 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I1003 02:35:54.693269 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_60000.caffemodel
I1003 02:35:55.100960 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_60000.solverstate
I1003 02:35:55.244232 30882 solver.cpp:514] Iteration 60000, Testing net (#0)
I1003 02:36:17.556358 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:36:17.644912 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172631 (* 1 = 0.172631 loss)
I1003 02:36:17.644937 30882 solver.cpp:580]     Test net output #1: prob = 0.953502
I1003 02:36:17.644943 30882 solver.cpp:593]     Max_acc: 0.953802  with iter: 58500
I1003 02:36:18.455639 30882 solver.cpp:357] Iteration 60000 (0.946312 iter/s, 105.673s/100 iters), loss = 0.00616901
I1003 02:36:18.455691 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.000808818 (* 1 = 0.000808818 loss)
I1003 02:36:18.455701 30882 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I1003 02:37:41.268328 30882 solver.cpp:357] Iteration 60100 (1.20766 iter/s, 82.8046s/100 iters), loss = 0.00666184
I1003 02:37:41.268427 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00872093 (* 1 = 0.00872093 loss)
I1003 02:37:41.268440 30882 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I1003 02:38:25.639809 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:39:04.181948 30882 solver.cpp:357] Iteration 60200 (1.20618 iter/s, 82.9067s/100 iters), loss = 0.00306285
I1003 02:39:04.182067 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00270659 (* 1 = 0.00270659 loss)
I1003 02:39:04.182078 30882 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I1003 02:40:27.023754 30882 solver.cpp:357] Iteration 60300 (1.20721 iter/s, 82.8359s/100 iters), loss = 0.00724594
I1003 02:40:27.023897 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00234703 (* 1 = 0.00234703 loss)
I1003 02:40:27.023908 30882 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I1003 02:41:49.773458 30882 solver.cpp:357] Iteration 60400 (1.20854 iter/s, 82.7446s/100 iters), loss = 0.00445129
I1003 02:41:49.773599 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0040113 (* 1 = 0.0040113 loss)
I1003 02:41:49.773612 30882 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I1003 02:43:11.843997 30882 solver.cpp:514] Iteration 60500, Testing net (#0)
I1003 02:43:34.161274 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:43:34.249481 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172436 (* 1 = 0.172436 loss)
I1003 02:43:34.249507 30882 solver.cpp:580]     Test net output #1: prob = 0.954002
I1003 02:43:34.249519 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_60500.caffemodel
I1003 02:43:34.661329 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_60500.solverstate
I1003 02:43:34.801607 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 02:43:35.614295 30882 solver.cpp:357] Iteration 60500 (0.944864 iter/s, 105.835s/100 iters), loss = 0.00340104
I1003 02:43:35.614351 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00275124 (* 1 = 0.00275124 loss)
I1003 02:43:35.614362 30882 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I1003 02:44:12.115571 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:44:58.348305 30882 solver.cpp:357] Iteration 60600 (1.20874 iter/s, 82.7305s/100 iters), loss = 0.00250974
I1003 02:44:58.350076 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00229654 (* 1 = 0.00229654 loss)
I1003 02:44:58.350090 30882 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I1003 02:46:21.158448 30882 solver.cpp:357] Iteration 60700 (1.20765 iter/s, 82.8054s/100 iters), loss = 0.00689053
I1003 02:46:21.158588 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104345 (* 1 = 0.0104345 loss)
I1003 02:46:21.158599 30882 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I1003 02:47:43.980545 30882 solver.cpp:357] Iteration 60800 (1.20745 iter/s, 82.8194s/100 iters), loss = 0.00474004
I1003 02:47:43.980684 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00602483 (* 1 = 0.00602483 loss)
I1003 02:47:43.980695 30882 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I1003 02:49:06.492208 30882 solver.cpp:357] Iteration 60900 (1.21198 iter/s, 82.5093s/100 iters), loss = 0.015158
I1003 02:49:06.492344 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0287144 (* 1 = 0.0287144 loss)
I1003 02:49:06.492355 30882 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I1003 02:49:35.170322 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:50:28.521924 30882 solver.cpp:514] Iteration 61000, Testing net (#0)
I1003 02:50:50.845046 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:50:50.933706 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.171707 (* 1 = 0.171707 loss)
I1003 02:50:50.933732 30882 solver.cpp:580]     Test net output #1: prob = 0.953502
I1003 02:50:50.933737 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 02:50:51.753623 30882 solver.cpp:357] Iteration 61000 (0.950038 iter/s, 105.259s/100 iters), loss = 0.00935291
I1003 02:50:51.753674 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00486435 (* 1 = 0.00486435 loss)
I1003 02:50:51.753686 30882 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I1003 02:52:14.551296 30882 solver.cpp:357] Iteration 61100 (1.20779 iter/s, 82.7961s/100 iters), loss = 0.00872728
I1003 02:52:14.551439 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0128466 (* 1 = 0.0128466 loss)
I1003 02:52:14.551448 30882 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I1003 02:53:37.214493 30882 solver.cpp:357] Iteration 61200 (1.20975 iter/s, 82.6617s/100 iters), loss = 0.0123983
I1003 02:53:37.214601 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00311154 (* 1 = 0.00311154 loss)
I1003 02:53:37.214612 30882 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I1003 02:55:00.108250 30882 solver.cpp:357] Iteration 61300 (1.20638 iter/s, 82.8925s/100 iters), loss = 0.0123682
I1003 02:55:00.108371 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0142227 (* 1 = 0.0142227 loss)
I1003 02:55:00.108381 30882 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I1003 02:55:21.254003 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:56:22.798496 30882 solver.cpp:357] Iteration 61400 (1.20935 iter/s, 82.6891s/100 iters), loss = 0.0105778
I1003 02:56:22.798645 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0139976 (* 1 = 0.0139976 loss)
I1003 02:56:22.798655 30882 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I1003 02:57:44.673537 30882 solver.cpp:514] Iteration 61500, Testing net (#0)
I1003 02:58:06.994074 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 02:58:07.082840 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172462 (* 1 = 0.172462 loss)
I1003 02:58:07.082866 30882 solver.cpp:580]     Test net output #1: prob = 0.953602
I1003 02:58:07.082872 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 02:58:07.904747 30882 solver.cpp:357] Iteration 61500 (0.951429 iter/s, 105.105s/100 iters), loss = 0.00359444
I1003 02:58:07.904796 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00440481 (* 1 = 0.00440481 loss)
I1003 02:58:07.904812 30882 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I1003 02:59:30.775878 30882 solver.cpp:357] Iteration 61600 (1.2067 iter/s, 82.8703s/100 iters), loss = 0.00405147
I1003 02:59:30.776363 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0052536 (* 1 = 0.0052536 loss)
I1003 02:59:30.776378 30882 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I1003 03:00:53.372017 30882 solver.cpp:357] Iteration 61700 (1.21073 iter/s, 82.595s/100 iters), loss = 0.00965053
I1003 03:00:53.372161 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00244458 (* 1 = 0.00244458 loss)
I1003 03:00:53.372172 30882 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I1003 03:01:06.598835 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:02:16.276046 30882 solver.cpp:357] Iteration 61800 (1.20622 iter/s, 82.9033s/100 iters), loss = 0.00667883
I1003 03:02:16.276193 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00519798 (* 1 = 0.00519798 loss)
I1003 03:02:16.276204 30882 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I1003 03:03:38.835137 30882 solver.cpp:357] Iteration 61900 (1.21126 iter/s, 82.5584s/100 iters), loss = 0.00465161
I1003 03:03:38.835278 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00568517 (* 1 = 0.00568517 loss)
I1003 03:03:38.835289 30882 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I1003 03:05:00.752040 30882 solver.cpp:514] Iteration 62000, Testing net (#0)
I1003 03:05:23.067154 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:05:23.155102 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172205 (* 1 = 0.172205 loss)
I1003 03:05:23.155128 30882 solver.cpp:580]     Test net output #1: prob = 0.953102
I1003 03:05:23.155134 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 03:05:23.975239 30882 solver.cpp:357] Iteration 62000 (0.951104 iter/s, 105.141s/100 iters), loss = 0.0132049
I1003 03:05:23.975291 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00476672 (* 1 = 0.00476672 loss)
I1003 03:05:23.975303 30882 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I1003 03:06:46.653357 30882 solver.cpp:357] Iteration 62100 (1.20923 iter/s, 82.697s/100 iters), loss = 0.00701395
I1003 03:06:46.653494 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00918361 (* 1 = 0.00918361 loss)
I1003 03:06:46.653509 30882 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I1003 03:06:52.039167 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:08:09.369019 30882 solver.cpp:357] Iteration 62200 (1.20872 iter/s, 82.7321s/100 iters), loss = 0.00607648
I1003 03:08:09.369112 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00487403 (* 1 = 0.00487403 loss)
I1003 03:08:09.369124 30882 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I1003 03:09:32.220525 30882 solver.cpp:357] Iteration 62300 (1.20677 iter/s, 82.866s/100 iters), loss = 0.00363467
I1003 03:09:32.220659 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00224786 (* 1 = 0.00224786 loss)
I1003 03:09:32.220669 30882 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I1003 03:10:54.844156 30882 solver.cpp:357] Iteration 62400 (1.21012 iter/s, 82.6363s/100 iters), loss = 0.00541234
I1003 03:10:54.844280 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00439981 (* 1 = 0.00439981 loss)
I1003 03:10:54.844290 30882 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I1003 03:12:15.142067 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:12:16.790077 30882 solver.cpp:514] Iteration 62500, Testing net (#0)
I1003 03:12:39.096217 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:12:39.183848 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174736 (* 1 = 0.174736 loss)
I1003 03:12:39.183876 30882 solver.cpp:580]     Test net output #1: prob = 0.953902
I1003 03:12:39.183881 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 03:12:40.007364 30882 solver.cpp:357] Iteration 62500 (0.950775 iter/s, 105.177s/100 iters), loss = 0.00595372
I1003 03:12:40.007413 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00446813 (* 1 = 0.00446813 loss)
I1003 03:12:40.007424 30882 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I1003 03:14:02.824652 30882 solver.cpp:357] Iteration 62600 (1.20733 iter/s, 82.8271s/100 iters), loss = 0.00476774
I1003 03:14:02.824800 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0032639 (* 1 = 0.0032639 loss)
I1003 03:14:02.824810 30882 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I1003 03:15:25.676005 30882 solver.cpp:357] Iteration 62700 (1.20685 iter/s, 82.86s/100 iters), loss = 0.0133888
I1003 03:15:25.676151 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0172575 (* 1 = 0.0172575 loss)
I1003 03:15:25.676160 30882 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I1003 03:16:48.432683 30882 solver.cpp:357] Iteration 62800 (1.20825 iter/s, 82.7645s/100 iters), loss = 0.00784065
I1003 03:16:48.432780 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0134427 (* 1 = 0.0134427 loss)
I1003 03:16:48.432790 30882 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I1003 03:18:01.127494 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:18:11.083228 30882 solver.cpp:357] Iteration 62900 (1.20981 iter/s, 82.6577s/100 iters), loss = 0.0122123
I1003 03:18:11.083290 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0114461 (* 1 = 0.0114461 loss)
I1003 03:18:11.083299 30882 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I1003 03:19:33.137362 30882 solver.cpp:514] Iteration 63000, Testing net (#0)
I1003 03:19:55.449023 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:19:55.537392 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.172099 (* 1 = 0.172099 loss)
I1003 03:19:55.537418 30882 solver.cpp:580]     Test net output #1: prob = 0.953502
I1003 03:19:55.537425 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 03:19:56.359017 30882 solver.cpp:357] Iteration 63000 (0.949811 iter/s, 105.284s/100 iters), loss = 0.00321139
I1003 03:19:56.359067 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00141383 (* 1 = 0.00141383 loss)
I1003 03:19:56.359079 30882 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I1003 03:21:19.212642 30882 solver.cpp:357] Iteration 63100 (1.20686 iter/s, 82.8596s/100 iters), loss = 0.00604281
I1003 03:21:19.212767 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00636374 (* 1 = 0.00636374 loss)
I1003 03:21:19.212777 30882 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I1003 03:22:42.016214 30882 solver.cpp:357] Iteration 63200 (1.2076 iter/s, 82.809s/100 iters), loss = 0.013011
I1003 03:22:42.016353 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00459917 (* 1 = 0.00459917 loss)
I1003 03:22:42.016363 30882 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I1003 03:23:47.055830 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:24:04.803306 30882 solver.cpp:357] Iteration 63300 (1.20784 iter/s, 82.7921s/100 iters), loss = 0.00339166
I1003 03:24:04.803364 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00303748 (* 1 = 0.00303748 loss)
I1003 03:24:04.803375 30882 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I1003 03:25:27.550271 30882 solver.cpp:357] Iteration 63400 (1.20843 iter/s, 82.7518s/100 iters), loss = 0.00739432
I1003 03:25:27.550410 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00764154 (* 1 = 0.00764154 loss)
I1003 03:25:27.550422 30882 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I1003 03:26:49.329952 30882 solver.cpp:514] Iteration 63500, Testing net (#0)
I1003 03:27:11.644187 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:27:11.732188 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.176259 (* 1 = 0.176259 loss)
I1003 03:27:11.732214 30882 solver.cpp:580]     Test net output #1: prob = 0.952702
I1003 03:27:11.732221 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 03:27:12.552114 30882 solver.cpp:357] Iteration 63500 (0.952313 iter/s, 105.008s/100 iters), loss = 0.00236416
I1003 03:27:12.552165 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00190854 (* 1 = 0.00190854 loss)
I1003 03:27:12.552181 30882 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I1003 03:28:35.210532 30882 solver.cpp:357] Iteration 63600 (1.20974 iter/s, 82.6627s/100 iters), loss = 0.00281747
I1003 03:28:35.210726 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00228555 (* 1 = 0.00228555 loss)
I1003 03:28:35.210775 30882 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I1003 03:29:32.260406 30889 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:29:57.904129 30882 solver.cpp:357] Iteration 63700 (1.20923 iter/s, 82.6975s/100 iters), loss = 0.00500074
I1003 03:29:57.904191 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00296417 (* 1 = 0.00296417 loss)
I1003 03:29:57.904201 30882 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I1003 03:31:20.573812 30882 solver.cpp:357] Iteration 63800 (1.20958 iter/s, 82.6736s/100 iters), loss = 0.00425751
I1003 03:31:20.573972 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.0065011 (* 1 = 0.0065011 loss)
I1003 03:31:20.573983 30882 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I1003 03:32:43.369798 30882 solver.cpp:357] Iteration 63900 (1.20773 iter/s, 82.7997s/100 iters), loss = 0.00784241
I1003 03:32:43.369966 30882 solver.cpp:376]     Train net output #0: Softmax1 = 0.00953217 (* 1 = 0.00953217 loss)
I1003 03:32:43.369976 30882 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I1003 03:34:05.238481 30882 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_WRN_20_iter_64000.caffemodel
I1003 03:34:05.646478 30882 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_WRN_20_iter_64000.solverstate
I1003 03:34:05.920462 30882 solver.cpp:472] Iteration 64000, loss = 0.00154943
I1003 03:34:05.920511 30882 solver.cpp:514] Iteration 64000, Testing net (#0)
I1003 03:34:28.225563 30899 data_layer.cpp:73] Restarting data prefetching from start.
I1003 03:34:28.313807 30882 solver.cpp:580]     Test net output #0: Softmax1 = 0.174442 (* 1 = 0.174442 loss)
I1003 03:34:28.313833 30882 solver.cpp:580]     Test net output #1: prob = 0.952602
I1003 03:34:28.313839 30882 solver.cpp:593]     Max_acc: 0.954002  with iter: 60500
I1003 03:34:28.313848 30882 solver.cpp:479] Optimization Done.
I1003 03:34:28.313851 30882 caffe.cpp:326] Optimization Done.
