WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 11:12:19.014246  2068 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0825 11:12:19.014390  2068 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0825 11:12:19.014395  2068 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0825 11:12:19.014398  2068 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0825 11:12:19.014402  2068 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0825 11:12:19.014405  2068 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0825 11:12:19.014463  2068 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0825 11:12:19.014629  2068 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0825 11:12:19.032928  2068 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0825 11:12:19.032980  2068 caffe.cpp:269] Using GPUs 0
I0825 11:12:19.039502  2068 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0825 11:12:20.040828  2068 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0825 11:12:20.041102  2068 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0825 11:12:20.258633  2068 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_56.prototxt"
test_net: "./test_ResNet_56.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_56"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 2
type: "Nesterov"
I0825 11:12:20.259270  2068 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_56.prototxt
I0825 11:12:20.261260  2068 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_56.prototxt
I0825 11:12:20.261342  2068 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:12:20.261809  2068 net.cpp:390] layer_param.include_size():1
I0825 11:12:20.261868  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.261905  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.261936  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.261968  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.261999  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262030  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262059  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262094  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262125  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262157  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262192  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262224  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262255  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262287  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262317  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262367  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262399  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262431  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262462  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262495  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262526  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262557  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262588  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262619  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262650  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262681  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262712  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262743  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262786  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262818  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262851  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262883  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262914  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.262945  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.262976  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263008  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263038  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263070  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263113  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263144  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263175  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263206  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263237  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263268  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263299  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263330  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263361  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263393  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263424  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263455  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263487  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263519  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263550  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263581  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263612  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263643  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263674  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263705  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263736  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263767  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263798  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263829  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263860  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263891  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263921  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.263953  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.263984  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264015  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264047  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264081  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264111  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264142  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264173  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264205  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264236  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264267  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264298  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264329  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264360  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264391  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264422  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264453  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264483  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264515  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264546  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264577  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264608  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264643  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264674  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264708  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264739  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264770  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264801  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264832  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264863  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264894  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264925  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.264962  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.264993  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265025  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265056  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265089  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265120  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265151  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265182  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265213  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265244  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265275  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265305  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265338  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265369  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265401  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265431  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265463  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265494  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265525  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265556  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265588  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265619  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265650  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265681  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265713  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265743  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265775  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265805  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265837  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265867  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265899  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265933  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.265964  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.265995  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266026  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266057  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266088  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266119  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266151  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266181  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266213  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266243  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266275  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266305  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266341  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266373  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266404  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266435  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266470  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266500  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266532  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266564  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266595  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266628  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266659  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266690  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266721  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266752  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266791  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266821  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266854  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266887  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266919  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.266950  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.266981  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267012  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267045  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267074  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267107  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267138  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267169  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267205  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267237  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267268  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267299  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267330  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267362  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267395  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267426  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267457  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267488  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267518  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267550  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267581  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267612  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267643  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267675  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267706  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267737  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267768  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267804  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267838  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267873  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267906  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.267940  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.267971  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268003  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268035  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268066  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268097  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268129  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268159  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268191  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268223  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268254  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268285  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268316  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268350  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268383  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268414  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268446  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268476  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268508  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268539  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268570  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268601  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268633  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268671  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268702  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268733  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268765  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268796  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268828  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268858  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268889  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268920  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.268952  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.268983  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269016  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269045  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269078  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269109  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269141  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269171  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269202  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269234  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269268  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269299  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269330  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269361  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269392  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269423  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269454  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269485  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269517  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269548  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269582  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269613  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269644  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269676  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269707  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269738  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269769  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269800  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269834  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269863  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269894  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269925  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.269956  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.269987  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270018  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270050  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270081  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270112  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270143  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270177  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270210  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270239  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270272  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270303  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270344  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270377  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270409  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270440  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270473  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270516  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270548  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270579  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270611  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270642  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270673  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270704  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270735  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270766  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270798  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270829  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270860  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270891  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270923  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.270954  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.270987  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271018  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271049  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271080  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271111  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271142  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271173  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271204  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271236  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271267  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271299  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271329  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271363  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271394  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271425  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271456  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271486  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271517  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271548  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271581  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271613  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271644  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271675  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271706  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271739  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271769  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271800  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271831  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271863  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271894  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271925  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.271955  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.271987  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272017  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272054  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272085  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272117  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272147  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272181  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272210  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272243  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272274  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272305  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272336  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272373  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272405  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272436  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272469  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272500  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272531  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272562  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272593  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272624  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272655  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272687  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272718  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272749  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272780  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272814  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272843  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272876  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272907  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.272939  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.272970  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273002  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273033  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273066  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273097  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273128  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273159  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273190  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273221  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273253  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273285  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273316  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273347  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273378  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273409  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273442  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273474  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273506  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273536  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273568  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273599  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273630  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273661  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273694  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273723  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273756  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273785  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273818  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273849  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273885  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273916  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.273947  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.273977  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274010  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274042  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274073  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274104  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274135  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274166  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274204  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274235  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274267  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274298  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274332  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274365  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274396  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274427  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274458  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274489  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274523  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274552  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274585  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274616  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274646  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274678  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274710  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274741  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274773  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274804  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274837  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274868  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274899  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274930  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.274961  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.274993  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275025  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275055  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275087  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275118  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275151  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275180  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275213  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275243  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275274  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275306  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275339  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275370  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275403  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275432  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275465  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275494  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275527  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275562  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275593  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275624  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275655  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275686  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275718  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275753  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275784  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275815  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275846  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275877  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275910  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.275941  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.275972  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276005  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276036  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276073  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276105  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276136  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276167  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276198  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276229  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276260  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276293  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276324  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276355  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276386  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276417  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276448  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276479  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276510  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276543  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276574  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276607  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276638  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276669  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276700  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276731  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276762  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276793  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276823  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276855  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276886  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276918  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.276948  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.276980  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277011  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277042  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277073  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277106  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277137  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277169  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277199  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277232  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277263  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277295  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277326  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277357  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277389  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277420  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277451  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277482  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277513  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277544  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277580  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277611  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277642  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277675  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277706  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277737  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277770  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277801  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277832  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277863  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277901  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277933  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.277963  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.277995  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.278026  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.278059  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:20.278090  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:20.279774  2068 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution31"
  top: "Convolution31"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Convolution31"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise15"
  type: "Eltwise"
  bottom: "Eltwise14"
  bottom: "Convolution32"
  top: "Eltwise15"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU31"
  type: "ReLU"
  bottom: "Eltwise
I0825 11:12:20.306347  2068 layer_factory.hpp:77] Creating layer Data1
I0825 11:12:20.306617  2068 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0825 11:12:20.306713  2068 net.cpp:128] Creating Layer Data1
I0825 11:12:20.306756  2068 net.cpp:522] Data1 -> Data1
I0825 11:12:20.306820  2068 net.cpp:522] Data1 -> Data2
I0825 11:12:20.308640  2068 data_layer.cpp:45] output data size: 64,3,32,32
I0825 11:12:20.336519  2068 net.cpp:172] Setting up Data1
I0825 11:12:20.336652  2068 net.cpp:186] Top shape: 64 3 32 32 (196608)
I0825 11:12:20.336693  2068 net.cpp:186] Top shape: 64 (64)
I0825 11:12:20.336724  2068 net.cpp:194] Memory required for data: 786688
I0825 11:12:20.336767  2068 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:12:20.336832  2068 net.cpp:128] Creating Layer Convolution1
I0825 11:12:20.336869  2068 net.cpp:558] Convolution1 <- Data1
I0825 11:12:20.336920  2068 net.cpp:522] Convolution1 -> Convolution1
I0825 11:12:21.631618  2068 net.cpp:172] Setting up Convolution1
I0825 11:12:21.631737  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.631759  2068 net.cpp:194] Memory required for data: 4980992
I0825 11:12:21.631816  2068 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:12:21.631850  2068 net.cpp:128] Creating Layer BatchNorm1
I0825 11:12:21.631870  2068 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:12:21.631893  2068 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:12:21.632151  2068 net.cpp:172] Setting up BatchNorm1
I0825 11:12:21.632175  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.632206  2068 net.cpp:194] Memory required for data: 9175296
I0825 11:12:21.632236  2068 layer_factory.hpp:77] Creating layer Scale1
I0825 11:12:21.632261  2068 net.cpp:128] Creating Layer Scale1
I0825 11:12:21.632279  2068 net.cpp:558] Scale1 <- Convolution1
I0825 11:12:21.632299  2068 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:12:21.632371  2068 layer_factory.hpp:77] Creating layer Scale1
I0825 11:12:21.632519  2068 net.cpp:172] Setting up Scale1
I0825 11:12:21.632541  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.632560  2068 net.cpp:194] Memory required for data: 13369600
I0825 11:12:21.632582  2068 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:12:21.632618  2068 net.cpp:128] Creating Layer ReLU1
I0825 11:12:21.632637  2068 net.cpp:558] ReLU1 <- Convolution1
I0825 11:12:21.632656  2068 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:12:21.633347  2068 net.cpp:172] Setting up ReLU1
I0825 11:12:21.633378  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.633396  2068 net.cpp:194] Memory required for data: 17563904
I0825 11:12:21.633414  2068 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:12:21.633437  2068 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:12:21.633456  2068 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:12:21.633477  2068 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:12:21.633502  2068 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:12:21.633568  2068 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:12:21.633589  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.633608  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.633625  2068 net.cpp:194] Memory required for data: 25952512
I0825 11:12:21.633643  2068 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:12:21.633671  2068 net.cpp:128] Creating Layer Convolution2
I0825 11:12:21.633689  2068 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:12:21.633710  2068 net.cpp:522] Convolution2 -> Convolution2
I0825 11:12:21.638247  2068 net.cpp:172] Setting up Convolution2
I0825 11:12:21.638298  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.638315  2068 net.cpp:194] Memory required for data: 30146816
I0825 11:12:21.638350  2068 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:12:21.638375  2068 net.cpp:128] Creating Layer BatchNorm2
I0825 11:12:21.638392  2068 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:12:21.638412  2068 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:12:21.638655  2068 net.cpp:172] Setting up BatchNorm2
I0825 11:12:21.638676  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.638692  2068 net.cpp:194] Memory required for data: 34341120
I0825 11:12:21.638716  2068 layer_factory.hpp:77] Creating layer Scale2
I0825 11:12:21.638737  2068 net.cpp:128] Creating Layer Scale2
I0825 11:12:21.638754  2068 net.cpp:558] Scale2 <- Convolution2
I0825 11:12:21.638773  2068 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:12:21.638833  2068 layer_factory.hpp:77] Creating layer Scale2
I0825 11:12:21.638972  2068 net.cpp:172] Setting up Scale2
I0825 11:12:21.638993  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.639009  2068 net.cpp:194] Memory required for data: 38535424
I0825 11:12:21.639030  2068 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:12:21.639050  2068 net.cpp:128] Creating Layer ReLU2
I0825 11:12:21.639073  2068 net.cpp:558] ReLU2 <- Convolution2
I0825 11:12:21.639092  2068 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:12:21.640437  2068 net.cpp:172] Setting up ReLU2
I0825 11:12:21.640467  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.640485  2068 net.cpp:194] Memory required for data: 42729728
I0825 11:12:21.640502  2068 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:12:21.640528  2068 net.cpp:128] Creating Layer Convolution3
I0825 11:12:21.640558  2068 net.cpp:558] Convolution3 <- Convolution2
I0825 11:12:21.640579  2068 net.cpp:522] Convolution3 -> Convolution3
I0825 11:12:21.653664  2068 net.cpp:172] Setting up Convolution3
I0825 11:12:21.653709  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.653728  2068 net.cpp:194] Memory required for data: 46924032
I0825 11:12:21.653751  2068 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:12:21.653772  2068 net.cpp:128] Creating Layer BatchNorm3
I0825 11:12:21.653790  2068 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:12:21.653810  2068 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:12:21.654053  2068 net.cpp:172] Setting up BatchNorm3
I0825 11:12:21.654088  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.654105  2068 net.cpp:194] Memory required for data: 51118336
I0825 11:12:21.654132  2068 layer_factory.hpp:77] Creating layer Scale3
I0825 11:12:21.654153  2068 net.cpp:128] Creating Layer Scale3
I0825 11:12:21.654170  2068 net.cpp:558] Scale3 <- Convolution3
I0825 11:12:21.654189  2068 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:12:21.654249  2068 layer_factory.hpp:77] Creating layer Scale3
I0825 11:12:21.654402  2068 net.cpp:172] Setting up Scale3
I0825 11:12:21.654424  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.654441  2068 net.cpp:194] Memory required for data: 55312640
I0825 11:12:21.654464  2068 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:12:21.654485  2068 net.cpp:128] Creating Layer Eltwise1
I0825 11:12:21.654502  2068 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:12:21.654520  2068 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:12:21.654541  2068 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:12:21.654589  2068 net.cpp:172] Setting up Eltwise1
I0825 11:12:21.654611  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.654628  2068 net.cpp:194] Memory required for data: 59506944
I0825 11:12:21.654644  2068 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:12:21.654664  2068 net.cpp:128] Creating Layer ReLU3
I0825 11:12:21.654680  2068 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:12:21.654700  2068 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:12:21.657905  2068 net.cpp:172] Setting up ReLU3
I0825 11:12:21.657946  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.657964  2068 net.cpp:194] Memory required for data: 63701248
I0825 11:12:21.657982  2068 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:12:21.658004  2068 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:12:21.658021  2068 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:12:21.658042  2068 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:12:21.658066  2068 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:12:21.658130  2068 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:12:21.658152  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.658171  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.658188  2068 net.cpp:194] Memory required for data: 72089856
I0825 11:12:21.658205  2068 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:12:21.658231  2068 net.cpp:128] Creating Layer Convolution4
I0825 11:12:21.658248  2068 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:12:21.658268  2068 net.cpp:522] Convolution4 -> Convolution4
I0825 11:12:21.671197  2068 net.cpp:172] Setting up Convolution4
I0825 11:12:21.671241  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.671257  2068 net.cpp:194] Memory required for data: 76284160
I0825 11:12:21.671281  2068 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:12:21.671303  2068 net.cpp:128] Creating Layer BatchNorm4
I0825 11:12:21.671320  2068 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:12:21.671340  2068 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:12:21.671586  2068 net.cpp:172] Setting up BatchNorm4
I0825 11:12:21.671608  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.671633  2068 net.cpp:194] Memory required for data: 80478464
I0825 11:12:21.671656  2068 layer_factory.hpp:77] Creating layer Scale4
I0825 11:12:21.671677  2068 net.cpp:128] Creating Layer Scale4
I0825 11:12:21.671694  2068 net.cpp:558] Scale4 <- Convolution4
I0825 11:12:21.671712  2068 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:12:21.671773  2068 layer_factory.hpp:77] Creating layer Scale4
I0825 11:12:21.671917  2068 net.cpp:172] Setting up Scale4
I0825 11:12:21.671939  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.671955  2068 net.cpp:194] Memory required for data: 84672768
I0825 11:12:21.671977  2068 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:12:21.672004  2068 net.cpp:128] Creating Layer ReLU4
I0825 11:12:21.672021  2068 net.cpp:558] ReLU4 <- Convolution4
I0825 11:12:21.672040  2068 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:12:21.673527  2068 net.cpp:172] Setting up ReLU4
I0825 11:12:21.673555  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.673573  2068 net.cpp:194] Memory required for data: 88867072
I0825 11:12:21.673589  2068 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:12:21.673614  2068 net.cpp:128] Creating Layer Convolution5
I0825 11:12:21.673632  2068 net.cpp:558] Convolution5 <- Convolution4
I0825 11:12:21.673652  2068 net.cpp:522] Convolution5 -> Convolution5
I0825 11:12:21.680537  2068 net.cpp:172] Setting up Convolution5
I0825 11:12:21.680579  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.680596  2068 net.cpp:194] Memory required for data: 93061376
I0825 11:12:21.680619  2068 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:12:21.680642  2068 net.cpp:128] Creating Layer BatchNorm5
I0825 11:12:21.680660  2068 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:12:21.680680  2068 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:12:21.680935  2068 net.cpp:172] Setting up BatchNorm5
I0825 11:12:21.680958  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.680974  2068 net.cpp:194] Memory required for data: 97255680
I0825 11:12:21.681001  2068 layer_factory.hpp:77] Creating layer Scale5
I0825 11:12:21.681020  2068 net.cpp:128] Creating Layer Scale5
I0825 11:12:21.681037  2068 net.cpp:558] Scale5 <- Convolution5
I0825 11:12:21.681058  2068 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:12:21.681120  2068 layer_factory.hpp:77] Creating layer Scale5
I0825 11:12:21.681270  2068 net.cpp:172] Setting up Scale5
I0825 11:12:21.681291  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.681306  2068 net.cpp:194] Memory required for data: 101449984
I0825 11:12:21.681327  2068 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:12:21.681347  2068 net.cpp:128] Creating Layer Eltwise2
I0825 11:12:21.681363  2068 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:12:21.681380  2068 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:12:21.681398  2068 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:12:21.681442  2068 net.cpp:172] Setting up Eltwise2
I0825 11:12:21.681462  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.681478  2068 net.cpp:194] Memory required for data: 105644288
I0825 11:12:21.681493  2068 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:12:21.681510  2068 net.cpp:128] Creating Layer ReLU5
I0825 11:12:21.681526  2068 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:12:21.681543  2068 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:12:21.681815  2068 net.cpp:172] Setting up ReLU5
I0825 11:12:21.681843  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.681857  2068 net.cpp:194] Memory required for data: 109838592
I0825 11:12:21.681874  2068 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:12:21.681895  2068 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:12:21.681913  2068 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:12:21.681931  2068 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:12:21.681952  2068 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:12:21.682026  2068 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:12:21.682047  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.682065  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.682081  2068 net.cpp:194] Memory required for data: 118227200
I0825 11:12:21.682098  2068 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:12:21.682123  2068 net.cpp:128] Creating Layer Convolution6
I0825 11:12:21.682144  2068 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:12:21.682168  2068 net.cpp:522] Convolution6 -> Convolution6
I0825 11:12:21.687299  2068 net.cpp:172] Setting up Convolution6
I0825 11:12:21.687360  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.687376  2068 net.cpp:194] Memory required for data: 122421504
I0825 11:12:21.687402  2068 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:12:21.687422  2068 net.cpp:128] Creating Layer BatchNorm6
I0825 11:12:21.687438  2068 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:12:21.687461  2068 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:12:21.687722  2068 net.cpp:172] Setting up BatchNorm6
I0825 11:12:21.687744  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.687760  2068 net.cpp:194] Memory required for data: 126615808
I0825 11:12:21.687783  2068 layer_factory.hpp:77] Creating layer Scale6
I0825 11:12:21.687804  2068 net.cpp:128] Creating Layer Scale6
I0825 11:12:21.687821  2068 net.cpp:558] Scale6 <- Convolution6
I0825 11:12:21.687841  2068 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:12:21.687903  2068 layer_factory.hpp:77] Creating layer Scale6
I0825 11:12:21.688057  2068 net.cpp:172] Setting up Scale6
I0825 11:12:21.688081  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.688097  2068 net.cpp:194] Memory required for data: 130810112
I0825 11:12:21.688117  2068 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:12:21.688138  2068 net.cpp:128] Creating Layer ReLU6
I0825 11:12:21.688154  2068 net.cpp:558] ReLU6 <- Convolution6
I0825 11:12:21.688172  2068 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:12:21.689431  2068 net.cpp:172] Setting up ReLU6
I0825 11:12:21.689474  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.689491  2068 net.cpp:194] Memory required for data: 135004416
I0825 11:12:21.689508  2068 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:12:21.689535  2068 net.cpp:128] Creating Layer Convolution7
I0825 11:12:21.689553  2068 net.cpp:558] Convolution7 <- Convolution6
I0825 11:12:21.689574  2068 net.cpp:522] Convolution7 -> Convolution7
I0825 11:12:21.697849  2068 net.cpp:172] Setting up Convolution7
I0825 11:12:21.697896  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.697913  2068 net.cpp:194] Memory required for data: 139198720
I0825 11:12:21.697937  2068 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:12:21.697962  2068 net.cpp:128] Creating Layer BatchNorm7
I0825 11:12:21.697979  2068 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:12:21.697999  2068 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:12:21.698266  2068 net.cpp:172] Setting up BatchNorm7
I0825 11:12:21.698288  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.698305  2068 net.cpp:194] Memory required for data: 143393024
I0825 11:12:21.698329  2068 layer_factory.hpp:77] Creating layer Scale7
I0825 11:12:21.698360  2068 net.cpp:128] Creating Layer Scale7
I0825 11:12:21.698379  2068 net.cpp:558] Scale7 <- Convolution7
I0825 11:12:21.698398  2068 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:12:21.698463  2068 layer_factory.hpp:77] Creating layer Scale7
I0825 11:12:21.698618  2068 net.cpp:172] Setting up Scale7
I0825 11:12:21.698642  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.698658  2068 net.cpp:194] Memory required for data: 147587328
I0825 11:12:21.698680  2068 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:12:21.698701  2068 net.cpp:128] Creating Layer Eltwise3
I0825 11:12:21.698726  2068 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:12:21.698745  2068 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:12:21.698765  2068 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:12:21.698812  2068 net.cpp:172] Setting up Eltwise3
I0825 11:12:21.698833  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.698849  2068 net.cpp:194] Memory required for data: 151781632
I0825 11:12:21.698865  2068 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:12:21.698884  2068 net.cpp:128] Creating Layer ReLU7
I0825 11:12:21.698900  2068 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:12:21.698918  2068 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:12:21.699990  2068 net.cpp:172] Setting up ReLU7
I0825 11:12:21.700018  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.700036  2068 net.cpp:194] Memory required for data: 155975936
I0825 11:12:21.700052  2068 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:12:21.700073  2068 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:12:21.700093  2068 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:12:21.700114  2068 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:12:21.700134  2068 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:12:21.700201  2068 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:12:21.700222  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.700242  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.700258  2068 net.cpp:194] Memory required for data: 164364544
I0825 11:12:21.700273  2068 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:12:21.700299  2068 net.cpp:128] Creating Layer Convolution8
I0825 11:12:21.700317  2068 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:12:21.700340  2068 net.cpp:522] Convolution8 -> Convolution8
I0825 11:12:21.706943  2068 net.cpp:172] Setting up Convolution8
I0825 11:12:21.706986  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.707003  2068 net.cpp:194] Memory required for data: 168558848
I0825 11:12:21.707027  2068 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:12:21.707047  2068 net.cpp:128] Creating Layer BatchNorm8
I0825 11:12:21.707063  2068 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:12:21.707085  2068 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:12:21.707351  2068 net.cpp:172] Setting up BatchNorm8
I0825 11:12:21.707373  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.707389  2068 net.cpp:194] Memory required for data: 172753152
I0825 11:12:21.707413  2068 layer_factory.hpp:77] Creating layer Scale8
I0825 11:12:21.707433  2068 net.cpp:128] Creating Layer Scale8
I0825 11:12:21.707450  2068 net.cpp:558] Scale8 <- Convolution8
I0825 11:12:21.707471  2068 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:12:21.707531  2068 layer_factory.hpp:77] Creating layer Scale8
I0825 11:12:21.707690  2068 net.cpp:172] Setting up Scale8
I0825 11:12:21.707711  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.707727  2068 net.cpp:194] Memory required for data: 176947456
I0825 11:12:21.707748  2068 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:12:21.707767  2068 net.cpp:128] Creating Layer ReLU8
I0825 11:12:21.707782  2068 net.cpp:558] ReLU8 <- Convolution8
I0825 11:12:21.707800  2068 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:12:21.709123  2068 net.cpp:172] Setting up ReLU8
I0825 11:12:21.709153  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.709170  2068 net.cpp:194] Memory required for data: 181141760
I0825 11:12:21.709187  2068 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:12:21.709213  2068 net.cpp:128] Creating Layer Convolution9
I0825 11:12:21.709233  2068 net.cpp:558] Convolution9 <- Convolution8
I0825 11:12:21.709257  2068 net.cpp:522] Convolution9 -> Convolution9
I0825 11:12:21.722398  2068 net.cpp:172] Setting up Convolution9
I0825 11:12:21.722442  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.722465  2068 net.cpp:194] Memory required for data: 185336064
I0825 11:12:21.722489  2068 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:12:21.722512  2068 net.cpp:128] Creating Layer BatchNorm9
I0825 11:12:21.722528  2068 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:12:21.722550  2068 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:12:21.722823  2068 net.cpp:172] Setting up BatchNorm9
I0825 11:12:21.722846  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.722862  2068 net.cpp:194] Memory required for data: 189530368
I0825 11:12:21.722887  2068 layer_factory.hpp:77] Creating layer Scale9
I0825 11:12:21.722906  2068 net.cpp:128] Creating Layer Scale9
I0825 11:12:21.722932  2068 net.cpp:558] Scale9 <- Convolution9
I0825 11:12:21.722951  2068 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:12:21.723017  2068 layer_factory.hpp:77] Creating layer Scale9
I0825 11:12:21.723173  2068 net.cpp:172] Setting up Scale9
I0825 11:12:21.723194  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.723210  2068 net.cpp:194] Memory required for data: 193724672
I0825 11:12:21.723232  2068 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:12:21.723254  2068 net.cpp:128] Creating Layer Eltwise4
I0825 11:12:21.723271  2068 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:12:21.723289  2068 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:12:21.723309  2068 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:12:21.723354  2068 net.cpp:172] Setting up Eltwise4
I0825 11:12:21.723376  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.723390  2068 net.cpp:194] Memory required for data: 197918976
I0825 11:12:21.723407  2068 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:12:21.723426  2068 net.cpp:128] Creating Layer ReLU9
I0825 11:12:21.723443  2068 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:12:21.723462  2068 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:12:21.726645  2068 net.cpp:172] Setting up ReLU9
I0825 11:12:21.726675  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.726692  2068 net.cpp:194] Memory required for data: 202113280
I0825 11:12:21.726709  2068 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:12:21.726730  2068 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:12:21.726747  2068 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:12:21.726769  2068 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:12:21.726791  2068 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:12:21.726860  2068 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:12:21.726881  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.726898  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.726914  2068 net.cpp:194] Memory required for data: 210501888
I0825 11:12:21.726932  2068 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:12:21.726959  2068 net.cpp:128] Creating Layer Convolution10
I0825 11:12:21.726977  2068 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:12:21.727000  2068 net.cpp:522] Convolution10 -> Convolution10
I0825 11:12:21.739874  2068 net.cpp:172] Setting up Convolution10
I0825 11:12:21.739922  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.739939  2068 net.cpp:194] Memory required for data: 214696192
I0825 11:12:21.739971  2068 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:12:21.739995  2068 net.cpp:128] Creating Layer BatchNorm10
I0825 11:12:21.740013  2068 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:12:21.740033  2068 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:12:21.740305  2068 net.cpp:172] Setting up BatchNorm10
I0825 11:12:21.740326  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.740344  2068 net.cpp:194] Memory required for data: 218890496
I0825 11:12:21.740366  2068 layer_factory.hpp:77] Creating layer Scale10
I0825 11:12:21.740388  2068 net.cpp:128] Creating Layer Scale10
I0825 11:12:21.740406  2068 net.cpp:558] Scale10 <- Convolution10
I0825 11:12:21.740432  2068 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:12:21.740496  2068 layer_factory.hpp:77] Creating layer Scale10
I0825 11:12:21.740654  2068 net.cpp:172] Setting up Scale10
I0825 11:12:21.740677  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.740694  2068 net.cpp:194] Memory required for data: 223084800
I0825 11:12:21.740715  2068 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:12:21.740734  2068 net.cpp:128] Creating Layer ReLU10
I0825 11:12:21.740751  2068 net.cpp:558] ReLU10 <- Convolution10
I0825 11:12:21.740772  2068 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:12:21.742182  2068 net.cpp:172] Setting up ReLU10
I0825 11:12:21.742234  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.742251  2068 net.cpp:194] Memory required for data: 227279104
I0825 11:12:21.742269  2068 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:12:21.742295  2068 net.cpp:128] Creating Layer Convolution11
I0825 11:12:21.742312  2068 net.cpp:558] Convolution11 <- Convolution10
I0825 11:12:21.742343  2068 net.cpp:522] Convolution11 -> Convolution11
I0825 11:12:21.749140  2068 net.cpp:172] Setting up Convolution11
I0825 11:12:21.749184  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.749200  2068 net.cpp:194] Memory required for data: 231473408
I0825 11:12:21.749223  2068 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:12:21.749248  2068 net.cpp:128] Creating Layer BatchNorm11
I0825 11:12:21.749265  2068 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:12:21.749287  2068 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:12:21.749560  2068 net.cpp:172] Setting up BatchNorm11
I0825 11:12:21.749583  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.749598  2068 net.cpp:194] Memory required for data: 235667712
I0825 11:12:21.749620  2068 layer_factory.hpp:77] Creating layer Scale11
I0825 11:12:21.749642  2068 net.cpp:128] Creating Layer Scale11
I0825 11:12:21.749660  2068 net.cpp:558] Scale11 <- Convolution11
I0825 11:12:21.749677  2068 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:12:21.749742  2068 layer_factory.hpp:77] Creating layer Scale11
I0825 11:12:21.749902  2068 net.cpp:172] Setting up Scale11
I0825 11:12:21.749922  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.749938  2068 net.cpp:194] Memory required for data: 239862016
I0825 11:12:21.749960  2068 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:12:21.749981  2068 net.cpp:128] Creating Layer Eltwise5
I0825 11:12:21.750000  2068 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:12:21.750016  2068 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:12:21.750036  2068 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:12:21.750082  2068 net.cpp:172] Setting up Eltwise5
I0825 11:12:21.750102  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.750118  2068 net.cpp:194] Memory required for data: 244056320
I0825 11:12:21.750133  2068 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:12:21.750151  2068 net.cpp:128] Creating Layer ReLU11
I0825 11:12:21.750169  2068 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:12:21.750187  2068 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:12:21.751293  2068 net.cpp:172] Setting up ReLU11
I0825 11:12:21.751323  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.751340  2068 net.cpp:194] Memory required for data: 248250624
I0825 11:12:21.751356  2068 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:12:21.751379  2068 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:12:21.751397  2068 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:12:21.751416  2068 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:12:21.751438  2068 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:12:21.751505  2068 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:12:21.751526  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.751550  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.751567  2068 net.cpp:194] Memory required for data: 256639232
I0825 11:12:21.751585  2068 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:12:21.751610  2068 net.cpp:128] Creating Layer Convolution12
I0825 11:12:21.751627  2068 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:12:21.751652  2068 net.cpp:522] Convolution12 -> Convolution12
I0825 11:12:21.766611  2068 net.cpp:172] Setting up Convolution12
I0825 11:12:21.766659  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.766676  2068 net.cpp:194] Memory required for data: 260833536
I0825 11:12:21.766700  2068 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:12:21.766739  2068 net.cpp:128] Creating Layer BatchNorm12
I0825 11:12:21.766757  2068 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:12:21.766777  2068 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:12:21.767058  2068 net.cpp:172] Setting up BatchNorm12
I0825 11:12:21.767083  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.767099  2068 net.cpp:194] Memory required for data: 265027840
I0825 11:12:21.767123  2068 layer_factory.hpp:77] Creating layer Scale12
I0825 11:12:21.767143  2068 net.cpp:128] Creating Layer Scale12
I0825 11:12:21.767160  2068 net.cpp:558] Scale12 <- Convolution12
I0825 11:12:21.767179  2068 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:12:21.767247  2068 layer_factory.hpp:77] Creating layer Scale12
I0825 11:12:21.767405  2068 net.cpp:172] Setting up Scale12
I0825 11:12:21.767427  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.767443  2068 net.cpp:194] Memory required for data: 269222144
I0825 11:12:21.767465  2068 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:12:21.767488  2068 net.cpp:128] Creating Layer ReLU12
I0825 11:12:21.767504  2068 net.cpp:558] ReLU12 <- Convolution12
I0825 11:12:21.767525  2068 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0825 11:12:21.772941  2068 net.cpp:172] Setting up ReLU12
I0825 11:12:21.772971  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.772987  2068 net.cpp:194] Memory required for data: 273416448
I0825 11:12:21.773005  2068 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:12:21.773032  2068 net.cpp:128] Creating Layer Convolution13
I0825 11:12:21.773051  2068 net.cpp:558] Convolution13 <- Convolution12
I0825 11:12:21.773073  2068 net.cpp:522] Convolution13 -> Convolution13
I0825 11:12:21.792508  2068 net.cpp:172] Setting up Convolution13
I0825 11:12:21.792551  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.792567  2068 net.cpp:194] Memory required for data: 277610752
I0825 11:12:21.792590  2068 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:12:21.792613  2068 net.cpp:128] Creating Layer BatchNorm13
I0825 11:12:21.792631  2068 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:12:21.792654  2068 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:12:21.792919  2068 net.cpp:172] Setting up BatchNorm13
I0825 11:12:21.792942  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.792958  2068 net.cpp:194] Memory required for data: 281805056
I0825 11:12:21.792981  2068 layer_factory.hpp:77] Creating layer Scale13
I0825 11:12:21.793001  2068 net.cpp:128] Creating Layer Scale13
I0825 11:12:21.793020  2068 net.cpp:558] Scale13 <- Convolution13
I0825 11:12:21.793038  2068 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:12:21.793102  2068 layer_factory.hpp:77] Creating layer Scale13
I0825 11:12:21.793264  2068 net.cpp:172] Setting up Scale13
I0825 11:12:21.793287  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.793303  2068 net.cpp:194] Memory required for data: 285999360
I0825 11:12:21.793323  2068 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:12:21.793345  2068 net.cpp:128] Creating Layer Eltwise6
I0825 11:12:21.793362  2068 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:12:21.793380  2068 net.cpp:558] Eltwise6 <- Convolution13
I0825 11:12:21.793406  2068 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:12:21.793458  2068 net.cpp:172] Setting up Eltwise6
I0825 11:12:21.793480  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.793495  2068 net.cpp:194] Memory required for data: 290193664
I0825 11:12:21.793511  2068 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:12:21.793535  2068 net.cpp:128] Creating Layer ReLU13
I0825 11:12:21.793551  2068 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:12:21.793570  2068 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:12:21.798903  2068 net.cpp:172] Setting up ReLU13
I0825 11:12:21.798943  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.798959  2068 net.cpp:194] Memory required for data: 294387968
I0825 11:12:21.798988  2068 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:12:21.799011  2068 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:12:21.799031  2068 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:12:21.799049  2068 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:12:21.799072  2068 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:12:21.799144  2068 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:12:21.799163  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.799182  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.799198  2068 net.cpp:194] Memory required for data: 302776576
I0825 11:12:21.799214  2068 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:12:21.799243  2068 net.cpp:128] Creating Layer Convolution14
I0825 11:12:21.799260  2068 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0825 11:12:21.799281  2068 net.cpp:522] Convolution14 -> Convolution14
I0825 11:12:21.812732  2068 net.cpp:172] Setting up Convolution14
I0825 11:12:21.812775  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.812791  2068 net.cpp:194] Memory required for data: 306970880
I0825 11:12:21.812814  2068 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:12:21.812837  2068 net.cpp:128] Creating Layer BatchNorm14
I0825 11:12:21.812855  2068 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:12:21.812873  2068 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:12:21.813148  2068 net.cpp:172] Setting up BatchNorm14
I0825 11:12:21.813170  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.813186  2068 net.cpp:194] Memory required for data: 311165184
I0825 11:12:21.813210  2068 layer_factory.hpp:77] Creating layer Scale14
I0825 11:12:21.813230  2068 net.cpp:128] Creating Layer Scale14
I0825 11:12:21.813246  2068 net.cpp:558] Scale14 <- Convolution14
I0825 11:12:21.813266  2068 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:12:21.813331  2068 layer_factory.hpp:77] Creating layer Scale14
I0825 11:12:21.813490  2068 net.cpp:172] Setting up Scale14
I0825 11:12:21.813513  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.813529  2068 net.cpp:194] Memory required for data: 315359488
I0825 11:12:21.813549  2068 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:12:21.813568  2068 net.cpp:128] Creating Layer ReLU14
I0825 11:12:21.813585  2068 net.cpp:558] ReLU14 <- Convolution14
I0825 11:12:21.813606  2068 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0825 11:12:21.816936  2068 net.cpp:172] Setting up ReLU14
I0825 11:12:21.816967  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.816983  2068 net.cpp:194] Memory required for data: 319553792
I0825 11:12:21.817000  2068 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:12:21.817026  2068 net.cpp:128] Creating Layer Convolution15
I0825 11:12:21.817044  2068 net.cpp:558] Convolution15 <- Convolution14
I0825 11:12:21.817066  2068 net.cpp:522] Convolution15 -> Convolution15
I0825 11:12:21.827368  2068 net.cpp:172] Setting up Convolution15
I0825 11:12:21.827414  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.827430  2068 net.cpp:194] Memory required for data: 323748096
I0825 11:12:21.827453  2068 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:12:21.827488  2068 net.cpp:128] Creating Layer BatchNorm15
I0825 11:12:21.827505  2068 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:12:21.827527  2068 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:12:21.827802  2068 net.cpp:172] Setting up BatchNorm15
I0825 11:12:21.827826  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.827842  2068 net.cpp:194] Memory required for data: 327942400
I0825 11:12:21.827867  2068 layer_factory.hpp:77] Creating layer Scale15
I0825 11:12:21.827885  2068 net.cpp:128] Creating Layer Scale15
I0825 11:12:21.827903  2068 net.cpp:558] Scale15 <- Convolution15
I0825 11:12:21.827931  2068 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:12:21.827994  2068 layer_factory.hpp:77] Creating layer Scale15
I0825 11:12:21.828156  2068 net.cpp:172] Setting up Scale15
I0825 11:12:21.828181  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.828197  2068 net.cpp:194] Memory required for data: 332136704
I0825 11:12:21.828218  2068 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:12:21.828239  2068 net.cpp:128] Creating Layer Eltwise7
I0825 11:12:21.828256  2068 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:12:21.828274  2068 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:12:21.828294  2068 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:12:21.828341  2068 net.cpp:172] Setting up Eltwise7
I0825 11:12:21.828361  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.828377  2068 net.cpp:194] Memory required for data: 336331008
I0825 11:12:21.828392  2068 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:12:21.828413  2068 net.cpp:128] Creating Layer ReLU15
I0825 11:12:21.828428  2068 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:12:21.828447  2068 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:12:21.831657  2068 net.cpp:172] Setting up ReLU15
I0825 11:12:21.831688  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.831704  2068 net.cpp:194] Memory required for data: 340525312
I0825 11:12:21.831722  2068 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:12:21.831744  2068 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:12:21.831761  2068 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:12:21.831781  2068 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:12:21.831804  2068 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:12:21.831871  2068 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:12:21.831892  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.831910  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.831926  2068 net.cpp:194] Memory required for data: 348913920
I0825 11:12:21.831943  2068 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:12:21.831967  2068 net.cpp:128] Creating Layer Convolution16
I0825 11:12:21.831984  2068 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0825 11:12:21.832007  2068 net.cpp:522] Convolution16 -> Convolution16
I0825 11:12:21.847280  2068 net.cpp:172] Setting up Convolution16
I0825 11:12:21.847326  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.847343  2068 net.cpp:194] Memory required for data: 353108224
I0825 11:12:21.847368  2068 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:12:21.847389  2068 net.cpp:128] Creating Layer BatchNorm16
I0825 11:12:21.847406  2068 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:12:21.847427  2068 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:12:21.847700  2068 net.cpp:172] Setting up BatchNorm16
I0825 11:12:21.847723  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.847739  2068 net.cpp:194] Memory required for data: 357302528
I0825 11:12:21.847764  2068 layer_factory.hpp:77] Creating layer Scale16
I0825 11:12:21.847786  2068 net.cpp:128] Creating Layer Scale16
I0825 11:12:21.847805  2068 net.cpp:558] Scale16 <- Convolution16
I0825 11:12:21.847822  2068 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:12:21.847896  2068 layer_factory.hpp:77] Creating layer Scale16
I0825 11:12:21.848058  2068 net.cpp:172] Setting up Scale16
I0825 11:12:21.848080  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.848096  2068 net.cpp:194] Memory required for data: 361496832
I0825 11:12:21.848117  2068 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:12:21.848143  2068 net.cpp:128] Creating Layer ReLU16
I0825 11:12:21.848160  2068 net.cpp:558] ReLU16 <- Convolution16
I0825 11:12:21.848179  2068 net.cpp:509] ReLU16 -> Convolution16 (in-place)
I0825 11:12:21.853339  2068 net.cpp:172] Setting up ReLU16
I0825 11:12:21.853381  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.853410  2068 net.cpp:194] Memory required for data: 365691136
I0825 11:12:21.853427  2068 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:12:21.853453  2068 net.cpp:128] Creating Layer Convolution17
I0825 11:12:21.853471  2068 net.cpp:558] Convolution17 <- Convolution16
I0825 11:12:21.853493  2068 net.cpp:522] Convolution17 -> Convolution17
I0825 11:12:21.867662  2068 net.cpp:172] Setting up Convolution17
I0825 11:12:21.867704  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.867722  2068 net.cpp:194] Memory required for data: 369885440
I0825 11:12:21.867745  2068 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:12:21.867769  2068 net.cpp:128] Creating Layer BatchNorm17
I0825 11:12:21.867785  2068 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:12:21.867807  2068 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:12:21.868088  2068 net.cpp:172] Setting up BatchNorm17
I0825 11:12:21.868109  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.868126  2068 net.cpp:194] Memory required for data: 374079744
I0825 11:12:21.868149  2068 layer_factory.hpp:77] Creating layer Scale17
I0825 11:12:21.868168  2068 net.cpp:128] Creating Layer Scale17
I0825 11:12:21.868185  2068 net.cpp:558] Scale17 <- Convolution17
I0825 11:12:21.868204  2068 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:12:21.868269  2068 layer_factory.hpp:77] Creating layer Scale17
I0825 11:12:21.868429  2068 net.cpp:172] Setting up Scale17
I0825 11:12:21.868453  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.868468  2068 net.cpp:194] Memory required for data: 378274048
I0825 11:12:21.868489  2068 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:12:21.868511  2068 net.cpp:128] Creating Layer Eltwise8
I0825 11:12:21.868528  2068 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:12:21.868546  2068 net.cpp:558] Eltwise8 <- Convolution17
I0825 11:12:21.868566  2068 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:12:21.868611  2068 net.cpp:172] Setting up Eltwise8
I0825 11:12:21.868631  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.868647  2068 net.cpp:194] Memory required for data: 382468352
I0825 11:12:21.868664  2068 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:12:21.868683  2068 net.cpp:128] Creating Layer ReLU17
I0825 11:12:21.868700  2068 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:12:21.868721  2068 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:12:21.869817  2068 net.cpp:172] Setting up ReLU17
I0825 11:12:21.869843  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.869860  2068 net.cpp:194] Memory required for data: 386662656
I0825 11:12:21.869877  2068 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:12:21.869897  2068 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:12:21.869915  2068 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:12:21.869938  2068 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:12:21.869961  2068 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:12:21.870028  2068 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:12:21.870049  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.870067  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.870090  2068 net.cpp:194] Memory required for data: 395051264
I0825 11:12:21.870107  2068 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:12:21.870132  2068 net.cpp:128] Creating Layer Convolution18
I0825 11:12:21.870149  2068 net.cpp:558] Convolution18 <- Eltwise8_ReLU17_0_split_0
I0825 11:12:21.870172  2068 net.cpp:522] Convolution18 -> Convolution18
I0825 11:12:21.876703  2068 net.cpp:172] Setting up Convolution18
I0825 11:12:21.876750  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.876768  2068 net.cpp:194] Memory required for data: 399245568
I0825 11:12:21.876792  2068 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:12:21.876813  2068 net.cpp:128] Creating Layer BatchNorm18
I0825 11:12:21.876842  2068 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:12:21.876864  2068 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:12:21.877149  2068 net.cpp:172] Setting up BatchNorm18
I0825 11:12:21.877172  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.877188  2068 net.cpp:194] Memory required for data: 403439872
I0825 11:12:21.877213  2068 layer_factory.hpp:77] Creating layer Scale18
I0825 11:12:21.877233  2068 net.cpp:128] Creating Layer Scale18
I0825 11:12:21.877250  2068 net.cpp:558] Scale18 <- Convolution18
I0825 11:12:21.877270  2068 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:12:21.877334  2068 layer_factory.hpp:77] Creating layer Scale18
I0825 11:12:21.877494  2068 net.cpp:172] Setting up Scale18
I0825 11:12:21.877516  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.877532  2068 net.cpp:194] Memory required for data: 407634176
I0825 11:12:21.877552  2068 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:12:21.877573  2068 net.cpp:128] Creating Layer ReLU18
I0825 11:12:21.877590  2068 net.cpp:558] ReLU18 <- Convolution18
I0825 11:12:21.877609  2068 net.cpp:509] ReLU18 -> Convolution18 (in-place)
I0825 11:12:21.878835  2068 net.cpp:172] Setting up ReLU18
I0825 11:12:21.878865  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.878882  2068 net.cpp:194] Memory required for data: 411828480
I0825 11:12:21.878901  2068 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:12:21.878931  2068 net.cpp:128] Creating Layer Convolution19
I0825 11:12:21.878948  2068 net.cpp:558] Convolution19 <- Convolution18
I0825 11:12:21.878969  2068 net.cpp:522] Convolution19 -> Convolution19
I0825 11:12:21.889989  2068 net.cpp:172] Setting up Convolution19
I0825 11:12:21.890033  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.890049  2068 net.cpp:194] Memory required for data: 416022784
I0825 11:12:21.890072  2068 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:12:21.890096  2068 net.cpp:128] Creating Layer BatchNorm19
I0825 11:12:21.890115  2068 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:12:21.890134  2068 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:12:21.890426  2068 net.cpp:172] Setting up BatchNorm19
I0825 11:12:21.890455  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.890472  2068 net.cpp:194] Memory required for data: 420217088
I0825 11:12:21.890511  2068 layer_factory.hpp:77] Creating layer Scale19
I0825 11:12:21.890534  2068 net.cpp:128] Creating Layer Scale19
I0825 11:12:21.890552  2068 net.cpp:558] Scale19 <- Convolution19
I0825 11:12:21.890570  2068 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:12:21.890640  2068 layer_factory.hpp:77] Creating layer Scale19
I0825 11:12:21.890802  2068 net.cpp:172] Setting up Scale19
I0825 11:12:21.890825  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.890841  2068 net.cpp:194] Memory required for data: 424411392
I0825 11:12:21.890861  2068 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:12:21.890880  2068 net.cpp:128] Creating Layer Eltwise9
I0825 11:12:21.890897  2068 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:12:21.890915  2068 net.cpp:558] Eltwise9 <- Convolution19
I0825 11:12:21.890936  2068 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:12:21.890990  2068 net.cpp:172] Setting up Eltwise9
I0825 11:12:21.891012  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.891028  2068 net.cpp:194] Memory required for data: 428605696
I0825 11:12:21.891046  2068 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:12:21.891063  2068 net.cpp:128] Creating Layer ReLU19
I0825 11:12:21.891079  2068 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:12:21.891098  2068 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:12:21.894239  2068 net.cpp:172] Setting up ReLU19
I0825 11:12:21.894268  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.894285  2068 net.cpp:194] Memory required for data: 432800000
I0825 11:12:21.894301  2068 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:12:21.894338  2068 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:12:21.894358  2068 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:12:21.894379  2068 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:12:21.894402  2068 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:12:21.894474  2068 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:12:21.894495  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.894513  2068 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:12:21.894531  2068 net.cpp:194] Memory required for data: 441188608
I0825 11:12:21.894546  2068 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:12:21.894577  2068 net.cpp:128] Creating Layer Convolution20
I0825 11:12:21.894594  2068 net.cpp:558] Convolution20 <- Eltwise9_ReLU19_0_split_0
I0825 11:12:21.894614  2068 net.cpp:522] Convolution20 -> Convolution20
I0825 11:12:21.907702  2068 net.cpp:172] Setting up Convolution20
I0825 11:12:21.907747  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.907763  2068 net.cpp:194] Memory required for data: 443285760
I0825 11:12:21.907788  2068 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:12:21.907807  2068 net.cpp:128] Creating Layer BatchNorm20
I0825 11:12:21.907825  2068 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:12:21.907846  2068 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:12:21.908129  2068 net.cpp:172] Setting up BatchNorm20
I0825 11:12:21.908150  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.908166  2068 net.cpp:194] Memory required for data: 445382912
I0825 11:12:21.908190  2068 layer_factory.hpp:77] Creating layer Scale20
I0825 11:12:21.908210  2068 net.cpp:128] Creating Layer Scale20
I0825 11:12:21.908226  2068 net.cpp:558] Scale20 <- Convolution20
I0825 11:12:21.908247  2068 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:12:21.908310  2068 layer_factory.hpp:77] Creating layer Scale20
I0825 11:12:21.908476  2068 net.cpp:172] Setting up Scale20
I0825 11:12:21.908498  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.908514  2068 net.cpp:194] Memory required for data: 447480064
I0825 11:12:21.908535  2068 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:12:21.908560  2068 net.cpp:128] Creating Layer Convolution21
I0825 11:12:21.908578  2068 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_1
I0825 11:12:21.908601  2068 net.cpp:522] Convolution21 -> Convolution21
I0825 11:12:21.916635  2068 net.cpp:172] Setting up Convolution21
I0825 11:12:21.916682  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.916700  2068 net.cpp:194] Memory required for data: 449577216
I0825 11:12:21.916724  2068 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:12:21.916748  2068 net.cpp:128] Creating Layer BatchNorm21
I0825 11:12:21.916766  2068 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:12:21.916785  2068 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:12:21.917063  2068 net.cpp:172] Setting up BatchNorm21
I0825 11:12:21.917085  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.917101  2068 net.cpp:194] Memory required for data: 451674368
I0825 11:12:21.917124  2068 layer_factory.hpp:77] Creating layer Scale21
I0825 11:12:21.917155  2068 net.cpp:128] Creating Layer Scale21
I0825 11:12:21.917172  2068 net.cpp:558] Scale21 <- Convolution21
I0825 11:12:21.917191  2068 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:12:21.917259  2068 layer_factory.hpp:77] Creating layer Scale21
I0825 11:12:21.917428  2068 net.cpp:172] Setting up Scale21
I0825 11:12:21.917449  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.917465  2068 net.cpp:194] Memory required for data: 453771520
I0825 11:12:21.917487  2068 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:12:21.917510  2068 net.cpp:128] Creating Layer ReLU20
I0825 11:12:21.917526  2068 net.cpp:558] ReLU20 <- Convolution21
I0825 11:12:21.917558  2068 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:12:21.918792  2068 net.cpp:172] Setting up ReLU20
I0825 11:12:21.918833  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.918851  2068 net.cpp:194] Memory required for data: 455868672
I0825 11:12:21.918869  2068 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:12:21.918895  2068 net.cpp:128] Creating Layer Convolution22
I0825 11:12:21.918913  2068 net.cpp:558] Convolution22 <- Convolution21
I0825 11:12:21.918936  2068 net.cpp:522] Convolution22 -> Convolution22
I0825 11:12:21.925819  2068 net.cpp:172] Setting up Convolution22
I0825 11:12:21.925914  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.925947  2068 net.cpp:194] Memory required for data: 457965824
I0825 11:12:21.925987  2068 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:12:21.926026  2068 net.cpp:128] Creating Layer BatchNorm22
I0825 11:12:21.926059  2068 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:12:21.926098  2068 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:12:21.926414  2068 net.cpp:172] Setting up BatchNorm22
I0825 11:12:21.926460  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.926491  2068 net.cpp:194] Memory required for data: 460062976
I0825 11:12:21.926530  2068 layer_factory.hpp:77] Creating layer Scale22
I0825 11:12:21.926565  2068 net.cpp:128] Creating Layer Scale22
I0825 11:12:21.926596  2068 net.cpp:558] Scale22 <- Convolution22
I0825 11:12:21.926630  2068 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:12:21.926717  2068 layer_factory.hpp:77] Creating layer Scale22
I0825 11:12:21.926905  2068 net.cpp:172] Setting up Scale22
I0825 11:12:21.926944  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.926975  2068 net.cpp:194] Memory required for data: 462160128
I0825 11:12:21.927013  2068 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:12:21.927052  2068 net.cpp:128] Creating Layer Eltwise10
I0825 11:12:21.927085  2068 net.cpp:558] Eltwise10 <- Convolution20
I0825 11:12:21.927119  2068 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:12:21.927153  2068 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:12:21.927214  2068 net.cpp:172] Setting up Eltwise10
I0825 11:12:21.927250  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.927281  2068 net.cpp:194] Memory required for data: 464257280
I0825 11:12:21.927314  2068 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:12:21.927347  2068 net.cpp:128] Creating Layer ReLU21
I0825 11:12:21.927378  2068 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:12:21.927414  2068 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:12:21.927922  2068 net.cpp:172] Setting up ReLU21
I0825 11:12:21.927969  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.928000  2068 net.cpp:194] Memory required for data: 466354432
I0825 11:12:21.928033  2068 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:12:21.928073  2068 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:12:21.928107  2068 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:12:21.928145  2068 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:12:21.928185  2068 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:12:21.928275  2068 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:12:21.928320  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.928354  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.928385  2068 net.cpp:194] Memory required for data: 470548736
I0825 11:12:21.928418  2068 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:12:21.928462  2068 net.cpp:128] Creating Layer Convolution23
I0825 11:12:21.928494  2068 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:12:21.928534  2068 net.cpp:522] Convolution23 -> Convolution23
I0825 11:12:21.941388  2068 net.cpp:172] Setting up Convolution23
I0825 11:12:21.941457  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.941501  2068 net.cpp:194] Memory required for data: 472645888
I0825 11:12:21.941540  2068 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:12:21.941579  2068 net.cpp:128] Creating Layer BatchNorm23
I0825 11:12:21.941612  2068 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:12:21.941646  2068 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:12:21.941947  2068 net.cpp:172] Setting up BatchNorm23
I0825 11:12:21.941988  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.942020  2068 net.cpp:194] Memory required for data: 474743040
I0825 11:12:21.942059  2068 layer_factory.hpp:77] Creating layer Scale23
I0825 11:12:21.942096  2068 net.cpp:128] Creating Layer Scale23
I0825 11:12:21.942128  2068 net.cpp:558] Scale23 <- Convolution23
I0825 11:12:21.942162  2068 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:12:21.942245  2068 layer_factory.hpp:77] Creating layer Scale23
I0825 11:12:21.942441  2068 net.cpp:172] Setting up Scale23
I0825 11:12:21.942483  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.942514  2068 net.cpp:194] Memory required for data: 476840192
I0825 11:12:21.942551  2068 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:12:21.942591  2068 net.cpp:128] Creating Layer ReLU22
I0825 11:12:21.942623  2068 net.cpp:558] ReLU22 <- Convolution23
I0825 11:12:21.942661  2068 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0825 11:12:21.945367  2068 net.cpp:172] Setting up ReLU22
I0825 11:12:21.945420  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.945452  2068 net.cpp:194] Memory required for data: 478937344
I0825 11:12:21.945484  2068 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:12:21.945528  2068 net.cpp:128] Creating Layer Convolution24
I0825 11:12:21.945561  2068 net.cpp:558] Convolution24 <- Convolution23
I0825 11:12:21.945600  2068 net.cpp:522] Convolution24 -> Convolution24
I0825 11:12:21.960731  2068 net.cpp:172] Setting up Convolution24
I0825 11:12:21.960798  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.960830  2068 net.cpp:194] Memory required for data: 481034496
I0825 11:12:21.960870  2068 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:12:21.960908  2068 net.cpp:128] Creating Layer BatchNorm24
I0825 11:12:21.960940  2068 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:12:21.960976  2068 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:12:21.961275  2068 net.cpp:172] Setting up BatchNorm24
I0825 11:12:21.961314  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.961345  2068 net.cpp:194] Memory required for data: 483131648
I0825 11:12:21.961385  2068 layer_factory.hpp:77] Creating layer Scale24
I0825 11:12:21.961423  2068 net.cpp:128] Creating Layer Scale24
I0825 11:12:21.961455  2068 net.cpp:558] Scale24 <- Convolution24
I0825 11:12:21.961488  2068 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:12:21.961573  2068 layer_factory.hpp:77] Creating layer Scale24
I0825 11:12:21.961760  2068 net.cpp:172] Setting up Scale24
I0825 11:12:21.961798  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.961830  2068 net.cpp:194] Memory required for data: 485228800
I0825 11:12:21.961868  2068 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:12:21.961905  2068 net.cpp:128] Creating Layer Eltwise11
I0825 11:12:21.961936  2068 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0825 11:12:21.961977  2068 net.cpp:558] Eltwise11 <- Convolution24
I0825 11:12:21.962011  2068 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:12:21.962072  2068 net.cpp:172] Setting up Eltwise11
I0825 11:12:21.962108  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.962139  2068 net.cpp:194] Memory required for data: 487325952
I0825 11:12:21.962172  2068 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:12:21.962206  2068 net.cpp:128] Creating Layer ReLU23
I0825 11:12:21.962237  2068 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:12:21.962272  2068 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:12:21.965123  2068 net.cpp:172] Setting up ReLU23
I0825 11:12:21.965205  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.965239  2068 net.cpp:194] Memory required for data: 489423104
I0825 11:12:21.965271  2068 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:12:21.965307  2068 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:12:21.965340  2068 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:12:21.965378  2068 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:12:21.965417  2068 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:12:21.965512  2068 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:12:21.965549  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.965582  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.965613  2068 net.cpp:194] Memory required for data: 493617408
I0825 11:12:21.965646  2068 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:12:21.965685  2068 net.cpp:128] Creating Layer Convolution25
I0825 11:12:21.965718  2068 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0825 11:12:21.965756  2068 net.cpp:522] Convolution25 -> Convolution25
I0825 11:12:21.977138  2068 net.cpp:172] Setting up Convolution25
I0825 11:12:21.977206  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.977239  2068 net.cpp:194] Memory required for data: 495714560
I0825 11:12:21.977277  2068 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:12:21.977313  2068 net.cpp:128] Creating Layer BatchNorm25
I0825 11:12:21.977347  2068 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:12:21.977385  2068 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:12:21.977687  2068 net.cpp:172] Setting up BatchNorm25
I0825 11:12:21.977728  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.977759  2068 net.cpp:194] Memory required for data: 497811712
I0825 11:12:21.977799  2068 layer_factory.hpp:77] Creating layer Scale25
I0825 11:12:21.977834  2068 net.cpp:128] Creating Layer Scale25
I0825 11:12:21.977866  2068 net.cpp:558] Scale25 <- Convolution25
I0825 11:12:21.977900  2068 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:12:21.977985  2068 layer_factory.hpp:77] Creating layer Scale25
I0825 11:12:21.978169  2068 net.cpp:172] Setting up Scale25
I0825 11:12:21.978209  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.978241  2068 net.cpp:194] Memory required for data: 499908864
I0825 11:12:21.978278  2068 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:12:21.978313  2068 net.cpp:128] Creating Layer ReLU24
I0825 11:12:21.978354  2068 net.cpp:558] ReLU24 <- Convolution25
I0825 11:12:21.978389  2068 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0825 11:12:21.981385  2068 net.cpp:172] Setting up ReLU24
I0825 11:12:21.981437  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.981470  2068 net.cpp:194] Memory required for data: 502006016
I0825 11:12:21.981503  2068 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:12:21.981547  2068 net.cpp:128] Creating Layer Convolution26
I0825 11:12:21.981580  2068 net.cpp:558] Convolution26 <- Convolution25
I0825 11:12:21.981616  2068 net.cpp:522] Convolution26 -> Convolution26
I0825 11:12:21.996824  2068 net.cpp:172] Setting up Convolution26
I0825 11:12:21.996891  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.996933  2068 net.cpp:194] Memory required for data: 504103168
I0825 11:12:21.996973  2068 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:12:21.997011  2068 net.cpp:128] Creating Layer BatchNorm26
I0825 11:12:21.997043  2068 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:12:21.997078  2068 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:12:21.997388  2068 net.cpp:172] Setting up BatchNorm26
I0825 11:12:21.997429  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.997460  2068 net.cpp:194] Memory required for data: 506200320
I0825 11:12:21.997500  2068 layer_factory.hpp:77] Creating layer Scale26
I0825 11:12:21.997535  2068 net.cpp:128] Creating Layer Scale26
I0825 11:12:21.997577  2068 net.cpp:558] Scale26 <- Convolution26
I0825 11:12:21.997611  2068 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:12:21.997701  2068 layer_factory.hpp:77] Creating layer Scale26
I0825 11:12:21.997890  2068 net.cpp:172] Setting up Scale26
I0825 11:12:21.997928  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.997961  2068 net.cpp:194] Memory required for data: 508297472
I0825 11:12:21.997997  2068 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:12:21.998035  2068 net.cpp:128] Creating Layer Eltwise12
I0825 11:12:21.998067  2068 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:12:21.998101  2068 net.cpp:558] Eltwise12 <- Convolution26
I0825 11:12:21.998136  2068 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:12:21.998193  2068 net.cpp:172] Setting up Eltwise12
I0825 11:12:21.998229  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:21.998260  2068 net.cpp:194] Memory required for data: 510394624
I0825 11:12:21.998291  2068 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:12:21.998329  2068 net.cpp:128] Creating Layer ReLU25
I0825 11:12:21.998368  2068 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:12:21.998401  2068 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:12:22.003168  2068 net.cpp:172] Setting up ReLU25
I0825 11:12:22.003219  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.003252  2068 net.cpp:194] Memory required for data: 512491776
I0825 11:12:22.003283  2068 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:12:22.003335  2068 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:12:22.003368  2068 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:12:22.003404  2068 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:12:22.003451  2068 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:12:22.003542  2068 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:12:22.003582  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.003615  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.003646  2068 net.cpp:194] Memory required for data: 516686080
I0825 11:12:22.003679  2068 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:12:22.003718  2068 net.cpp:128] Creating Layer Convolution27
I0825 11:12:22.003751  2068 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0825 11:12:22.003790  2068 net.cpp:522] Convolution27 -> Convolution27
I0825 11:12:22.022169  2068 net.cpp:172] Setting up Convolution27
I0825 11:12:22.022239  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.022272  2068 net.cpp:194] Memory required for data: 518783232
I0825 11:12:22.022312  2068 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:12:22.022359  2068 net.cpp:128] Creating Layer BatchNorm27
I0825 11:12:22.022393  2068 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:12:22.022428  2068 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:12:22.022737  2068 net.cpp:172] Setting up BatchNorm27
I0825 11:12:22.022776  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.022807  2068 net.cpp:194] Memory required for data: 520880384
I0825 11:12:22.022846  2068 layer_factory.hpp:77] Creating layer Scale27
I0825 11:12:22.022881  2068 net.cpp:128] Creating Layer Scale27
I0825 11:12:22.022920  2068 net.cpp:558] Scale27 <- Convolution27
I0825 11:12:22.022958  2068 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:12:22.023042  2068 layer_factory.hpp:77] Creating layer Scale27
I0825 11:12:22.023232  2068 net.cpp:172] Setting up Scale27
I0825 11:12:22.023270  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.023301  2068 net.cpp:194] Memory required for data: 522977536
I0825 11:12:22.023339  2068 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:12:22.023373  2068 net.cpp:128] Creating Layer ReLU26
I0825 11:12:22.023404  2068 net.cpp:558] ReLU26 <- Convolution27
I0825 11:12:22.023442  2068 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0825 11:12:22.026403  2068 net.cpp:172] Setting up ReLU26
I0825 11:12:22.026465  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.026497  2068 net.cpp:194] Memory required for data: 525074688
I0825 11:12:22.026530  2068 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:12:22.026573  2068 net.cpp:128] Creating Layer Convolution28
I0825 11:12:22.026607  2068 net.cpp:558] Convolution28 <- Convolution27
I0825 11:12:22.026645  2068 net.cpp:522] Convolution28 -> Convolution28
I0825 11:12:22.037734  2068 net.cpp:172] Setting up Convolution28
I0825 11:12:22.037799  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.037832  2068 net.cpp:194] Memory required for data: 527171840
I0825 11:12:22.037870  2068 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:12:22.037910  2068 net.cpp:128] Creating Layer BatchNorm28
I0825 11:12:22.037942  2068 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:12:22.037979  2068 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:12:22.038286  2068 net.cpp:172] Setting up BatchNorm28
I0825 11:12:22.038326  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.038367  2068 net.cpp:194] Memory required for data: 529268992
I0825 11:12:22.038406  2068 layer_factory.hpp:77] Creating layer Scale28
I0825 11:12:22.038445  2068 net.cpp:128] Creating Layer Scale28
I0825 11:12:22.038476  2068 net.cpp:558] Scale28 <- Convolution28
I0825 11:12:22.038509  2068 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:12:22.038599  2068 layer_factory.hpp:77] Creating layer Scale28
I0825 11:12:22.038794  2068 net.cpp:172] Setting up Scale28
I0825 11:12:22.038831  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.038862  2068 net.cpp:194] Memory required for data: 531366144
I0825 11:12:22.038900  2068 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:12:22.038939  2068 net.cpp:128] Creating Layer Eltwise13
I0825 11:12:22.038975  2068 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:12:22.039011  2068 net.cpp:558] Eltwise13 <- Convolution28
I0825 11:12:22.039047  2068 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:12:22.039113  2068 net.cpp:172] Setting up Eltwise13
I0825 11:12:22.039150  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.039182  2068 net.cpp:194] Memory required for data: 533463296
I0825 11:12:22.039214  2068 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:12:22.039248  2068 net.cpp:128] Creating Layer ReLU27
I0825 11:12:22.039280  2068 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:12:22.039315  2068 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:12:22.039849  2068 net.cpp:172] Setting up ReLU27
I0825 11:12:22.039896  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.039928  2068 net.cpp:194] Memory required for data: 535560448
I0825 11:12:22.039960  2068 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:12:22.039997  2068 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:12:22.040030  2068 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:12:22.040067  2068 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:12:22.040107  2068 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:12:22.040200  2068 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:12:22.040238  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.040280  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.040313  2068 net.cpp:194] Memory required for data: 539754752
I0825 11:12:22.040344  2068 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:12:22.040386  2068 net.cpp:128] Creating Layer Convolution29
I0825 11:12:22.040418  2068 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0825 11:12:22.040460  2068 net.cpp:522] Convolution29 -> Convolution29
I0825 11:12:22.048893  2068 net.cpp:172] Setting up Convolution29
I0825 11:12:22.048961  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.048993  2068 net.cpp:194] Memory required for data: 541851904
I0825 11:12:22.049043  2068 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:12:22.049082  2068 net.cpp:128] Creating Layer BatchNorm29
I0825 11:12:22.049113  2068 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:12:22.049149  2068 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:12:22.049461  2068 net.cpp:172] Setting up BatchNorm29
I0825 11:12:22.049502  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.049533  2068 net.cpp:194] Memory required for data: 543949056
I0825 11:12:22.049572  2068 layer_factory.hpp:77] Creating layer Scale29
I0825 11:12:22.049610  2068 net.cpp:128] Creating Layer Scale29
I0825 11:12:22.049641  2068 net.cpp:558] Scale29 <- Convolution29
I0825 11:12:22.049674  2068 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:12:22.049760  2068 layer_factory.hpp:77] Creating layer Scale29
I0825 11:12:22.049952  2068 net.cpp:172] Setting up Scale29
I0825 11:12:22.049989  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.050020  2068 net.cpp:194] Memory required for data: 546046208
I0825 11:12:22.050057  2068 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:12:22.050096  2068 net.cpp:128] Creating Layer ReLU28
I0825 11:12:22.050127  2068 net.cpp:558] ReLU28 <- Convolution29
I0825 11:12:22.050163  2068 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0825 11:12:22.053146  2068 net.cpp:172] Setting up ReLU28
I0825 11:12:22.053203  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.053236  2068 net.cpp:194] Memory required for data: 548143360
I0825 11:12:22.053268  2068 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:12:22.053314  2068 net.cpp:128] Creating Layer Convolution30
I0825 11:12:22.053346  2068 net.cpp:558] Convolution30 <- Convolution29
I0825 11:12:22.053386  2068 net.cpp:522] Convolution30 -> Convolution30
I0825 11:12:22.072731  2068 net.cpp:172] Setting up Convolution30
I0825 11:12:22.072798  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.072831  2068 net.cpp:194] Memory required for data: 550240512
I0825 11:12:22.072870  2068 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:12:22.072908  2068 net.cpp:128] Creating Layer BatchNorm30
I0825 11:12:22.072942  2068 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:12:22.072978  2068 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:12:22.073292  2068 net.cpp:172] Setting up BatchNorm30
I0825 11:12:22.073333  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.073364  2068 net.cpp:194] Memory required for data: 552337664
I0825 11:12:22.073402  2068 layer_factory.hpp:77] Creating layer Scale30
I0825 11:12:22.073439  2068 net.cpp:128] Creating Layer Scale30
I0825 11:12:22.073472  2068 net.cpp:558] Scale30 <- Convolution30
I0825 11:12:22.073504  2068 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:12:22.073591  2068 layer_factory.hpp:77] Creating layer Scale30
I0825 11:12:22.073781  2068 net.cpp:172] Setting up Scale30
I0825 11:12:22.073819  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.073850  2068 net.cpp:194] Memory required for data: 554434816
I0825 11:12:22.073887  2068 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:12:22.073925  2068 net.cpp:128] Creating Layer Eltwise14
I0825 11:12:22.073957  2068 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:12:22.074000  2068 net.cpp:558] Eltwise14 <- Convolution30
I0825 11:12:22.074034  2068 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:12:22.074097  2068 net.cpp:172] Setting up Eltwise14
I0825 11:12:22.074133  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.074164  2068 net.cpp:194] Memory required for data: 556531968
I0825 11:12:22.074195  2068 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:12:22.074229  2068 net.cpp:128] Creating Layer ReLU29
I0825 11:12:22.074260  2068 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:12:22.074295  2068 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:12:22.079046  2068 net.cpp:172] Setting up ReLU29
I0825 11:12:22.079107  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.079149  2068 net.cpp:194] Memory required for data: 558629120
I0825 11:12:22.079182  2068 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:12:22.079219  2068 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:12:22.079253  2068 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:12:22.079290  2068 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:12:22.079329  2068 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:12:22.079423  2068 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:12:22.079461  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.079495  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.079525  2068 net.cpp:194] Memory required for data: 562823424
I0825 11:12:22.079557  2068 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:12:22.079601  2068 net.cpp:128] Creating Layer Convolution31
I0825 11:12:22.079632  2068 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0825 11:12:22.079672  2068 net.cpp:522] Convolution31 -> Convolution31
I0825 11:12:22.098651  2068 net.cpp:172] Setting up Convolution31
I0825 11:12:22.098717  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.098749  2068 net.cpp:194] Memory required for data: 564920576
I0825 11:12:22.098788  2068 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:12:22.098827  2068 net.cpp:128] Creating Layer BatchNorm31
I0825 11:12:22.098860  2068 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:12:22.098896  2068 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:12:22.099203  2068 net.cpp:172] Setting up BatchNorm31
I0825 11:12:22.099241  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.099273  2068 net.cpp:194] Memory required for data: 567017728
I0825 11:12:22.099313  2068 layer_factory.hpp:77] Creating layer Scale31
I0825 11:12:22.099352  2068 net.cpp:128] Creating Layer Scale31
I0825 11:12:22.099385  2068 net.cpp:558] Scale31 <- Convolution31
I0825 11:12:22.099417  2068 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:12:22.099506  2068 layer_factory.hpp:77] Creating layer Scale31
I0825 11:12:22.099699  2068 net.cpp:172] Setting up Scale31
I0825 11:12:22.099736  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.099767  2068 net.cpp:194] Memory required for data: 569114880
I0825 11:12:22.099804  2068 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:12:22.099841  2068 net.cpp:128] Creating Layer ReLU30
I0825 11:12:22.099874  2068 net.cpp:558] ReLU30 <- Convolution31
I0825 11:12:22.099906  2068 net.cpp:509] ReLU30 -> Convolution31 (in-place)
I0825 11:12:22.103379  2068 net.cpp:172] Setting up ReLU30
I0825 11:12:22.103444  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.103477  2068 net.cpp:194] Memory required for data: 571212032
I0825 11:12:22.103510  2068 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:12:22.103551  2068 net.cpp:128] Creating Layer Convolution32
I0825 11:12:22.103585  2068 net.cpp:558] Convolution32 <- Convolution31
I0825 11:12:22.103621  2068 net.cpp:522] Convolution32 -> Convolution32
I0825 11:12:22.116677  2068 net.cpp:172] Setting up Convolution32
I0825 11:12:22.116744  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.116776  2068 net.cpp:194] Memory required for data: 573309184
I0825 11:12:22.116827  2068 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:12:22.116866  2068 net.cpp:128] Creating Layer BatchNorm32
I0825 11:12:22.116899  2068 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:12:22.116933  2068 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:12:22.117244  2068 net.cpp:172] Setting up BatchNorm32
I0825 11:12:22.117285  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.117316  2068 net.cpp:194] Memory required for data: 575406336
I0825 11:12:22.117354  2068 layer_factory.hpp:77] Creating layer Scale32
I0825 11:12:22.117391  2068 net.cpp:128] Creating Layer Scale32
I0825 11:12:22.117432  2068 net.cpp:558] Scale32 <- Convolution32
I0825 11:12:22.117466  2068 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:12:22.117554  2068 layer_factory.hpp:77] Creating layer Scale32
I0825 11:12:22.117756  2068 net.cpp:172] Setting up Scale32
I0825 11:12:22.117797  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.117828  2068 net.cpp:194] Memory required for data: 577503488
I0825 11:12:22.117866  2068 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:12:22.117900  2068 net.cpp:128] Creating Layer Eltwise15
I0825 11:12:22.117933  2068 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0825 11:12:22.117966  2068 net.cpp:558] Eltwise15 <- Convolution32
I0825 11:12:22.118003  2068 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:12:22.118062  2068 net.cpp:172] Setting up Eltwise15
I0825 11:12:22.118098  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.118129  2068 net.cpp:194] Memory required for data: 579600640
I0825 11:12:22.118160  2068 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:12:22.118197  2068 net.cpp:128] Creating Layer ReLU31
I0825 11:12:22.118228  2068 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:12:22.118260  2068 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:12:22.121179  2068 net.cpp:172] Setting up ReLU31
I0825 11:12:22.121250  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.121284  2068 net.cpp:194] Memory required for data: 581697792
I0825 11:12:22.121316  2068 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0825 11:12:22.121352  2068 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0825 11:12:22.121385  2068 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0825 11:12:22.121421  2068 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0825 11:12:22.121462  2068 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0825 11:12:22.121559  2068 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0825 11:12:22.121596  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.121630  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.121660  2068 net.cpp:194] Memory required for data: 585892096
I0825 11:12:22.121693  2068 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:12:22.121737  2068 net.cpp:128] Creating Layer Convolution33
I0825 11:12:22.121768  2068 net.cpp:558] Convolution33 <- Eltwise15_ReLU31_0_split_0
I0825 11:12:22.121805  2068 net.cpp:522] Convolution33 -> Convolution33
I0825 11:12:22.136607  2068 net.cpp:172] Setting up Convolution33
I0825 11:12:22.136678  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.136711  2068 net.cpp:194] Memory required for data: 587989248
I0825 11:12:22.136750  2068 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:12:22.136787  2068 net.cpp:128] Creating Layer BatchNorm33
I0825 11:12:22.136819  2068 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:12:22.136855  2068 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:12:22.137177  2068 net.cpp:172] Setting up BatchNorm33
I0825 11:12:22.137217  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.137248  2068 net.cpp:194] Memory required for data: 590086400
I0825 11:12:22.137286  2068 layer_factory.hpp:77] Creating layer Scale33
I0825 11:12:22.137322  2068 net.cpp:128] Creating Layer Scale33
I0825 11:12:22.137353  2068 net.cpp:558] Scale33 <- Convolution33
I0825 11:12:22.137393  2068 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:12:22.137482  2068 layer_factory.hpp:77] Creating layer Scale33
I0825 11:12:22.137678  2068 net.cpp:172] Setting up Scale33
I0825 11:12:22.137719  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.137751  2068 net.cpp:194] Memory required for data: 592183552
I0825 11:12:22.137789  2068 layer_factory.hpp:77] Creating layer ReLU32
I0825 11:12:22.137822  2068 net.cpp:128] Creating Layer ReLU32
I0825 11:12:22.137853  2068 net.cpp:558] ReLU32 <- Convolution33
I0825 11:12:22.137887  2068 net.cpp:509] ReLU32 -> Convolution33 (in-place)
I0825 11:12:22.142662  2068 net.cpp:172] Setting up ReLU32
I0825 11:12:22.142729  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.142761  2068 net.cpp:194] Memory required for data: 594280704
I0825 11:12:22.142793  2068 layer_factory.hpp:77] Creating layer Convolution34
I0825 11:12:22.142838  2068 net.cpp:128] Creating Layer Convolution34
I0825 11:12:22.142869  2068 net.cpp:558] Convolution34 <- Convolution33
I0825 11:12:22.142905  2068 net.cpp:522] Convolution34 -> Convolution34
I0825 11:12:22.162196  2068 net.cpp:172] Setting up Convolution34
I0825 11:12:22.162261  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.162294  2068 net.cpp:194] Memory required for data: 596377856
I0825 11:12:22.162339  2068 layer_factory.hpp:77] Creating layer BatchNorm34
I0825 11:12:22.162379  2068 net.cpp:128] Creating Layer BatchNorm34
I0825 11:12:22.162411  2068 net.cpp:558] BatchNorm34 <- Convolution34
I0825 11:12:22.162446  2068 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0825 11:12:22.162760  2068 net.cpp:172] Setting up BatchNorm34
I0825 11:12:22.162801  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.162832  2068 net.cpp:194] Memory required for data: 598475008
I0825 11:12:22.162871  2068 layer_factory.hpp:77] Creating layer Scale34
I0825 11:12:22.162906  2068 net.cpp:128] Creating Layer Scale34
I0825 11:12:22.162937  2068 net.cpp:558] Scale34 <- Convolution34
I0825 11:12:22.162971  2068 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0825 11:12:22.163060  2068 layer_factory.hpp:77] Creating layer Scale34
I0825 11:12:22.163252  2068 net.cpp:172] Setting up Scale34
I0825 11:12:22.163291  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.163322  2068 net.cpp:194] Memory required for data: 600572160
I0825 11:12:22.163358  2068 layer_factory.hpp:77] Creating layer Eltwise16
I0825 11:12:22.163398  2068 net.cpp:128] Creating Layer Eltwise16
I0825 11:12:22.163429  2068 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0825 11:12:22.163462  2068 net.cpp:558] Eltwise16 <- Convolution34
I0825 11:12:22.163497  2068 net.cpp:522] Eltwise16 -> Eltwise16
I0825 11:12:22.163556  2068 net.cpp:172] Setting up Eltwise16
I0825 11:12:22.163592  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.163624  2068 net.cpp:194] Memory required for data: 602669312
I0825 11:12:22.163655  2068 layer_factory.hpp:77] Creating layer ReLU33
I0825 11:12:22.163692  2068 net.cpp:128] Creating Layer ReLU33
I0825 11:12:22.163724  2068 net.cpp:558] ReLU33 <- Eltwise16
I0825 11:12:22.163758  2068 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0825 11:12:22.168584  2068 net.cpp:172] Setting up ReLU33
I0825 11:12:22.168649  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.168682  2068 net.cpp:194] Memory required for data: 604766464
I0825 11:12:22.168715  2068 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0825 11:12:22.168751  2068 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0825 11:12:22.168783  2068 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0825 11:12:22.168818  2068 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0825 11:12:22.168859  2068 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0825 11:12:22.168952  2068 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0825 11:12:22.168990  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.169030  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.169062  2068 net.cpp:194] Memory required for data: 608960768
I0825 11:12:22.169093  2068 layer_factory.hpp:77] Creating layer Convolution35
I0825 11:12:22.169137  2068 net.cpp:128] Creating Layer Convolution35
I0825 11:12:22.169169  2068 net.cpp:558] Convolution35 <- Eltwise16_ReLU33_0_split_0
I0825 11:12:22.169204  2068 net.cpp:522] Convolution35 -> Convolution35
I0825 11:12:22.184530  2068 net.cpp:172] Setting up Convolution35
I0825 11:12:22.184595  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.184628  2068 net.cpp:194] Memory required for data: 611057920
I0825 11:12:22.184676  2068 layer_factory.hpp:77] Creating layer BatchNorm35
I0825 11:12:22.184716  2068 net.cpp:128] Creating Layer BatchNorm35
I0825 11:12:22.184749  2068 net.cpp:558] BatchNorm35 <- Convolution35
I0825 11:12:22.184785  2068 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0825 11:12:22.185102  2068 net.cpp:172] Setting up BatchNorm35
I0825 11:12:22.185142  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.185173  2068 net.cpp:194] Memory required for data: 613155072
I0825 11:12:22.185212  2068 layer_factory.hpp:77] Creating layer Scale35
I0825 11:12:22.185250  2068 net.cpp:128] Creating Layer Scale35
I0825 11:12:22.185281  2068 net.cpp:558] Scale35 <- Convolution35
I0825 11:12:22.185314  2068 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0825 11:12:22.185401  2068 layer_factory.hpp:77] Creating layer Scale35
I0825 11:12:22.185597  2068 net.cpp:172] Setting up Scale35
I0825 11:12:22.185634  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.185667  2068 net.cpp:194] Memory required for data: 615252224
I0825 11:12:22.185703  2068 layer_factory.hpp:77] Creating layer ReLU34
I0825 11:12:22.185739  2068 net.cpp:128] Creating Layer ReLU34
I0825 11:12:22.185771  2068 net.cpp:558] ReLU34 <- Convolution35
I0825 11:12:22.185804  2068 net.cpp:509] ReLU34 -> Convolution35 (in-place)
I0825 11:12:22.188774  2068 net.cpp:172] Setting up ReLU34
I0825 11:12:22.188828  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.188860  2068 net.cpp:194] Memory required for data: 617349376
I0825 11:12:22.188894  2068 layer_factory.hpp:77] Creating layer Convolution36
I0825 11:12:22.188937  2068 net.cpp:128] Creating Layer Convolution36
I0825 11:12:22.188971  2068 net.cpp:558] Convolution36 <- Convolution35
I0825 11:12:22.189007  2068 net.cpp:522] Convolution36 -> Convolution36
I0825 11:12:22.202121  2068 net.cpp:172] Setting up Convolution36
I0825 11:12:22.202185  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.202219  2068 net.cpp:194] Memory required for data: 619446528
I0825 11:12:22.202258  2068 layer_factory.hpp:77] Creating layer BatchNorm36
I0825 11:12:22.202296  2068 net.cpp:128] Creating Layer BatchNorm36
I0825 11:12:22.202330  2068 net.cpp:558] BatchNorm36 <- Convolution36
I0825 11:12:22.202371  2068 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0825 11:12:22.202692  2068 net.cpp:172] Setting up BatchNorm36
I0825 11:12:22.202734  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.202765  2068 net.cpp:194] Memory required for data: 621543680
I0825 11:12:22.202805  2068 layer_factory.hpp:77] Creating layer Scale36
I0825 11:12:22.202841  2068 net.cpp:128] Creating Layer Scale36
I0825 11:12:22.202872  2068 net.cpp:558] Scale36 <- Convolution36
I0825 11:12:22.202908  2068 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0825 11:12:22.202993  2068 layer_factory.hpp:77] Creating layer Scale36
I0825 11:12:22.203187  2068 net.cpp:172] Setting up Scale36
I0825 11:12:22.203225  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.203258  2068 net.cpp:194] Memory required for data: 623640832
I0825 11:12:22.203294  2068 layer_factory.hpp:77] Creating layer Eltwise17
I0825 11:12:22.203330  2068 net.cpp:128] Creating Layer Eltwise17
I0825 11:12:22.203362  2068 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0825 11:12:22.203395  2068 net.cpp:558] Eltwise17 <- Convolution36
I0825 11:12:22.203439  2068 net.cpp:522] Eltwise17 -> Eltwise17
I0825 11:12:22.203498  2068 net.cpp:172] Setting up Eltwise17
I0825 11:12:22.203534  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.203567  2068 net.cpp:194] Memory required for data: 625737984
I0825 11:12:22.203598  2068 layer_factory.hpp:77] Creating layer ReLU35
I0825 11:12:22.203636  2068 net.cpp:128] Creating Layer ReLU35
I0825 11:12:22.203668  2068 net.cpp:558] ReLU35 <- Eltwise17
I0825 11:12:22.203701  2068 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0825 11:12:22.208461  2068 net.cpp:172] Setting up ReLU35
I0825 11:12:22.208513  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.208554  2068 net.cpp:194] Memory required for data: 627835136
I0825 11:12:22.208586  2068 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0825 11:12:22.208626  2068 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0825 11:12:22.208659  2068 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0825 11:12:22.208695  2068 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0825 11:12:22.208734  2068 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0825 11:12:22.208829  2068 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0825 11:12:22.208868  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.208902  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.208933  2068 net.cpp:194] Memory required for data: 632029440
I0825 11:12:22.208966  2068 layer_factory.hpp:77] Creating layer Convolution37
I0825 11:12:22.209012  2068 net.cpp:128] Creating Layer Convolution37
I0825 11:12:22.209044  2068 net.cpp:558] Convolution37 <- Eltwise17_ReLU35_0_split_0
I0825 11:12:22.209080  2068 net.cpp:522] Convolution37 -> Convolution37
I0825 11:12:22.228103  2068 net.cpp:172] Setting up Convolution37
I0825 11:12:22.228168  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.228200  2068 net.cpp:194] Memory required for data: 634126592
I0825 11:12:22.228238  2068 layer_factory.hpp:77] Creating layer BatchNorm37
I0825 11:12:22.228277  2068 net.cpp:128] Creating Layer BatchNorm37
I0825 11:12:22.228310  2068 net.cpp:558] BatchNorm37 <- Convolution37
I0825 11:12:22.228346  2068 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0825 11:12:22.228659  2068 net.cpp:172] Setting up BatchNorm37
I0825 11:12:22.228699  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.228730  2068 net.cpp:194] Memory required for data: 636223744
I0825 11:12:22.228792  2068 layer_factory.hpp:77] Creating layer Scale37
I0825 11:12:22.228829  2068 net.cpp:128] Creating Layer Scale37
I0825 11:12:22.228862  2068 net.cpp:558] Scale37 <- Convolution37
I0825 11:12:22.228894  2068 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0825 11:12:22.228986  2068 layer_factory.hpp:77] Creating layer Scale37
I0825 11:12:22.229180  2068 net.cpp:172] Setting up Scale37
I0825 11:12:22.229219  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.229250  2068 net.cpp:194] Memory required for data: 638320896
I0825 11:12:22.229287  2068 layer_factory.hpp:77] Creating layer ReLU36
I0825 11:12:22.229321  2068 net.cpp:128] Creating Layer ReLU36
I0825 11:12:22.229352  2068 net.cpp:558] ReLU36 <- Convolution37
I0825 11:12:22.229388  2068 net.cpp:509] ReLU36 -> Convolution37 (in-place)
I0825 11:12:22.234413  2068 net.cpp:172] Setting up ReLU36
I0825 11:12:22.234475  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.234508  2068 net.cpp:194] Memory required for data: 640418048
I0825 11:12:22.234540  2068 layer_factory.hpp:77] Creating layer Convolution38
I0825 11:12:22.234583  2068 net.cpp:128] Creating Layer Convolution38
I0825 11:12:22.234616  2068 net.cpp:558] Convolution38 <- Convolution37
I0825 11:12:22.234654  2068 net.cpp:522] Convolution38 -> Convolution38
I0825 11:12:22.244755  2068 net.cpp:172] Setting up Convolution38
I0825 11:12:22.244822  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.244854  2068 net.cpp:194] Memory required for data: 642515200
I0825 11:12:22.244899  2068 layer_factory.hpp:77] Creating layer BatchNorm38
I0825 11:12:22.244936  2068 net.cpp:128] Creating Layer BatchNorm38
I0825 11:12:22.244968  2068 net.cpp:558] BatchNorm38 <- Convolution38
I0825 11:12:22.245003  2068 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0825 11:12:22.245319  2068 net.cpp:172] Setting up BatchNorm38
I0825 11:12:22.245359  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.245391  2068 net.cpp:194] Memory required for data: 644612352
I0825 11:12:22.245429  2068 layer_factory.hpp:77] Creating layer Scale38
I0825 11:12:22.245467  2068 net.cpp:128] Creating Layer Scale38
I0825 11:12:22.245498  2068 net.cpp:558] Scale38 <- Convolution38
I0825 11:12:22.245543  2068 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0825 11:12:22.245633  2068 layer_factory.hpp:77] Creating layer Scale38
I0825 11:12:22.245827  2068 net.cpp:172] Setting up Scale38
I0825 11:12:22.245865  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.245898  2068 net.cpp:194] Memory required for data: 646709504
I0825 11:12:22.245934  2068 layer_factory.hpp:77] Creating layer Eltwise18
I0825 11:12:22.245972  2068 net.cpp:128] Creating Layer Eltwise18
I0825 11:12:22.246004  2068 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0825 11:12:22.246037  2068 net.cpp:558] Eltwise18 <- Convolution38
I0825 11:12:22.246074  2068 net.cpp:522] Eltwise18 -> Eltwise18
I0825 11:12:22.246132  2068 net.cpp:172] Setting up Eltwise18
I0825 11:12:22.246168  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.246199  2068 net.cpp:194] Memory required for data: 648806656
I0825 11:12:22.246232  2068 layer_factory.hpp:77] Creating layer ReLU37
I0825 11:12:22.246265  2068 net.cpp:128] Creating Layer ReLU37
I0825 11:12:22.246296  2068 net.cpp:558] ReLU37 <- Eltwise18
I0825 11:12:22.246340  2068 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0825 11:12:22.246623  2068 net.cpp:172] Setting up ReLU37
I0825 11:12:22.246676  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.246709  2068 net.cpp:194] Memory required for data: 650903808
I0825 11:12:22.246742  2068 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0825 11:12:22.246778  2068 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0825 11:12:22.246811  2068 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0825 11:12:22.246850  2068 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0825 11:12:22.246887  2068 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0825 11:12:22.246981  2068 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0825 11:12:22.247020  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.247052  2068 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:12:22.247083  2068 net.cpp:194] Memory required for data: 655098112
I0825 11:12:22.247115  2068 layer_factory.hpp:77] Creating layer Convolution39
I0825 11:12:22.247156  2068 net.cpp:128] Creating Layer Convolution39
I0825 11:12:22.247189  2068 net.cpp:558] Convolution39 <- Eltwise18_ReLU37_0_split_0
I0825 11:12:22.247228  2068 net.cpp:522] Convolution39 -> Convolution39
I0825 11:12:22.252704  2068 net.cpp:172] Setting up Convolution39
I0825 11:12:22.252771  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.252804  2068 net.cpp:194] Memory required for data: 656146688
I0825 11:12:22.252842  2068 layer_factory.hpp:77] Creating layer BatchNorm39
I0825 11:12:22.252882  2068 net.cpp:128] Creating Layer BatchNorm39
I0825 11:12:22.252914  2068 net.cpp:558] BatchNorm39 <- Convolution39
I0825 11:12:22.252954  2068 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0825 11:12:22.253280  2068 net.cpp:172] Setting up BatchNorm39
I0825 11:12:22.253321  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.253352  2068 net.cpp:194] Memory required for data: 657195264
I0825 11:12:22.253391  2068 layer_factory.hpp:77] Creating layer Scale39
I0825 11:12:22.253425  2068 net.cpp:128] Creating Layer Scale39
I0825 11:12:22.253456  2068 net.cpp:558] Scale39 <- Convolution39
I0825 11:12:22.253500  2068 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0825 11:12:22.253585  2068 layer_factory.hpp:77] Creating layer Scale39
I0825 11:12:22.253782  2068 net.cpp:172] Setting up Scale39
I0825 11:12:22.253819  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.253851  2068 net.cpp:194] Memory required for data: 658243840
I0825 11:12:22.253888  2068 layer_factory.hpp:77] Creating layer Convolution40
I0825 11:12:22.253929  2068 net.cpp:128] Creating Layer Convolution40
I0825 11:12:22.253962  2068 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_1
I0825 11:12:22.254000  2068 net.cpp:522] Convolution40 -> Convolution40
I0825 11:12:22.259785  2068 net.cpp:172] Setting up Convolution40
I0825 11:12:22.259867  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.259899  2068 net.cpp:194] Memory required for data: 659292416
I0825 11:12:22.259938  2068 layer_factory.hpp:77] Creating layer BatchNorm40
I0825 11:12:22.259977  2068 net.cpp:128] Creating Layer BatchNorm40
I0825 11:12:22.260010  2068 net.cpp:558] BatchNorm40 <- Convolution40
I0825 11:12:22.260046  2068 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0825 11:12:22.260377  2068 net.cpp:172] Setting up BatchNorm40
I0825 11:12:22.260417  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.260448  2068 net.cpp:194] Memory required for data: 660340992
I0825 11:12:22.260488  2068 layer_factory.hpp:77] Creating layer Scale40
I0825 11:12:22.260524  2068 net.cpp:128] Creating Layer Scale40
I0825 11:12:22.260556  2068 net.cpp:558] Scale40 <- Convolution40
I0825 11:12:22.260589  2068 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0825 11:12:22.260676  2068 layer_factory.hpp:77] Creating layer Scale40
I0825 11:12:22.260884  2068 net.cpp:172] Setting up Scale40
I0825 11:12:22.260922  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.260954  2068 net.cpp:194] Memory required for data: 661389568
I0825 11:12:22.260993  2068 layer_factory.hpp:77] Creating layer ReLU38
I0825 11:12:22.261031  2068 net.cpp:128] Creating Layer ReLU38
I0825 11:12:22.261062  2068 net.cpp:558] ReLU38 <- Convolution40
I0825 11:12:22.261096  2068 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0825 11:12:22.261384  2068 net.cpp:172] Setting up ReLU38
I0825 11:12:22.261433  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.261464  2068 net.cpp:194] Memory required for data: 662438144
I0825 11:12:22.261497  2068 layer_factory.hpp:77] Creating layer Convolution41
I0825 11:12:22.261541  2068 net.cpp:128] Creating Layer Convolution41
I0825 11:12:22.261574  2068 net.cpp:558] Convolution41 <- Convolution40
I0825 11:12:22.261611  2068 net.cpp:522] Convolution41 -> Convolution41
I0825 11:12:22.270473  2068 net.cpp:172] Setting up Convolution41
I0825 11:12:22.270561  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.270596  2068 net.cpp:194] Memory required for data: 663486720
I0825 11:12:22.270638  2068 layer_factory.hpp:77] Creating layer BatchNorm41
I0825 11:12:22.270679  2068 net.cpp:128] Creating Layer BatchNorm41
I0825 11:12:22.270714  2068 net.cpp:558] BatchNorm41 <- Convolution41
I0825 11:12:22.270751  2068 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0825 11:12:22.271090  2068 net.cpp:172] Setting up BatchNorm41
I0825 11:12:22.271131  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.271162  2068 net.cpp:194] Memory required for data: 664535296
I0825 11:12:22.271201  2068 layer_factory.hpp:77] Creating layer Scale41
I0825 11:12:22.271240  2068 net.cpp:128] Creating Layer Scale41
I0825 11:12:22.271272  2068 net.cpp:558] Scale41 <- Convolution41
I0825 11:12:22.271306  2068 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0825 11:12:22.271395  2068 layer_factory.hpp:77] Creating layer Scale41
I0825 11:12:22.271601  2068 net.cpp:172] Setting up Scale41
I0825 11:12:22.271641  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.271672  2068 net.cpp:194] Memory required for data: 665583872
I0825 11:12:22.271710  2068 layer_factory.hpp:77] Creating layer Eltwise19
I0825 11:12:22.271747  2068 net.cpp:128] Creating Layer Eltwise19
I0825 11:12:22.271795  2068 net.cpp:558] Eltwise19 <- Convolution39
I0825 11:12:22.271828  2068 net.cpp:558] Eltwise19 <- Convolution41
I0825 11:12:22.271863  2068 net.cpp:522] Eltwise19 -> Eltwise19
I0825 11:12:22.271932  2068 net.cpp:172] Setting up Eltwise19
I0825 11:12:22.271970  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.272001  2068 net.cpp:194] Memory required for data: 666632448
I0825 11:12:22.272033  2068 layer_factory.hpp:77] Creating layer ReLU39
I0825 11:12:22.272068  2068 net.cpp:128] Creating Layer ReLU39
I0825 11:12:22.272099  2068 net.cpp:558] ReLU39 <- Eltwise19
I0825 11:12:22.272132  2068 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0825 11:12:22.274412  2068 net.cpp:172] Setting up ReLU39
I0825 11:12:22.274719  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.274752  2068 net.cpp:194] Memory required for data: 667681024
I0825 11:12:22.274786  2068 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0825 11:12:22.274825  2068 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0825 11:12:22.274859  2068 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0825 11:12:22.274899  2068 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0825 11:12:22.274951  2068 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0825 11:12:22.275058  2068 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0825 11:12:22.275096  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.275130  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.275161  2068 net.cpp:194] Memory required for data: 669778176
I0825 11:12:22.275193  2068 layer_factory.hpp:77] Creating layer Convolution42
I0825 11:12:22.275236  2068 net.cpp:128] Creating Layer Convolution42
I0825 11:12:22.275269  2068 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0825 11:12:22.275308  2068 net.cpp:522] Convolution42 -> Convolution42
I0825 11:12:22.286916  2068 net.cpp:172] Setting up Convolution42
I0825 11:12:22.286983  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.287015  2068 net.cpp:194] Memory required for data: 670826752
I0825 11:12:22.287055  2068 layer_factory.hpp:77] Creating layer BatchNorm42
I0825 11:12:22.287092  2068 net.cpp:128] Creating Layer BatchNorm42
I0825 11:12:22.287124  2068 net.cpp:558] BatchNorm42 <- Convolution42
I0825 11:12:22.287161  2068 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0825 11:12:22.287492  2068 net.cpp:172] Setting up BatchNorm42
I0825 11:12:22.287533  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.287564  2068 net.cpp:194] Memory required for data: 671875328
I0825 11:12:22.287605  2068 layer_factory.hpp:77] Creating layer Scale42
I0825 11:12:22.287639  2068 net.cpp:128] Creating Layer Scale42
I0825 11:12:22.287670  2068 net.cpp:558] Scale42 <- Convolution42
I0825 11:12:22.287706  2068 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0825 11:12:22.287794  2068 layer_factory.hpp:77] Creating layer Scale42
I0825 11:12:22.287998  2068 net.cpp:172] Setting up Scale42
I0825 11:12:22.288036  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.288067  2068 net.cpp:194] Memory required for data: 672923904
I0825 11:12:22.288105  2068 layer_factory.hpp:77] Creating layer ReLU40
I0825 11:12:22.288142  2068 net.cpp:128] Creating Layer ReLU40
I0825 11:12:22.288173  2068 net.cpp:558] ReLU40 <- Convolution42
I0825 11:12:22.288208  2068 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0825 11:12:22.289065  2068 net.cpp:172] Setting up ReLU40
I0825 11:12:22.289130  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.289162  2068 net.cpp:194] Memory required for data: 673972480
I0825 11:12:22.289196  2068 layer_factory.hpp:77] Creating layer Convolution43
I0825 11:12:22.289239  2068 net.cpp:128] Creating Layer Convolution43
I0825 11:12:22.289273  2068 net.cpp:558] Convolution43 <- Convolution42
I0825 11:12:22.289309  2068 net.cpp:522] Convolution43 -> Convolution43
I0825 11:12:22.296228  2068 net.cpp:172] Setting up Convolution43
I0825 11:12:22.296301  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.296334  2068 net.cpp:194] Memory required for data: 675021056
I0825 11:12:22.296373  2068 layer_factory.hpp:77] Creating layer BatchNorm43
I0825 11:12:22.296411  2068 net.cpp:128] Creating Layer BatchNorm43
I0825 11:12:22.296443  2068 net.cpp:558] BatchNorm43 <- Convolution43
I0825 11:12:22.296480  2068 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0825 11:12:22.296820  2068 net.cpp:172] Setting up BatchNorm43
I0825 11:12:22.296862  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.296893  2068 net.cpp:194] Memory required for data: 676069632
I0825 11:12:22.296932  2068 layer_factory.hpp:77] Creating layer Scale43
I0825 11:12:22.296977  2068 net.cpp:128] Creating Layer Scale43
I0825 11:12:22.297009  2068 net.cpp:558] Scale43 <- Convolution43
I0825 11:12:22.297044  2068 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0825 11:12:22.297139  2068 layer_factory.hpp:77] Creating layer Scale43
I0825 11:12:22.297345  2068 net.cpp:172] Setting up Scale43
I0825 11:12:22.297384  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.297415  2068 net.cpp:194] Memory required for data: 677118208
I0825 11:12:22.297451  2068 layer_factory.hpp:77] Creating layer Eltwise20
I0825 11:12:22.297489  2068 net.cpp:128] Creating Layer Eltwise20
I0825 11:12:22.297521  2068 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0825 11:12:22.297555  2068 net.cpp:558] Eltwise20 <- Convolution43
I0825 11:12:22.297590  2068 net.cpp:522] Eltwise20 -> Eltwise20
I0825 11:12:22.297657  2068 net.cpp:172] Setting up Eltwise20
I0825 11:12:22.297693  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.297724  2068 net.cpp:194] Memory required for data: 678166784
I0825 11:12:22.297755  2068 layer_factory.hpp:77] Creating layer ReLU41
I0825 11:12:22.297788  2068 net.cpp:128] Creating Layer ReLU41
I0825 11:12:22.297821  2068 net.cpp:558] ReLU41 <- Eltwise20
I0825 11:12:22.297854  2068 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0825 11:12:22.298142  2068 net.cpp:172] Setting up ReLU41
I0825 11:12:22.298193  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.298224  2068 net.cpp:194] Memory required for data: 679215360
I0825 11:12:22.298257  2068 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0825 11:12:22.298295  2068 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0825 11:12:22.298327  2068 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0825 11:12:22.298375  2068 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0825 11:12:22.298415  2068 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0825 11:12:22.298514  2068 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0825 11:12:22.298552  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.298586  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.298617  2068 net.cpp:194] Memory required for data: 681312512
I0825 11:12:22.298650  2068 layer_factory.hpp:77] Creating layer Convolution44
I0825 11:12:22.298691  2068 net.cpp:128] Creating Layer Convolution44
I0825 11:12:22.298722  2068 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0825 11:12:22.298761  2068 net.cpp:522] Convolution44 -> Convolution44
I0825 11:12:22.310741  2068 net.cpp:172] Setting up Convolution44
I0825 11:12:22.310811  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.310843  2068 net.cpp:194] Memory required for data: 682361088
I0825 11:12:22.310883  2068 layer_factory.hpp:77] Creating layer BatchNorm44
I0825 11:12:22.310921  2068 net.cpp:128] Creating Layer BatchNorm44
I0825 11:12:22.310953  2068 net.cpp:558] BatchNorm44 <- Convolution44
I0825 11:12:22.310988  2068 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0825 11:12:22.311327  2068 net.cpp:172] Setting up BatchNorm44
I0825 11:12:22.311367  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.311399  2068 net.cpp:194] Memory required for data: 683409664
I0825 11:12:22.311439  2068 layer_factory.hpp:77] Creating layer Scale44
I0825 11:12:22.311484  2068 net.cpp:128] Creating Layer Scale44
I0825 11:12:22.311517  2068 net.cpp:558] Scale44 <- Convolution44
I0825 11:12:22.311550  2068 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0825 11:12:22.311640  2068 layer_factory.hpp:77] Creating layer Scale44
I0825 11:12:22.311848  2068 net.cpp:172] Setting up Scale44
I0825 11:12:22.311887  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.311919  2068 net.cpp:194] Memory required for data: 684458240
I0825 11:12:22.311957  2068 layer_factory.hpp:77] Creating layer ReLU42
I0825 11:12:22.311995  2068 net.cpp:128] Creating Layer ReLU42
I0825 11:12:22.312027  2068 net.cpp:558] ReLU42 <- Convolution44
I0825 11:12:22.312070  2068 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0825 11:12:22.314996  2068 net.cpp:172] Setting up ReLU42
I0825 11:12:22.315048  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.315080  2068 net.cpp:194] Memory required for data: 685506816
I0825 11:12:22.315114  2068 layer_factory.hpp:77] Creating layer Convolution45
I0825 11:12:22.315157  2068 net.cpp:128] Creating Layer Convolution45
I0825 11:12:22.315191  2068 net.cpp:558] Convolution45 <- Convolution44
I0825 11:12:22.315227  2068 net.cpp:522] Convolution45 -> Convolution45
I0825 11:12:22.328586  2068 net.cpp:172] Setting up Convolution45
I0825 11:12:22.328652  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.328685  2068 net.cpp:194] Memory required for data: 686555392
I0825 11:12:22.328723  2068 layer_factory.hpp:77] Creating layer BatchNorm45
I0825 11:12:22.328759  2068 net.cpp:128] Creating Layer BatchNorm45
I0825 11:12:22.328791  2068 net.cpp:558] BatchNorm45 <- Convolution45
I0825 11:12:22.328830  2068 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0825 11:12:22.329174  2068 net.cpp:172] Setting up BatchNorm45
I0825 11:12:22.329212  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.329244  2068 net.cpp:194] Memory required for data: 687603968
I0825 11:12:22.329283  2068 layer_factory.hpp:77] Creating layer Scale45
I0825 11:12:22.329321  2068 net.cpp:128] Creating Layer Scale45
I0825 11:12:22.329354  2068 net.cpp:558] Scale45 <- Convolution45
I0825 11:12:22.329386  2068 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0825 11:12:22.329474  2068 layer_factory.hpp:77] Creating layer Scale45
I0825 11:12:22.329684  2068 net.cpp:172] Setting up Scale45
I0825 11:12:22.329721  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.329751  2068 net.cpp:194] Memory required for data: 688652544
I0825 11:12:22.329789  2068 layer_factory.hpp:77] Creating layer Eltwise21
I0825 11:12:22.329828  2068 net.cpp:128] Creating Layer Eltwise21
I0825 11:12:22.329860  2068 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0825 11:12:22.329895  2068 net.cpp:558] Eltwise21 <- Convolution45
I0825 11:12:22.329931  2068 net.cpp:522] Eltwise21 -> Eltwise21
I0825 11:12:22.329994  2068 net.cpp:172] Setting up Eltwise21
I0825 11:12:22.330030  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.330061  2068 net.cpp:194] Memory required for data: 689701120
I0825 11:12:22.330093  2068 layer_factory.hpp:77] Creating layer ReLU43
I0825 11:12:22.330130  2068 net.cpp:128] Creating Layer ReLU43
I0825 11:12:22.330162  2068 net.cpp:558] ReLU43 <- Eltwise21
I0825 11:12:22.330194  2068 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0825 11:12:22.332734  2068 net.cpp:172] Setting up ReLU43
I0825 11:12:22.332800  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.332832  2068 net.cpp:194] Memory required for data: 690749696
I0825 11:12:22.332865  2068 layer_factory.hpp:77] Creating layer Eltwise21_ReLU43_0_split
I0825 11:12:22.332904  2068 net.cpp:128] Creating Layer Eltwise21_ReLU43_0_split
I0825 11:12:22.332937  2068 net.cpp:558] Eltwise21_ReLU43_0_split <- Eltwise21
I0825 11:12:22.332973  2068 net.cpp:522] Eltwise21_ReLU43_0_split -> Eltwise21_ReLU43_0_split_0
I0825 11:12:22.333011  2068 net.cpp:522] Eltwise21_ReLU43_0_split -> Eltwise21_ReLU43_0_split_1
I0825 11:12:22.333111  2068 net.cpp:172] Setting up Eltwise21_ReLU43_0_split
I0825 11:12:22.333153  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.333187  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.333217  2068 net.cpp:194] Memory required for data: 692846848
I0825 11:12:22.333250  2068 layer_factory.hpp:77] Creating layer Convolution46
I0825 11:12:22.333292  2068 net.cpp:128] Creating Layer Convolution46
I0825 11:12:22.333323  2068 net.cpp:558] Convolution46 <- Eltwise21_ReLU43_0_split_0
I0825 11:12:22.333362  2068 net.cpp:522] Convolution46 -> Convolution46
I0825 11:12:22.345994  2068 net.cpp:172] Setting up Convolution46
I0825 11:12:22.346060  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.346092  2068 net.cpp:194] Memory required for data: 693895424
I0825 11:12:22.346141  2068 layer_factory.hpp:77] Creating layer BatchNorm46
I0825 11:12:22.346179  2068 net.cpp:128] Creating Layer BatchNorm46
I0825 11:12:22.346212  2068 net.cpp:558] BatchNorm46 <- Convolution46
I0825 11:12:22.346251  2068 net.cpp:509] BatchNorm46 -> Convolution46 (in-place)
I0825 11:12:22.346598  2068 net.cpp:172] Setting up BatchNorm46
I0825 11:12:22.346640  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.346673  2068 net.cpp:194] Memory required for data: 694944000
I0825 11:12:22.346711  2068 layer_factory.hpp:77] Creating layer Scale46
I0825 11:12:22.346748  2068 net.cpp:128] Creating Layer Scale46
I0825 11:12:22.346781  2068 net.cpp:558] Scale46 <- Convolution46
I0825 11:12:22.346813  2068 net.cpp:509] Scale46 -> Convolution46 (in-place)
I0825 11:12:22.346904  2068 layer_factory.hpp:77] Creating layer Scale46
I0825 11:12:22.347115  2068 net.cpp:172] Setting up Scale46
I0825 11:12:22.347153  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.347185  2068 net.cpp:194] Memory required for data: 695992576
I0825 11:12:22.347223  2068 layer_factory.hpp:77] Creating layer ReLU44
I0825 11:12:22.347256  2068 net.cpp:128] Creating Layer ReLU44
I0825 11:12:22.347288  2068 net.cpp:558] ReLU44 <- Convolution46
I0825 11:12:22.347323  2068 net.cpp:509] ReLU44 -> Convolution46 (in-place)
I0825 11:12:22.350185  2068 net.cpp:172] Setting up ReLU44
I0825 11:12:22.350235  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.350267  2068 net.cpp:194] Memory required for data: 697041152
I0825 11:12:22.350301  2068 layer_factory.hpp:77] Creating layer Convolution47
I0825 11:12:22.350349  2068 net.cpp:128] Creating Layer Convolution47
I0825 11:12:22.350383  2068 net.cpp:558] Convolution47 <- Convolution46
I0825 11:12:22.350421  2068 net.cpp:522] Convolution47 -> Convolution47
I0825 11:12:22.367945  2068 net.cpp:172] Setting up Convolution47
I0825 11:12:22.368014  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.368046  2068 net.cpp:194] Memory required for data: 698089728
I0825 11:12:22.368086  2068 layer_factory.hpp:77] Creating layer BatchNorm47
I0825 11:12:22.368124  2068 net.cpp:128] Creating Layer BatchNorm47
I0825 11:12:22.368156  2068 net.cpp:558] BatchNorm47 <- Convolution47
I0825 11:12:22.368191  2068 net.cpp:509] BatchNorm47 -> Convolution47 (in-place)
I0825 11:12:22.368542  2068 net.cpp:172] Setting up BatchNorm47
I0825 11:12:22.368583  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.368614  2068 net.cpp:194] Memory required for data: 699138304
I0825 11:12:22.368654  2068 layer_factory.hpp:77] Creating layer Scale47
I0825 11:12:22.368691  2068 net.cpp:128] Creating Layer Scale47
I0825 11:12:22.368722  2068 net.cpp:558] Scale47 <- Convolution47
I0825 11:12:22.368757  2068 net.cpp:509] Scale47 -> Convolution47 (in-place)
I0825 11:12:22.368845  2068 layer_factory.hpp:77] Creating layer Scale47
I0825 11:12:22.369058  2068 net.cpp:172] Setting up Scale47
I0825 11:12:22.369098  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.369129  2068 net.cpp:194] Memory required for data: 700186880
I0825 11:12:22.369168  2068 layer_factory.hpp:77] Creating layer Eltwise22
I0825 11:12:22.369202  2068 net.cpp:128] Creating Layer Eltwise22
I0825 11:12:22.369235  2068 net.cpp:558] Eltwise22 <- Eltwise21_ReLU43_0_split_1
I0825 11:12:22.369276  2068 net.cpp:558] Eltwise22 <- Convolution47
I0825 11:12:22.369313  2068 net.cpp:522] Eltwise22 -> Eltwise22
I0825 11:12:22.369379  2068 net.cpp:172] Setting up Eltwise22
I0825 11:12:22.369415  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.369446  2068 net.cpp:194] Memory required for data: 701235456
I0825 11:12:22.369477  2068 layer_factory.hpp:77] Creating layer ReLU45
I0825 11:12:22.369514  2068 net.cpp:128] Creating Layer ReLU45
I0825 11:12:22.369546  2068 net.cpp:558] ReLU45 <- Eltwise22
I0825 11:12:22.369578  2068 net.cpp:509] ReLU45 -> Eltwise22 (in-place)
I0825 11:12:22.373322  2068 net.cpp:172] Setting up ReLU45
I0825 11:12:22.373374  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.373419  2068 net.cpp:194] Memory required for data: 702284032
I0825 11:12:22.373450  2068 layer_factory.hpp:77] Creating layer Eltwise22_ReLU45_0_split
I0825 11:12:22.373492  2068 net.cpp:128] Creating Layer Eltwise22_ReLU45_0_split
I0825 11:12:22.373524  2068 net.cpp:558] Eltwise22_ReLU45_0_split <- Eltwise22
I0825 11:12:22.373564  2068 net.cpp:522] Eltwise22_ReLU45_0_split -> Eltwise22_ReLU45_0_split_0
I0825 11:12:22.373602  2068 net.cpp:522] Eltwise22_ReLU45_0_split -> Eltwise22_ReLU45_0_split_1
I0825 11:12:22.373700  2068 net.cpp:172] Setting up Eltwise22_ReLU45_0_split
I0825 11:12:22.373740  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.373775  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.373805  2068 net.cpp:194] Memory required for data: 704381184
I0825 11:12:22.373837  2068 layer_factory.hpp:77] Creating layer Convolution48
I0825 11:12:22.373878  2068 net.cpp:128] Creating Layer Convolution48
I0825 11:12:22.373910  2068 net.cpp:558] Convolution48 <- Eltwise22_ReLU45_0_split_0
I0825 11:12:22.373947  2068 net.cpp:522] Convolution48 -> Convolution48
I0825 11:12:22.386602  2068 net.cpp:172] Setting up Convolution48
I0825 11:12:22.386670  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.386703  2068 net.cpp:194] Memory required for data: 705429760
I0825 11:12:22.386742  2068 layer_factory.hpp:77] Creating layer BatchNorm48
I0825 11:12:22.386780  2068 net.cpp:128] Creating Layer BatchNorm48
I0825 11:12:22.386811  2068 net.cpp:558] BatchNorm48 <- Convolution48
I0825 11:12:22.386849  2068 net.cpp:509] BatchNorm48 -> Convolution48 (in-place)
I0825 11:12:22.387195  2068 net.cpp:172] Setting up BatchNorm48
I0825 11:12:22.387236  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.387267  2068 net.cpp:194] Memory required for data: 706478336
I0825 11:12:22.387307  2068 layer_factory.hpp:77] Creating layer Scale48
I0825 11:12:22.387341  2068 net.cpp:128] Creating Layer Scale48
I0825 11:12:22.387373  2068 net.cpp:558] Scale48 <- Convolution48
I0825 11:12:22.387409  2068 net.cpp:509] Scale48 -> Convolution48 (in-place)
I0825 11:12:22.387496  2068 layer_factory.hpp:77] Creating layer Scale48
I0825 11:12:22.387709  2068 net.cpp:172] Setting up Scale48
I0825 11:12:22.387747  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.387778  2068 net.cpp:194] Memory required for data: 707526912
I0825 11:12:22.387815  2068 layer_factory.hpp:77] Creating layer ReLU46
I0825 11:12:22.387850  2068 net.cpp:128] Creating Layer ReLU46
I0825 11:12:22.387881  2068 net.cpp:558] ReLU46 <- Convolution48
I0825 11:12:22.387917  2068 net.cpp:509] ReLU46 -> Convolution48 (in-place)
I0825 11:12:22.392887  2068 net.cpp:172] Setting up ReLU46
I0825 11:12:22.392953  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.392985  2068 net.cpp:194] Memory required for data: 708575488
I0825 11:12:22.393018  2068 layer_factory.hpp:77] Creating layer Convolution49
I0825 11:12:22.393059  2068 net.cpp:128] Creating Layer Convolution49
I0825 11:12:22.393092  2068 net.cpp:558] Convolution49 <- Convolution48
I0825 11:12:22.393131  2068 net.cpp:522] Convolution49 -> Convolution49
I0825 11:12:22.406937  2068 net.cpp:172] Setting up Convolution49
I0825 11:12:22.407006  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.407037  2068 net.cpp:194] Memory required for data: 709624064
I0825 11:12:22.407083  2068 layer_factory.hpp:77] Creating layer BatchNorm49
I0825 11:12:22.407122  2068 net.cpp:128] Creating Layer BatchNorm49
I0825 11:12:22.407155  2068 net.cpp:558] BatchNorm49 <- Convolution49
I0825 11:12:22.407189  2068 net.cpp:509] BatchNorm49 -> Convolution49 (in-place)
I0825 11:12:22.407542  2068 net.cpp:172] Setting up BatchNorm49
I0825 11:12:22.407582  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.407613  2068 net.cpp:194] Memory required for data: 710672640
I0825 11:12:22.407651  2068 layer_factory.hpp:77] Creating layer Scale49
I0825 11:12:22.407686  2068 net.cpp:128] Creating Layer Scale49
I0825 11:12:22.407727  2068 net.cpp:558] Scale49 <- Convolution49
I0825 11:12:22.407766  2068 net.cpp:509] Scale49 -> Convolution49 (in-place)
I0825 11:12:22.407860  2068 layer_factory.hpp:77] Creating layer Scale49
I0825 11:12:22.408072  2068 net.cpp:172] Setting up Scale49
I0825 11:12:22.408110  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.408141  2068 net.cpp:194] Memory required for data: 711721216
I0825 11:12:22.408179  2068 layer_factory.hpp:77] Creating layer Eltwise23
I0825 11:12:22.408215  2068 net.cpp:128] Creating Layer Eltwise23
I0825 11:12:22.408248  2068 net.cpp:558] Eltwise23 <- Eltwise22_ReLU45_0_split_1
I0825 11:12:22.408282  2068 net.cpp:558] Eltwise23 <- Convolution49
I0825 11:12:22.408318  2068 net.cpp:522] Eltwise23 -> Eltwise23
I0825 11:12:22.408387  2068 net.cpp:172] Setting up Eltwise23
I0825 11:12:22.408423  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.408454  2068 net.cpp:194] Memory required for data: 712769792
I0825 11:12:22.408485  2068 layer_factory.hpp:77] Creating layer ReLU47
I0825 11:12:22.408519  2068 net.cpp:128] Creating Layer ReLU47
I0825 11:12:22.408551  2068 net.cpp:558] ReLU47 <- Eltwise23
I0825 11:12:22.408584  2068 net.cpp:509] ReLU47 -> Eltwise23 (in-place)
I0825 11:12:22.410923  2068 net.cpp:172] Setting up ReLU47
I0825 11:12:22.410976  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.411007  2068 net.cpp:194] Memory required for data: 713818368
I0825 11:12:22.411039  2068 layer_factory.hpp:77] Creating layer Eltwise23_ReLU47_0_split
I0825 11:12:22.411077  2068 net.cpp:128] Creating Layer Eltwise23_ReLU47_0_split
I0825 11:12:22.411109  2068 net.cpp:558] Eltwise23_ReLU47_0_split <- Eltwise23
I0825 11:12:22.411149  2068 net.cpp:522] Eltwise23_ReLU47_0_split -> Eltwise23_ReLU47_0_split_0
I0825 11:12:22.411187  2068 net.cpp:522] Eltwise23_ReLU47_0_split -> Eltwise23_ReLU47_0_split_1
I0825 11:12:22.411284  2068 net.cpp:172] Setting up Eltwise23_ReLU47_0_split
I0825 11:12:22.411322  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.411355  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.411386  2068 net.cpp:194] Memory required for data: 715915520
I0825 11:12:22.411418  2068 layer_factory.hpp:77] Creating layer Convolution50
I0825 11:12:22.411461  2068 net.cpp:128] Creating Layer Convolution50
I0825 11:12:22.411494  2068 net.cpp:558] Convolution50 <- Eltwise23_ReLU47_0_split_0
I0825 11:12:22.411530  2068 net.cpp:522] Convolution50 -> Convolution50
I0825 11:12:22.426358  2068 net.cpp:172] Setting up Convolution50
I0825 11:12:22.426425  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.426457  2068 net.cpp:194] Memory required for data: 716964096
I0825 11:12:22.426497  2068 layer_factory.hpp:77] Creating layer BatchNorm50
I0825 11:12:22.426537  2068 net.cpp:128] Creating Layer BatchNorm50
I0825 11:12:22.426569  2068 net.cpp:558] BatchNorm50 <- Convolution50
I0825 11:12:22.426606  2068 net.cpp:509] BatchNorm50 -> Convolution50 (in-place)
I0825 11:12:22.426954  2068 net.cpp:172] Setting up BatchNorm50
I0825 11:12:22.426993  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.427024  2068 net.cpp:194] Memory required for data: 718012672
I0825 11:12:22.427063  2068 layer_factory.hpp:77] Creating layer Scale50
I0825 11:12:22.427098  2068 net.cpp:128] Creating Layer Scale50
I0825 11:12:22.427130  2068 net.cpp:558] Scale50 <- Convolution50
I0825 11:12:22.427172  2068 net.cpp:509] Scale50 -> Convolution50 (in-place)
I0825 11:12:22.427271  2068 layer_factory.hpp:77] Creating layer Scale50
I0825 11:12:22.427482  2068 net.cpp:172] Setting up Scale50
I0825 11:12:22.427520  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.427551  2068 net.cpp:194] Memory required for data: 719061248
I0825 11:12:22.427592  2068 layer_factory.hpp:77] Creating layer ReLU48
I0825 11:12:22.427629  2068 net.cpp:128] Creating Layer ReLU48
I0825 11:12:22.427661  2068 net.cpp:558] ReLU48 <- Convolution50
I0825 11:12:22.427695  2068 net.cpp:509] ReLU48 -> Convolution50 (in-place)
I0825 11:12:22.432602  2068 net.cpp:172] Setting up ReLU48
I0825 11:12:22.432669  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.432701  2068 net.cpp:194] Memory required for data: 720109824
I0825 11:12:22.432734  2068 layer_factory.hpp:77] Creating layer Convolution51
I0825 11:12:22.432777  2068 net.cpp:128] Creating Layer Convolution51
I0825 11:12:22.432811  2068 net.cpp:558] Convolution51 <- Convolution50
I0825 11:12:22.432848  2068 net.cpp:522] Convolution51 -> Convolution51
I0825 11:12:22.452500  2068 net.cpp:172] Setting up Convolution51
I0825 11:12:22.452569  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.452601  2068 net.cpp:194] Memory required for data: 721158400
I0825 11:12:22.452641  2068 layer_factory.hpp:77] Creating layer BatchNorm51
I0825 11:12:22.452679  2068 net.cpp:128] Creating Layer BatchNorm51
I0825 11:12:22.452713  2068 net.cpp:558] BatchNorm51 <- Convolution51
I0825 11:12:22.452749  2068 net.cpp:509] BatchNorm51 -> Convolution51 (in-place)
I0825 11:12:22.453104  2068 net.cpp:172] Setting up BatchNorm51
I0825 11:12:22.453145  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.453176  2068 net.cpp:194] Memory required for data: 722206976
I0825 11:12:22.453215  2068 layer_factory.hpp:77] Creating layer Scale51
I0825 11:12:22.453249  2068 net.cpp:128] Creating Layer Scale51
I0825 11:12:22.453280  2068 net.cpp:558] Scale51 <- Convolution51
I0825 11:12:22.453315  2068 net.cpp:509] Scale51 -> Convolution51 (in-place)
I0825 11:12:22.453405  2068 layer_factory.hpp:77] Creating layer Scale51
I0825 11:12:22.453621  2068 net.cpp:172] Setting up Scale51
I0825 11:12:22.453660  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.453691  2068 net.cpp:194] Memory required for data: 723255552
I0825 11:12:22.453728  2068 layer_factory.hpp:77] Creating layer Eltwise24
I0825 11:12:22.453766  2068 net.cpp:128] Creating Layer Eltwise24
I0825 11:12:22.453799  2068 net.cpp:558] Eltwise24 <- Eltwise23_ReLU47_0_split_1
I0825 11:12:22.453832  2068 net.cpp:558] Eltwise24 <- Convolution51
I0825 11:12:22.453867  2068 net.cpp:522] Eltwise24 -> Eltwise24
I0825 11:12:22.453936  2068 net.cpp:172] Setting up Eltwise24
I0825 11:12:22.453972  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.454002  2068 net.cpp:194] Memory required for data: 724304128
I0825 11:12:22.454035  2068 layer_factory.hpp:77] Creating layer ReLU49
I0825 11:12:22.454068  2068 net.cpp:128] Creating Layer ReLU49
I0825 11:12:22.454099  2068 net.cpp:558] ReLU49 <- Eltwise24
I0825 11:12:22.454133  2068 net.cpp:509] ReLU49 -> Eltwise24 (in-place)
I0825 11:12:22.458513  2068 net.cpp:172] Setting up ReLU49
I0825 11:12:22.458566  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.458598  2068 net.cpp:194] Memory required for data: 725352704
I0825 11:12:22.458631  2068 layer_factory.hpp:77] Creating layer Eltwise24_ReLU49_0_split
I0825 11:12:22.458668  2068 net.cpp:128] Creating Layer Eltwise24_ReLU49_0_split
I0825 11:12:22.458701  2068 net.cpp:558] Eltwise24_ReLU49_0_split <- Eltwise24
I0825 11:12:22.458739  2068 net.cpp:522] Eltwise24_ReLU49_0_split -> Eltwise24_ReLU49_0_split_0
I0825 11:12:22.458777  2068 net.cpp:522] Eltwise24_ReLU49_0_split -> Eltwise24_ReLU49_0_split_1
I0825 11:12:22.458878  2068 net.cpp:172] Setting up Eltwise24_ReLU49_0_split
I0825 11:12:22.458915  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.458950  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.458986  2068 net.cpp:194] Memory required for data: 727449856
I0825 11:12:22.459018  2068 layer_factory.hpp:77] Creating layer Convolution52
I0825 11:12:22.459059  2068 net.cpp:128] Creating Layer Convolution52
I0825 11:12:22.459091  2068 net.cpp:558] Convolution52 <- Eltwise24_ReLU49_0_split_0
I0825 11:12:22.459131  2068 net.cpp:522] Convolution52 -> Convolution52
I0825 11:12:22.474160  2068 net.cpp:172] Setting up Convolution52
I0825 11:12:22.474205  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.474221  2068 net.cpp:194] Memory required for data: 728498432
I0825 11:12:22.474247  2068 layer_factory.hpp:77] Creating layer BatchNorm52
I0825 11:12:22.474279  2068 net.cpp:128] Creating Layer BatchNorm52
I0825 11:12:22.474297  2068 net.cpp:558] BatchNorm52 <- Convolution52
I0825 11:12:22.474318  2068 net.cpp:509] BatchNorm52 -> Convolution52 (in-place)
I0825 11:12:22.474659  2068 net.cpp:172] Setting up BatchNorm52
I0825 11:12:22.474684  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.474700  2068 net.cpp:194] Memory required for data: 729547008
I0825 11:12:22.474723  2068 layer_factory.hpp:77] Creating layer Scale52
I0825 11:12:22.474745  2068 net.cpp:128] Creating Layer Scale52
I0825 11:12:22.474762  2068 net.cpp:558] Scale52 <- Convolution52
I0825 11:12:22.474781  2068 net.cpp:509] Scale52 -> Convolution52 (in-place)
I0825 11:12:22.474854  2068 layer_factory.hpp:77] Creating layer Scale52
I0825 11:12:22.475052  2068 net.cpp:172] Setting up Scale52
I0825 11:12:22.475075  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.475091  2068 net.cpp:194] Memory required for data: 730595584
I0825 11:12:22.475112  2068 layer_factory.hpp:77] Creating layer ReLU50
I0825 11:12:22.475137  2068 net.cpp:128] Creating Layer ReLU50
I0825 11:12:22.475154  2068 net.cpp:558] ReLU50 <- Convolution52
I0825 11:12:22.475172  2068 net.cpp:509] ReLU50 -> Convolution52 (in-place)
I0825 11:12:22.478392  2068 net.cpp:172] Setting up ReLU50
I0825 11:12:22.478436  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.478453  2068 net.cpp:194] Memory required for data: 731644160
I0825 11:12:22.478471  2068 layer_factory.hpp:77] Creating layer Convolution53
I0825 11:12:22.478515  2068 net.cpp:128] Creating Layer Convolution53
I0825 11:12:22.478533  2068 net.cpp:558] Convolution53 <- Convolution52
I0825 11:12:22.478554  2068 net.cpp:522] Convolution53 -> Convolution53
I0825 11:12:22.491904  2068 net.cpp:172] Setting up Convolution53
I0825 11:12:22.491950  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.491966  2068 net.cpp:194] Memory required for data: 732692736
I0825 11:12:22.491991  2068 layer_factory.hpp:77] Creating layer BatchNorm53
I0825 11:12:22.492013  2068 net.cpp:128] Creating Layer BatchNorm53
I0825 11:12:22.492030  2068 net.cpp:558] BatchNorm53 <- Convolution53
I0825 11:12:22.492048  2068 net.cpp:509] BatchNorm53 -> Convolution53 (in-place)
I0825 11:12:22.492385  2068 net.cpp:172] Setting up BatchNorm53
I0825 11:12:22.492408  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.492424  2068 net.cpp:194] Memory required for data: 733741312
I0825 11:12:22.492446  2068 layer_factory.hpp:77] Creating layer Scale53
I0825 11:12:22.492468  2068 net.cpp:128] Creating Layer Scale53
I0825 11:12:22.492485  2068 net.cpp:558] Scale53 <- Convolution53
I0825 11:12:22.492502  2068 net.cpp:509] Scale53 -> Convolution53 (in-place)
I0825 11:12:22.492574  2068 layer_factory.hpp:77] Creating layer Scale53
I0825 11:12:22.492775  2068 net.cpp:172] Setting up Scale53
I0825 11:12:22.492799  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.492815  2068 net.cpp:194] Memory required for data: 734789888
I0825 11:12:22.492836  2068 layer_factory.hpp:77] Creating layer Eltwise25
I0825 11:12:22.492856  2068 net.cpp:128] Creating Layer Eltwise25
I0825 11:12:22.492872  2068 net.cpp:558] Eltwise25 <- Eltwise24_ReLU49_0_split_1
I0825 11:12:22.492890  2068 net.cpp:558] Eltwise25 <- Convolution53
I0825 11:12:22.492911  2068 net.cpp:522] Eltwise25 -> Eltwise25
I0825 11:12:22.492969  2068 net.cpp:172] Setting up Eltwise25
I0825 11:12:22.492988  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.493005  2068 net.cpp:194] Memory required for data: 735838464
I0825 11:12:22.493022  2068 layer_factory.hpp:77] Creating layer ReLU51
I0825 11:12:22.493042  2068 net.cpp:128] Creating Layer ReLU51
I0825 11:12:22.493060  2068 net.cpp:558] ReLU51 <- Eltwise25
I0825 11:12:22.493078  2068 net.cpp:509] ReLU51 -> Eltwise25 (in-place)
I0825 11:12:22.495826  2068 net.cpp:172] Setting up ReLU51
I0825 11:12:22.495865  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.495882  2068 net.cpp:194] Memory required for data: 736887040
I0825 11:12:22.495909  2068 layer_factory.hpp:77] Creating layer Eltwise25_ReLU51_0_split
I0825 11:12:22.495932  2068 net.cpp:128] Creating Layer Eltwise25_ReLU51_0_split
I0825 11:12:22.495949  2068 net.cpp:558] Eltwise25_ReLU51_0_split <- Eltwise25
I0825 11:12:22.495970  2068 net.cpp:522] Eltwise25_ReLU51_0_split -> Eltwise25_ReLU51_0_split_0
I0825 11:12:22.495991  2068 net.cpp:522] Eltwise25_ReLU51_0_split -> Eltwise25_ReLU51_0_split_1
I0825 11:12:22.496070  2068 net.cpp:172] Setting up Eltwise25_ReLU51_0_split
I0825 11:12:22.496091  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.496110  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.496125  2068 net.cpp:194] Memory required for data: 738984192
I0825 11:12:22.496141  2068 layer_factory.hpp:77] Creating layer Convolution54
I0825 11:12:22.496165  2068 net.cpp:128] Creating Layer Convolution54
I0825 11:12:22.496182  2068 net.cpp:558] Convolution54 <- Eltwise25_ReLU51_0_split_0
I0825 11:12:22.496202  2068 net.cpp:522] Convolution54 -> Convolution54
I0825 11:12:22.515488  2068 net.cpp:172] Setting up Convolution54
I0825 11:12:22.515542  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.515558  2068 net.cpp:194] Memory required for data: 740032768
I0825 11:12:22.515585  2068 layer_factory.hpp:77] Creating layer BatchNorm54
I0825 11:12:22.515610  2068 net.cpp:128] Creating Layer BatchNorm54
I0825 11:12:22.515628  2068 net.cpp:558] BatchNorm54 <- Convolution54
I0825 11:12:22.515650  2068 net.cpp:509] BatchNorm54 -> Convolution54 (in-place)
I0825 11:12:22.515985  2068 net.cpp:172] Setting up BatchNorm54
I0825 11:12:22.516006  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.516022  2068 net.cpp:194] Memory required for data: 741081344
I0825 11:12:22.516047  2068 layer_factory.hpp:77] Creating layer Scale54
I0825 11:12:22.516067  2068 net.cpp:128] Creating Layer Scale54
I0825 11:12:22.516084  2068 net.cpp:558] Scale54 <- Convolution54
I0825 11:12:22.516103  2068 net.cpp:509] Scale54 -> Convolution54 (in-place)
I0825 11:12:22.516180  2068 layer_factory.hpp:77] Creating layer Scale54
I0825 11:12:22.516372  2068 net.cpp:172] Setting up Scale54
I0825 11:12:22.516397  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.516413  2068 net.cpp:194] Memory required for data: 742129920
I0825 11:12:22.516434  2068 layer_factory.hpp:77] Creating layer ReLU52
I0825 11:12:22.516454  2068 net.cpp:128] Creating Layer ReLU52
I0825 11:12:22.516471  2068 net.cpp:558] ReLU52 <- Convolution54
I0825 11:12:22.516489  2068 net.cpp:509] ReLU52 -> Convolution54 (in-place)
I0825 11:12:22.521806  2068 net.cpp:172] Setting up ReLU52
I0825 11:12:22.521841  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.521858  2068 net.cpp:194] Memory required for data: 743178496
I0825 11:12:22.521875  2068 layer_factory.hpp:77] Creating layer Convolution55
I0825 11:12:22.521903  2068 net.cpp:128] Creating Layer Convolution55
I0825 11:12:22.521919  2068 net.cpp:558] Convolution55 <- Convolution54
I0825 11:12:22.521941  2068 net.cpp:522] Convolution55 -> Convolution55
I0825 11:12:22.540717  2068 net.cpp:172] Setting up Convolution55
I0825 11:12:22.540766  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.540781  2068 net.cpp:194] Memory required for data: 744227072
I0825 11:12:22.540807  2068 layer_factory.hpp:77] Creating layer BatchNorm55
I0825 11:12:22.540838  2068 net.cpp:128] Creating Layer BatchNorm55
I0825 11:12:22.540855  2068 net.cpp:558] BatchNorm55 <- Convolution55
I0825 11:12:22.540877  2068 net.cpp:509] BatchNorm55 -> Convolution55 (in-place)
I0825 11:12:22.541223  2068 net.cpp:172] Setting up BatchNorm55
I0825 11:12:22.541245  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.541262  2068 net.cpp:194] Memory required for data: 745275648
I0825 11:12:22.541285  2068 layer_factory.hpp:77] Creating layer Scale55
I0825 11:12:22.541306  2068 net.cpp:128] Creating Layer Scale55
I0825 11:12:22.541321  2068 net.cpp:558] Scale55 <- Convolution55
I0825 11:12:22.541339  2068 net.cpp:509] Scale55 -> Convolution55 (in-place)
I0825 11:12:22.541429  2068 layer_factory.hpp:77] Creating layer Scale55
I0825 11:12:22.541623  2068 net.cpp:172] Setting up Scale55
I0825 11:12:22.541644  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.541661  2068 net.cpp:194] Memory required for data: 746324224
I0825 11:12:22.541682  2068 layer_factory.hpp:77] Creating layer Eltwise26
I0825 11:12:22.541705  2068 net.cpp:128] Creating Layer Eltwise26
I0825 11:12:22.541723  2068 net.cpp:558] Eltwise26 <- Eltwise25_ReLU51_0_split_1
I0825 11:12:22.541740  2068 net.cpp:558] Eltwise26 <- Convolution55
I0825 11:12:22.541759  2068 net.cpp:522] Eltwise26 -> Eltwise26
I0825 11:12:22.541813  2068 net.cpp:172] Setting up Eltwise26
I0825 11:12:22.541833  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.541851  2068 net.cpp:194] Memory required for data: 747372800
I0825 11:12:22.541867  2068 layer_factory.hpp:77] Creating layer ReLU53
I0825 11:12:22.541884  2068 net.cpp:128] Creating Layer ReLU53
I0825 11:12:22.541901  2068 net.cpp:558] ReLU53 <- Eltwise26
I0825 11:12:22.541919  2068 net.cpp:509] ReLU53 -> Eltwise26 (in-place)
I0825 11:12:22.542920  2068 net.cpp:172] Setting up ReLU53
I0825 11:12:22.542968  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.542984  2068 net.cpp:194] Memory required for data: 748421376
I0825 11:12:22.543001  2068 layer_factory.hpp:77] Creating layer Eltwise26_ReLU53_0_split
I0825 11:12:22.543021  2068 net.cpp:128] Creating Layer Eltwise26_ReLU53_0_split
I0825 11:12:22.543040  2068 net.cpp:558] Eltwise26_ReLU53_0_split <- Eltwise26
I0825 11:12:22.543061  2068 net.cpp:522] Eltwise26_ReLU53_0_split -> Eltwise26_ReLU53_0_split_0
I0825 11:12:22.543082  2068 net.cpp:522] Eltwise26_ReLU53_0_split -> Eltwise26_ReLU53_0_split_1
I0825 11:12:22.543165  2068 net.cpp:172] Setting up Eltwise26_ReLU53_0_split
I0825 11:12:22.543186  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.543205  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.543220  2068 net.cpp:194] Memory required for data: 750518528
I0825 11:12:22.543236  2068 layer_factory.hpp:77] Creating layer Convolution56
I0825 11:12:22.543260  2068 net.cpp:128] Creating Layer Convolution56
I0825 11:12:22.543277  2068 net.cpp:558] Convolution56 <- Eltwise26_ReLU53_0_split_0
I0825 11:12:22.543299  2068 net.cpp:522] Convolution56 -> Convolution56
I0825 11:12:22.549944  2068 net.cpp:172] Setting up Convolution56
I0825 11:12:22.549989  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.550006  2068 net.cpp:194] Memory required for data: 751567104
I0825 11:12:22.550029  2068 layer_factory.hpp:77] Creating layer BatchNorm56
I0825 11:12:22.550052  2068 net.cpp:128] Creating Layer BatchNorm56
I0825 11:12:22.550071  2068 net.cpp:558] BatchNorm56 <- Convolution56
I0825 11:12:22.550088  2068 net.cpp:509] BatchNorm56 -> Convolution56 (in-place)
I0825 11:12:22.550431  2068 net.cpp:172] Setting up BatchNorm56
I0825 11:12:22.550457  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.550472  2068 net.cpp:194] Memory required for data: 752615680
I0825 11:12:22.550495  2068 layer_factory.hpp:77] Creating layer Scale56
I0825 11:12:22.550515  2068 net.cpp:128] Creating Layer Scale56
I0825 11:12:22.550532  2068 net.cpp:558] Scale56 <- Convolution56
I0825 11:12:22.550550  2068 net.cpp:509] Scale56 -> Convolution56 (in-place)
I0825 11:12:22.550634  2068 layer_factory.hpp:77] Creating layer Scale56
I0825 11:12:22.550832  2068 net.cpp:172] Setting up Scale56
I0825 11:12:22.550854  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.550871  2068 net.cpp:194] Memory required for data: 753664256
I0825 11:12:22.550892  2068 layer_factory.hpp:77] Creating layer ReLU54
I0825 11:12:22.550911  2068 net.cpp:128] Creating Layer ReLU54
I0825 11:12:22.550928  2068 net.cpp:558] ReLU54 <- Convolution56
I0825 11:12:22.550946  2068 net.cpp:509] ReLU54 -> Convolution56 (in-place)
I0825 11:12:22.552047  2068 net.cpp:172] Setting up ReLU54
I0825 11:12:22.552083  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.552109  2068 net.cpp:194] Memory required for data: 754712832
I0825 11:12:22.552127  2068 layer_factory.hpp:77] Creating layer Convolution57
I0825 11:12:22.552152  2068 net.cpp:128] Creating Layer Convolution57
I0825 11:12:22.552170  2068 net.cpp:558] Convolution57 <- Convolution56
I0825 11:12:22.552193  2068 net.cpp:522] Convolution57 -> Convolution57
I0825 11:12:22.560950  2068 net.cpp:172] Setting up Convolution57
I0825 11:12:22.561005  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.561023  2068 net.cpp:194] Memory required for data: 755761408
I0825 11:12:22.561049  2068 layer_factory.hpp:77] Creating layer BatchNorm57
I0825 11:12:22.561074  2068 net.cpp:128] Creating Layer BatchNorm57
I0825 11:12:22.561091  2068 net.cpp:558] BatchNorm57 <- Convolution57
I0825 11:12:22.561112  2068 net.cpp:509] BatchNorm57 -> Convolution57 (in-place)
I0825 11:12:22.561455  2068 net.cpp:172] Setting up BatchNorm57
I0825 11:12:22.561477  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.561494  2068 net.cpp:194] Memory required for data: 756809984
I0825 11:12:22.561517  2068 layer_factory.hpp:77] Creating layer Scale57
I0825 11:12:22.561537  2068 net.cpp:128] Creating Layer Scale57
I0825 11:12:22.561554  2068 net.cpp:558] Scale57 <- Convolution57
I0825 11:12:22.561573  2068 net.cpp:509] Scale57 -> Convolution57 (in-place)
I0825 11:12:22.561650  2068 layer_factory.hpp:77] Creating layer Scale57
I0825 11:12:22.561851  2068 net.cpp:172] Setting up Scale57
I0825 11:12:22.561877  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.561894  2068 net.cpp:194] Memory required for data: 757858560
I0825 11:12:22.561916  2068 layer_factory.hpp:77] Creating layer Eltwise27
I0825 11:12:22.561936  2068 net.cpp:128] Creating Layer Eltwise27
I0825 11:12:22.561954  2068 net.cpp:558] Eltwise27 <- Eltwise26_ReLU53_0_split_1
I0825 11:12:22.561972  2068 net.cpp:558] Eltwise27 <- Convolution57
I0825 11:12:22.561991  2068 net.cpp:522] Eltwise27 -> Eltwise27
I0825 11:12:22.562043  2068 net.cpp:172] Setting up Eltwise27
I0825 11:12:22.562064  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.562080  2068 net.cpp:194] Memory required for data: 758907136
I0825 11:12:22.562098  2068 layer_factory.hpp:77] Creating layer ReLU55
I0825 11:12:22.562119  2068 net.cpp:128] Creating Layer ReLU55
I0825 11:12:22.562137  2068 net.cpp:558] ReLU55 <- Eltwise27
I0825 11:12:22.562155  2068 net.cpp:509] ReLU55 -> Eltwise27 (in-place)
I0825 11:12:22.564852  2068 net.cpp:172] Setting up ReLU55
I0825 11:12:22.564887  2068 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:12:22.564903  2068 net.cpp:194] Memory required for data: 759955712
I0825 11:12:22.564919  2068 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:12:22.564944  2068 net.cpp:128] Creating Layer Pooling1
I0825 11:12:22.564961  2068 net.cpp:558] Pooling1 <- Eltwise27
I0825 11:12:22.564982  2068 net.cpp:522] Pooling1 -> Pooling1
I0825 11:12:22.569308  2068 net.cpp:172] Setting up Pooling1
I0825 11:12:22.569356  2068 net.cpp:186] Top shape: 64 64 1 1 (4096)
I0825 11:12:22.569373  2068 net.cpp:194] Memory required for data: 759972096
I0825 11:12:22.569391  2068 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:12:22.569417  2068 net.cpp:128] Creating Layer InnerProduct1
I0825 11:12:22.569435  2068 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:12:22.569458  2068 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:12:22.569699  2068 net.cpp:172] Setting up InnerProduct1
I0825 11:12:22.569723  2068 net.cpp:186] Top shape: 64 10 (640)
I0825 11:12:22.569739  2068 net.cpp:194] Memory required for data: 759974656
I0825 11:12:22.569763  2068 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:12:22.569787  2068 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:12:22.569803  2068 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0825 11:12:22.569823  2068 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0825 11:12:22.569844  2068 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:12:22.569872  2068 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:12:22.573752  2068 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:12:22.573792  2068 net.cpp:186] Top shape: (1)
I0825 11:12:22.573809  2068 net.cpp:189]     with loss weight 1
I0825 11:12:22.573854  2068 net.cpp:194] Memory required for data: 759974660
I0825 11:12:22.573873  2068 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:12:22.573894  2068 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:12:22.573911  2068 net.cpp:301] Pooling1 needs backward computation.
I0825 11:12:22.573928  2068 net.cpp:301] ReLU55 needs backward computation.
I0825 11:12:22.573945  2068 net.cpp:301] Eltwise27 needs backward computation.
I0825 11:12:22.573962  2068 net.cpp:301] Scale57 needs backward computation.
I0825 11:12:22.573979  2068 net.cpp:301] BatchNorm57 needs backward computation.
I0825 11:12:22.573995  2068 net.cpp:301] Convolution57 needs backward computation.
I0825 11:12:22.574012  2068 net.cpp:301] ReLU54 needs backward computation.
I0825 11:12:22.574028  2068 net.cpp:301] Scale56 needs backward computation.
I0825 11:12:22.574044  2068 net.cpp:301] BatchNorm56 needs backward computation.
I0825 11:12:22.574060  2068 net.cpp:301] Convolution56 needs backward computation.
I0825 11:12:22.574079  2068 net.cpp:301] Eltwise26_ReLU53_0_split needs backward computation.
I0825 11:12:22.574095  2068 net.cpp:301] ReLU53 needs backward computation.
I0825 11:12:22.574111  2068 net.cpp:301] Eltwise26 needs backward computation.
I0825 11:12:22.574131  2068 net.cpp:301] Scale55 needs backward computation.
I0825 11:12:22.574148  2068 net.cpp:301] BatchNorm55 needs backward computation.
I0825 11:12:22.574165  2068 net.cpp:301] Convolution55 needs backward computation.
I0825 11:12:22.574182  2068 net.cpp:301] ReLU52 needs backward computation.
I0825 11:12:22.574199  2068 net.cpp:301] Scale54 needs backward computation.
I0825 11:12:22.574214  2068 net.cpp:301] BatchNorm54 needs backward computation.
I0825 11:12:22.574230  2068 net.cpp:301] Convolution54 needs backward computation.
I0825 11:12:22.574247  2068 net.cpp:301] Eltwise25_ReLU51_0_split needs backward computation.
I0825 11:12:22.574265  2068 net.cpp:301] ReLU51 needs backward computation.
I0825 11:12:22.574281  2068 net.cpp:301] Eltwise25 needs backward computation.
I0825 11:12:22.574299  2068 net.cpp:301] Scale53 needs backward computation.
I0825 11:12:22.574316  2068 net.cpp:301] BatchNorm53 needs backward computation.
I0825 11:12:22.574340  2068 net.cpp:301] Convolution53 needs backward computation.
I0825 11:12:22.574359  2068 net.cpp:301] ReLU50 needs backward computation.
I0825 11:12:22.574376  2068 net.cpp:301] Scale52 needs backward computation.
I0825 11:12:22.574393  2068 net.cpp:301] BatchNorm52 needs backward computation.
I0825 11:12:22.574410  2068 net.cpp:301] Convolution52 needs backward computation.
I0825 11:12:22.574427  2068 net.cpp:301] Eltwise24_ReLU49_0_split needs backward computation.
I0825 11:12:22.574446  2068 net.cpp:301] ReLU49 needs backward computation.
I0825 11:12:22.574462  2068 net.cpp:301] Eltwise24 needs backward computation.
I0825 11:12:22.574481  2068 net.cpp:301] Scale51 needs backward computation.
I0825 11:12:22.574497  2068 net.cpp:301] BatchNorm51 needs backward computation.
I0825 11:12:22.574514  2068 net.cpp:301] Convolution51 needs backward computation.
I0825 11:12:22.574532  2068 net.cpp:301] ReLU48 needs backward computation.
I0825 11:12:22.574555  2068 net.cpp:301] Scale50 needs backward computation.
I0825 11:12:22.574573  2068 net.cpp:301] BatchNorm50 needs backward computation.
I0825 11:12:22.574590  2068 net.cpp:301] Convolution50 needs backward computation.
I0825 11:12:22.574607  2068 net.cpp:301] Eltwise23_ReLU47_0_split needs backward computation.
I0825 11:12:22.574625  2068 net.cpp:301] ReLU47 needs backward computation.
I0825 11:12:22.574641  2068 net.cpp:301] Eltwise23 needs backward computation.
I0825 11:12:22.574659  2068 net.cpp:301] Scale49 needs backward computation.
I0825 11:12:22.574676  2068 net.cpp:301] BatchNorm49 needs backward computation.
I0825 11:12:22.574693  2068 net.cpp:301] Convolution49 needs backward computation.
I0825 11:12:22.574718  2068 net.cpp:301] ReLU46 needs backward computation.
I0825 11:12:22.574736  2068 net.cpp:301] Scale48 needs backward computation.
I0825 11:12:22.574753  2068 net.cpp:301] BatchNorm48 needs backward computation.
I0825 11:12:22.574769  2068 net.cpp:301] Convolution48 needs backward computation.
I0825 11:12:22.574827  2068 net.cpp:301] Eltwise22_ReLU45_0_split needs backward computation.
I0825 11:12:22.574851  2068 net.cpp:301] ReLU45 needs backward computation.
I0825 11:12:22.574867  2068 net.cpp:301] Eltwise22 needs backward computation.
I0825 11:12:22.574885  2068 net.cpp:301] Scale47 needs backward computation.
I0825 11:12:22.574903  2068 net.cpp:301] BatchNorm47 needs backward computation.
I0825 11:12:22.574919  2068 net.cpp:301] Convolution47 needs backward computation.
I0825 11:12:22.574936  2068 net.cpp:301] ReLU44 needs backward computation.
I0825 11:12:22.574954  2068 net.cpp:301] Scale46 needs backward computation.
I0825 11:12:22.574971  2068 net.cpp:301] BatchNorm46 needs backward computation.
I0825 11:12:22.574987  2068 net.cpp:301] Convolution46 needs backward computation.
I0825 11:12:22.575006  2068 net.cpp:301] Eltwise21_ReLU43_0_split needs backward computation.
I0825 11:12:22.575023  2068 net.cpp:301] ReLU43 needs backward computation.
I0825 11:12:22.575040  2068 net.cpp:301] Eltwise21 needs backward computation.
I0825 11:12:22.575058  2068 net.cpp:301] Scale45 needs backward computation.
I0825 11:12:22.575074  2068 net.cpp:301] BatchNorm45 needs backward computation.
I0825 11:12:22.575090  2068 net.cpp:301] Convolution45 needs backward computation.
I0825 11:12:22.575109  2068 net.cpp:301] ReLU42 needs backward computation.
I0825 11:12:22.575125  2068 net.cpp:301] Scale44 needs backward computation.
I0825 11:12:22.575142  2068 net.cpp:301] BatchNorm44 needs backward computation.
I0825 11:12:22.575160  2068 net.cpp:301] Convolution44 needs backward computation.
I0825 11:12:22.575177  2068 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0825 11:12:22.575196  2068 net.cpp:301] ReLU41 needs backward computation.
I0825 11:12:22.575212  2068 net.cpp:301] Eltwise20 needs backward computation.
I0825 11:12:22.575230  2068 net.cpp:301] Scale43 needs backward computation.
I0825 11:12:22.575247  2068 net.cpp:301] BatchNorm43 needs backward computation.
I0825 11:12:22.575263  2068 net.cpp:301] Convolution43 needs backward computation.
I0825 11:12:22.575280  2068 net.cpp:301] ReLU40 needs backward computation.
I0825 11:12:22.575297  2068 net.cpp:301] Scale42 needs backward computation.
I0825 11:12:22.575314  2068 net.cpp:301] BatchNorm42 needs backward computation.
I0825 11:12:22.575330  2068 net.cpp:301] Convolution42 needs backward computation.
I0825 11:12:22.575348  2068 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0825 11:12:22.575366  2068 net.cpp:301] ReLU39 needs backward computation.
I0825 11:12:22.575383  2068 net.cpp:301] Eltwise19 needs backward computation.
I0825 11:12:22.575400  2068 net.cpp:301] Scale41 needs backward computation.
I0825 11:12:22.575418  2068 net.cpp:301] BatchNorm41 needs backward computation.
I0825 11:12:22.575435  2068 net.cpp:301] Convolution41 needs backward computation.
I0825 11:12:22.575453  2068 net.cpp:301] ReLU38 needs backward computation.
I0825 11:12:22.575470  2068 net.cpp:301] Scale40 needs backward computation.
I0825 11:12:22.575490  2068 net.cpp:301] BatchNorm40 needs backward computation.
I0825 11:12:22.575508  2068 net.cpp:301] Convolution40 needs backward computation.
I0825 11:12:22.575526  2068 net.cpp:301] Scale39 needs backward computation.
I0825 11:12:22.575542  2068 net.cpp:301] BatchNorm39 needs backward computation.
I0825 11:12:22.575561  2068 net.cpp:301] Convolution39 needs backward computation.
I0825 11:12:22.575578  2068 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0825 11:12:22.575595  2068 net.cpp:301] ReLU37 needs backward computation.
I0825 11:12:22.575611  2068 net.cpp:301] Eltwise18 needs backward computation.
I0825 11:12:22.575633  2068 net.cpp:301] Scale38 needs backward computation.
I0825 11:12:22.575659  2068 net.cpp:301] BatchNorm38 needs backward computation.
I0825 11:12:22.575676  2068 net.cpp:301] Convolution38 needs backward computation.
I0825 11:12:22.575695  2068 net.cpp:301] ReLU36 needs backward computation.
I0825 11:12:22.575711  2068 net.cpp:301] Scale37 needs backward computation.
I0825 11:12:22.575727  2068 net.cpp:301] BatchNorm37 needs backward computation.
I0825 11:12:22.575744  2068 net.cpp:301] Convolution37 needs backward computation.
I0825 11:12:22.575762  2068 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0825 11:12:22.575778  2068 net.cpp:301] ReLU35 needs backward computation.
I0825 11:12:22.575794  2068 net.cpp:301] Eltwise17 needs backward computation.
I0825 11:12:22.575811  2068 net.cpp:301] Scale36 needs backward computation.
I0825 11:12:22.575827  2068 net.cpp:301] BatchNorm36 needs backward computation.
I0825 11:12:22.575845  2068 net.cpp:301] Convolution36 needs backward computation.
I0825 11:12:22.575862  2068 net.cpp:301] ReLU34 needs backward computation.
I0825 11:12:22.575878  2068 net.cpp:301] Scale35 needs backward computation.
I0825 11:12:22.575896  2068 net.cpp:301] BatchNorm35 needs backward computation.
I0825 11:12:22.575913  2068 net.cpp:301] Convolution35 needs backward computation.
I0825 11:12:22.575930  2068 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0825 11:12:22.575948  2068 net.cpp:301] ReLU33 needs backward computation.
I0825 11:12:22.575965  2068 net.cpp:301] Eltwise16 needs backward computation.
I0825 11:12:22.575983  2068 net.cpp:301] Scale34 needs backward computation.
I0825 11:12:22.575999  2068 net.cpp:301] BatchNorm34 needs backward computation.
I0825 11:12:22.576015  2068 net.cpp:301] Convolution34 needs backward computation.
I0825 11:12:22.576032  2068 net.cpp:301] ReLU32 needs backward computation.
I0825 11:12:22.576050  2068 net.cpp:301] Scale33 needs backward computation.
I0825 11:12:22.576067  2068 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:12:22.576083  2068 net.cpp:301] Convolution33 needs backward computation.
I0825 11:12:22.576100  2068 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0825 11:12:22.576117  2068 net.cpp:301] ReLU31 needs backward computation.
I0825 11:12:22.576134  2068 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:12:22.576151  2068 net.cpp:301] Scale32 needs backward computation.
I0825 11:12:22.576169  2068 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:12:22.576185  2068 net.cpp:301] Convolution32 needs backward computation.
I0825 11:12:22.576202  2068 net.cpp:301] ReLU30 needs backward computation.
I0825 11:12:22.576220  2068 net.cpp:301] Scale31 needs backward computation.
I0825 11:12:22.576237  2068 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:12:22.576256  2068 net.cpp:301] Convolution31 needs backward computation.
I0825 11:12:22.576272  2068 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:12:22.576290  2068 net.cpp:301] ReLU29 needs backward computation.
I0825 11:12:22.576310  2068 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:12:22.576328  2068 net.cpp:301] Scale30 needs backward computation.
I0825 11:12:22.576344  2068 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:12:22.576360  2068 net.cpp:301] Convolution30 needs backward computation.
I0825 11:12:22.576381  2068 net.cpp:301] ReLU28 needs backward computation.
I0825 11:12:22.576400  2068 net.cpp:301] Scale29 needs backward computation.
I0825 11:12:22.576418  2068 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:12:22.576434  2068 net.cpp:301] Convolution29 needs backward computation.
I0825 11:12:22.576452  2068 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:12:22.576468  2068 net.cpp:301] ReLU27 needs backward computation.
I0825 11:12:22.576486  2068 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:12:22.576504  2068 net.cpp:301] Scale28 needs backward computation.
I0825 11:12:22.576526  2068 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:12:22.576544  2068 net.cpp:301] Convolution28 needs backward computation.
I0825 11:12:22.576560  2068 net.cpp:301] ReLU26 needs backward computation.
I0825 11:12:22.576577  2068 net.cpp:301] Scale27 needs backward computation.
I0825 11:12:22.576594  2068 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:12:22.576611  2068 net.cpp:301] Convolution27 needs backward computation.
I0825 11:12:22.576628  2068 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:12:22.576645  2068 net.cpp:301] ReLU25 needs backward computation.
I0825 11:12:22.576663  2068 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:12:22.576680  2068 net.cpp:301] Scale26 needs backward computation.
I0825 11:12:22.576697  2068 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:12:22.576714  2068 net.cpp:301] Convolution26 needs backward computation.
I0825 11:12:22.576732  2068 net.cpp:301] ReLU24 needs backward computation.
I0825 11:12:22.576748  2068 net.cpp:301] Scale25 needs backward computation.
I0825 11:12:22.576764  2068 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:12:22.576781  2068 net.cpp:301] Convolution25 needs backward computation.
I0825 11:12:22.576798  2068 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:12:22.576815  2068 net.cpp:301] ReLU23 needs backward computation.
I0825 11:12:22.576833  2068 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:12:22.576849  2068 net.cpp:301] Scale24 needs backward computation.
I0825 11:12:22.576866  2068 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:12:22.576882  2068 net.cpp:301] Convolution24 needs backward computation.
I0825 11:12:22.576900  2068 net.cpp:301] ReLU22 needs backward computation.
I0825 11:12:22.576917  2068 net.cpp:301] Scale23 needs backward computation.
I0825 11:12:22.576933  2068 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:12:22.576951  2068 net.cpp:301] Convolution23 needs backward computation.
I0825 11:12:22.576968  2068 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:12:22.576985  2068 net.cpp:301] ReLU21 needs backward computation.
I0825 11:12:22.577003  2068 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:12:22.577020  2068 net.cpp:301] Scale22 needs backward computation.
I0825 11:12:22.577038  2068 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:12:22.577054  2068 net.cpp:301] Convolution22 needs backward computation.
I0825 11:12:22.577071  2068 net.cpp:301] ReLU20 needs backward computation.
I0825 11:12:22.577088  2068 net.cpp:301] Scale21 needs backward computation.
I0825 11:12:22.577105  2068 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:12:22.577122  2068 net.cpp:301] Convolution21 needs backward computation.
I0825 11:12:22.577142  2068 net.cpp:301] Scale20 needs backward computation.
I0825 11:12:22.577158  2068 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:12:22.577174  2068 net.cpp:301] Convolution20 needs backward computation.
I0825 11:12:22.577191  2068 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:12:22.577209  2068 net.cpp:301] ReLU19 needs backward computation.
I0825 11:12:22.577227  2068 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:12:22.577248  2068 net.cpp:301] Scale19 needs backward computation.
I0825 11:12:22.577265  2068 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:12:22.577283  2068 net.cpp:301] Convolution19 needs backward computation.
I0825 11:12:22.577301  2068 net.cpp:301] ReLU18 needs backward computation.
I0825 11:12:22.577317  2068 net.cpp:301] Scale18 needs backward computation.
I0825 11:12:22.577333  2068 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:12:22.577349  2068 net.cpp:301] Convolution18 needs backward computation.
I0825 11:12:22.577368  2068 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:12:22.577385  2068 net.cpp:301] ReLU17 needs backward computation.
I0825 11:12:22.577407  2068 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:12:22.577425  2068 net.cpp:301] Scale17 needs backward computation.
I0825 11:12:22.577447  2068 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:12:22.577466  2068 net.cpp:301] Convolution17 needs backward computation.
I0825 11:12:22.577482  2068 net.cpp:301] ReLU16 needs backward computation.
I0825 11:12:22.577498  2068 net.cpp:301] Scale16 needs backward computation.
I0825 11:12:22.577515  2068 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:12:22.577533  2068 net.cpp:301] Convolution16 needs backward computation.
I0825 11:12:22.577549  2068 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:12:22.577567  2068 net.cpp:301] ReLU15 needs backward computation.
I0825 11:12:22.577584  2068 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:12:22.577602  2068 net.cpp:301] Scale15 needs backward computation.
I0825 11:12:22.577618  2068 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:12:22.577636  2068 net.cpp:301] Convolution15 needs backward computation.
I0825 11:12:22.577653  2068 net.cpp:301] ReLU14 needs backward computation.
I0825 11:12:22.577670  2068 net.cpp:301] Scale14 needs backward computation.
I0825 11:12:22.577687  2068 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:12:22.577704  2068 net.cpp:301] Convolution14 needs backward computation.
I0825 11:12:22.577723  2068 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:12:22.577740  2068 net.cpp:301] ReLU13 needs backward computation.
I0825 11:12:22.577756  2068 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:12:22.577776  2068 net.cpp:301] Scale13 needs backward computation.
I0825 11:12:22.577793  2068 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:12:22.577810  2068 net.cpp:301] Convolution13 needs backward computation.
I0825 11:12:22.577827  2068 net.cpp:301] ReLU12 needs backward computation.
I0825 11:12:22.577844  2068 net.cpp:301] Scale12 needs backward computation.
I0825 11:12:22.577860  2068 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:12:22.577877  2068 net.cpp:301] Convolution12 needs backward computation.
I0825 11:12:22.577894  2068 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:12:22.577913  2068 net.cpp:301] ReLU11 needs backward computation.
I0825 11:12:22.577929  2068 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:12:22.577945  2068 net.cpp:301] Scale11 needs backward computation.
I0825 11:12:22.577961  2068 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:12:22.577977  2068 net.cpp:301] Convolution11 needs backward computation.
I0825 11:12:22.577993  2068 net.cpp:301] ReLU10 needs backward computation.
I0825 11:12:22.578009  2068 net.cpp:301] Scale10 needs backward computation.
I0825 11:12:22.578027  2068 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:12:22.578043  2068 net.cpp:301] Convolution10 needs backward computation.
I0825 11:12:22.578061  2068 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:12:22.578079  2068 net.cpp:301] ReLU9 needs backward computation.
I0825 11:12:22.578096  2068 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:12:22.578114  2068 net.cpp:301] Scale9 needs backward computation.
I0825 11:12:22.578135  2068 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:12:22.578152  2068 net.cpp:301] Convolution9 needs backward computation.
I0825 11:12:22.578171  2068 net.cpp:301] ReLU8 needs backward computation.
I0825 11:12:22.578187  2068 net.cpp:301] Scale8 needs backward computation.
I0825 11:12:22.578204  2068 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:12:22.578220  2068 net.cpp:301] Convolution8 needs backward computation.
I0825 11:12:22.578238  2068 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:12:22.578256  2068 net.cpp:301] ReLU7 needs backward computation.
I0825 11:12:22.578274  2068 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:12:22.578299  2068 net.cpp:301] Scale7 needs backward computation.
I0825 11:12:22.578315  2068 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:12:22.578337  2068 net.cpp:301] Convolution7 needs backward computation.
I0825 11:12:22.578356  2068 net.cpp:301] ReLU6 needs backward computation.
I0825 11:12:22.578372  2068 net.cpp:301] Scale6 needs backward computation.
I0825 11:12:22.578389  2068 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:12:22.578407  2068 net.cpp:301] Convolution6 needs backward computation.
I0825 11:12:22.578424  2068 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:12:22.578442  2068 net.cpp:301] ReLU5 needs backward computation.
I0825 11:12:22.578459  2068 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:12:22.578477  2068 net.cpp:301] Scale5 needs backward computation.
I0825 11:12:22.578495  2068 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:12:22.578511  2068 net.cpp:301] Convolution5 needs backward computation.
I0825 11:12:22.578528  2068 net.cpp:301] ReLU4 needs backward computation.
I0825 11:12:22.578546  2068 net.cpp:301] Scale4 needs backward computation.
I0825 11:12:22.578562  2068 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:12:22.578579  2068 net.cpp:301] Convolution4 needs backward computation.
I0825 11:12:22.578599  2068 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:12:22.578618  2068 net.cpp:301] ReLU3 needs backward computation.
I0825 11:12:22.578634  2068 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:12:22.578652  2068 net.cpp:301] Scale3 needs backward computation.
I0825 11:12:22.578670  2068 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:12:22.578686  2068 net.cpp:301] Convolution3 needs backward computation.
I0825 11:12:22.578703  2068 net.cpp:301] ReLU2 needs backward computation.
I0825 11:12:22.578721  2068 net.cpp:301] Scale2 needs backward computation.
I0825 11:12:22.578737  2068 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:12:22.578753  2068 net.cpp:301] Convolution2 needs backward computation.
I0825 11:12:22.578771  2068 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:12:22.578789  2068 net.cpp:301] ReLU1 needs backward computation.
I0825 11:12:22.578805  2068 net.cpp:301] Scale1 needs backward computation.
I0825 11:12:22.578822  2068 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:12:22.578840  2068 net.cpp:301] Convolution1 needs backward computation.
I0825 11:12:22.578857  2068 net.cpp:303] Data1 does not need backward computation.
I0825 11:12:22.578874  2068 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:12:22.579027  2068 net.cpp:363] Network initialization done.
I0825 11:12:22.581863  2068 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_56.prototxt
I0825 11:12:22.581915  2068 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:12:22.581938  2068 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_56.prototxt
I0825 11:12:22.582173  2068 net.cpp:390] layer_param.include_size():1
I0825 11:12:22.582204  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582223  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582245  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582263  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582278  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582294  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582309  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582325  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582356  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582373  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582389  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582406  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582430  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582446  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582463  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582479  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582494  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582510  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582526  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582543  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582559  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582576  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582592  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582608  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582623  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582639  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582655  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582672  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582690  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582706  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582721  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582738  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582754  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582772  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582787  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582803  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582818  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582837  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582852  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582868  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582885  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582901  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582916  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582932  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582948  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582964  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.582980  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.582998  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583014  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583030  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583046  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583062  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583077  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583094  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583111  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583127  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583143  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583159  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583175  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583192  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583209  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583230  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583245  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583261  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583276  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583293  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583309  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583326  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583343  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583359  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583375  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583397  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583413  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583431  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583446  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583462  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583478  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583494  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583510  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583528  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583542  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583559  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583575  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583591  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583607  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583624  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583640  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583657  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583673  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583688  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583704  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583722  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583739  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583756  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583771  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583787  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583803  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583820  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583835  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583851  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583868  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583884  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583900  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583919  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583935  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583950  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583966  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.583981  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.583997  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584014  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584030  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584048  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584064  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584079  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584095  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584111  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584127  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584144  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584161  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584180  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584197  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584213  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584229  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584246  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584262  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584278  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584295  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584311  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584326  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584348  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584364  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584380  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584396  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584414  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584429  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584444  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584460  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584475  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584491  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584506  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584522  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584537  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584553  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584568  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584584  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584601  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584616  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584632  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584647  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584663  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584679  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584695  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584710  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584728  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584743  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584760  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584776  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584792  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584807  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584823  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584838  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584854  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584869  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584887  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584903  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584919  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584934  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584950  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584965  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.584981  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.584996  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585012  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585028  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585044  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585060  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585077  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585091  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585108  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585126  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585142  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585158  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585173  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585189  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585206  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585219  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585235  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585250  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585268  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585289  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585306  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585321  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585338  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585353  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585369  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585384  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585400  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585415  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585431  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585446  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585463  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585477  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585494  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585510  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585525  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585541  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585556  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585572  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585588  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585603  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585620  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585635  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585651  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585666  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585683  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585698  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585714  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585729  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585745  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585760  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585777  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585793  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585808  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585829  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585846  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585861  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585878  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585893  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585909  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585925  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585942  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585958  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.585973  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.585988  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586004  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586019  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586035  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586056  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586072  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586087  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586103  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586119  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586135  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586150  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586166  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586181  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586199  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586221  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586238  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586254  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586271  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586287  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586303  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586319  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586351  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586369  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586385  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586400  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586416  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586432  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586449  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586465  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586483  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586498  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586513  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586529  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586544  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586560  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586575  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586591  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586606  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586621  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586637  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586652  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586668  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586684  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586700  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586717  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586735  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586750  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586766  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586781  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586797  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586813  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586829  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586845  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586861  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586879  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586895  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586911  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586927  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586942  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586959  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.586975  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.586992  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587007  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587028  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587044  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587060  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587075  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587091  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587107  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587122  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587137  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587154  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587170  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587193  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587209  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587226  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587242  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587258  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587275  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587291  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587306  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587324  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587339  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587357  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587373  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587389  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587404  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587421  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587437  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587453  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587468  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587486  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587502  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587518  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587534  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587550  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587566  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587582  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587599  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587615  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587631  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587647  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587663  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587679  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587695  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587713  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587728  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587744  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587760  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587777  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587792  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587808  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587824  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587841  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587855  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587872  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587888  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587904  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587921  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587937  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587954  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.587973  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.587990  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588006  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588022  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588039  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588055  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588071  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588088  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588104  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588119  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588141  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588157  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588174  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588191  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588207  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588222  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588239  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588255  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588271  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588286  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588304  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588320  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588336  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588352  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588368  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588384  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588400  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588416  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588433  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588448  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588464  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588479  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588497  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588512  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588529  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588546  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588562  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588577  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588594  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588610  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588626  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588641  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588657  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588672  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588690  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588704  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588721  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588737  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588754  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588769  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588785  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588799  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588816  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588830  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588846  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588862  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588878  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588892  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588909  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588928  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588945  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588963  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.588979  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.588994  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589010  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589026  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589043  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589059  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589076  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589097  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589114  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589130  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589146  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589162  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589179  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589195  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589210  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589226  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589242  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589258  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589274  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589289  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589306  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589321  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589339  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589355  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589371  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589387  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589404  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589421  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589437  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589452  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589468  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589483  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589499  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589514  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589530  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589545  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589562  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589577  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589593  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589609  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589625  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589642  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589658  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589673  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589689  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589704  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589720  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589735  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589752  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589768  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589784  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589800  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589817  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589833  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589849  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589867  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589884  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589901  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589918  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589933  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589951  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589965  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.589982  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.589998  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590015  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590036  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590052  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590070  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590085  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590101  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590116  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590132  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590150  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590165  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590181  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590198  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590214  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590230  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590246  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590261  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590277  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590293  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590309  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590324  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590353  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590369  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590385  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590401  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590417  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590433  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590451  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590466  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590481  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590497  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590513  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590529  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590546  2068 net.cpp:390] layer_param.include_size():0
I0825 11:12:22.590562  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.590579  2068 net.cpp:390] layer_param.include_size():1
I0825 11:12:22.590595  2068 net.cpp:391] layer_param.exclude_size():0
I0825 11:12:22.592162  2068 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution17"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution19"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Convolution20"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution31"
  top: "Convolution31"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Convolution31"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise15"
  type: "Eltwise"
  bottom: "Eltwise14"
  bottom: "Convolution32"
  top: "Eltwise15"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU31"
  type: "ReLU"
  bottom: "Eltwise15"
  top: "Eltwise15"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Eltwise15"
  t
I0825 11:12:22.601987  2068 layer_factory.hpp:77] Creating layer Data1
I0825 11:12:22.602130  2068 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0825 11:12:22.602180  2068 net.cpp:128] Creating Layer Data1
I0825 11:12:22.602207  2068 net.cpp:522] Data1 -> Data1
I0825 11:12:22.602241  2068 net.cpp:522] Data1 -> Data2
I0825 11:12:22.602571  2068 data_layer.cpp:45] output data size: 10,3,32,32
I0825 11:12:22.618979  2068 net.cpp:172] Setting up Data1
I0825 11:12:22.619031  2068 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0825 11:12:22.619057  2068 net.cpp:186] Top shape: 10 (10)
I0825 11:12:22.619079  2068 net.cpp:194] Memory required for data: 122920
I0825 11:12:22.619102  2068 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0825 11:12:22.619132  2068 net.cpp:128] Creating Layer Data2_Data1_1_split
I0825 11:12:22.619156  2068 net.cpp:558] Data2_Data1_1_split <- Data2
I0825 11:12:22.619185  2068 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0825 11:12:22.619215  2068 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0825 11:12:22.619340  2068 net.cpp:172] Setting up Data2_Data1_1_split
I0825 11:12:22.619370  2068 net.cpp:186] Top shape: 10 (10)
I0825 11:12:22.619392  2068 net.cpp:186] Top shape: 10 (10)
I0825 11:12:22.619412  2068 net.cpp:194] Memory required for data: 123000
I0825 11:12:22.619434  2068 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:12:22.619477  2068 net.cpp:128] Creating Layer Convolution1
I0825 11:12:22.619500  2068 net.cpp:558] Convolution1 <- Data1
I0825 11:12:22.619529  2068 net.cpp:522] Convolution1 -> Convolution1
I0825 11:12:22.633524  2068 net.cpp:172] Setting up Convolution1
I0825 11:12:22.633576  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.633595  2068 net.cpp:194] Memory required for data: 778360
I0825 11:12:22.633627  2068 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:12:22.633654  2068 net.cpp:128] Creating Layer BatchNorm1
I0825 11:12:22.633673  2068 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:12:22.633693  2068 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:12:22.634037  2068 net.cpp:172] Setting up BatchNorm1
I0825 11:12:22.634061  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.634078  2068 net.cpp:194] Memory required for data: 1433720
I0825 11:12:22.634105  2068 layer_factory.hpp:77] Creating layer Scale1
I0825 11:12:22.634130  2068 net.cpp:128] Creating Layer Scale1
I0825 11:12:22.634146  2068 net.cpp:558] Scale1 <- Convolution1
I0825 11:12:22.634166  2068 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:12:22.634249  2068 layer_factory.hpp:77] Creating layer Scale1
I0825 11:12:22.634456  2068 net.cpp:172] Setting up Scale1
I0825 11:12:22.634481  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.634498  2068 net.cpp:194] Memory required for data: 2089080
I0825 11:12:22.634521  2068 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:12:22.634543  2068 net.cpp:128] Creating Layer ReLU1
I0825 11:12:22.634562  2068 net.cpp:558] ReLU1 <- Convolution1
I0825 11:12:22.634580  2068 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:12:22.637662  2068 net.cpp:172] Setting up ReLU1
I0825 11:12:22.637693  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.637711  2068 net.cpp:194] Memory required for data: 2744440
I0825 11:12:22.637728  2068 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:12:22.637753  2068 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:12:22.637770  2068 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:12:22.637790  2068 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:12:22.637822  2068 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:12:22.637904  2068 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:12:22.637926  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.637945  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.637961  2068 net.cpp:194] Memory required for data: 4055160
I0825 11:12:22.637979  2068 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:12:22.638003  2068 net.cpp:128] Creating Layer Convolution2
I0825 11:12:22.638021  2068 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:12:22.638043  2068 net.cpp:522] Convolution2 -> Convolution2
I0825 11:12:22.653069  2068 net.cpp:172] Setting up Convolution2
I0825 11:12:22.653116  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.653134  2068 net.cpp:194] Memory required for data: 4710520
I0825 11:12:22.653161  2068 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:12:22.653185  2068 net.cpp:128] Creating Layer BatchNorm2
I0825 11:12:22.653203  2068 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:12:22.653225  2068 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:12:22.653564  2068 net.cpp:172] Setting up BatchNorm2
I0825 11:12:22.653589  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.653605  2068 net.cpp:194] Memory required for data: 5365880
I0825 11:12:22.653628  2068 layer_factory.hpp:77] Creating layer Scale2
I0825 11:12:22.653650  2068 net.cpp:128] Creating Layer Scale2
I0825 11:12:22.653667  2068 net.cpp:558] Scale2 <- Convolution2
I0825 11:12:22.653687  2068 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:12:22.653762  2068 layer_factory.hpp:77] Creating layer Scale2
I0825 11:12:22.653959  2068 net.cpp:172] Setting up Scale2
I0825 11:12:22.653983  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.654000  2068 net.cpp:194] Memory required for data: 6021240
I0825 11:12:22.654022  2068 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:12:22.654043  2068 net.cpp:128] Creating Layer ReLU2
I0825 11:12:22.654060  2068 net.cpp:558] ReLU2 <- Convolution2
I0825 11:12:22.654079  2068 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:12:22.659343  2068 net.cpp:172] Setting up ReLU2
I0825 11:12:22.659376  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.659394  2068 net.cpp:194] Memory required for data: 6676600
I0825 11:12:22.659411  2068 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:12:22.659441  2068 net.cpp:128] Creating Layer Convolution3
I0825 11:12:22.659458  2068 net.cpp:558] Convolution3 <- Convolution2
I0825 11:12:22.659479  2068 net.cpp:522] Convolution3 -> Convolution3
I0825 11:12:22.674151  2068 net.cpp:172] Setting up Convolution3
I0825 11:12:22.674196  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.674213  2068 net.cpp:194] Memory required for data: 7331960
I0825 11:12:22.674237  2068 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:12:22.674260  2068 net.cpp:128] Creating Layer BatchNorm3
I0825 11:12:22.674278  2068 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:12:22.674299  2068 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:12:22.674646  2068 net.cpp:172] Setting up BatchNorm3
I0825 11:12:22.674672  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.674688  2068 net.cpp:194] Memory required for data: 7987320
I0825 11:12:22.674717  2068 layer_factory.hpp:77] Creating layer Scale3
I0825 11:12:22.674737  2068 net.cpp:128] Creating Layer Scale3
I0825 11:12:22.674753  2068 net.cpp:558] Scale3 <- Convolution3
I0825 11:12:22.674772  2068 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:12:22.674854  2068 layer_factory.hpp:77] Creating layer Scale3
I0825 11:12:22.675055  2068 net.cpp:172] Setting up Scale3
I0825 11:12:22.675079  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.675096  2068 net.cpp:194] Memory required for data: 8642680
I0825 11:12:22.675118  2068 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:12:22.675141  2068 net.cpp:128] Creating Layer Eltwise1
I0825 11:12:22.675165  2068 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:12:22.675184  2068 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:12:22.675205  2068 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:12:22.675257  2068 net.cpp:172] Setting up Eltwise1
I0825 11:12:22.675278  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.675294  2068 net.cpp:194] Memory required for data: 9298040
I0825 11:12:22.675312  2068 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:12:22.675333  2068 net.cpp:128] Creating Layer ReLU3
I0825 11:12:22.675349  2068 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:12:22.675369  2068 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:12:22.678367  2068 net.cpp:172] Setting up ReLU3
I0825 11:12:22.678411  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.678428  2068 net.cpp:194] Memory required for data: 9953400
I0825 11:12:22.678445  2068 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:12:22.678468  2068 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:12:22.678485  2068 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:12:22.678508  2068 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:12:22.678530  2068 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:12:22.678616  2068 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:12:22.678637  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.678654  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.678670  2068 net.cpp:194] Memory required for data: 11264120
I0825 11:12:22.678686  2068 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:12:22.678712  2068 net.cpp:128] Creating Layer Convolution4
I0825 11:12:22.678728  2068 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:12:22.678750  2068 net.cpp:522] Convolution4 -> Convolution4
I0825 11:12:22.692090  2068 net.cpp:172] Setting up Convolution4
I0825 11:12:22.692133  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.692150  2068 net.cpp:194] Memory required for data: 11919480
I0825 11:12:22.692174  2068 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:12:22.692198  2068 net.cpp:128] Creating Layer BatchNorm4
I0825 11:12:22.692215  2068 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:12:22.692237  2068 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:12:22.692581  2068 net.cpp:172] Setting up BatchNorm4
I0825 11:12:22.692606  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.692622  2068 net.cpp:194] Memory required for data: 12574840
I0825 11:12:22.692646  2068 layer_factory.hpp:77] Creating layer Scale4
I0825 11:12:22.692669  2068 net.cpp:128] Creating Layer Scale4
I0825 11:12:22.692687  2068 net.cpp:558] Scale4 <- Convolution4
I0825 11:12:22.692704  2068 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:12:22.692781  2068 layer_factory.hpp:77] Creating layer Scale4
I0825 11:12:22.692981  2068 net.cpp:172] Setting up Scale4
I0825 11:12:22.693003  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.693019  2068 net.cpp:194] Memory required for data: 13230200
I0825 11:12:22.693040  2068 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:12:22.693063  2068 net.cpp:128] Creating Layer ReLU4
I0825 11:12:22.693079  2068 net.cpp:558] ReLU4 <- Convolution4
I0825 11:12:22.693099  2068 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:12:22.696310  2068 net.cpp:172] Setting up ReLU4
I0825 11:12:22.696341  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.696359  2068 net.cpp:194] Memory required for data: 13885560
I0825 11:12:22.696375  2068 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:12:22.696403  2068 net.cpp:128] Creating Layer Convolution5
I0825 11:12:22.696421  2068 net.cpp:558] Convolution5 <- Convolution4
I0825 11:12:22.696441  2068 net.cpp:522] Convolution5 -> Convolution5
I0825 11:12:22.709537  2068 net.cpp:172] Setting up Convolution5
I0825 11:12:22.709583  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.709609  2068 net.cpp:194] Memory required for data: 14540920
I0825 11:12:22.709632  2068 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:12:22.709656  2068 net.cpp:128] Creating Layer BatchNorm5
I0825 11:12:22.709674  2068 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:12:22.709695  2068 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:12:22.710044  2068 net.cpp:172] Setting up BatchNorm5
I0825 11:12:22.710068  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.710085  2068 net.cpp:194] Memory required for data: 15196280
I0825 11:12:22.710114  2068 layer_factory.hpp:77] Creating layer Scale5
I0825 11:12:22.710144  2068 net.cpp:128] Creating Layer Scale5
I0825 11:12:22.710161  2068 net.cpp:558] Scale5 <- Convolution5
I0825 11:12:22.710180  2068 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:12:22.710263  2068 layer_factory.hpp:77] Creating layer Scale5
I0825 11:12:22.710471  2068 net.cpp:172] Setting up Scale5
I0825 11:12:22.710503  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.710520  2068 net.cpp:194] Memory required for data: 15851640
I0825 11:12:22.710542  2068 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:12:22.710566  2068 net.cpp:128] Creating Layer Eltwise2
I0825 11:12:22.710583  2068 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:12:22.710602  2068 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:12:22.710620  2068 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:12:22.710675  2068 net.cpp:172] Setting up Eltwise2
I0825 11:12:22.710696  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.710711  2068 net.cpp:194] Memory required for data: 16507000
I0825 11:12:22.710728  2068 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:12:22.710747  2068 net.cpp:128] Creating Layer ReLU5
I0825 11:12:22.710763  2068 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:12:22.710781  2068 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:12:22.715873  2068 net.cpp:172] Setting up ReLU5
I0825 11:12:22.715905  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.715922  2068 net.cpp:194] Memory required for data: 17162360
I0825 11:12:22.715939  2068 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:12:22.715960  2068 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:12:22.715977  2068 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:12:22.715999  2068 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:12:22.716022  2068 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:12:22.716105  2068 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:12:22.716126  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.716145  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.716161  2068 net.cpp:194] Memory required for data: 18473080
I0825 11:12:22.716177  2068 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:12:22.716205  2068 net.cpp:128] Creating Layer Convolution6
I0825 11:12:22.716223  2068 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:12:22.716241  2068 net.cpp:522] Convolution6 -> Convolution6
I0825 11:12:22.735781  2068 net.cpp:172] Setting up Convolution6
I0825 11:12:22.735826  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.735843  2068 net.cpp:194] Memory required for data: 19128440
I0825 11:12:22.735867  2068 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:12:22.735891  2068 net.cpp:128] Creating Layer BatchNorm6
I0825 11:12:22.735909  2068 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:12:22.735929  2068 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:12:22.736284  2068 net.cpp:172] Setting up BatchNorm6
I0825 11:12:22.736307  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.736323  2068 net.cpp:194] Memory required for data: 19783800
I0825 11:12:22.736347  2068 layer_factory.hpp:77] Creating layer Scale6
I0825 11:12:22.736367  2068 net.cpp:128] Creating Layer Scale6
I0825 11:12:22.736384  2068 net.cpp:558] Scale6 <- Convolution6
I0825 11:12:22.736410  2068 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:12:22.736490  2068 layer_factory.hpp:77] Creating layer Scale6
I0825 11:12:22.736691  2068 net.cpp:172] Setting up Scale6
I0825 11:12:22.736716  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.736733  2068 net.cpp:194] Memory required for data: 20439160
I0825 11:12:22.736754  2068 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:12:22.736774  2068 net.cpp:128] Creating Layer ReLU6
I0825 11:12:22.736790  2068 net.cpp:558] ReLU6 <- Convolution6
I0825 11:12:22.736811  2068 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:12:22.741798  2068 net.cpp:172] Setting up ReLU6
I0825 11:12:22.741850  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.741868  2068 net.cpp:194] Memory required for data: 21094520
I0825 11:12:22.741886  2068 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:12:22.741914  2068 net.cpp:128] Creating Layer Convolution7
I0825 11:12:22.741931  2068 net.cpp:558] Convolution7 <- Convolution6
I0825 11:12:22.741955  2068 net.cpp:522] Convolution7 -> Convolution7
I0825 11:12:22.759845  2068 net.cpp:172] Setting up Convolution7
I0825 11:12:22.759888  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.759905  2068 net.cpp:194] Memory required for data: 21749880
I0825 11:12:22.759928  2068 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:12:22.759954  2068 net.cpp:128] Creating Layer BatchNorm7
I0825 11:12:22.759971  2068 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:12:22.759991  2068 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:12:22.760334  2068 net.cpp:172] Setting up BatchNorm7
I0825 11:12:22.760359  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.760375  2068 net.cpp:194] Memory required for data: 22405240
I0825 11:12:22.760398  2068 layer_factory.hpp:77] Creating layer Scale7
I0825 11:12:22.760421  2068 net.cpp:128] Creating Layer Scale7
I0825 11:12:22.760437  2068 net.cpp:558] Scale7 <- Convolution7
I0825 11:12:22.760457  2068 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:12:22.760532  2068 layer_factory.hpp:77] Creating layer Scale7
I0825 11:12:22.760727  2068 net.cpp:172] Setting up Scale7
I0825 11:12:22.760751  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.760768  2068 net.cpp:194] Memory required for data: 23060600
I0825 11:12:22.760789  2068 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:12:22.760810  2068 net.cpp:128] Creating Layer Eltwise3
I0825 11:12:22.760828  2068 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:12:22.760845  2068 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:12:22.760865  2068 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:12:22.760918  2068 net.cpp:172] Setting up Eltwise3
I0825 11:12:22.760939  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.760956  2068 net.cpp:194] Memory required for data: 23715960
I0825 11:12:22.760972  2068 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:12:22.760990  2068 net.cpp:128] Creating Layer ReLU7
I0825 11:12:22.761006  2068 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:12:22.761029  2068 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:12:22.764087  2068 net.cpp:172] Setting up ReLU7
I0825 11:12:22.764120  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.764137  2068 net.cpp:194] Memory required for data: 24371320
I0825 11:12:22.764153  2068 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:12:22.764175  2068 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:12:22.764191  2068 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:12:22.764214  2068 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:12:22.764236  2068 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:12:22.764319  2068 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:12:22.764340  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.764358  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.764380  2068 net.cpp:194] Memory required for data: 25682040
I0825 11:12:22.764397  2068 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:12:22.764423  2068 net.cpp:128] Creating Layer Convolution8
I0825 11:12:22.764441  2068 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:12:22.764466  2068 net.cpp:522] Convolution8 -> Convolution8
I0825 11:12:22.777365  2068 net.cpp:172] Setting up Convolution8
I0825 11:12:22.777412  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.777431  2068 net.cpp:194] Memory required for data: 26337400
I0825 11:12:22.777453  2068 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:12:22.777475  2068 net.cpp:128] Creating Layer BatchNorm8
I0825 11:12:22.777503  2068 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:12:22.777525  2068 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:12:22.777879  2068 net.cpp:172] Setting up BatchNorm8
I0825 11:12:22.777904  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.777922  2068 net.cpp:194] Memory required for data: 26992760
I0825 11:12:22.777946  2068 layer_factory.hpp:77] Creating layer Scale8
I0825 11:12:22.777966  2068 net.cpp:128] Creating Layer Scale8
I0825 11:12:22.777982  2068 net.cpp:558] Scale8 <- Convolution8
I0825 11:12:22.778004  2068 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:12:22.778079  2068 layer_factory.hpp:77] Creating layer Scale8
I0825 11:12:22.778276  2068 net.cpp:172] Setting up Scale8
I0825 11:12:22.778302  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.778318  2068 net.cpp:194] Memory required for data: 27648120
I0825 11:12:22.778347  2068 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:12:22.778367  2068 net.cpp:128] Creating Layer ReLU8
I0825 11:12:22.778384  2068 net.cpp:558] ReLU8 <- Convolution8
I0825 11:12:22.778403  2068 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:12:22.781574  2068 net.cpp:172] Setting up ReLU8
I0825 11:12:22.781613  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.781631  2068 net.cpp:194] Memory required for data: 28303480
I0825 11:12:22.781648  2068 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:12:22.781673  2068 net.cpp:128] Creating Layer Convolution9
I0825 11:12:22.781692  2068 net.cpp:558] Convolution9 <- Convolution8
I0825 11:12:22.781713  2068 net.cpp:522] Convolution9 -> Convolution9
I0825 11:12:22.797878  2068 net.cpp:172] Setting up Convolution9
I0825 11:12:22.797920  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.797938  2068 net.cpp:194] Memory required for data: 28958840
I0825 11:12:22.797962  2068 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:12:22.797986  2068 net.cpp:128] Creating Layer BatchNorm9
I0825 11:12:22.798002  2068 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:12:22.798022  2068 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:12:22.798374  2068 net.cpp:172] Setting up BatchNorm9
I0825 11:12:22.798400  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.798418  2068 net.cpp:194] Memory required for data: 29614200
I0825 11:12:22.798446  2068 layer_factory.hpp:77] Creating layer Scale9
I0825 11:12:22.798465  2068 net.cpp:128] Creating Layer Scale9
I0825 11:12:22.798483  2068 net.cpp:558] Scale9 <- Convolution9
I0825 11:12:22.798501  2068 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:12:22.798581  2068 layer_factory.hpp:77] Creating layer Scale9
I0825 11:12:22.798782  2068 net.cpp:172] Setting up Scale9
I0825 11:12:22.798805  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.798821  2068 net.cpp:194] Memory required for data: 30269560
I0825 11:12:22.798843  2068 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:12:22.798866  2068 net.cpp:128] Creating Layer Eltwise4
I0825 11:12:22.798883  2068 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:12:22.798902  2068 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:12:22.798923  2068 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:12:22.798975  2068 net.cpp:172] Setting up Eltwise4
I0825 11:12:22.799002  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.799019  2068 net.cpp:194] Memory required for data: 30924920
I0825 11:12:22.799036  2068 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:12:22.799055  2068 net.cpp:128] Creating Layer ReLU9
I0825 11:12:22.799072  2068 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:12:22.799093  2068 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:12:22.802084  2068 net.cpp:172] Setting up ReLU9
I0825 11:12:22.802127  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.802145  2068 net.cpp:194] Memory required for data: 31580280
I0825 11:12:22.802162  2068 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:12:22.802193  2068 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:12:22.802211  2068 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:12:22.802233  2068 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:12:22.802256  2068 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:12:22.802347  2068 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:12:22.802371  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.802388  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.802405  2068 net.cpp:194] Memory required for data: 32891000
I0825 11:12:22.802422  2068 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:12:22.802453  2068 net.cpp:128] Creating Layer Convolution10
I0825 11:12:22.802471  2068 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:12:22.802494  2068 net.cpp:522] Convolution10 -> Convolution10
I0825 11:12:22.815408  2068 net.cpp:172] Setting up Convolution10
I0825 11:12:22.815452  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.815469  2068 net.cpp:194] Memory required for data: 33546360
I0825 11:12:22.815501  2068 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:12:22.815526  2068 net.cpp:128] Creating Layer BatchNorm10
I0825 11:12:22.815542  2068 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:12:22.815562  2068 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:12:22.815914  2068 net.cpp:172] Setting up BatchNorm10
I0825 11:12:22.815938  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.815955  2068 net.cpp:194] Memory required for data: 34201720
I0825 11:12:22.815979  2068 layer_factory.hpp:77] Creating layer Scale10
I0825 11:12:22.815999  2068 net.cpp:128] Creating Layer Scale10
I0825 11:12:22.816015  2068 net.cpp:558] Scale10 <- Convolution10
I0825 11:12:22.816033  2068 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:12:22.816112  2068 layer_factory.hpp:77] Creating layer Scale10
I0825 11:12:22.816314  2068 net.cpp:172] Setting up Scale10
I0825 11:12:22.816339  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.816354  2068 net.cpp:194] Memory required for data: 34857080
I0825 11:12:22.816376  2068 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:12:22.816395  2068 net.cpp:128] Creating Layer ReLU10
I0825 11:12:22.816412  2068 net.cpp:558] ReLU10 <- Convolution10
I0825 11:12:22.816434  2068 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:12:22.819586  2068 net.cpp:172] Setting up ReLU10
I0825 11:12:22.819617  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.819634  2068 net.cpp:194] Memory required for data: 35512440
I0825 11:12:22.819651  2068 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:12:22.819677  2068 net.cpp:128] Creating Layer Convolution11
I0825 11:12:22.819695  2068 net.cpp:558] Convolution11 <- Convolution10
I0825 11:12:22.819717  2068 net.cpp:522] Convolution11 -> Convolution11
I0825 11:12:22.827035  2068 net.cpp:172] Setting up Convolution11
I0825 11:12:22.827078  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.827095  2068 net.cpp:194] Memory required for data: 36167800
I0825 11:12:22.827117  2068 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:12:22.827141  2068 net.cpp:128] Creating Layer BatchNorm11
I0825 11:12:22.827157  2068 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:12:22.827185  2068 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:12:22.827539  2068 net.cpp:172] Setting up BatchNorm11
I0825 11:12:22.827564  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.827579  2068 net.cpp:194] Memory required for data: 36823160
I0825 11:12:22.827602  2068 layer_factory.hpp:77] Creating layer Scale11
I0825 11:12:22.827623  2068 net.cpp:128] Creating Layer Scale11
I0825 11:12:22.827641  2068 net.cpp:558] Scale11 <- Convolution11
I0825 11:12:22.827659  2068 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:12:22.827739  2068 layer_factory.hpp:77] Creating layer Scale11
I0825 11:12:22.827939  2068 net.cpp:172] Setting up Scale11
I0825 11:12:22.827972  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.827989  2068 net.cpp:194] Memory required for data: 37478520
I0825 11:12:22.828011  2068 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:12:22.828032  2068 net.cpp:128] Creating Layer Eltwise5
I0825 11:12:22.828050  2068 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:12:22.828068  2068 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:12:22.828088  2068 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:12:22.828142  2068 net.cpp:172] Setting up Eltwise5
I0825 11:12:22.828162  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.828178  2068 net.cpp:194] Memory required for data: 38133880
I0825 11:12:22.828196  2068 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:12:22.828215  2068 net.cpp:128] Creating Layer ReLU11
I0825 11:12:22.828233  2068 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:12:22.828253  2068 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:12:22.829154  2068 net.cpp:172] Setting up ReLU11
I0825 11:12:22.829183  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.829197  2068 net.cpp:194] Memory required for data: 38789240
I0825 11:12:22.829212  2068 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:12:22.829229  2068 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:12:22.829246  2068 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:12:22.829267  2068 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:12:22.829289  2068 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:12:22.829372  2068 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:12:22.829393  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.829411  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.829427  2068 net.cpp:194] Memory required for data: 40099960
I0825 11:12:22.829444  2068 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:12:22.829469  2068 net.cpp:128] Creating Layer Convolution12
I0825 11:12:22.829486  2068 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:12:22.829509  2068 net.cpp:522] Convolution12 -> Convolution12
I0825 11:12:22.838282  2068 net.cpp:172] Setting up Convolution12
I0825 11:12:22.838330  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.838353  2068 net.cpp:194] Memory required for data: 40755320
I0825 11:12:22.838376  2068 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:12:22.838397  2068 net.cpp:128] Creating Layer BatchNorm12
I0825 11:12:22.838414  2068 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:12:22.838436  2068 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:12:22.838794  2068 net.cpp:172] Setting up BatchNorm12
I0825 11:12:22.838819  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.838836  2068 net.cpp:194] Memory required for data: 41410680
I0825 11:12:22.838860  2068 layer_factory.hpp:77] Creating layer Scale12
I0825 11:12:22.838879  2068 net.cpp:128] Creating Layer Scale12
I0825 11:12:22.838896  2068 net.cpp:558] Scale12 <- Convolution12
I0825 11:12:22.838917  2068 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:12:22.838994  2068 layer_factory.hpp:77] Creating layer Scale12
I0825 11:12:22.839196  2068 net.cpp:172] Setting up Scale12
I0825 11:12:22.839228  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.839246  2068 net.cpp:194] Memory required for data: 42066040
I0825 11:12:22.839267  2068 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:12:22.839288  2068 net.cpp:128] Creating Layer ReLU12
I0825 11:12:22.839304  2068 net.cpp:558] ReLU12 <- Convolution12
I0825 11:12:22.839323  2068 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0825 11:12:22.842481  2068 net.cpp:172] Setting up ReLU12
I0825 11:12:22.842514  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.842530  2068 net.cpp:194] Memory required for data: 42721400
I0825 11:12:22.842548  2068 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:12:22.842586  2068 net.cpp:128] Creating Layer Convolution13
I0825 11:12:22.842604  2068 net.cpp:558] Convolution13 <- Convolution12
I0825 11:12:22.842627  2068 net.cpp:522] Convolution13 -> Convolution13
I0825 11:12:22.855775  2068 net.cpp:172] Setting up Convolution13
I0825 11:12:22.855819  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.855836  2068 net.cpp:194] Memory required for data: 43376760
I0825 11:12:22.855860  2068 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:12:22.855883  2068 net.cpp:128] Creating Layer BatchNorm13
I0825 11:12:22.855901  2068 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:12:22.855921  2068 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:12:22.856279  2068 net.cpp:172] Setting up BatchNorm13
I0825 11:12:22.856305  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.856323  2068 net.cpp:194] Memory required for data: 44032120
I0825 11:12:22.856346  2068 layer_factory.hpp:77] Creating layer Scale13
I0825 11:12:22.856367  2068 net.cpp:128] Creating Layer Scale13
I0825 11:12:22.856384  2068 net.cpp:558] Scale13 <- Convolution13
I0825 11:12:22.856405  2068 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:12:22.856487  2068 layer_factory.hpp:77] Creating layer Scale13
I0825 11:12:22.856688  2068 net.cpp:172] Setting up Scale13
I0825 11:12:22.856712  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.856729  2068 net.cpp:194] Memory required for data: 44687480
I0825 11:12:22.856750  2068 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:12:22.856779  2068 net.cpp:128] Creating Layer Eltwise6
I0825 11:12:22.856796  2068 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:12:22.856815  2068 net.cpp:558] Eltwise6 <- Convolution13
I0825 11:12:22.856835  2068 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:12:22.856886  2068 net.cpp:172] Setting up Eltwise6
I0825 11:12:22.856907  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.856923  2068 net.cpp:194] Memory required for data: 45342840
I0825 11:12:22.856941  2068 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:12:22.856963  2068 net.cpp:128] Creating Layer ReLU13
I0825 11:12:22.856981  2068 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:12:22.856998  2068 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:12:22.860002  2068 net.cpp:172] Setting up ReLU13
I0825 11:12:22.860041  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.860059  2068 net.cpp:194] Memory required for data: 45998200
I0825 11:12:22.860076  2068 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:12:22.860098  2068 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:12:22.860116  2068 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:12:22.860136  2068 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:12:22.860157  2068 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:12:22.860242  2068 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:12:22.860265  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.860282  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.860298  2068 net.cpp:194] Memory required for data: 47308920
I0825 11:12:22.860314  2068 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:12:22.860338  2068 net.cpp:128] Creating Layer Convolution14
I0825 11:12:22.860363  2068 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0825 11:12:22.860385  2068 net.cpp:522] Convolution14 -> Convolution14
I0825 11:12:22.864253  2068 net.cpp:172] Setting up Convolution14
I0825 11:12:22.864297  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.864315  2068 net.cpp:194] Memory required for data: 47964280
I0825 11:12:22.864336  2068 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:12:22.864356  2068 net.cpp:128] Creating Layer BatchNorm14
I0825 11:12:22.864373  2068 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:12:22.864395  2068 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:12:22.864768  2068 net.cpp:172] Setting up BatchNorm14
I0825 11:12:22.864794  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.864810  2068 net.cpp:194] Memory required for data: 48619640
I0825 11:12:22.864835  2068 layer_factory.hpp:77] Creating layer Scale14
I0825 11:12:22.864853  2068 net.cpp:128] Creating Layer Scale14
I0825 11:12:22.864871  2068 net.cpp:558] Scale14 <- Convolution14
I0825 11:12:22.864890  2068 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:12:22.864967  2068 layer_factory.hpp:77] Creating layer Scale14
I0825 11:12:22.865176  2068 net.cpp:172] Setting up Scale14
I0825 11:12:22.865200  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.865216  2068 net.cpp:194] Memory required for data: 49275000
I0825 11:12:22.865237  2068 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:12:22.865257  2068 net.cpp:128] Creating Layer ReLU14
I0825 11:12:22.865274  2068 net.cpp:558] ReLU14 <- Convolution14
I0825 11:12:22.865293  2068 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0825 11:12:22.865576  2068 net.cpp:172] Setting up ReLU14
I0825 11:12:22.865607  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.865624  2068 net.cpp:194] Memory required for data: 49930360
I0825 11:12:22.865641  2068 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:12:22.865669  2068 net.cpp:128] Creating Layer Convolution15
I0825 11:12:22.865686  2068 net.cpp:558] Convolution15 <- Convolution14
I0825 11:12:22.865710  2068 net.cpp:522] Convolution15 -> Convolution15
I0825 11:12:22.871851  2068 net.cpp:172] Setting up Convolution15
I0825 11:12:22.871893  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.871912  2068 net.cpp:194] Memory required for data: 50585720
I0825 11:12:22.871934  2068 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:12:22.871958  2068 net.cpp:128] Creating Layer BatchNorm15
I0825 11:12:22.871975  2068 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:12:22.871996  2068 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:12:22.872360  2068 net.cpp:172] Setting up BatchNorm15
I0825 11:12:22.872385  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.872401  2068 net.cpp:194] Memory required for data: 51241080
I0825 11:12:22.872424  2068 layer_factory.hpp:77] Creating layer Scale15
I0825 11:12:22.872445  2068 net.cpp:128] Creating Layer Scale15
I0825 11:12:22.872462  2068 net.cpp:558] Scale15 <- Convolution15
I0825 11:12:22.872480  2068 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:12:22.872560  2068 layer_factory.hpp:77] Creating layer Scale15
I0825 11:12:22.872767  2068 net.cpp:172] Setting up Scale15
I0825 11:12:22.872792  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.872809  2068 net.cpp:194] Memory required for data: 51896440
I0825 11:12:22.872830  2068 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:12:22.872853  2068 net.cpp:128] Creating Layer Eltwise7
I0825 11:12:22.872871  2068 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:12:22.872889  2068 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:12:22.872908  2068 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:12:22.872963  2068 net.cpp:172] Setting up Eltwise7
I0825 11:12:22.872984  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.873001  2068 net.cpp:194] Memory required for data: 52551800
I0825 11:12:22.873024  2068 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:12:22.873044  2068 net.cpp:128] Creating Layer ReLU15
I0825 11:12:22.873061  2068 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:12:22.873082  2068 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:12:22.873952  2068 net.cpp:172] Setting up ReLU15
I0825 11:12:22.873982  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.873998  2068 net.cpp:194] Memory required for data: 53207160
I0825 11:12:22.874016  2068 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:12:22.874037  2068 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:12:22.874053  2068 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:12:22.874085  2068 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:12:22.874107  2068 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:12:22.874193  2068 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:12:22.874215  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.874234  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.874250  2068 net.cpp:194] Memory required for data: 54517880
I0825 11:12:22.874269  2068 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:12:22.874292  2068 net.cpp:128] Creating Layer Convolution16
I0825 11:12:22.874310  2068 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0825 11:12:22.874337  2068 net.cpp:522] Convolution16 -> Convolution16
I0825 11:12:22.879146  2068 net.cpp:172] Setting up Convolution16
I0825 11:12:22.879192  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.879209  2068 net.cpp:194] Memory required for data: 55173240
I0825 11:12:22.879231  2068 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:12:22.879251  2068 net.cpp:128] Creating Layer BatchNorm16
I0825 11:12:22.879268  2068 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:12:22.879289  2068 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:12:22.881667  2068 net.cpp:172] Setting up BatchNorm16
I0825 11:12:22.881696  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.881701  2068 net.cpp:194] Memory required for data: 55828600
I0825 11:12:22.881717  2068 layer_factory.hpp:77] Creating layer Scale16
I0825 11:12:22.881726  2068 net.cpp:128] Creating Layer Scale16
I0825 11:12:22.881731  2068 net.cpp:558] Scale16 <- Convolution16
I0825 11:12:22.881742  2068 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:12:22.881810  2068 layer_factory.hpp:77] Creating layer Scale16
I0825 11:12:22.882009  2068 net.cpp:172] Setting up Scale16
I0825 11:12:22.882019  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.882022  2068 net.cpp:194] Memory required for data: 56483960
I0825 11:12:22.882030  2068 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:12:22.882037  2068 net.cpp:128] Creating Layer ReLU16
I0825 11:12:22.882042  2068 net.cpp:558] ReLU16 <- Convolution16
I0825 11:12:22.882047  2068 net.cpp:509] ReLU16 -> Convolution16 (in-place)
I0825 11:12:22.882727  2068 net.cpp:172] Setting up ReLU16
I0825 11:12:22.882750  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.882786  2068 net.cpp:194] Memory required for data: 57139320
I0825 11:12:22.882794  2068 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:12:22.882812  2068 net.cpp:128] Creating Layer Convolution17
I0825 11:12:22.882834  2068 net.cpp:558] Convolution17 <- Convolution16
I0825 11:12:22.882848  2068 net.cpp:522] Convolution17 -> Convolution17
I0825 11:12:22.895614  2068 net.cpp:172] Setting up Convolution17
I0825 11:12:22.895664  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.895684  2068 net.cpp:194] Memory required for data: 57794680
I0825 11:12:22.895705  2068 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:12:22.895730  2068 net.cpp:128] Creating Layer BatchNorm17
I0825 11:12:22.895748  2068 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:12:22.895771  2068 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:12:22.896140  2068 net.cpp:172] Setting up BatchNorm17
I0825 11:12:22.896178  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.896196  2068 net.cpp:194] Memory required for data: 58450040
I0825 11:12:22.896220  2068 layer_factory.hpp:77] Creating layer Scale17
I0825 11:12:22.896245  2068 net.cpp:128] Creating Layer Scale17
I0825 11:12:22.896262  2068 net.cpp:558] Scale17 <- Convolution17
I0825 11:12:22.896281  2068 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:12:22.896366  2068 layer_factory.hpp:77] Creating layer Scale17
I0825 11:12:22.896579  2068 net.cpp:172] Setting up Scale17
I0825 11:12:22.896602  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.896620  2068 net.cpp:194] Memory required for data: 59105400
I0825 11:12:22.896656  2068 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:12:22.896677  2068 net.cpp:128] Creating Layer Eltwise8
I0825 11:12:22.896697  2068 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:12:22.896716  2068 net.cpp:558] Eltwise8 <- Convolution17
I0825 11:12:22.896739  2068 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:12:22.896798  2068 net.cpp:172] Setting up Eltwise8
I0825 11:12:22.896819  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.896836  2068 net.cpp:194] Memory required for data: 59760760
I0825 11:12:22.896852  2068 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:12:22.896872  2068 net.cpp:128] Creating Layer ReLU17
I0825 11:12:22.896890  2068 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:12:22.896914  2068 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:12:22.901918  2068 net.cpp:172] Setting up ReLU17
I0825 11:12:22.901952  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.901969  2068 net.cpp:194] Memory required for data: 60416120
I0825 11:12:22.901988  2068 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:12:22.902009  2068 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:12:22.902026  2068 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:12:22.902048  2068 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:12:22.902071  2068 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:12:22.902158  2068 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:12:22.902179  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.902199  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.902213  2068 net.cpp:194] Memory required for data: 61726840
I0825 11:12:22.902230  2068 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:12:22.902256  2068 net.cpp:128] Creating Layer Convolution18
I0825 11:12:22.902274  2068 net.cpp:558] Convolution18 <- Eltwise8_ReLU17_0_split_0
I0825 11:12:22.902297  2068 net.cpp:522] Convolution18 -> Convolution18
I0825 11:12:22.918794  2068 net.cpp:172] Setting up Convolution18
I0825 11:12:22.918838  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.918854  2068 net.cpp:194] Memory required for data: 62382200
I0825 11:12:22.918879  2068 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:12:22.918903  2068 net.cpp:128] Creating Layer BatchNorm18
I0825 11:12:22.918920  2068 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:12:22.918942  2068 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:12:22.919312  2068 net.cpp:172] Setting up BatchNorm18
I0825 11:12:22.919337  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.919353  2068 net.cpp:194] Memory required for data: 63037560
I0825 11:12:22.919378  2068 layer_factory.hpp:77] Creating layer Scale18
I0825 11:12:22.919400  2068 net.cpp:128] Creating Layer Scale18
I0825 11:12:22.919417  2068 net.cpp:558] Scale18 <- Convolution18
I0825 11:12:22.919435  2068 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:12:22.919515  2068 layer_factory.hpp:77] Creating layer Scale18
I0825 11:12:22.919728  2068 net.cpp:172] Setting up Scale18
I0825 11:12:22.919750  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.919766  2068 net.cpp:194] Memory required for data: 63692920
I0825 11:12:22.919796  2068 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:12:22.919819  2068 net.cpp:128] Creating Layer ReLU18
I0825 11:12:22.919836  2068 net.cpp:558] ReLU18 <- Convolution18
I0825 11:12:22.919855  2068 net.cpp:509] ReLU18 -> Convolution18 (in-place)
I0825 11:12:22.923025  2068 net.cpp:172] Setting up ReLU18
I0825 11:12:22.923058  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.923076  2068 net.cpp:194] Memory required for data: 64348280
I0825 11:12:22.923094  2068 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:12:22.923125  2068 net.cpp:128] Creating Layer Convolution19
I0825 11:12:22.923142  2068 net.cpp:558] Convolution19 <- Convolution18
I0825 11:12:22.923173  2068 net.cpp:522] Convolution19 -> Convolution19
I0825 11:12:22.936386  2068 net.cpp:172] Setting up Convolution19
I0825 11:12:22.936429  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.936446  2068 net.cpp:194] Memory required for data: 65003640
I0825 11:12:22.936470  2068 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:12:22.936492  2068 net.cpp:128] Creating Layer BatchNorm19
I0825 11:12:22.936511  2068 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:12:22.936532  2068 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:12:22.936903  2068 net.cpp:172] Setting up BatchNorm19
I0825 11:12:22.936928  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.936944  2068 net.cpp:194] Memory required for data: 65659000
I0825 11:12:22.936982  2068 layer_factory.hpp:77] Creating layer Scale19
I0825 11:12:22.937006  2068 net.cpp:128] Creating Layer Scale19
I0825 11:12:22.937024  2068 net.cpp:558] Scale19 <- Convolution19
I0825 11:12:22.937043  2068 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:12:22.937129  2068 layer_factory.hpp:77] Creating layer Scale19
I0825 11:12:22.937336  2068 net.cpp:172] Setting up Scale19
I0825 11:12:22.937358  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.937374  2068 net.cpp:194] Memory required for data: 66314360
I0825 11:12:22.937397  2068 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:12:22.937417  2068 net.cpp:128] Creating Layer Eltwise9
I0825 11:12:22.937434  2068 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:12:22.937453  2068 net.cpp:558] Eltwise9 <- Convolution19
I0825 11:12:22.937474  2068 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:12:22.937528  2068 net.cpp:172] Setting up Eltwise9
I0825 11:12:22.937548  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.937564  2068 net.cpp:194] Memory required for data: 66969720
I0825 11:12:22.937582  2068 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:12:22.937603  2068 net.cpp:128] Creating Layer ReLU19
I0825 11:12:22.937620  2068 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:12:22.937639  2068 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:12:22.940641  2068 net.cpp:172] Setting up ReLU19
I0825 11:12:22.940683  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.940701  2068 net.cpp:194] Memory required for data: 67625080
I0825 11:12:22.940718  2068 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:12:22.940739  2068 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:12:22.940757  2068 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:12:22.940780  2068 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:12:22.940804  2068 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:12:22.940889  2068 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:12:22.940913  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.940932  2068 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:12:22.940948  2068 net.cpp:194] Memory required for data: 68935800
I0825 11:12:22.940963  2068 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:12:22.940991  2068 net.cpp:128] Creating Layer Convolution20
I0825 11:12:22.941009  2068 net.cpp:558] Convolution20 <- Eltwise9_ReLU19_0_split_0
I0825 11:12:22.941030  2068 net.cpp:522] Convolution20 -> Convolution20
I0825 11:12:22.955202  2068 net.cpp:172] Setting up Convolution20
I0825 11:12:22.955245  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.955261  2068 net.cpp:194] Memory required for data: 69263480
I0825 11:12:22.955284  2068 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:12:22.955308  2068 net.cpp:128] Creating Layer BatchNorm20
I0825 11:12:22.955327  2068 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:12:22.955348  2068 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:12:22.955706  2068 net.cpp:172] Setting up BatchNorm20
I0825 11:12:22.955730  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.955756  2068 net.cpp:194] Memory required for data: 69591160
I0825 11:12:22.955780  2068 layer_factory.hpp:77] Creating layer Scale20
I0825 11:12:22.955802  2068 net.cpp:128] Creating Layer Scale20
I0825 11:12:22.955819  2068 net.cpp:558] Scale20 <- Convolution20
I0825 11:12:22.955837  2068 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:12:22.955920  2068 layer_factory.hpp:77] Creating layer Scale20
I0825 11:12:22.956130  2068 net.cpp:172] Setting up Scale20
I0825 11:12:22.956151  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.956167  2068 net.cpp:194] Memory required for data: 69918840
I0825 11:12:22.956188  2068 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:12:22.956214  2068 net.cpp:128] Creating Layer Convolution21
I0825 11:12:22.956231  2068 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_1
I0825 11:12:22.956251  2068 net.cpp:522] Convolution21 -> Convolution21
I0825 11:12:22.967551  2068 net.cpp:172] Setting up Convolution21
I0825 11:12:22.967595  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.967612  2068 net.cpp:194] Memory required for data: 70246520
I0825 11:12:22.967635  2068 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:12:22.967658  2068 net.cpp:128] Creating Layer BatchNorm21
I0825 11:12:22.967676  2068 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:12:22.967697  2068 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:12:22.968061  2068 net.cpp:172] Setting up BatchNorm21
I0825 11:12:22.968086  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.968102  2068 net.cpp:194] Memory required for data: 70574200
I0825 11:12:22.968127  2068 layer_factory.hpp:77] Creating layer Scale21
I0825 11:12:22.968149  2068 net.cpp:128] Creating Layer Scale21
I0825 11:12:22.968166  2068 net.cpp:558] Scale21 <- Convolution21
I0825 11:12:22.968185  2068 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:12:22.968267  2068 layer_factory.hpp:77] Creating layer Scale21
I0825 11:12:22.968480  2068 net.cpp:172] Setting up Scale21
I0825 11:12:22.968503  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.968519  2068 net.cpp:194] Memory required for data: 70901880
I0825 11:12:22.968541  2068 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:12:22.968564  2068 net.cpp:128] Creating Layer ReLU20
I0825 11:12:22.968580  2068 net.cpp:558] ReLU20 <- Convolution21
I0825 11:12:22.968598  2068 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:12:22.971714  2068 net.cpp:172] Setting up ReLU20
I0825 11:12:22.971747  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.971765  2068 net.cpp:194] Memory required for data: 71229560
I0825 11:12:22.971781  2068 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:12:22.971812  2068 net.cpp:128] Creating Layer Convolution22
I0825 11:12:22.971829  2068 net.cpp:558] Convolution22 <- Convolution21
I0825 11:12:22.971850  2068 net.cpp:522] Convolution22 -> Convolution22
I0825 11:12:22.985595  2068 net.cpp:172] Setting up Convolution22
I0825 11:12:22.985651  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.985672  2068 net.cpp:194] Memory required for data: 71557240
I0825 11:12:22.985702  2068 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:12:22.985731  2068 net.cpp:128] Creating Layer BatchNorm22
I0825 11:12:22.985754  2068 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:12:22.985791  2068 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:12:22.986157  2068 net.cpp:172] Setting up BatchNorm22
I0825 11:12:22.986189  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.986210  2068 net.cpp:194] Memory required for data: 71884920
I0825 11:12:22.986240  2068 layer_factory.hpp:77] Creating layer Scale22
I0825 11:12:22.986264  2068 net.cpp:128] Creating Layer Scale22
I0825 11:12:22.986286  2068 net.cpp:558] Scale22 <- Convolution22
I0825 11:12:22.986311  2068 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:12:22.986407  2068 layer_factory.hpp:77] Creating layer Scale22
I0825 11:12:22.986630  2068 net.cpp:172] Setting up Scale22
I0825 11:12:22.986673  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.986696  2068 net.cpp:194] Memory required for data: 72212600
I0825 11:12:22.986724  2068 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:12:22.986752  2068 net.cpp:128] Creating Layer Eltwise10
I0825 11:12:22.986773  2068 net.cpp:558] Eltwise10 <- Convolution20
I0825 11:12:22.986795  2068 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:12:22.986822  2068 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:12:22.986879  2068 net.cpp:172] Setting up Eltwise10
I0825 11:12:22.986908  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.986929  2068 net.cpp:194] Memory required for data: 72540280
I0825 11:12:22.986958  2068 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:12:22.986982  2068 net.cpp:128] Creating Layer ReLU21
I0825 11:12:22.987004  2068 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:12:22.987026  2068 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:12:22.989714  2068 net.cpp:172] Setting up ReLU21
I0825 11:12:22.989753  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.989776  2068 net.cpp:194] Memory required for data: 72867960
I0825 11:12:22.989799  2068 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:12:22.989823  2068 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:12:22.989845  2068 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:12:22.989871  2068 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:12:22.989898  2068 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:12:22.989989  2068 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:12:22.990015  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.990039  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:22.990061  2068 net.cpp:194] Memory required for data: 73523320
I0825 11:12:22.990082  2068 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:12:22.990118  2068 net.cpp:128] Creating Layer Convolution23
I0825 11:12:22.990140  2068 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:12:22.990165  2068 net.cpp:522] Convolution23 -> Convolution23
I0825 11:12:23.005396  2068 net.cpp:172] Setting up Convolution23
I0825 11:12:23.005450  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.005481  2068 net.cpp:194] Memory required for data: 73851000
I0825 11:12:23.005514  2068 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:12:23.005545  2068 net.cpp:128] Creating Layer BatchNorm23
I0825 11:12:23.005568  2068 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:12:23.005597  2068 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:12:23.005973  2068 net.cpp:172] Setting up BatchNorm23
I0825 11:12:23.006006  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.006027  2068 net.cpp:194] Memory required for data: 74178680
I0825 11:12:23.006057  2068 layer_factory.hpp:77] Creating layer Scale23
I0825 11:12:23.006083  2068 net.cpp:128] Creating Layer Scale23
I0825 11:12:23.006106  2068 net.cpp:558] Scale23 <- Convolution23
I0825 11:12:23.006130  2068 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:12:23.006223  2068 layer_factory.hpp:77] Creating layer Scale23
I0825 11:12:23.006453  2068 net.cpp:172] Setting up Scale23
I0825 11:12:23.006495  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.006517  2068 net.cpp:194] Memory required for data: 74506360
I0825 11:12:23.006543  2068 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:12:23.006572  2068 net.cpp:128] Creating Layer ReLU22
I0825 11:12:23.006593  2068 net.cpp:558] ReLU22 <- Convolution23
I0825 11:12:23.006616  2068 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0825 11:12:23.011389  2068 net.cpp:172] Setting up ReLU22
I0825 11:12:23.011428  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.011451  2068 net.cpp:194] Memory required for data: 74834040
I0825 11:12:23.011471  2068 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:12:23.011517  2068 net.cpp:128] Creating Layer Convolution24
I0825 11:12:23.011540  2068 net.cpp:558] Convolution24 <- Convolution23
I0825 11:12:23.011569  2068 net.cpp:522] Convolution24 -> Convolution24
I0825 11:12:23.030974  2068 net.cpp:172] Setting up Convolution24
I0825 11:12:23.031046  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.031080  2068 net.cpp:194] Memory required for data: 75161720
I0825 11:12:23.031119  2068 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:12:23.031158  2068 net.cpp:128] Creating Layer BatchNorm24
I0825 11:12:23.031191  2068 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:12:23.031225  2068 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:12:23.031605  2068 net.cpp:172] Setting up BatchNorm24
I0825 11:12:23.031646  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.031678  2068 net.cpp:194] Memory required for data: 75489400
I0825 11:12:23.031718  2068 layer_factory.hpp:77] Creating layer Scale24
I0825 11:12:23.031756  2068 net.cpp:128] Creating Layer Scale24
I0825 11:12:23.031788  2068 net.cpp:558] Scale24 <- Convolution24
I0825 11:12:23.031822  2068 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:12:23.031924  2068 layer_factory.hpp:77] Creating layer Scale24
I0825 11:12:23.032157  2068 net.cpp:172] Setting up Scale24
I0825 11:12:23.032198  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.032229  2068 net.cpp:194] Memory required for data: 75817080
I0825 11:12:23.032268  2068 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:12:23.032306  2068 net.cpp:128] Creating Layer Eltwise11
I0825 11:12:23.032339  2068 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0825 11:12:23.032373  2068 net.cpp:558] Eltwise11 <- Convolution24
I0825 11:12:23.032409  2068 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:12:23.032474  2068 net.cpp:172] Setting up Eltwise11
I0825 11:12:23.032510  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.032541  2068 net.cpp:194] Memory required for data: 76144760
I0825 11:12:23.032573  2068 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:12:23.032608  2068 net.cpp:128] Creating Layer ReLU23
I0825 11:12:23.032639  2068 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:12:23.032675  2068 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:12:23.037292  2068 net.cpp:172] Setting up ReLU23
I0825 11:12:23.037355  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.037389  2068 net.cpp:194] Memory required for data: 76472440
I0825 11:12:23.037422  2068 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:12:23.037461  2068 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:12:23.037494  2068 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:12:23.037529  2068 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:12:23.037569  2068 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:12:23.037675  2068 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:12:23.037714  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.037747  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.037778  2068 net.cpp:194] Memory required for data: 77127800
I0825 11:12:23.037811  2068 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:12:23.037852  2068 net.cpp:128] Creating Layer Convolution25
I0825 11:12:23.037889  2068 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0825 11:12:23.037928  2068 net.cpp:522] Convolution25 -> Convolution25
I0825 11:12:23.053432  2068 net.cpp:172] Setting up Convolution25
I0825 11:12:23.053499  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.053534  2068 net.cpp:194] Memory required for data: 77455480
I0825 11:12:23.053572  2068 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:12:23.053611  2068 net.cpp:128] Creating Layer BatchNorm25
I0825 11:12:23.053643  2068 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:12:23.053679  2068 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:12:23.054088  2068 net.cpp:172] Setting up BatchNorm25
I0825 11:12:23.054134  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.054167  2068 net.cpp:194] Memory required for data: 77783160
I0825 11:12:23.054206  2068 layer_factory.hpp:77] Creating layer Scale25
I0825 11:12:23.054241  2068 net.cpp:128] Creating Layer Scale25
I0825 11:12:23.054273  2068 net.cpp:558] Scale25 <- Convolution25
I0825 11:12:23.054307  2068 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:12:23.054420  2068 layer_factory.hpp:77] Creating layer Scale25
I0825 11:12:23.054656  2068 net.cpp:172] Setting up Scale25
I0825 11:12:23.054697  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.054729  2068 net.cpp:194] Memory required for data: 78110840
I0825 11:12:23.054766  2068 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:12:23.054801  2068 net.cpp:128] Creating Layer ReLU24
I0825 11:12:23.054836  2068 net.cpp:558] ReLU24 <- Convolution25
I0825 11:12:23.054868  2068 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0825 11:12:23.057382  2068 net.cpp:172] Setting up ReLU24
I0825 11:12:23.057435  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.057468  2068 net.cpp:194] Memory required for data: 78438520
I0825 11:12:23.057502  2068 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:12:23.057543  2068 net.cpp:128] Creating Layer Convolution26
I0825 11:12:23.057576  2068 net.cpp:558] Convolution26 <- Convolution25
I0825 11:12:23.057615  2068 net.cpp:522] Convolution26 -> Convolution26
I0825 11:12:23.070708  2068 net.cpp:172] Setting up Convolution26
I0825 11:12:23.070773  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.070806  2068 net.cpp:194] Memory required for data: 78766200
I0825 11:12:23.070845  2068 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:12:23.070884  2068 net.cpp:128] Creating Layer BatchNorm26
I0825 11:12:23.070917  2068 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:12:23.070953  2068 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:12:23.071338  2068 net.cpp:172] Setting up BatchNorm26
I0825 11:12:23.071382  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.071413  2068 net.cpp:194] Memory required for data: 79093880
I0825 11:12:23.071452  2068 layer_factory.hpp:77] Creating layer Scale26
I0825 11:12:23.071490  2068 net.cpp:128] Creating Layer Scale26
I0825 11:12:23.071521  2068 net.cpp:558] Scale26 <- Convolution26
I0825 11:12:23.071555  2068 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:12:23.071657  2068 layer_factory.hpp:77] Creating layer Scale26
I0825 11:12:23.071887  2068 net.cpp:172] Setting up Scale26
I0825 11:12:23.071928  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.071959  2068 net.cpp:194] Memory required for data: 79421560
I0825 11:12:23.071996  2068 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:12:23.072034  2068 net.cpp:128] Creating Layer Eltwise12
I0825 11:12:23.072067  2068 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:12:23.072100  2068 net.cpp:558] Eltwise12 <- Convolution26
I0825 11:12:23.072135  2068 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:12:23.072202  2068 net.cpp:172] Setting up Eltwise12
I0825 11:12:23.072238  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.072269  2068 net.cpp:194] Memory required for data: 79749240
I0825 11:12:23.072309  2068 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:12:23.072350  2068 net.cpp:128] Creating Layer ReLU25
I0825 11:12:23.072382  2068 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:12:23.072415  2068 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:12:23.074955  2068 net.cpp:172] Setting up ReLU25
I0825 11:12:23.075011  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.075042  2068 net.cpp:194] Memory required for data: 80076920
I0825 11:12:23.075076  2068 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:12:23.075112  2068 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:12:23.075146  2068 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:12:23.075196  2068 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:12:23.075237  2068 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:12:23.075345  2068 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:12:23.075384  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.075418  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.075449  2068 net.cpp:194] Memory required for data: 80732280
I0825 11:12:23.075481  2068 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:12:23.075522  2068 net.cpp:128] Creating Layer Convolution27
I0825 11:12:23.075556  2068 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0825 11:12:23.075595  2068 net.cpp:522] Convolution27 -> Convolution27
I0825 11:12:23.094566  2068 net.cpp:172] Setting up Convolution27
I0825 11:12:23.094635  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.094666  2068 net.cpp:194] Memory required for data: 81059960
I0825 11:12:23.094705  2068 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:12:23.094741  2068 net.cpp:128] Creating Layer BatchNorm27
I0825 11:12:23.094774  2068 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:12:23.094810  2068 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:12:23.095198  2068 net.cpp:172] Setting up BatchNorm27
I0825 11:12:23.095240  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.095273  2068 net.cpp:194] Memory required for data: 81387640
I0825 11:12:23.095312  2068 layer_factory.hpp:77] Creating layer Scale27
I0825 11:12:23.095347  2068 net.cpp:128] Creating Layer Scale27
I0825 11:12:23.095378  2068 net.cpp:558] Scale27 <- Convolution27
I0825 11:12:23.095412  2068 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:12:23.095513  2068 layer_factory.hpp:77] Creating layer Scale27
I0825 11:12:23.095746  2068 net.cpp:172] Setting up Scale27
I0825 11:12:23.095787  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.095818  2068 net.cpp:194] Memory required for data: 81715320
I0825 11:12:23.095854  2068 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:12:23.095890  2068 net.cpp:128] Creating Layer ReLU26
I0825 11:12:23.095922  2068 net.cpp:558] ReLU26 <- Convolution27
I0825 11:12:23.095955  2068 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0825 11:12:23.100860  2068 net.cpp:172] Setting up ReLU26
I0825 11:12:23.100922  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.100955  2068 net.cpp:194] Memory required for data: 82043000
I0825 11:12:23.100987  2068 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:12:23.101032  2068 net.cpp:128] Creating Layer Convolution28
I0825 11:12:23.101063  2068 net.cpp:558] Convolution28 <- Convolution27
I0825 11:12:23.101099  2068 net.cpp:522] Convolution28 -> Convolution28
I0825 11:12:23.120494  2068 net.cpp:172] Setting up Convolution28
I0825 11:12:23.120558  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.120590  2068 net.cpp:194] Memory required for data: 82370680
I0825 11:12:23.120630  2068 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:12:23.120666  2068 net.cpp:128] Creating Layer BatchNorm28
I0825 11:12:23.120699  2068 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:12:23.120736  2068 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:12:23.121129  2068 net.cpp:172] Setting up BatchNorm28
I0825 11:12:23.121173  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.121204  2068 net.cpp:194] Memory required for data: 82698360
I0825 11:12:23.121243  2068 layer_factory.hpp:77] Creating layer Scale28
I0825 11:12:23.121279  2068 net.cpp:128] Creating Layer Scale28
I0825 11:12:23.121309  2068 net.cpp:558] Scale28 <- Convolution28
I0825 11:12:23.121345  2068 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:12:23.121448  2068 layer_factory.hpp:77] Creating layer Scale28
I0825 11:12:23.121683  2068 net.cpp:172] Setting up Scale28
I0825 11:12:23.121723  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.121764  2068 net.cpp:194] Memory required for data: 83026040
I0825 11:12:23.121803  2068 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:12:23.121839  2068 net.cpp:128] Creating Layer Eltwise13
I0825 11:12:23.121870  2068 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:12:23.121906  2068 net.cpp:558] Eltwise13 <- Convolution28
I0825 11:12:23.121940  2068 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:12:23.122007  2068 net.cpp:172] Setting up Eltwise13
I0825 11:12:23.122046  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.122078  2068 net.cpp:194] Memory required for data: 83353720
I0825 11:12:23.122109  2068 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:12:23.122143  2068 net.cpp:128] Creating Layer ReLU27
I0825 11:12:23.122174  2068 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:12:23.122207  2068 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:12:23.125200  2068 net.cpp:172] Setting up ReLU27
I0825 11:12:23.125259  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.125291  2068 net.cpp:194] Memory required for data: 83681400
I0825 11:12:23.125324  2068 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:12:23.125360  2068 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:12:23.125392  2068 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:12:23.125430  2068 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:12:23.125469  2068 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:12:23.125573  2068 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:12:23.125612  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.125646  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.125676  2068 net.cpp:194] Memory required for data: 84336760
I0825 11:12:23.125708  2068 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:12:23.125751  2068 net.cpp:128] Creating Layer Convolution29
I0825 11:12:23.125784  2068 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0825 11:12:23.125820  2068 net.cpp:522] Convolution29 -> Convolution29
I0825 11:12:23.138543  2068 net.cpp:172] Setting up Convolution29
I0825 11:12:23.138607  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.138639  2068 net.cpp:194] Memory required for data: 84664440
I0825 11:12:23.138679  2068 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:12:23.138720  2068 net.cpp:128] Creating Layer BatchNorm29
I0825 11:12:23.138752  2068 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:12:23.138788  2068 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:12:23.139178  2068 net.cpp:172] Setting up BatchNorm29
I0825 11:12:23.139220  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.139252  2068 net.cpp:194] Memory required for data: 84992120
I0825 11:12:23.139292  2068 layer_factory.hpp:77] Creating layer Scale29
I0825 11:12:23.139329  2068 net.cpp:128] Creating Layer Scale29
I0825 11:12:23.139360  2068 net.cpp:558] Scale29 <- Convolution29
I0825 11:12:23.139394  2068 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:12:23.139495  2068 layer_factory.hpp:77] Creating layer Scale29
I0825 11:12:23.139727  2068 net.cpp:172] Setting up Scale29
I0825 11:12:23.139766  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.139798  2068 net.cpp:194] Memory required for data: 85319800
I0825 11:12:23.139842  2068 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:12:23.139880  2068 net.cpp:128] Creating Layer ReLU28
I0825 11:12:23.139912  2068 net.cpp:558] ReLU28 <- Convolution29
I0825 11:12:23.139945  2068 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0825 11:12:23.141237  2068 net.cpp:172] Setting up ReLU28
I0825 11:12:23.141288  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.141320  2068 net.cpp:194] Memory required for data: 85647480
I0825 11:12:23.141352  2068 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:12:23.141398  2068 net.cpp:128] Creating Layer Convolution30
I0825 11:12:23.141430  2068 net.cpp:558] Convolution30 <- Convolution29
I0825 11:12:23.141477  2068 net.cpp:522] Convolution30 -> Convolution30
I0825 11:12:23.158807  2068 net.cpp:172] Setting up Convolution30
I0825 11:12:23.158874  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.158906  2068 net.cpp:194] Memory required for data: 85975160
I0825 11:12:23.158946  2068 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:12:23.158984  2068 net.cpp:128] Creating Layer BatchNorm30
I0825 11:12:23.159016  2068 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:12:23.159052  2068 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:12:23.159440  2068 net.cpp:172] Setting up BatchNorm30
I0825 11:12:23.159483  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.159514  2068 net.cpp:194] Memory required for data: 86302840
I0825 11:12:23.159554  2068 layer_factory.hpp:77] Creating layer Scale30
I0825 11:12:23.159588  2068 net.cpp:128] Creating Layer Scale30
I0825 11:12:23.159620  2068 net.cpp:558] Scale30 <- Convolution30
I0825 11:12:23.159656  2068 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:12:23.159756  2068 layer_factory.hpp:77] Creating layer Scale30
I0825 11:12:23.159994  2068 net.cpp:172] Setting up Scale30
I0825 11:12:23.160034  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.160065  2068 net.cpp:194] Memory required for data: 86630520
I0825 11:12:23.160104  2068 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:12:23.160140  2068 net.cpp:128] Creating Layer Eltwise14
I0825 11:12:23.160171  2068 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:12:23.160205  2068 net.cpp:558] Eltwise14 <- Convolution30
I0825 11:12:23.160241  2068 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:12:23.160307  2068 net.cpp:172] Setting up Eltwise14
I0825 11:12:23.160346  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.160377  2068 net.cpp:194] Memory required for data: 86958200
I0825 11:12:23.160408  2068 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:12:23.160442  2068 net.cpp:128] Creating Layer ReLU29
I0825 11:12:23.160473  2068 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:12:23.160506  2068 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:12:23.165148  2068 net.cpp:172] Setting up ReLU29
I0825 11:12:23.165211  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.165243  2068 net.cpp:194] Memory required for data: 87285880
I0825 11:12:23.165277  2068 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:12:23.165315  2068 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:12:23.165347  2068 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:12:23.165382  2068 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:12:23.165422  2068 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:12:23.165529  2068 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:12:23.165566  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.165601  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.165630  2068 net.cpp:194] Memory required for data: 87941240
I0825 11:12:23.165661  2068 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:12:23.165702  2068 net.cpp:128] Creating Layer Convolution31
I0825 11:12:23.165733  2068 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0825 11:12:23.165779  2068 net.cpp:522] Convolution31 -> Convolution31
I0825 11:12:23.184809  2068 net.cpp:172] Setting up Convolution31
I0825 11:12:23.184873  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.184906  2068 net.cpp:194] Memory required for data: 88268920
I0825 11:12:23.184944  2068 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:12:23.184983  2068 net.cpp:128] Creating Layer BatchNorm31
I0825 11:12:23.185015  2068 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:12:23.185051  2068 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:12:23.185442  2068 net.cpp:172] Setting up BatchNorm31
I0825 11:12:23.185484  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.185526  2068 net.cpp:194] Memory required for data: 88596600
I0825 11:12:23.185566  2068 layer_factory.hpp:77] Creating layer Scale31
I0825 11:12:23.185601  2068 net.cpp:128] Creating Layer Scale31
I0825 11:12:23.185632  2068 net.cpp:558] Scale31 <- Convolution31
I0825 11:12:23.185667  2068 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:12:23.185771  2068 layer_factory.hpp:77] Creating layer Scale31
I0825 11:12:23.186007  2068 net.cpp:172] Setting up Scale31
I0825 11:12:23.186048  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.186079  2068 net.cpp:194] Memory required for data: 88924280
I0825 11:12:23.186117  2068 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:12:23.186154  2068 net.cpp:128] Creating Layer ReLU30
I0825 11:12:23.186185  2068 net.cpp:558] ReLU30 <- Convolution31
I0825 11:12:23.186219  2068 net.cpp:509] ReLU30 -> Convolution31 (in-place)
I0825 11:12:23.190124  2068 net.cpp:172] Setting up ReLU30
I0825 11:12:23.190179  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.190212  2068 net.cpp:194] Memory required for data: 89251960
I0825 11:12:23.190243  2068 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:12:23.190285  2068 net.cpp:128] Creating Layer Convolution32
I0825 11:12:23.190317  2068 net.cpp:558] Convolution32 <- Convolution31
I0825 11:12:23.190362  2068 net.cpp:522] Convolution32 -> Convolution32
I0825 11:12:23.197773  2068 net.cpp:172] Setting up Convolution32
I0825 11:12:23.197839  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.197870  2068 net.cpp:194] Memory required for data: 89579640
I0825 11:12:23.197909  2068 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:12:23.197947  2068 net.cpp:128] Creating Layer BatchNorm32
I0825 11:12:23.197979  2068 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:12:23.198014  2068 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:12:23.198407  2068 net.cpp:172] Setting up BatchNorm32
I0825 11:12:23.198451  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.198482  2068 net.cpp:194] Memory required for data: 89907320
I0825 11:12:23.198523  2068 layer_factory.hpp:77] Creating layer Scale32
I0825 11:12:23.198562  2068 net.cpp:128] Creating Layer Scale32
I0825 11:12:23.198593  2068 net.cpp:558] Scale32 <- Convolution32
I0825 11:12:23.198627  2068 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:12:23.198729  2068 layer_factory.hpp:77] Creating layer Scale32
I0825 11:12:23.198972  2068 net.cpp:172] Setting up Scale32
I0825 11:12:23.199013  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.199044  2068 net.cpp:194] Memory required for data: 90235000
I0825 11:12:23.199082  2068 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:12:23.199117  2068 net.cpp:128] Creating Layer Eltwise15
I0825 11:12:23.199148  2068 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0825 11:12:23.199182  2068 net.cpp:558] Eltwise15 <- Convolution32
I0825 11:12:23.199219  2068 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:12:23.199285  2068 net.cpp:172] Setting up Eltwise15
I0825 11:12:23.199321  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.199352  2068 net.cpp:194] Memory required for data: 90562680
I0825 11:12:23.199384  2068 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:12:23.199419  2068 net.cpp:128] Creating Layer ReLU31
I0825 11:12:23.199455  2068 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:12:23.199491  2068 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:12:23.199820  2068 net.cpp:172] Setting up ReLU31
I0825 11:12:23.199872  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.199903  2068 net.cpp:194] Memory required for data: 90890360
I0825 11:12:23.199935  2068 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0825 11:12:23.199973  2068 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0825 11:12:23.200006  2068 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0825 11:12:23.200040  2068 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0825 11:12:23.200091  2068 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0825 11:12:23.200198  2068 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0825 11:12:23.200237  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.200270  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.200301  2068 net.cpp:194] Memory required for data: 91545720
I0825 11:12:23.200332  2068 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:12:23.200376  2068 net.cpp:128] Creating Layer Convolution33
I0825 11:12:23.200407  2068 net.cpp:558] Convolution33 <- Eltwise15_ReLU31_0_split_0
I0825 11:12:23.200445  2068 net.cpp:522] Convolution33 -> Convolution33
I0825 11:12:23.205679  2068 net.cpp:172] Setting up Convolution33
I0825 11:12:23.205745  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.205778  2068 net.cpp:194] Memory required for data: 91873400
I0825 11:12:23.205816  2068 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:12:23.205857  2068 net.cpp:128] Creating Layer BatchNorm33
I0825 11:12:23.205889  2068 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:12:23.205924  2068 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:12:23.206318  2068 net.cpp:172] Setting up BatchNorm33
I0825 11:12:23.206368  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.206400  2068 net.cpp:194] Memory required for data: 92201080
I0825 11:12:23.206440  2068 layer_factory.hpp:77] Creating layer Scale33
I0825 11:12:23.206476  2068 net.cpp:128] Creating Layer Scale33
I0825 11:12:23.206508  2068 net.cpp:558] Scale33 <- Convolution33
I0825 11:12:23.206542  2068 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:12:23.206652  2068 layer_factory.hpp:77] Creating layer Scale33
I0825 11:12:23.206892  2068 net.cpp:172] Setting up Scale33
I0825 11:12:23.206933  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.206964  2068 net.cpp:194] Memory required for data: 92528760
I0825 11:12:23.207001  2068 layer_factory.hpp:77] Creating layer ReLU32
I0825 11:12:23.207036  2068 net.cpp:128] Creating Layer ReLU32
I0825 11:12:23.207067  2068 net.cpp:558] ReLU32 <- Convolution33
I0825 11:12:23.207103  2068 net.cpp:509] ReLU32 -> Convolution33 (in-place)
I0825 11:12:23.207765  2068 net.cpp:172] Setting up ReLU32
I0825 11:12:23.207818  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.207850  2068 net.cpp:194] Memory required for data: 92856440
I0825 11:12:23.207883  2068 layer_factory.hpp:77] Creating layer Convolution34
I0825 11:12:23.207926  2068 net.cpp:128] Creating Layer Convolution34
I0825 11:12:23.207959  2068 net.cpp:558] Convolution34 <- Convolution33
I0825 11:12:23.207998  2068 net.cpp:522] Convolution34 -> Convolution34
I0825 11:12:23.215834  2068 net.cpp:172] Setting up Convolution34
I0825 11:12:23.215900  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.215932  2068 net.cpp:194] Memory required for data: 93184120
I0825 11:12:23.215970  2068 layer_factory.hpp:77] Creating layer BatchNorm34
I0825 11:12:23.216009  2068 net.cpp:128] Creating Layer BatchNorm34
I0825 11:12:23.216042  2068 net.cpp:558] BatchNorm34 <- Convolution34
I0825 11:12:23.216078  2068 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0825 11:12:23.216471  2068 net.cpp:172] Setting up BatchNorm34
I0825 11:12:23.216513  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.216555  2068 net.cpp:194] Memory required for data: 93511800
I0825 11:12:23.216595  2068 layer_factory.hpp:77] Creating layer Scale34
I0825 11:12:23.216634  2068 net.cpp:128] Creating Layer Scale34
I0825 11:12:23.216665  2068 net.cpp:558] Scale34 <- Convolution34
I0825 11:12:23.216699  2068 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0825 11:12:23.216805  2068 layer_factory.hpp:77] Creating layer Scale34
I0825 11:12:23.217046  2068 net.cpp:172] Setting up Scale34
I0825 11:12:23.217085  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.217116  2068 net.cpp:194] Memory required for data: 93839480
I0825 11:12:23.217154  2068 layer_factory.hpp:77] Creating layer Eltwise16
I0825 11:12:23.217206  2068 net.cpp:128] Creating Layer Eltwise16
I0825 11:12:23.217237  2068 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0825 11:12:23.217272  2068 net.cpp:558] Eltwise16 <- Convolution34
I0825 11:12:23.217310  2068 net.cpp:522] Eltwise16 -> Eltwise16
I0825 11:12:23.217378  2068 net.cpp:172] Setting up Eltwise16
I0825 11:12:23.217417  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.217447  2068 net.cpp:194] Memory required for data: 94167160
I0825 11:12:23.217479  2068 layer_factory.hpp:77] Creating layer ReLU33
I0825 11:12:23.217514  2068 net.cpp:128] Creating Layer ReLU33
I0825 11:12:23.217545  2068 net.cpp:558] ReLU33 <- Eltwise16
I0825 11:12:23.217581  2068 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0825 11:12:23.218161  2068 net.cpp:172] Setting up ReLU33
I0825 11:12:23.218225  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.218257  2068 net.cpp:194] Memory required for data: 94494840
I0825 11:12:23.218291  2068 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0825 11:12:23.218327  2068 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0825 11:12:23.218366  2068 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0825 11:12:23.218405  2068 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0825 11:12:23.218449  2068 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0825 11:12:23.218564  2068 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0825 11:12:23.218602  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.218636  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.218667  2068 net.cpp:194] Memory required for data: 95150200
I0825 11:12:23.218698  2068 layer_factory.hpp:77] Creating layer Convolution35
I0825 11:12:23.218739  2068 net.cpp:128] Creating Layer Convolution35
I0825 11:12:23.218771  2068 net.cpp:558] Convolution35 <- Eltwise16_ReLU33_0_split_0
I0825 11:12:23.218809  2068 net.cpp:522] Convolution35 -> Convolution35
I0825 11:12:23.225193  2068 net.cpp:172] Setting up Convolution35
I0825 11:12:23.225260  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.225292  2068 net.cpp:194] Memory required for data: 95477880
I0825 11:12:23.225332  2068 layer_factory.hpp:77] Creating layer BatchNorm35
I0825 11:12:23.225370  2068 net.cpp:128] Creating Layer BatchNorm35
I0825 11:12:23.225402  2068 net.cpp:558] BatchNorm35 <- Convolution35
I0825 11:12:23.225436  2068 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0825 11:12:23.225848  2068 net.cpp:172] Setting up BatchNorm35
I0825 11:12:23.225891  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.225924  2068 net.cpp:194] Memory required for data: 95805560
I0825 11:12:23.225962  2068 layer_factory.hpp:77] Creating layer Scale35
I0825 11:12:23.225997  2068 net.cpp:128] Creating Layer Scale35
I0825 11:12:23.226029  2068 net.cpp:558] Scale35 <- Convolution35
I0825 11:12:23.226063  2068 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0825 11:12:23.226168  2068 layer_factory.hpp:77] Creating layer Scale35
I0825 11:12:23.226415  2068 net.cpp:172] Setting up Scale35
I0825 11:12:23.226459  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.226490  2068 net.cpp:194] Memory required for data: 96133240
I0825 11:12:23.226527  2068 layer_factory.hpp:77] Creating layer ReLU34
I0825 11:12:23.226569  2068 net.cpp:128] Creating Layer ReLU34
I0825 11:12:23.226601  2068 net.cpp:558] ReLU34 <- Convolution35
I0825 11:12:23.226637  2068 net.cpp:509] ReLU34 -> Convolution35 (in-place)
I0825 11:12:23.226999  2068 net.cpp:172] Setting up ReLU34
I0825 11:12:23.227056  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.227087  2068 net.cpp:194] Memory required for data: 96460920
I0825 11:12:23.227119  2068 layer_factory.hpp:77] Creating layer Convolution36
I0825 11:12:23.227164  2068 net.cpp:128] Creating Layer Convolution36
I0825 11:12:23.227195  2068 net.cpp:558] Convolution36 <- Convolution35
I0825 11:12:23.227239  2068 net.cpp:522] Convolution36 -> Convolution36
I0825 11:12:23.246641  2068 net.cpp:172] Setting up Convolution36
I0825 11:12:23.246709  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.246742  2068 net.cpp:194] Memory required for data: 96788600
I0825 11:12:23.246781  2068 layer_factory.hpp:77] Creating layer BatchNorm36
I0825 11:12:23.246822  2068 net.cpp:128] Creating Layer BatchNorm36
I0825 11:12:23.246855  2068 net.cpp:558] BatchNorm36 <- Convolution36
I0825 11:12:23.246892  2068 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0825 11:12:23.247287  2068 net.cpp:172] Setting up BatchNorm36
I0825 11:12:23.247328  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.247360  2068 net.cpp:194] Memory required for data: 97116280
I0825 11:12:23.247400  2068 layer_factory.hpp:77] Creating layer Scale36
I0825 11:12:23.247437  2068 net.cpp:128] Creating Layer Scale36
I0825 11:12:23.247469  2068 net.cpp:558] Scale36 <- Convolution36
I0825 11:12:23.247503  2068 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0825 11:12:23.247608  2068 layer_factory.hpp:77] Creating layer Scale36
I0825 11:12:23.247844  2068 net.cpp:172] Setting up Scale36
I0825 11:12:23.247885  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.247917  2068 net.cpp:194] Memory required for data: 97443960
I0825 11:12:23.247956  2068 layer_factory.hpp:77] Creating layer Eltwise17
I0825 11:12:23.247993  2068 net.cpp:128] Creating Layer Eltwise17
I0825 11:12:23.248026  2068 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0825 11:12:23.248059  2068 net.cpp:558] Eltwise17 <- Convolution36
I0825 11:12:23.248097  2068 net.cpp:522] Eltwise17 -> Eltwise17
I0825 11:12:23.248163  2068 net.cpp:172] Setting up Eltwise17
I0825 11:12:23.248199  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.248230  2068 net.cpp:194] Memory required for data: 97771640
I0825 11:12:23.248262  2068 layer_factory.hpp:77] Creating layer ReLU35
I0825 11:12:23.248297  2068 net.cpp:128] Creating Layer ReLU35
I0825 11:12:23.248328  2068 net.cpp:558] ReLU35 <- Eltwise17
I0825 11:12:23.248363  2068 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0825 11:12:23.252969  2068 net.cpp:172] Setting up ReLU35
I0825 11:12:23.253022  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.253053  2068 net.cpp:194] Memory required for data: 98099320
I0825 11:12:23.253085  2068 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0825 11:12:23.253123  2068 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0825 11:12:23.253156  2068 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0825 11:12:23.253193  2068 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0825 11:12:23.253233  2068 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0825 11:12:23.253341  2068 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0825 11:12:23.253379  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.253412  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.253443  2068 net.cpp:194] Memory required for data: 98754680
I0825 11:12:23.253475  2068 layer_factory.hpp:77] Creating layer Convolution37
I0825 11:12:23.253516  2068 net.cpp:128] Creating Layer Convolution37
I0825 11:12:23.253549  2068 net.cpp:558] Convolution37 <- Eltwise17_ReLU35_0_split_0
I0825 11:12:23.253587  2068 net.cpp:522] Convolution37 -> Convolution37
I0825 11:12:23.272601  2068 net.cpp:172] Setting up Convolution37
I0825 11:12:23.272675  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.272707  2068 net.cpp:194] Memory required for data: 99082360
I0825 11:12:23.272745  2068 layer_factory.hpp:77] Creating layer BatchNorm37
I0825 11:12:23.272783  2068 net.cpp:128] Creating Layer BatchNorm37
I0825 11:12:23.272815  2068 net.cpp:558] BatchNorm37 <- Convolution37
I0825 11:12:23.272850  2068 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0825 11:12:23.274612  2068 net.cpp:172] Setting up BatchNorm37
I0825 11:12:23.274677  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.274708  2068 net.cpp:194] Memory required for data: 99410040
I0825 11:12:23.274790  2068 layer_factory.hpp:77] Creating layer Scale37
I0825 11:12:23.274827  2068 net.cpp:128] Creating Layer Scale37
I0825 11:12:23.274859  2068 net.cpp:558] Scale37 <- Convolution37
I0825 11:12:23.274893  2068 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0825 11:12:23.274986  2068 layer_factory.hpp:77] Creating layer Scale37
I0825 11:12:23.275153  2068 net.cpp:172] Setting up Scale37
I0825 11:12:23.275192  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.275223  2068 net.cpp:194] Memory required for data: 99737720
I0825 11:12:23.275260  2068 layer_factory.hpp:77] Creating layer ReLU36
I0825 11:12:23.275295  2068 net.cpp:128] Creating Layer ReLU36
I0825 11:12:23.275326  2068 net.cpp:558] ReLU36 <- Convolution37
I0825 11:12:23.275362  2068 net.cpp:509] ReLU36 -> Convolution37 (in-place)
I0825 11:12:23.277140  2068 net.cpp:172] Setting up ReLU36
I0825 11:12:23.277204  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.277236  2068 net.cpp:194] Memory required for data: 100065400
I0825 11:12:23.277269  2068 layer_factory.hpp:77] Creating layer Convolution38
I0825 11:12:23.277314  2068 net.cpp:128] Creating Layer Convolution38
I0825 11:12:23.277348  2068 net.cpp:558] Convolution38 <- Convolution37
I0825 11:12:23.277385  2068 net.cpp:522] Convolution38 -> Convolution38
I0825 11:12:23.290405  2068 net.cpp:172] Setting up Convolution38
I0825 11:12:23.290468  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.290501  2068 net.cpp:194] Memory required for data: 100393080
I0825 11:12:23.290540  2068 layer_factory.hpp:77] Creating layer BatchNorm38
I0825 11:12:23.290578  2068 net.cpp:128] Creating Layer BatchNorm38
I0825 11:12:23.290611  2068 net.cpp:558] BatchNorm38 <- Convolution38
I0825 11:12:23.290648  2068 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0825 11:12:23.290910  2068 net.cpp:172] Setting up BatchNorm38
I0825 11:12:23.290948  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.290979  2068 net.cpp:194] Memory required for data: 100720760
I0825 11:12:23.291018  2068 layer_factory.hpp:77] Creating layer Scale38
I0825 11:12:23.291055  2068 net.cpp:128] Creating Layer Scale38
I0825 11:12:23.291087  2068 net.cpp:558] Scale38 <- Convolution38
I0825 11:12:23.291121  2068 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0825 11:12:23.291200  2068 layer_factory.hpp:77] Creating layer Scale38
I0825 11:12:23.291365  2068 net.cpp:172] Setting up Scale38
I0825 11:12:23.291401  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.291434  2068 net.cpp:194] Memory required for data: 101048440
I0825 11:12:23.291471  2068 layer_factory.hpp:77] Creating layer Eltwise18
I0825 11:12:23.291508  2068 net.cpp:128] Creating Layer Eltwise18
I0825 11:12:23.291540  2068 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0825 11:12:23.291574  2068 net.cpp:558] Eltwise18 <- Convolution38
I0825 11:12:23.291609  2068 net.cpp:522] Eltwise18 -> Eltwise18
I0825 11:12:23.291666  2068 net.cpp:172] Setting up Eltwise18
I0825 11:12:23.291700  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.291733  2068 net.cpp:194] Memory required for data: 101376120
I0825 11:12:23.291764  2068 layer_factory.hpp:77] Creating layer ReLU37
I0825 11:12:23.291798  2068 net.cpp:128] Creating Layer ReLU37
I0825 11:12:23.291829  2068 net.cpp:558] ReLU37 <- Eltwise18
I0825 11:12:23.291864  2068 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0825 11:12:23.294636  2068 net.cpp:172] Setting up ReLU37
I0825 11:12:23.294690  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.294723  2068 net.cpp:194] Memory required for data: 101703800
I0825 11:12:23.294754  2068 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0825 11:12:23.294791  2068 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0825 11:12:23.294824  2068 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0825 11:12:23.294862  2068 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0825 11:12:23.294903  2068 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0825 11:12:23.294996  2068 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0825 11:12:23.295033  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.295066  2068 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:12:23.295097  2068 net.cpp:194] Memory required for data: 102359160
I0825 11:12:23.295130  2068 layer_factory.hpp:77] Creating layer Convolution39
I0825 11:12:23.295171  2068 net.cpp:128] Creating Layer Convolution39
I0825 11:12:23.295204  2068 net.cpp:558] Convolution39 <- Eltwise18_ReLU37_0_split_0
I0825 11:12:23.295244  2068 net.cpp:522] Convolution39 -> Convolution39
I0825 11:12:23.305212  2068 net.cpp:172] Setting up Convolution39
I0825 11:12:23.305286  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.305320  2068 net.cpp:194] Memory required for data: 102523000
I0825 11:12:23.305361  2068 layer_factory.hpp:77] Creating layer BatchNorm39
I0825 11:12:23.305400  2068 net.cpp:128] Creating Layer BatchNorm39
I0825 11:12:23.305433  2068 net.cpp:558] BatchNorm39 <- Convolution39
I0825 11:12:23.305469  2068 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0825 11:12:23.305742  2068 net.cpp:172] Setting up BatchNorm39
I0825 11:12:23.305780  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.305811  2068 net.cpp:194] Memory required for data: 102686840
I0825 11:12:23.305850  2068 layer_factory.hpp:77] Creating layer Scale39
I0825 11:12:23.305888  2068 net.cpp:128] Creating Layer Scale39
I0825 11:12:23.305922  2068 net.cpp:558] Scale39 <- Convolution39
I0825 11:12:23.305954  2068 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0825 11:12:23.306038  2068 layer_factory.hpp:77] Creating layer Scale39
I0825 11:12:23.306208  2068 net.cpp:172] Setting up Scale39
I0825 11:12:23.306246  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.306277  2068 net.cpp:194] Memory required for data: 102850680
I0825 11:12:23.306313  2068 layer_factory.hpp:77] Creating layer Convolution40
I0825 11:12:23.306365  2068 net.cpp:128] Creating Layer Convolution40
I0825 11:12:23.306398  2068 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_1
I0825 11:12:23.306435  2068 net.cpp:522] Convolution40 -> Convolution40
I0825 11:12:23.318035  2068 net.cpp:172] Setting up Convolution40
I0825 11:12:23.318101  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.318133  2068 net.cpp:194] Memory required for data: 103014520
I0825 11:12:23.318172  2068 layer_factory.hpp:77] Creating layer BatchNorm40
I0825 11:12:23.318213  2068 net.cpp:128] Creating Layer BatchNorm40
I0825 11:12:23.318245  2068 net.cpp:558] BatchNorm40 <- Convolution40
I0825 11:12:23.318280  2068 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0825 11:12:23.318555  2068 net.cpp:172] Setting up BatchNorm40
I0825 11:12:23.318595  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.318626  2068 net.cpp:194] Memory required for data: 103178360
I0825 11:12:23.318665  2068 layer_factory.hpp:77] Creating layer Scale40
I0825 11:12:23.318701  2068 net.cpp:128] Creating Layer Scale40
I0825 11:12:23.318732  2068 net.cpp:558] Scale40 <- Convolution40
I0825 11:12:23.318765  2068 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0825 11:12:23.318850  2068 layer_factory.hpp:77] Creating layer Scale40
I0825 11:12:23.319021  2068 net.cpp:172] Setting up Scale40
I0825 11:12:23.319066  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.319108  2068 net.cpp:194] Memory required for data: 103342200
I0825 11:12:23.319145  2068 layer_factory.hpp:77] Creating layer ReLU38
I0825 11:12:23.319180  2068 net.cpp:128] Creating Layer ReLU38
I0825 11:12:23.319212  2068 net.cpp:558] ReLU38 <- Convolution40
I0825 11:12:23.319245  2068 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0825 11:12:23.324338  2068 net.cpp:172] Setting up ReLU38
I0825 11:12:23.324395  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.324427  2068 net.cpp:194] Memory required for data: 103506040
I0825 11:12:23.324460  2068 layer_factory.hpp:77] Creating layer Convolution41
I0825 11:12:23.324502  2068 net.cpp:128] Creating Layer Convolution41
I0825 11:12:23.324548  2068 net.cpp:558] Convolution41 <- Convolution40
I0825 11:12:23.324584  2068 net.cpp:522] Convolution41 -> Convolution41
I0825 11:12:23.341719  2068 net.cpp:172] Setting up Convolution41
I0825 11:12:23.341786  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.341820  2068 net.cpp:194] Memory required for data: 103669880
I0825 11:12:23.341857  2068 layer_factory.hpp:77] Creating layer BatchNorm41
I0825 11:12:23.341893  2068 net.cpp:128] Creating Layer BatchNorm41
I0825 11:12:23.341925  2068 net.cpp:558] BatchNorm41 <- Convolution41
I0825 11:12:23.341962  2068 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0825 11:12:23.342232  2068 net.cpp:172] Setting up BatchNorm41
I0825 11:12:23.342272  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.342303  2068 net.cpp:194] Memory required for data: 103833720
I0825 11:12:23.342347  2068 layer_factory.hpp:77] Creating layer Scale41
I0825 11:12:23.342383  2068 net.cpp:128] Creating Layer Scale41
I0825 11:12:23.342414  2068 net.cpp:558] Scale41 <- Convolution41
I0825 11:12:23.342450  2068 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0825 11:12:23.342530  2068 layer_factory.hpp:77] Creating layer Scale41
I0825 11:12:23.342700  2068 net.cpp:172] Setting up Scale41
I0825 11:12:23.342736  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.342767  2068 net.cpp:194] Memory required for data: 103997560
I0825 11:12:23.342804  2068 layer_factory.hpp:77] Creating layer Eltwise19
I0825 11:12:23.342842  2068 net.cpp:128] Creating Layer Eltwise19
I0825 11:12:23.342873  2068 net.cpp:558] Eltwise19 <- Convolution39
I0825 11:12:23.342906  2068 net.cpp:558] Eltwise19 <- Convolution41
I0825 11:12:23.342941  2068 net.cpp:522] Eltwise19 -> Eltwise19
I0825 11:12:23.343000  2068 net.cpp:172] Setting up Eltwise19
I0825 11:12:23.343035  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.343067  2068 net.cpp:194] Memory required for data: 104161400
I0825 11:12:23.343099  2068 layer_factory.hpp:77] Creating layer ReLU39
I0825 11:12:23.343132  2068 net.cpp:128] Creating Layer ReLU39
I0825 11:12:23.343163  2068 net.cpp:558] ReLU39 <- Eltwise19
I0825 11:12:23.343199  2068 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0825 11:12:23.345963  2068 net.cpp:172] Setting up ReLU39
I0825 11:12:23.346026  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.346058  2068 net.cpp:194] Memory required for data: 104325240
I0825 11:12:23.346091  2068 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0825 11:12:23.346127  2068 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0825 11:12:23.346160  2068 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0825 11:12:23.346199  2068 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0825 11:12:23.346237  2068 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0825 11:12:23.346324  2068 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0825 11:12:23.346367  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.346401  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.346432  2068 net.cpp:194] Memory required for data: 104652920
I0825 11:12:23.346464  2068 layer_factory.hpp:77] Creating layer Convolution42
I0825 11:12:23.346504  2068 net.cpp:128] Creating Layer Convolution42
I0825 11:12:23.346536  2068 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0825 11:12:23.346581  2068 net.cpp:522] Convolution42 -> Convolution42
I0825 11:12:23.359490  2068 net.cpp:172] Setting up Convolution42
I0825 11:12:23.359556  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.359588  2068 net.cpp:194] Memory required for data: 104816760
I0825 11:12:23.359627  2068 layer_factory.hpp:77] Creating layer BatchNorm42
I0825 11:12:23.359665  2068 net.cpp:128] Creating Layer BatchNorm42
I0825 11:12:23.359699  2068 net.cpp:558] BatchNorm42 <- Convolution42
I0825 11:12:23.359732  2068 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0825 11:12:23.360013  2068 net.cpp:172] Setting up BatchNorm42
I0825 11:12:23.360052  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.360095  2068 net.cpp:194] Memory required for data: 104980600
I0825 11:12:23.360134  2068 layer_factory.hpp:77] Creating layer Scale42
I0825 11:12:23.360169  2068 net.cpp:128] Creating Layer Scale42
I0825 11:12:23.360200  2068 net.cpp:558] Scale42 <- Convolution42
I0825 11:12:23.360234  2068 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0825 11:12:23.360319  2068 layer_factory.hpp:77] Creating layer Scale42
I0825 11:12:23.360496  2068 net.cpp:172] Setting up Scale42
I0825 11:12:23.360533  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.360564  2068 net.cpp:194] Memory required for data: 105144440
I0825 11:12:23.360601  2068 layer_factory.hpp:77] Creating layer ReLU40
I0825 11:12:23.360636  2068 net.cpp:128] Creating Layer ReLU40
I0825 11:12:23.360667  2068 net.cpp:558] ReLU40 <- Convolution42
I0825 11:12:23.360700  2068 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0825 11:12:23.365583  2068 net.cpp:172] Setting up ReLU40
I0825 11:12:23.365641  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.365674  2068 net.cpp:194] Memory required for data: 105308280
I0825 11:12:23.365705  2068 layer_factory.hpp:77] Creating layer Convolution43
I0825 11:12:23.365747  2068 net.cpp:128] Creating Layer Convolution43
I0825 11:12:23.365779  2068 net.cpp:558] Convolution43 <- Convolution42
I0825 11:12:23.365818  2068 net.cpp:522] Convolution43 -> Convolution43
I0825 11:12:23.381505  2068 net.cpp:172] Setting up Convolution43
I0825 11:12:23.381572  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.381604  2068 net.cpp:194] Memory required for data: 105472120
I0825 11:12:23.381642  2068 layer_factory.hpp:77] Creating layer BatchNorm43
I0825 11:12:23.381678  2068 net.cpp:128] Creating Layer BatchNorm43
I0825 11:12:23.381711  2068 net.cpp:558] BatchNorm43 <- Convolution43
I0825 11:12:23.381747  2068 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0825 11:12:23.382025  2068 net.cpp:172] Setting up BatchNorm43
I0825 11:12:23.382062  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.382093  2068 net.cpp:194] Memory required for data: 105635960
I0825 11:12:23.382133  2068 layer_factory.hpp:77] Creating layer Scale43
I0825 11:12:23.382167  2068 net.cpp:128] Creating Layer Scale43
I0825 11:12:23.382199  2068 net.cpp:558] Scale43 <- Convolution43
I0825 11:12:23.382232  2068 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0825 11:12:23.382315  2068 layer_factory.hpp:77] Creating layer Scale43
I0825 11:12:23.382500  2068 net.cpp:172] Setting up Scale43
I0825 11:12:23.382539  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.382568  2068 net.cpp:194] Memory required for data: 105799800
I0825 11:12:23.382606  2068 layer_factory.hpp:77] Creating layer Eltwise20
I0825 11:12:23.382642  2068 net.cpp:128] Creating Layer Eltwise20
I0825 11:12:23.382675  2068 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0825 11:12:23.382709  2068 net.cpp:558] Eltwise20 <- Convolution43
I0825 11:12:23.382743  2068 net.cpp:522] Eltwise20 -> Eltwise20
I0825 11:12:23.382804  2068 net.cpp:172] Setting up Eltwise20
I0825 11:12:23.382839  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.382874  2068 net.cpp:194] Memory required for data: 105963640
I0825 11:12:23.382906  2068 layer_factory.hpp:77] Creating layer ReLU41
I0825 11:12:23.382941  2068 net.cpp:128] Creating Layer ReLU41
I0825 11:12:23.382978  2068 net.cpp:558] ReLU41 <- Eltwise20
I0825 11:12:23.383013  2068 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0825 11:12:23.385736  2068 net.cpp:172] Setting up ReLU41
I0825 11:12:23.385789  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.385821  2068 net.cpp:194] Memory required for data: 106127480
I0825 11:12:23.385854  2068 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0825 11:12:23.385890  2068 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0825 11:12:23.385923  2068 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0825 11:12:23.385962  2068 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0825 11:12:23.386011  2068 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0825 11:12:23.386099  2068 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0825 11:12:23.386137  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.386170  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.386200  2068 net.cpp:194] Memory required for data: 106455160
I0825 11:12:23.386232  2068 layer_factory.hpp:77] Creating layer Convolution44
I0825 11:12:23.386272  2068 net.cpp:128] Creating Layer Convolution44
I0825 11:12:23.386304  2068 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0825 11:12:23.386350  2068 net.cpp:522] Convolution44 -> Convolution44
I0825 11:12:23.397677  2068 net.cpp:172] Setting up Convolution44
I0825 11:12:23.397742  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.397774  2068 net.cpp:194] Memory required for data: 106619000
I0825 11:12:23.397814  2068 layer_factory.hpp:77] Creating layer BatchNorm44
I0825 11:12:23.397852  2068 net.cpp:128] Creating Layer BatchNorm44
I0825 11:12:23.397884  2068 net.cpp:558] BatchNorm44 <- Convolution44
I0825 11:12:23.397922  2068 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0825 11:12:23.398206  2068 net.cpp:172] Setting up BatchNorm44
I0825 11:12:23.398245  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.398277  2068 net.cpp:194] Memory required for data: 106782840
I0825 11:12:23.398314  2068 layer_factory.hpp:77] Creating layer Scale44
I0825 11:12:23.398360  2068 net.cpp:128] Creating Layer Scale44
I0825 11:12:23.398391  2068 net.cpp:558] Scale44 <- Convolution44
I0825 11:12:23.398424  2068 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0825 11:12:23.398507  2068 layer_factory.hpp:77] Creating layer Scale44
I0825 11:12:23.398687  2068 net.cpp:172] Setting up Scale44
I0825 11:12:23.398725  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.398756  2068 net.cpp:194] Memory required for data: 106946680
I0825 11:12:23.398793  2068 layer_factory.hpp:77] Creating layer ReLU42
I0825 11:12:23.398828  2068 net.cpp:128] Creating Layer ReLU42
I0825 11:12:23.398859  2068 net.cpp:558] ReLU42 <- Convolution44
I0825 11:12:23.398895  2068 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0825 11:12:23.399530  2068 net.cpp:172] Setting up ReLU42
I0825 11:12:23.399583  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.399617  2068 net.cpp:194] Memory required for data: 107110520
I0825 11:12:23.399649  2068 layer_factory.hpp:77] Creating layer Convolution45
I0825 11:12:23.399691  2068 net.cpp:128] Creating Layer Convolution45
I0825 11:12:23.399724  2068 net.cpp:558] Convolution45 <- Convolution44
I0825 11:12:23.399761  2068 net.cpp:522] Convolution45 -> Convolution45
I0825 11:12:23.406505  2068 net.cpp:172] Setting up Convolution45
I0825 11:12:23.406569  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.406601  2068 net.cpp:194] Memory required for data: 107274360
I0825 11:12:23.406641  2068 layer_factory.hpp:77] Creating layer BatchNorm45
I0825 11:12:23.406678  2068 net.cpp:128] Creating Layer BatchNorm45
I0825 11:12:23.406711  2068 net.cpp:558] BatchNorm45 <- Convolution45
I0825 11:12:23.406745  2068 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0825 11:12:23.407022  2068 net.cpp:172] Setting up BatchNorm45
I0825 11:12:23.407061  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.407099  2068 net.cpp:194] Memory required for data: 107438200
I0825 11:12:23.407138  2068 layer_factory.hpp:77] Creating layer Scale45
I0825 11:12:23.407173  2068 net.cpp:128] Creating Layer Scale45
I0825 11:12:23.407204  2068 net.cpp:558] Scale45 <- Convolution45
I0825 11:12:23.407238  2068 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0825 11:12:23.407323  2068 layer_factory.hpp:77] Creating layer Scale45
I0825 11:12:23.407500  2068 net.cpp:172] Setting up Scale45
I0825 11:12:23.407537  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.407567  2068 net.cpp:194] Memory required for data: 107602040
I0825 11:12:23.407604  2068 layer_factory.hpp:77] Creating layer Eltwise21
I0825 11:12:23.407650  2068 net.cpp:128] Creating Layer Eltwise21
I0825 11:12:23.407683  2068 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0825 11:12:23.407716  2068 net.cpp:558] Eltwise21 <- Convolution45
I0825 11:12:23.407754  2068 net.cpp:522] Eltwise21 -> Eltwise21
I0825 11:12:23.407815  2068 net.cpp:172] Setting up Eltwise21
I0825 11:12:23.407853  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.407883  2068 net.cpp:194] Memory required for data: 107765880
I0825 11:12:23.407915  2068 layer_factory.hpp:77] Creating layer ReLU43
I0825 11:12:23.407949  2068 net.cpp:128] Creating Layer ReLU43
I0825 11:12:23.407980  2068 net.cpp:558] ReLU43 <- Eltwise21
I0825 11:12:23.408013  2068 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0825 11:12:23.408628  2068 net.cpp:172] Setting up ReLU43
I0825 11:12:23.408692  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.408725  2068 net.cpp:194] Memory required for data: 107929720
I0825 11:12:23.408757  2068 layer_factory.hpp:77] Creating layer Eltwise21_ReLU43_0_split
I0825 11:12:23.408795  2068 net.cpp:128] Creating Layer Eltwise21_ReLU43_0_split
I0825 11:12:23.408828  2068 net.cpp:558] Eltwise21_ReLU43_0_split <- Eltwise21
I0825 11:12:23.408864  2068 net.cpp:522] Eltwise21_ReLU43_0_split -> Eltwise21_ReLU43_0_split_0
I0825 11:12:23.408905  2068 net.cpp:522] Eltwise21_ReLU43_0_split -> Eltwise21_ReLU43_0_split_1
I0825 11:12:23.408989  2068 net.cpp:172] Setting up Eltwise21_ReLU43_0_split
I0825 11:12:23.409025  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.409060  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.409091  2068 net.cpp:194] Memory required for data: 108257400
I0825 11:12:23.409122  2068 layer_factory.hpp:77] Creating layer Convolution46
I0825 11:12:23.409163  2068 net.cpp:128] Creating Layer Convolution46
I0825 11:12:23.409195  2068 net.cpp:558] Convolution46 <- Eltwise21_ReLU43_0_split_0
I0825 11:12:23.409235  2068 net.cpp:522] Convolution46 -> Convolution46
I0825 11:12:23.420122  2068 net.cpp:172] Setting up Convolution46
I0825 11:12:23.420189  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.420222  2068 net.cpp:194] Memory required for data: 108421240
I0825 11:12:23.420260  2068 layer_factory.hpp:77] Creating layer BatchNorm46
I0825 11:12:23.420300  2068 net.cpp:128] Creating Layer BatchNorm46
I0825 11:12:23.420332  2068 net.cpp:558] BatchNorm46 <- Convolution46
I0825 11:12:23.420369  2068 net.cpp:509] BatchNorm46 -> Convolution46 (in-place)
I0825 11:12:23.420656  2068 net.cpp:172] Setting up BatchNorm46
I0825 11:12:23.420696  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.420727  2068 net.cpp:194] Memory required for data: 108585080
I0825 11:12:23.420765  2068 layer_factory.hpp:77] Creating layer Scale46
I0825 11:12:23.420800  2068 net.cpp:128] Creating Layer Scale46
I0825 11:12:23.420831  2068 net.cpp:558] Scale46 <- Convolution46
I0825 11:12:23.420864  2068 net.cpp:509] Scale46 -> Convolution46 (in-place)
I0825 11:12:23.420948  2068 layer_factory.hpp:77] Creating layer Scale46
I0825 11:12:23.421125  2068 net.cpp:172] Setting up Scale46
I0825 11:12:23.421162  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.421192  2068 net.cpp:194] Memory required for data: 108748920
I0825 11:12:23.421229  2068 layer_factory.hpp:77] Creating layer ReLU44
I0825 11:12:23.421273  2068 net.cpp:128] Creating Layer ReLU44
I0825 11:12:23.421305  2068 net.cpp:558] ReLU44 <- Convolution46
I0825 11:12:23.421339  2068 net.cpp:509] ReLU44 -> Convolution46 (in-place)
I0825 11:12:23.424057  2068 net.cpp:172] Setting up ReLU44
I0825 11:12:23.424110  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.424142  2068 net.cpp:194] Memory required for data: 108912760
I0825 11:12:23.424175  2068 layer_factory.hpp:77] Creating layer Convolution47
I0825 11:12:23.424217  2068 net.cpp:128] Creating Layer Convolution47
I0825 11:12:23.424249  2068 net.cpp:558] Convolution47 <- Convolution46
I0825 11:12:23.424288  2068 net.cpp:522] Convolution47 -> Convolution47
I0825 11:12:23.437350  2068 net.cpp:172] Setting up Convolution47
I0825 11:12:23.437417  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.437449  2068 net.cpp:194] Memory required for data: 109076600
I0825 11:12:23.437489  2068 layer_factory.hpp:77] Creating layer BatchNorm47
I0825 11:12:23.437527  2068 net.cpp:128] Creating Layer BatchNorm47
I0825 11:12:23.437559  2068 net.cpp:558] BatchNorm47 <- Convolution47
I0825 11:12:23.437597  2068 net.cpp:509] BatchNorm47 -> Convolution47 (in-place)
I0825 11:12:23.437889  2068 net.cpp:172] Setting up BatchNorm47
I0825 11:12:23.437929  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.437959  2068 net.cpp:194] Memory required for data: 109240440
I0825 11:12:23.437999  2068 layer_factory.hpp:77] Creating layer Scale47
I0825 11:12:23.438033  2068 net.cpp:128] Creating Layer Scale47
I0825 11:12:23.438064  2068 net.cpp:558] Scale47 <- Convolution47
I0825 11:12:23.438098  2068 net.cpp:509] Scale47 -> Convolution47 (in-place)
I0825 11:12:23.438182  2068 layer_factory.hpp:77] Creating layer Scale47
I0825 11:12:23.438371  2068 net.cpp:172] Setting up Scale47
I0825 11:12:23.438411  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.438442  2068 net.cpp:194] Memory required for data: 109404280
I0825 11:12:23.438479  2068 layer_factory.hpp:77] Creating layer Eltwise22
I0825 11:12:23.438519  2068 net.cpp:128] Creating Layer Eltwise22
I0825 11:12:23.438551  2068 net.cpp:558] Eltwise22 <- Eltwise21_ReLU43_0_split_1
I0825 11:12:23.438585  2068 net.cpp:558] Eltwise22 <- Convolution47
I0825 11:12:23.438621  2068 net.cpp:522] Eltwise22 -> Eltwise22
I0825 11:12:23.438684  2068 net.cpp:172] Setting up Eltwise22
I0825 11:12:23.438719  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.438750  2068 net.cpp:194] Memory required for data: 109568120
I0825 11:12:23.438781  2068 layer_factory.hpp:77] Creating layer ReLU45
I0825 11:12:23.438817  2068 net.cpp:128] Creating Layer ReLU45
I0825 11:12:23.438848  2068 net.cpp:558] ReLU45 <- Eltwise22
I0825 11:12:23.438879  2068 net.cpp:509] ReLU45 -> Eltwise22 (in-place)
I0825 11:12:23.441609  2068 net.cpp:172] Setting up ReLU45
I0825 11:12:23.441663  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.441694  2068 net.cpp:194] Memory required for data: 109731960
I0825 11:12:23.441726  2068 layer_factory.hpp:77] Creating layer Eltwise22_ReLU45_0_split
I0825 11:12:23.441763  2068 net.cpp:128] Creating Layer Eltwise22_ReLU45_0_split
I0825 11:12:23.441797  2068 net.cpp:558] Eltwise22_ReLU45_0_split <- Eltwise22
I0825 11:12:23.441834  2068 net.cpp:522] Eltwise22_ReLU45_0_split -> Eltwise22_ReLU45_0_split_0
I0825 11:12:23.441874  2068 net.cpp:522] Eltwise22_ReLU45_0_split -> Eltwise22_ReLU45_0_split_1
I0825 11:12:23.441960  2068 net.cpp:172] Setting up Eltwise22_ReLU45_0_split
I0825 11:12:23.441998  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.442030  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.442061  2068 net.cpp:194] Memory required for data: 110059640
I0825 11:12:23.442093  2068 layer_factory.hpp:77] Creating layer Convolution48
I0825 11:12:23.442134  2068 net.cpp:128] Creating Layer Convolution48
I0825 11:12:23.442167  2068 net.cpp:558] Convolution48 <- Eltwise22_ReLU45_0_split_0
I0825 11:12:23.442204  2068 net.cpp:522] Convolution48 -> Convolution48
I0825 11:12:23.448997  2068 net.cpp:172] Setting up Convolution48
I0825 11:12:23.449072  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.449105  2068 net.cpp:194] Memory required for data: 110223480
I0825 11:12:23.449146  2068 layer_factory.hpp:77] Creating layer BatchNorm48
I0825 11:12:23.449182  2068 net.cpp:128] Creating Layer BatchNorm48
I0825 11:12:23.449215  2068 net.cpp:558] BatchNorm48 <- Convolution48
I0825 11:12:23.449250  2068 net.cpp:509] BatchNorm48 -> Convolution48 (in-place)
I0825 11:12:23.449542  2068 net.cpp:172] Setting up BatchNorm48
I0825 11:12:23.449581  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.449612  2068 net.cpp:194] Memory required for data: 110387320
I0825 11:12:23.449661  2068 layer_factory.hpp:77] Creating layer Scale48
I0825 11:12:23.449697  2068 net.cpp:128] Creating Layer Scale48
I0825 11:12:23.449728  2068 net.cpp:558] Scale48 <- Convolution48
I0825 11:12:23.449761  2068 net.cpp:509] Scale48 -> Convolution48 (in-place)
I0825 11:12:23.449849  2068 layer_factory.hpp:77] Creating layer Scale48
I0825 11:12:23.450027  2068 net.cpp:172] Setting up Scale48
I0825 11:12:23.450064  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.450096  2068 net.cpp:194] Memory required for data: 110551160
I0825 11:12:23.450134  2068 layer_factory.hpp:77] Creating layer ReLU46
I0825 11:12:23.450167  2068 net.cpp:128] Creating Layer ReLU46
I0825 11:12:23.450198  2068 net.cpp:558] ReLU46 <- Convolution48
I0825 11:12:23.450233  2068 net.cpp:509] ReLU46 -> Convolution48 (in-place)
I0825 11:12:23.450877  2068 net.cpp:172] Setting up ReLU46
I0825 11:12:23.450947  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.450979  2068 net.cpp:194] Memory required for data: 110715000
I0825 11:12:23.451012  2068 layer_factory.hpp:77] Creating layer Convolution49
I0825 11:12:23.451056  2068 net.cpp:128] Creating Layer Convolution49
I0825 11:12:23.451088  2068 net.cpp:558] Convolution49 <- Convolution48
I0825 11:12:23.451126  2068 net.cpp:522] Convolution49 -> Convolution49
I0825 11:12:23.457861  2068 net.cpp:172] Setting up Convolution49
I0825 11:12:23.457928  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.457960  2068 net.cpp:194] Memory required for data: 110878840
I0825 11:12:23.458000  2068 layer_factory.hpp:77] Creating layer BatchNorm49
I0825 11:12:23.458036  2068 net.cpp:128] Creating Layer BatchNorm49
I0825 11:12:23.458068  2068 net.cpp:558] BatchNorm49 <- Convolution49
I0825 11:12:23.458103  2068 net.cpp:509] BatchNorm49 -> Convolution49 (in-place)
I0825 11:12:23.458389  2068 net.cpp:172] Setting up BatchNorm49
I0825 11:12:23.458429  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.458461  2068 net.cpp:194] Memory required for data: 111042680
I0825 11:12:23.458500  2068 layer_factory.hpp:77] Creating layer Scale49
I0825 11:12:23.458536  2068 net.cpp:128] Creating Layer Scale49
I0825 11:12:23.458567  2068 net.cpp:558] Scale49 <- Convolution49
I0825 11:12:23.458601  2068 net.cpp:509] Scale49 -> Convolution49 (in-place)
I0825 11:12:23.458683  2068 layer_factory.hpp:77] Creating layer Scale49
I0825 11:12:23.458860  2068 net.cpp:172] Setting up Scale49
I0825 11:12:23.458900  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.458932  2068 net.cpp:194] Memory required for data: 111206520
I0825 11:12:23.458969  2068 layer_factory.hpp:77] Creating layer Eltwise23
I0825 11:12:23.459005  2068 net.cpp:128] Creating Layer Eltwise23
I0825 11:12:23.459038  2068 net.cpp:558] Eltwise23 <- Eltwise22_ReLU45_0_split_1
I0825 11:12:23.459070  2068 net.cpp:558] Eltwise23 <- Convolution49
I0825 11:12:23.459105  2068 net.cpp:522] Eltwise23 -> Eltwise23
I0825 11:12:23.459167  2068 net.cpp:172] Setting up Eltwise23
I0825 11:12:23.459203  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.459233  2068 net.cpp:194] Memory required for data: 111370360
I0825 11:12:23.459265  2068 layer_factory.hpp:77] Creating layer ReLU47
I0825 11:12:23.459300  2068 net.cpp:128] Creating Layer ReLU47
I0825 11:12:23.459337  2068 net.cpp:558] ReLU47 <- Eltwise23
I0825 11:12:23.459370  2068 net.cpp:509] ReLU47 -> Eltwise23 (in-place)
I0825 11:12:23.462131  2068 net.cpp:172] Setting up ReLU47
I0825 11:12:23.462184  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.462216  2068 net.cpp:194] Memory required for data: 111534200
I0825 11:12:23.462249  2068 layer_factory.hpp:77] Creating layer Eltwise23_ReLU47_0_split
I0825 11:12:23.462288  2068 net.cpp:128] Creating Layer Eltwise23_ReLU47_0_split
I0825 11:12:23.462321  2068 net.cpp:558] Eltwise23_ReLU47_0_split <- Eltwise23
I0825 11:12:23.462366  2068 net.cpp:522] Eltwise23_ReLU47_0_split -> Eltwise23_ReLU47_0_split_0
I0825 11:12:23.462405  2068 net.cpp:522] Eltwise23_ReLU47_0_split -> Eltwise23_ReLU47_0_split_1
I0825 11:12:23.462505  2068 net.cpp:172] Setting up Eltwise23_ReLU47_0_split
I0825 11:12:23.462543  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.462576  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.462607  2068 net.cpp:194] Memory required for data: 111861880
I0825 11:12:23.462640  2068 layer_factory.hpp:77] Creating layer Convolution50
I0825 11:12:23.462680  2068 net.cpp:128] Creating Layer Convolution50
I0825 11:12:23.462713  2068 net.cpp:558] Convolution50 <- Eltwise23_ReLU47_0_split_0
I0825 11:12:23.462750  2068 net.cpp:522] Convolution50 -> Convolution50
I0825 11:12:23.473732  2068 net.cpp:172] Setting up Convolution50
I0825 11:12:23.473798  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.473830  2068 net.cpp:194] Memory required for data: 112025720
I0825 11:12:23.473870  2068 layer_factory.hpp:77] Creating layer BatchNorm50
I0825 11:12:23.473908  2068 net.cpp:128] Creating Layer BatchNorm50
I0825 11:12:23.473942  2068 net.cpp:558] BatchNorm50 <- Convolution50
I0825 11:12:23.473981  2068 net.cpp:509] BatchNorm50 -> Convolution50 (in-place)
I0825 11:12:23.474275  2068 net.cpp:172] Setting up BatchNorm50
I0825 11:12:23.474314  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.474352  2068 net.cpp:194] Memory required for data: 112189560
I0825 11:12:23.474392  2068 layer_factory.hpp:77] Creating layer Scale50
I0825 11:12:23.474427  2068 net.cpp:128] Creating Layer Scale50
I0825 11:12:23.474459  2068 net.cpp:558] Scale50 <- Convolution50
I0825 11:12:23.474494  2068 net.cpp:509] Scale50 -> Convolution50 (in-place)
I0825 11:12:23.474577  2068 layer_factory.hpp:77] Creating layer Scale50
I0825 11:12:23.474758  2068 net.cpp:172] Setting up Scale50
I0825 11:12:23.474795  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.474828  2068 net.cpp:194] Memory required for data: 112353400
I0825 11:12:23.474864  2068 layer_factory.hpp:77] Creating layer ReLU48
I0825 11:12:23.474898  2068 net.cpp:128] Creating Layer ReLU48
I0825 11:12:23.474930  2068 net.cpp:558] ReLU48 <- Convolution50
I0825 11:12:23.474966  2068 net.cpp:509] ReLU48 -> Convolution50 (in-place)
I0825 11:12:23.475584  2068 net.cpp:172] Setting up ReLU48
I0825 11:12:23.475637  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.475669  2068 net.cpp:194] Memory required for data: 112517240
I0825 11:12:23.475702  2068 layer_factory.hpp:77] Creating layer Convolution51
I0825 11:12:23.475744  2068 net.cpp:128] Creating Layer Convolution51
I0825 11:12:23.475776  2068 net.cpp:558] Convolution51 <- Convolution50
I0825 11:12:23.475813  2068 net.cpp:522] Convolution51 -> Convolution51
I0825 11:12:23.480471  2068 net.cpp:172] Setting up Convolution51
I0825 11:12:23.480536  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.480567  2068 net.cpp:194] Memory required for data: 112681080
I0825 11:12:23.480605  2068 layer_factory.hpp:77] Creating layer BatchNorm51
I0825 11:12:23.480643  2068 net.cpp:128] Creating Layer BatchNorm51
I0825 11:12:23.480674  2068 net.cpp:558] BatchNorm51 <- Convolution51
I0825 11:12:23.480708  2068 net.cpp:509] BatchNorm51 -> Convolution51 (in-place)
I0825 11:12:23.480996  2068 net.cpp:172] Setting up BatchNorm51
I0825 11:12:23.481035  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.481066  2068 net.cpp:194] Memory required for data: 112844920
I0825 11:12:23.481106  2068 layer_factory.hpp:77] Creating layer Scale51
I0825 11:12:23.481153  2068 net.cpp:128] Creating Layer Scale51
I0825 11:12:23.481184  2068 net.cpp:558] Scale51 <- Convolution51
I0825 11:12:23.481217  2068 net.cpp:509] Scale51 -> Convolution51 (in-place)
I0825 11:12:23.481302  2068 layer_factory.hpp:77] Creating layer Scale51
I0825 11:12:23.481484  2068 net.cpp:172] Setting up Scale51
I0825 11:12:23.481521  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.481552  2068 net.cpp:194] Memory required for data: 113008760
I0825 11:12:23.481588  2068 layer_factory.hpp:77] Creating layer Eltwise24
I0825 11:12:23.481626  2068 net.cpp:128] Creating Layer Eltwise24
I0825 11:12:23.481659  2068 net.cpp:558] Eltwise24 <- Eltwise23_ReLU47_0_split_1
I0825 11:12:23.481703  2068 net.cpp:558] Eltwise24 <- Convolution51
I0825 11:12:23.481740  2068 net.cpp:522] Eltwise24 -> Eltwise24
I0825 11:12:23.481801  2068 net.cpp:172] Setting up Eltwise24
I0825 11:12:23.481837  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.481868  2068 net.cpp:194] Memory required for data: 113172600
I0825 11:12:23.481899  2068 layer_factory.hpp:77] Creating layer ReLU49
I0825 11:12:23.481935  2068 net.cpp:128] Creating Layer ReLU49
I0825 11:12:23.481967  2068 net.cpp:558] ReLU49 <- Eltwise24
I0825 11:12:23.482000  2068 net.cpp:509] ReLU49 -> Eltwise24 (in-place)
I0825 11:12:23.482617  2068 net.cpp:172] Setting up ReLU49
I0825 11:12:23.482683  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.482714  2068 net.cpp:194] Memory required for data: 113336440
I0825 11:12:23.482748  2068 layer_factory.hpp:77] Creating layer Eltwise24_ReLU49_0_split
I0825 11:12:23.482787  2068 net.cpp:128] Creating Layer Eltwise24_ReLU49_0_split
I0825 11:12:23.482820  2068 net.cpp:558] Eltwise24_ReLU49_0_split <- Eltwise24
I0825 11:12:23.482856  2068 net.cpp:522] Eltwise24_ReLU49_0_split -> Eltwise24_ReLU49_0_split_0
I0825 11:12:23.482897  2068 net.cpp:522] Eltwise24_ReLU49_0_split -> Eltwise24_ReLU49_0_split_1
I0825 11:12:23.482995  2068 net.cpp:172] Setting up Eltwise24_ReLU49_0_split
I0825 11:12:23.483031  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.483067  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.483098  2068 net.cpp:194] Memory required for data: 113664120
I0825 11:12:23.483129  2068 layer_factory.hpp:77] Creating layer Convolution52
I0825 11:12:23.483167  2068 net.cpp:128] Creating Layer Convolution52
I0825 11:12:23.483199  2068 net.cpp:558] Convolution52 <- Eltwise24_ReLU49_0_split_0
I0825 11:12:23.483238  2068 net.cpp:522] Convolution52 -> Convolution52
I0825 11:12:23.492899  2068 net.cpp:172] Setting up Convolution52
I0825 11:12:23.494979  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.495021  2068 net.cpp:194] Memory required for data: 113827960
I0825 11:12:23.495067  2068 layer_factory.hpp:77] Creating layer BatchNorm52
I0825 11:12:23.495108  2068 net.cpp:128] Creating Layer BatchNorm52
I0825 11:12:23.495146  2068 net.cpp:558] BatchNorm52 <- Convolution52
I0825 11:12:23.495199  2068 net.cpp:509] BatchNorm52 -> Convolution52 (in-place)
I0825 11:12:23.495514  2068 net.cpp:172] Setting up BatchNorm52
I0825 11:12:23.495555  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.495587  2068 net.cpp:194] Memory required for data: 113991800
I0825 11:12:23.495627  2068 layer_factory.hpp:77] Creating layer Scale52
I0825 11:12:23.495664  2068 net.cpp:128] Creating Layer Scale52
I0825 11:12:23.495697  2068 net.cpp:558] Scale52 <- Convolution52
I0825 11:12:23.495730  2068 net.cpp:509] Scale52 -> Convolution52 (in-place)
I0825 11:12:23.495826  2068 layer_factory.hpp:77] Creating layer Scale52
I0825 11:12:23.496007  2068 net.cpp:172] Setting up Scale52
I0825 11:12:23.496048  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.496079  2068 net.cpp:194] Memory required for data: 114155640
I0825 11:12:23.496116  2068 layer_factory.hpp:77] Creating layer ReLU50
I0825 11:12:23.496163  2068 net.cpp:128] Creating Layer ReLU50
I0825 11:12:23.496196  2068 net.cpp:558] ReLU50 <- Convolution52
I0825 11:12:23.496229  2068 net.cpp:509] ReLU50 -> Convolution52 (in-place)
I0825 11:12:23.496799  2068 net.cpp:172] Setting up ReLU50
I0825 11:12:23.496855  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.496887  2068 net.cpp:194] Memory required for data: 114319480
I0825 11:12:23.496922  2068 layer_factory.hpp:77] Creating layer Convolution53
I0825 11:12:23.496965  2068 net.cpp:128] Creating Layer Convolution53
I0825 11:12:23.496999  2068 net.cpp:558] Convolution53 <- Convolution52
I0825 11:12:23.497035  2068 net.cpp:522] Convolution53 -> Convolution53
I0825 11:12:23.506008  2068 net.cpp:172] Setting up Convolution53
I0825 11:12:23.506083  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.506129  2068 net.cpp:194] Memory required for data: 114483320
I0825 11:12:23.506170  2068 layer_factory.hpp:77] Creating layer BatchNorm53
I0825 11:12:23.506206  2068 net.cpp:128] Creating Layer BatchNorm53
I0825 11:12:23.506238  2068 net.cpp:558] BatchNorm53 <- Convolution53
I0825 11:12:23.506276  2068 net.cpp:509] BatchNorm53 -> Convolution53 (in-place)
I0825 11:12:23.506589  2068 net.cpp:172] Setting up BatchNorm53
I0825 11:12:23.506634  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.506665  2068 net.cpp:194] Memory required for data: 114647160
I0825 11:12:23.506705  2068 layer_factory.hpp:77] Creating layer Scale53
I0825 11:12:23.506741  2068 net.cpp:128] Creating Layer Scale53
I0825 11:12:23.506772  2068 net.cpp:558] Scale53 <- Convolution53
I0825 11:12:23.506811  2068 net.cpp:509] Scale53 -> Convolution53 (in-place)
I0825 11:12:23.506896  2068 layer_factory.hpp:77] Creating layer Scale53
I0825 11:12:23.507081  2068 net.cpp:172] Setting up Scale53
I0825 11:12:23.507119  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.507150  2068 net.cpp:194] Memory required for data: 114811000
I0825 11:12:23.507189  2068 layer_factory.hpp:77] Creating layer Eltwise25
I0825 11:12:23.507228  2068 net.cpp:128] Creating Layer Eltwise25
I0825 11:12:23.507262  2068 net.cpp:558] Eltwise25 <- Eltwise24_ReLU49_0_split_1
I0825 11:12:23.507297  2068 net.cpp:558] Eltwise25 <- Convolution53
I0825 11:12:23.507331  2068 net.cpp:522] Eltwise25 -> Eltwise25
I0825 11:12:23.507395  2068 net.cpp:172] Setting up Eltwise25
I0825 11:12:23.507431  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.507462  2068 net.cpp:194] Memory required for data: 114974840
I0825 11:12:23.507493  2068 layer_factory.hpp:77] Creating layer ReLU51
I0825 11:12:23.507529  2068 net.cpp:128] Creating Layer ReLU51
I0825 11:12:23.507560  2068 net.cpp:558] ReLU51 <- Eltwise25
I0825 11:12:23.507594  2068 net.cpp:509] ReLU51 -> Eltwise25 (in-place)
I0825 11:12:23.508152  2068 net.cpp:172] Setting up ReLU51
I0825 11:12:23.508203  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.508235  2068 net.cpp:194] Memory required for data: 115138680
I0825 11:12:23.508268  2068 layer_factory.hpp:77] Creating layer Eltwise25_ReLU51_0_split
I0825 11:12:23.508306  2068 net.cpp:128] Creating Layer Eltwise25_ReLU51_0_split
I0825 11:12:23.508338  2068 net.cpp:558] Eltwise25_ReLU51_0_split <- Eltwise25
I0825 11:12:23.508376  2068 net.cpp:522] Eltwise25_ReLU51_0_split -> Eltwise25_ReLU51_0_split_0
I0825 11:12:23.508415  2068 net.cpp:522] Eltwise25_ReLU51_0_split -> Eltwise25_ReLU51_0_split_1
I0825 11:12:23.508507  2068 net.cpp:172] Setting up Eltwise25_ReLU51_0_split
I0825 11:12:23.508544  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.508577  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.508608  2068 net.cpp:194] Memory required for data: 115466360
I0825 11:12:23.508641  2068 layer_factory.hpp:77] Creating layer Convolution54
I0825 11:12:23.508682  2068 net.cpp:128] Creating Layer Convolution54
I0825 11:12:23.508714  2068 net.cpp:558] Convolution54 <- Eltwise25_ReLU51_0_split_0
I0825 11:12:23.508754  2068 net.cpp:522] Convolution54 -> Convolution54
I0825 11:12:23.518203  2068 net.cpp:172] Setting up Convolution54
I0825 11:12:23.518285  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.518318  2068 net.cpp:194] Memory required for data: 115630200
I0825 11:12:23.518384  2068 layer_factory.hpp:77] Creating layer BatchNorm54
I0825 11:12:23.518426  2068 net.cpp:128] Creating Layer BatchNorm54
I0825 11:12:23.518460  2068 net.cpp:558] BatchNorm54 <- Convolution54
I0825 11:12:23.518497  2068 net.cpp:509] BatchNorm54 -> Convolution54 (in-place)
I0825 11:12:23.518808  2068 net.cpp:172] Setting up BatchNorm54
I0825 11:12:23.518849  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.518880  2068 net.cpp:194] Memory required for data: 115794040
I0825 11:12:23.518925  2068 layer_factory.hpp:77] Creating layer Scale54
I0825 11:12:23.518970  2068 net.cpp:128] Creating Layer Scale54
I0825 11:12:23.519001  2068 net.cpp:558] Scale54 <- Convolution54
I0825 11:12:23.519047  2068 net.cpp:509] Scale54 -> Convolution54 (in-place)
I0825 11:12:23.519141  2068 layer_factory.hpp:77] Creating layer Scale54
I0825 11:12:23.519328  2068 net.cpp:172] Setting up Scale54
I0825 11:12:23.519366  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.519397  2068 net.cpp:194] Memory required for data: 115957880
I0825 11:12:23.519435  2068 layer_factory.hpp:77] Creating layer ReLU52
I0825 11:12:23.519472  2068 net.cpp:128] Creating Layer ReLU52
I0825 11:12:23.519505  2068 net.cpp:558] ReLU52 <- Convolution54
I0825 11:12:23.519539  2068 net.cpp:509] ReLU52 -> Convolution54 (in-place)
I0825 11:12:23.519876  2068 net.cpp:172] Setting up ReLU52
I0825 11:12:23.519932  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.519963  2068 net.cpp:194] Memory required for data: 116121720
I0825 11:12:23.519997  2068 layer_factory.hpp:77] Creating layer Convolution55
I0825 11:12:23.520041  2068 net.cpp:128] Creating Layer Convolution55
I0825 11:12:23.520076  2068 net.cpp:558] Convolution55 <- Convolution54
I0825 11:12:23.520113  2068 net.cpp:522] Convolution55 -> Convolution55
I0825 11:12:23.535971  2068 net.cpp:172] Setting up Convolution55
I0825 11:12:23.536048  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.536082  2068 net.cpp:194] Memory required for data: 116285560
I0825 11:12:23.536123  2068 layer_factory.hpp:77] Creating layer BatchNorm55
I0825 11:12:23.536164  2068 net.cpp:128] Creating Layer BatchNorm55
I0825 11:12:23.536195  2068 net.cpp:558] BatchNorm55 <- Convolution55
I0825 11:12:23.536232  2068 net.cpp:509] BatchNorm55 -> Convolution55 (in-place)
I0825 11:12:23.536532  2068 net.cpp:172] Setting up BatchNorm55
I0825 11:12:23.536571  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.536603  2068 net.cpp:194] Memory required for data: 116449400
I0825 11:12:23.536643  2068 layer_factory.hpp:77] Creating layer Scale55
I0825 11:12:23.536679  2068 net.cpp:128] Creating Layer Scale55
I0825 11:12:23.536710  2068 net.cpp:558] Scale55 <- Convolution55
I0825 11:12:23.536743  2068 net.cpp:509] Scale55 -> Convolution55 (in-place)
I0825 11:12:23.536834  2068 layer_factory.hpp:77] Creating layer Scale55
I0825 11:12:23.537022  2068 net.cpp:172] Setting up Scale55
I0825 11:12:23.537060  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.537091  2068 net.cpp:194] Memory required for data: 116613240
I0825 11:12:23.537128  2068 layer_factory.hpp:77] Creating layer Eltwise26
I0825 11:12:23.537168  2068 net.cpp:128] Creating Layer Eltwise26
I0825 11:12:23.537201  2068 net.cpp:558] Eltwise26 <- Eltwise25_ReLU51_0_split_1
I0825 11:12:23.537235  2068 net.cpp:558] Eltwise26 <- Convolution55
I0825 11:12:23.537271  2068 net.cpp:522] Eltwise26 -> Eltwise26
I0825 11:12:23.537334  2068 net.cpp:172] Setting up Eltwise26
I0825 11:12:23.537369  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.537400  2068 net.cpp:194] Memory required for data: 116777080
I0825 11:12:23.537432  2068 layer_factory.hpp:77] Creating layer ReLU53
I0825 11:12:23.537467  2068 net.cpp:128] Creating Layer ReLU53
I0825 11:12:23.537499  2068 net.cpp:558] ReLU53 <- Eltwise26
I0825 11:12:23.537533  2068 net.cpp:509] ReLU53 -> Eltwise26 (in-place)
I0825 11:12:23.540496  2068 net.cpp:172] Setting up ReLU53
I0825 11:12:23.540561  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.540604  2068 net.cpp:194] Memory required for data: 116940920
I0825 11:12:23.540637  2068 layer_factory.hpp:77] Creating layer Eltwise26_ReLU53_0_split
I0825 11:12:23.540676  2068 net.cpp:128] Creating Layer Eltwise26_ReLU53_0_split
I0825 11:12:23.540709  2068 net.cpp:558] Eltwise26_ReLU53_0_split <- Eltwise26
I0825 11:12:23.540745  2068 net.cpp:522] Eltwise26_ReLU53_0_split -> Eltwise26_ReLU53_0_split_0
I0825 11:12:23.540784  2068 net.cpp:522] Eltwise26_ReLU53_0_split -> Eltwise26_ReLU53_0_split_1
I0825 11:12:23.540876  2068 net.cpp:172] Setting up Eltwise26_ReLU53_0_split
I0825 11:12:23.540913  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.540957  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.540988  2068 net.cpp:194] Memory required for data: 117268600
I0825 11:12:23.541021  2068 layer_factory.hpp:77] Creating layer Convolution56
I0825 11:12:23.541062  2068 net.cpp:128] Creating Layer Convolution56
I0825 11:12:23.541095  2068 net.cpp:558] Convolution56 <- Eltwise26_ReLU53_0_split_0
I0825 11:12:23.541134  2068 net.cpp:522] Convolution56 -> Convolution56
I0825 11:12:23.551589  2068 net.cpp:172] Setting up Convolution56
I0825 11:12:23.551661  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.551695  2068 net.cpp:194] Memory required for data: 117432440
I0825 11:12:23.551738  2068 layer_factory.hpp:77] Creating layer BatchNorm56
I0825 11:12:23.551777  2068 net.cpp:128] Creating Layer BatchNorm56
I0825 11:12:23.551811  2068 net.cpp:558] BatchNorm56 <- Convolution56
I0825 11:12:23.551848  2068 net.cpp:509] BatchNorm56 -> Convolution56 (in-place)
I0825 11:12:23.552155  2068 net.cpp:172] Setting up BatchNorm56
I0825 11:12:23.552194  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.552225  2068 net.cpp:194] Memory required for data: 117596280
I0825 11:12:23.552264  2068 layer_factory.hpp:77] Creating layer Scale56
I0825 11:12:23.552300  2068 net.cpp:128] Creating Layer Scale56
I0825 11:12:23.552331  2068 net.cpp:558] Scale56 <- Convolution56
I0825 11:12:23.552367  2068 net.cpp:509] Scale56 -> Convolution56 (in-place)
I0825 11:12:23.552453  2068 layer_factory.hpp:77] Creating layer Scale56
I0825 11:12:23.552644  2068 net.cpp:172] Setting up Scale56
I0825 11:12:23.552682  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.552714  2068 net.cpp:194] Memory required for data: 117760120
I0825 11:12:23.552752  2068 layer_factory.hpp:77] Creating layer ReLU54
I0825 11:12:23.552785  2068 net.cpp:128] Creating Layer ReLU54
I0825 11:12:23.552817  2068 net.cpp:558] ReLU54 <- Convolution56
I0825 11:12:23.552853  2068 net.cpp:509] ReLU54 -> Convolution56 (in-place)
I0825 11:12:23.555510  2068 net.cpp:172] Setting up ReLU54
I0825 11:12:23.555567  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.555599  2068 net.cpp:194] Memory required for data: 117923960
I0825 11:12:23.555631  2068 layer_factory.hpp:77] Creating layer Convolution57
I0825 11:12:23.555675  2068 net.cpp:128] Creating Layer Convolution57
I0825 11:12:23.555708  2068 net.cpp:558] Convolution57 <- Convolution56
I0825 11:12:23.555747  2068 net.cpp:522] Convolution57 -> Convolution57
I0825 11:12:23.568737  2068 net.cpp:172] Setting up Convolution57
I0825 11:12:23.568783  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.568801  2068 net.cpp:194] Memory required for data: 118087800
I0825 11:12:23.568825  2068 layer_factory.hpp:77] Creating layer BatchNorm57
I0825 11:12:23.568850  2068 net.cpp:128] Creating Layer BatchNorm57
I0825 11:12:23.568871  2068 net.cpp:558] BatchNorm57 <- Convolution57
I0825 11:12:23.568891  2068 net.cpp:509] BatchNorm57 -> Convolution57 (in-place)
I0825 11:12:23.569175  2068 net.cpp:172] Setting up BatchNorm57
I0825 11:12:23.569196  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.569212  2068 net.cpp:194] Memory required for data: 118251640
I0825 11:12:23.569236  2068 layer_factory.hpp:77] Creating layer Scale57
I0825 11:12:23.569258  2068 net.cpp:128] Creating Layer Scale57
I0825 11:12:23.569275  2068 net.cpp:558] Scale57 <- Convolution57
I0825 11:12:23.569306  2068 net.cpp:509] Scale57 -> Convolution57 (in-place)
I0825 11:12:23.569377  2068 layer_factory.hpp:77] Creating layer Scale57
I0825 11:12:23.569550  2068 net.cpp:172] Setting up Scale57
I0825 11:12:23.569571  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.569587  2068 net.cpp:194] Memory required for data: 118415480
I0825 11:12:23.569608  2068 layer_factory.hpp:77] Creating layer Eltwise27
I0825 11:12:23.569633  2068 net.cpp:128] Creating Layer Eltwise27
I0825 11:12:23.569650  2068 net.cpp:558] Eltwise27 <- Eltwise26_ReLU53_0_split_1
I0825 11:12:23.569669  2068 net.cpp:558] Eltwise27 <- Convolution57
I0825 11:12:23.569692  2068 net.cpp:522] Eltwise27 -> Eltwise27
I0825 11:12:23.569751  2068 net.cpp:172] Setting up Eltwise27
I0825 11:12:23.569772  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.569787  2068 net.cpp:194] Memory required for data: 118579320
I0825 11:12:23.569804  2068 layer_factory.hpp:77] Creating layer ReLU55
I0825 11:12:23.569826  2068 net.cpp:128] Creating Layer ReLU55
I0825 11:12:23.569844  2068 net.cpp:558] ReLU55 <- Eltwise27
I0825 11:12:23.569861  2068 net.cpp:509] ReLU55 -> Eltwise27 (in-place)
I0825 11:12:23.575078  2068 net.cpp:172] Setting up ReLU55
I0825 11:12:23.575114  2068 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:12:23.575130  2068 net.cpp:194] Memory required for data: 118743160
I0825 11:12:23.575147  2068 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:12:23.575171  2068 net.cpp:128] Creating Layer Pooling1
I0825 11:12:23.575188  2068 net.cpp:558] Pooling1 <- Eltwise27
I0825 11:12:23.575211  2068 net.cpp:522] Pooling1 -> Pooling1
I0825 11:12:23.581596  2068 net.cpp:172] Setting up Pooling1
I0825 11:12:23.581638  2068 net.cpp:186] Top shape: 10 64 1 1 (640)
I0825 11:12:23.581656  2068 net.cpp:194] Memory required for data: 118745720
I0825 11:12:23.581673  2068 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:12:23.581698  2068 net.cpp:128] Creating Layer InnerProduct1
I0825 11:12:23.581717  2068 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:12:23.581738  2068 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:12:23.581946  2068 net.cpp:172] Setting up InnerProduct1
I0825 11:12:23.581974  2068 net.cpp:186] Top shape: 10 10 (100)
I0825 11:12:23.581991  2068 net.cpp:194] Memory required for data: 118746120
I0825 11:12:23.582015  2068 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0825 11:12:23.582034  2068 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0825 11:12:23.582051  2068 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0825 11:12:23.582072  2068 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0825 11:12:23.582096  2068 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0825 11:12:23.582161  2068 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0825 11:12:23.582181  2068 net.cpp:186] Top shape: 10 10 (100)
I0825 11:12:23.582199  2068 net.cpp:186] Top shape: 10 10 (100)
I0825 11:12:23.582214  2068 net.cpp:194] Memory required for data: 118746920
I0825 11:12:23.582232  2068 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:12:23.582254  2068 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:12:23.582271  2068 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0825 11:12:23.582289  2068 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0825 11:12:23.582311  2068 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:12:23.582341  2068 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:12:23.587595  2068 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:12:23.587653  2068 net.cpp:186] Top shape: (1)
I0825 11:12:23.587671  2068 net.cpp:189]     with loss weight 1
I0825 11:12:23.587702  2068 net.cpp:194] Memory required for data: 118746924
I0825 11:12:23.587720  2068 layer_factory.hpp:77] Creating layer Accuracy1
I0825 11:12:23.587744  2068 net.cpp:128] Creating Layer Accuracy1
I0825 11:12:23.587775  2068 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0825 11:12:23.587795  2068 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0825 11:12:23.587821  2068 net.cpp:522] Accuracy1 -> Accuracy1
I0825 11:12:23.587847  2068 net.cpp:172] Setting up Accuracy1
I0825 11:12:23.587867  2068 net.cpp:186] Top shape: (1)
I0825 11:12:23.587882  2068 net.cpp:194] Memory required for data: 118746928
I0825 11:12:23.587899  2068 net.cpp:303] Accuracy1 does not need backward computation.
I0825 11:12:23.587918  2068 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:12:23.587936  2068 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0825 11:12:23.587963  2068 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:12:23.587980  2068 net.cpp:301] Pooling1 needs backward computation.
I0825 11:12:23.587997  2068 net.cpp:301] ReLU55 needs backward computation.
I0825 11:12:23.588014  2068 net.cpp:301] Eltwise27 needs backward computation.
I0825 11:12:23.588032  2068 net.cpp:301] Scale57 needs backward computation.
I0825 11:12:23.588049  2068 net.cpp:301] BatchNorm57 needs backward computation.
I0825 11:12:23.588065  2068 net.cpp:301] Convolution57 needs backward computation.
I0825 11:12:23.588083  2068 net.cpp:301] ReLU54 needs backward computation.
I0825 11:12:23.588099  2068 net.cpp:301] Scale56 needs backward computation.
I0825 11:12:23.588116  2068 net.cpp:301] BatchNorm56 needs backward computation.
I0825 11:12:23.588133  2068 net.cpp:301] Convolution56 needs backward computation.
I0825 11:12:23.588150  2068 net.cpp:301] Eltwise26_ReLU53_0_split needs backward computation.
I0825 11:12:23.588169  2068 net.cpp:301] ReLU53 needs backward computation.
I0825 11:12:23.588187  2068 net.cpp:301] Eltwise26 needs backward computation.
I0825 11:12:23.588204  2068 net.cpp:301] Scale55 needs backward computation.
I0825 11:12:23.588222  2068 net.cpp:301] BatchNorm55 needs backward computation.
I0825 11:12:23.588238  2068 net.cpp:301] Convolution55 needs backward computation.
I0825 11:12:23.588255  2068 net.cpp:301] ReLU52 needs backward computation.
I0825 11:12:23.588273  2068 net.cpp:301] Scale54 needs backward computation.
I0825 11:12:23.588289  2068 net.cpp:301] BatchNorm54 needs backward computation.
I0825 11:12:23.588305  2068 net.cpp:301] Convolution54 needs backward computation.
I0825 11:12:23.588322  2068 net.cpp:301] Eltwise25_ReLU51_0_split needs backward computation.
I0825 11:12:23.588341  2068 net.cpp:301] ReLU51 needs backward computation.
I0825 11:12:23.588358  2068 net.cpp:301] Eltwise25 needs backward computation.
I0825 11:12:23.588376  2068 net.cpp:301] Scale53 needs backward computation.
I0825 11:12:23.588393  2068 net.cpp:301] BatchNorm53 needs backward computation.
I0825 11:12:23.588410  2068 net.cpp:301] Convolution53 needs backward computation.
I0825 11:12:23.588428  2068 net.cpp:301] ReLU50 needs backward computation.
I0825 11:12:23.588443  2068 net.cpp:301] Scale52 needs backward computation.
I0825 11:12:23.588460  2068 net.cpp:301] BatchNorm52 needs backward computation.
I0825 11:12:23.588476  2068 net.cpp:301] Convolution52 needs backward computation.
I0825 11:12:23.588493  2068 net.cpp:301] Eltwise24_ReLU49_0_split needs backward computation.
I0825 11:12:23.588511  2068 net.cpp:301] ReLU49 needs backward computation.
I0825 11:12:23.588529  2068 net.cpp:301] Eltwise24 needs backward computation.
I0825 11:12:23.588549  2068 net.cpp:301] Scale51 needs backward computation.
I0825 11:12:23.588567  2068 net.cpp:301] BatchNorm51 needs backward computation.
I0825 11:12:23.588583  2068 net.cpp:301] Convolution51 needs backward computation.
I0825 11:12:23.588601  2068 net.cpp:301] ReLU48 needs backward computation.
I0825 11:12:23.588619  2068 net.cpp:301] Scale50 needs backward computation.
I0825 11:12:23.588637  2068 net.cpp:301] BatchNorm50 needs backward computation.
I0825 11:12:23.588654  2068 net.cpp:301] Convolution50 needs backward computation.
I0825 11:12:23.588671  2068 net.cpp:301] Eltwise23_ReLU47_0_split needs backward computation.
I0825 11:12:23.588695  2068 net.cpp:301] ReLU47 needs backward computation.
I0825 11:12:23.588711  2068 net.cpp:301] Eltwise23 needs backward computation.
I0825 11:12:23.588729  2068 net.cpp:301] Scale49 needs backward computation.
I0825 11:12:23.588747  2068 net.cpp:301] BatchNorm49 needs backward computation.
I0825 11:12:23.588763  2068 net.cpp:301] Convolution49 needs backward computation.
I0825 11:12:23.588780  2068 net.cpp:301] ReLU46 needs backward computation.
I0825 11:12:23.588798  2068 net.cpp:301] Scale48 needs backward computation.
I0825 11:12:23.588814  2068 net.cpp:301] BatchNorm48 needs backward computation.
I0825 11:12:23.588830  2068 net.cpp:301] Convolution48 needs backward computation.
I0825 11:12:23.588855  2068 net.cpp:301] Eltwise22_ReLU45_0_split needs backward computation.
I0825 11:12:23.588873  2068 net.cpp:301] ReLU45 needs backward computation.
I0825 11:12:23.588891  2068 net.cpp:301] Eltwise22 needs backward computation.
I0825 11:12:23.588909  2068 net.cpp:301] Scale47 needs backward computation.
I0825 11:12:23.588927  2068 net.cpp:301] BatchNorm47 needs backward computation.
I0825 11:12:23.588943  2068 net.cpp:301] Convolution47 needs backward computation.
I0825 11:12:23.588961  2068 net.cpp:301] ReLU44 needs backward computation.
I0825 11:12:23.588979  2068 net.cpp:301] Scale46 needs backward computation.
I0825 11:12:23.588996  2068 net.cpp:301] BatchNorm46 needs backward computation.
I0825 11:12:23.589015  2068 net.cpp:301] Convolution46 needs backward computation.
I0825 11:12:23.589032  2068 net.cpp:301] Eltwise21_ReLU43_0_split needs backward computation.
I0825 11:12:23.589051  2068 net.cpp:301] ReLU43 needs backward computation.
I0825 11:12:23.589067  2068 net.cpp:301] Eltwise21 needs backward computation.
I0825 11:12:23.589087  2068 net.cpp:301] Scale45 needs backward computation.
I0825 11:12:23.589104  2068 net.cpp:301] BatchNorm45 needs backward computation.
I0825 11:12:23.589121  2068 net.cpp:301] Convolution45 needs backward computation.
I0825 11:12:23.589138  2068 net.cpp:301] ReLU42 needs backward computation.
I0825 11:12:23.589155  2068 net.cpp:301] Scale44 needs backward computation.
I0825 11:12:23.589172  2068 net.cpp:301] BatchNorm44 needs backward computation.
I0825 11:12:23.589190  2068 net.cpp:301] Convolution44 needs backward computation.
I0825 11:12:23.589208  2068 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0825 11:12:23.589226  2068 net.cpp:301] ReLU41 needs backward computation.
I0825 11:12:23.589244  2068 net.cpp:301] Eltwise20 needs backward computation.
I0825 11:12:23.589262  2068 net.cpp:301] Scale43 needs backward computation.
I0825 11:12:23.589279  2068 net.cpp:301] BatchNorm43 needs backward computation.
I0825 11:12:23.589296  2068 net.cpp:301] Convolution43 needs backward computation.
I0825 11:12:23.589313  2068 net.cpp:301] ReLU40 needs backward computation.
I0825 11:12:23.589330  2068 net.cpp:301] Scale42 needs backward computation.
I0825 11:12:23.589347  2068 net.cpp:301] BatchNorm42 needs backward computation.
I0825 11:12:23.589365  2068 net.cpp:301] Convolution42 needs backward computation.
I0825 11:12:23.589381  2068 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0825 11:12:23.589401  2068 net.cpp:301] ReLU39 needs backward computation.
I0825 11:12:23.589418  2068 net.cpp:301] Eltwise19 needs backward computation.
I0825 11:12:23.589437  2068 net.cpp:301] Scale41 needs backward computation.
I0825 11:12:23.589453  2068 net.cpp:301] BatchNorm41 needs backward computation.
I0825 11:12:23.589470  2068 net.cpp:301] Convolution41 needs backward computation.
I0825 11:12:23.589488  2068 net.cpp:301] ReLU38 needs backward computation.
I0825 11:12:23.589505  2068 net.cpp:301] Scale40 needs backward computation.
I0825 11:12:23.589522  2068 net.cpp:301] BatchNorm40 needs backward computation.
I0825 11:12:23.589540  2068 net.cpp:301] Convolution40 needs backward computation.
I0825 11:12:23.589558  2068 net.cpp:301] Scale39 needs backward computation.
I0825 11:12:23.589576  2068 net.cpp:301] BatchNorm39 needs backward computation.
I0825 11:12:23.589596  2068 net.cpp:301] Convolution39 needs backward computation.
I0825 11:12:23.589614  2068 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0825 11:12:23.589632  2068 net.cpp:301] ReLU37 needs backward computation.
I0825 11:12:23.589649  2068 net.cpp:301] Eltwise18 needs backward computation.
I0825 11:12:23.589668  2068 net.cpp:301] Scale38 needs backward computation.
I0825 11:12:23.589684  2068 net.cpp:301] BatchNorm38 needs backward computation.
I0825 11:12:23.589700  2068 net.cpp:301] Convolution38 needs backward computation.
I0825 11:12:23.589717  2068 net.cpp:301] ReLU36 needs backward computation.
I0825 11:12:23.589735  2068 net.cpp:301] Scale37 needs backward computation.
I0825 11:12:23.589758  2068 net.cpp:301] BatchNorm37 needs backward computation.
I0825 11:12:23.589776  2068 net.cpp:301] Convolution37 needs backward computation.
I0825 11:12:23.589793  2068 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0825 11:12:23.589812  2068 net.cpp:301] ReLU35 needs backward computation.
I0825 11:12:23.589828  2068 net.cpp:301] Eltwise17 needs backward computation.
I0825 11:12:23.589846  2068 net.cpp:301] Scale36 needs backward computation.
I0825 11:12:23.589864  2068 net.cpp:301] BatchNorm36 needs backward computation.
I0825 11:12:23.589881  2068 net.cpp:301] Convolution36 needs backward computation.
I0825 11:12:23.589898  2068 net.cpp:301] ReLU34 needs backward computation.
I0825 11:12:23.589916  2068 net.cpp:301] Scale35 needs backward computation.
I0825 11:12:23.589932  2068 net.cpp:301] BatchNorm35 needs backward computation.
I0825 11:12:23.589949  2068 net.cpp:301] Convolution35 needs backward computation.
I0825 11:12:23.589967  2068 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0825 11:12:23.589984  2068 net.cpp:301] ReLU33 needs backward computation.
I0825 11:12:23.590000  2068 net.cpp:301] Eltwise16 needs backward computation.
I0825 11:12:23.590021  2068 net.cpp:301] Scale34 needs backward computation.
I0825 11:12:23.590039  2068 net.cpp:301] BatchNorm34 needs backward computation.
I0825 11:12:23.590056  2068 net.cpp:301] Convolution34 needs backward computation.
I0825 11:12:23.590075  2068 net.cpp:301] ReLU32 needs backward computation.
I0825 11:12:23.590091  2068 net.cpp:301] Scale33 needs backward computation.
I0825 11:12:23.590107  2068 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:12:23.590126  2068 net.cpp:301] Convolution33 needs backward computation.
I0825 11:12:23.590143  2068 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0825 11:12:23.590162  2068 net.cpp:301] ReLU31 needs backward computation.
I0825 11:12:23.590178  2068 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:12:23.590196  2068 net.cpp:301] Scale32 needs backward computation.
I0825 11:12:23.590214  2068 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:12:23.590231  2068 net.cpp:301] Convolution32 needs backward computation.
I0825 11:12:23.590250  2068 net.cpp:301] ReLU30 needs backward computation.
I0825 11:12:23.590266  2068 net.cpp:301] Scale31 needs backward computation.
I0825 11:12:23.590283  2068 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:12:23.590301  2068 net.cpp:301] Convolution31 needs backward computation.
I0825 11:12:23.590317  2068 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:12:23.590342  2068 net.cpp:301] ReLU29 needs backward computation.
I0825 11:12:23.590360  2068 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:12:23.590378  2068 net.cpp:301] Scale30 needs backward computation.
I0825 11:12:23.590396  2068 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:12:23.590412  2068 net.cpp:301] Convolution30 needs backward computation.
I0825 11:12:23.590430  2068 net.cpp:301] ReLU28 needs backward computation.
I0825 11:12:23.590447  2068 net.cpp:301] Scale29 needs backward computation.
I0825 11:12:23.590464  2068 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:12:23.590481  2068 net.cpp:301] Convolution29 needs backward computation.
I0825 11:12:23.590502  2068 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:12:23.590520  2068 net.cpp:301] ReLU27 needs backward computation.
I0825 11:12:23.590538  2068 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:12:23.590556  2068 net.cpp:301] Scale28 needs backward computation.
I0825 11:12:23.590572  2068 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:12:23.590590  2068 net.cpp:301] Convolution28 needs backward computation.
I0825 11:12:23.590606  2068 net.cpp:301] ReLU26 needs backward computation.
I0825 11:12:23.590625  2068 net.cpp:301] Scale27 needs backward computation.
I0825 11:12:23.590641  2068 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:12:23.590664  2068 net.cpp:301] Convolution27 needs backward computation.
I0825 11:12:23.590682  2068 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:12:23.590701  2068 net.cpp:301] ReLU25 needs backward computation.
I0825 11:12:23.590718  2068 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:12:23.590736  2068 net.cpp:301] Scale26 needs backward computation.
I0825 11:12:23.590754  2068 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:12:23.590771  2068 net.cpp:301] Convolution26 needs backward computation.
I0825 11:12:23.590788  2068 net.cpp:301] ReLU24 needs backward computation.
I0825 11:12:23.590806  2068 net.cpp:301] Scale25 needs backward computation.
I0825 11:12:23.590823  2068 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:12:23.590840  2068 net.cpp:301] Convolution25 needs backward computation.
I0825 11:12:23.590858  2068 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:12:23.590875  2068 net.cpp:301] ReLU23 needs backward computation.
I0825 11:12:23.590893  2068 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:12:23.590910  2068 net.cpp:301] Scale24 needs backward computation.
I0825 11:12:23.590929  2068 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:12:23.590945  2068 net.cpp:301] Convolution24 needs backward computation.
I0825 11:12:23.590962  2068 net.cpp:301] ReLU22 needs backward computation.
I0825 11:12:23.590978  2068 net.cpp:301] Scale23 needs backward computation.
I0825 11:12:23.590996  2068 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:12:23.591013  2068 net.cpp:301] Convolution23 needs backward computation.
I0825 11:12:23.591030  2068 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:12:23.591048  2068 net.cpp:301] ReLU21 needs backward computation.
I0825 11:12:23.591064  2068 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:12:23.591082  2068 net.cpp:301] Scale22 needs backward computation.
I0825 11:12:23.591100  2068 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:12:23.591117  2068 net.cpp:301] Convolution22 needs backward computation.
I0825 11:12:23.591135  2068 net.cpp:301] ReLU20 needs backward computation.
I0825 11:12:23.591152  2068 net.cpp:301] Scale21 needs backward computation.
I0825 11:12:23.591171  2068 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:12:23.591187  2068 net.cpp:301] Convolution21 needs backward computation.
I0825 11:12:23.591205  2068 net.cpp:301] Scale20 needs backward computation.
I0825 11:12:23.591223  2068 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:12:23.591240  2068 net.cpp:301] Convolution20 needs backward computation.
I0825 11:12:23.591259  2068 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:12:23.591276  2068 net.cpp:301] ReLU19 needs backward computation.
I0825 11:12:23.591295  2068 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:12:23.591312  2068 net.cpp:301] Scale19 needs backward computation.
I0825 11:12:23.591328  2068 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:12:23.591346  2068 net.cpp:301] Convolution19 needs backward computation.
I0825 11:12:23.591363  2068 net.cpp:301] ReLU18 needs backward computation.
I0825 11:12:23.591380  2068 net.cpp:301] Scale18 needs backward computation.
I0825 11:12:23.591400  2068 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:12:23.591418  2068 net.cpp:301] Convolution18 needs backward computation.
I0825 11:12:23.591436  2068 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:12:23.591454  2068 net.cpp:301] ReLU17 needs backward computation.
I0825 11:12:23.591471  2068 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:12:23.591491  2068 net.cpp:301] Scale17 needs backward computation.
I0825 11:12:23.591508  2068 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:12:23.591526  2068 net.cpp:301] Convolution17 needs backward computation.
I0825 11:12:23.591552  2068 net.cpp:301] ReLU16 needs backward computation.
I0825 11:12:23.591569  2068 net.cpp:301] Scale16 needs backward computation.
I0825 11:12:23.591585  2068 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:12:23.591601  2068 net.cpp:301] Convolution16 needs backward computation.
I0825 11:12:23.591619  2068 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:12:23.591634  2068 net.cpp:301] ReLU15 needs backward computation.
I0825 11:12:23.591651  2068 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:12:23.591670  2068 net.cpp:301] Scale15 needs backward computation.
I0825 11:12:23.591686  2068 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:12:23.591702  2068 net.cpp:301] Convolution15 needs backward computation.
I0825 11:12:23.591720  2068 net.cpp:301] ReLU14 needs backward computation.
I0825 11:12:23.591737  2068 net.cpp:301] Scale14 needs backward computation.
I0825 11:12:23.591754  2068 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:12:23.591771  2068 net.cpp:301] Convolution14 needs backward computation.
I0825 11:12:23.591789  2068 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:12:23.591806  2068 net.cpp:301] ReLU13 needs backward computation.
I0825 11:12:23.591825  2068 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:12:23.591842  2068 net.cpp:301] Scale13 needs backward computation.
I0825 11:12:23.591858  2068 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:12:23.591876  2068 net.cpp:301] Convolution13 needs backward computation.
I0825 11:12:23.591893  2068 net.cpp:301] ReLU12 needs backward computation.
I0825 11:12:23.591910  2068 net.cpp:301] Scale12 needs backward computation.
I0825 11:12:23.591926  2068 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:12:23.591944  2068 net.cpp:301] Convolution12 needs backward computation.
I0825 11:12:23.591965  2068 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:12:23.591984  2068 net.cpp:301] ReLU11 needs backward computation.
I0825 11:12:23.592000  2068 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:12:23.592015  2068 net.cpp:301] Scale11 needs backward computation.
I0825 11:12:23.592030  2068 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:12:23.592046  2068 net.cpp:301] Convolution11 needs backward computation.
I0825 11:12:23.592061  2068 net.cpp:301] ReLU10 needs backward computation.
I0825 11:12:23.592078  2068 net.cpp:301] Scale10 needs backward computation.
I0825 11:12:23.592095  2068 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:12:23.592113  2068 net.cpp:301] Convolution10 needs backward computation.
I0825 11:12:23.592130  2068 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:12:23.592149  2068 net.cpp:301] ReLU9 needs backward computation.
I0825 11:12:23.592167  2068 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:12:23.592186  2068 net.cpp:301] Scale9 needs backward computation.
I0825 11:12:23.592203  2068 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:12:23.592221  2068 net.cpp:301] Convolution9 needs backward computation.
I0825 11:12:23.592237  2068 net.cpp:301] ReLU8 needs backward computation.
I0825 11:12:23.592253  2068 net.cpp:301] Scale8 needs backward computation.
I0825 11:12:23.592270  2068 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:12:23.592291  2068 net.cpp:301] Convolution8 needs backward computation.
I0825 11:12:23.592325  2068 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:12:23.592345  2068 net.cpp:301] ReLU7 needs backward computation.
I0825 11:12:23.592362  2068 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:12:23.592381  2068 net.cpp:301] Scale7 needs backward computation.
I0825 11:12:23.592397  2068 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:12:23.592414  2068 net.cpp:301] Convolution7 needs backward computation.
I0825 11:12:23.592432  2068 net.cpp:301] ReLU6 needs backward computation.
I0825 11:12:23.592455  2068 net.cpp:301] Scale6 needs backward computation.
I0825 11:12:23.592473  2068 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:12:23.592489  2068 net.cpp:301] Convolution6 needs backward computation.
I0825 11:12:23.592507  2068 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:12:23.592525  2068 net.cpp:301] ReLU5 needs backward computation.
I0825 11:12:23.592541  2068 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:12:23.592559  2068 net.cpp:301] Scale5 needs backward computation.
I0825 11:12:23.592576  2068 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:12:23.592592  2068 net.cpp:301] Convolution5 needs backward computation.
I0825 11:12:23.592612  2068 net.cpp:301] ReLU4 needs backward computation.
I0825 11:12:23.592628  2068 net.cpp:301] Scale4 needs backward computation.
I0825 11:12:23.592644  2068 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:12:23.592661  2068 net.cpp:301] Convolution4 needs backward computation.
I0825 11:12:23.592679  2068 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:12:23.592695  2068 net.cpp:301] ReLU3 needs backward computation.
I0825 11:12:23.592712  2068 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:12:23.592731  2068 net.cpp:301] Scale3 needs backward computation.
I0825 11:12:23.592747  2068 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:12:23.592766  2068 net.cpp:301] Convolution3 needs backward computation.
I0825 11:12:23.592783  2068 net.cpp:301] ReLU2 needs backward computation.
I0825 11:12:23.592800  2068 net.cpp:301] Scale2 needs backward computation.
I0825 11:12:23.592818  2068 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:12:23.592834  2068 net.cpp:301] Convolution2 needs backward computation.
I0825 11:12:23.592852  2068 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:12:23.592869  2068 net.cpp:301] ReLU1 needs backward computation.
I0825 11:12:23.592885  2068 net.cpp:301] Scale1 needs backward computation.
I0825 11:12:23.592903  2068 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:12:23.592919  2068 net.cpp:301] Convolution1 needs backward computation.
I0825 11:12:23.592938  2068 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0825 11:12:23.592958  2068 net.cpp:303] Data1 does not need backward computation.
I0825 11:12:23.592974  2068 net.cpp:348] This network produces output Accuracy1
I0825 11:12:23.592991  2068 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:12:23.593140  2068 net.cpp:363] Network initialization done.
I0825 11:12:23.594240  2068 solver.cpp:110] Solver scaffolding done.
I0825 11:12:23.613163  2068 caffe.cpp:313] Starting Optimization
I0825 11:12:23.613214  2068 solver.cpp:425] Solving resnet_cifar10
I0825 11:12:23.613245  2068 solver.cpp:427] Learning Rate Policy: multistep
I0825 11:12:23.624007  2068 solver.cpp:514] Iteration 0, Testing net (#0)
I0825 11:13:51.015280  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:13:51.374752  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0
I0825 11:13:51.374917  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.30259 (* 1 = 2.30259 loss)
I0825 11:13:52.991729  2068 solver.cpp:357] Iteration 0 (0 iter/s, 89.327s/100 iters), loss = 3.70221
I0825 11:13:52.991914  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 3.6419 (* 1 = 3.6419 loss)
I0825 11:13:52.992005  2068 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0825 11:16:11.143666  2068 solver.cpp:357] Iteration 100 (0.723869 iter/s, 138.147s/100 iters), loss = 1.86258
I0825 11:16:11.143908  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.95741 (* 1 = 1.95741 loss)
I0825 11:16:11.143954  2068 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0825 11:18:28.196718  2068 solver.cpp:357] Iteration 200 (0.729692 iter/s, 137.044s/100 iters), loss = 1.70576
I0825 11:18:28.196931  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.75537 (* 1 = 1.75537 loss)
I0825 11:18:28.196943  2068 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0825 11:20:45.113283  2068 solver.cpp:357] Iteration 300 (0.730438 iter/s, 136.904s/100 iters), loss = 1.63446
I0825 11:20:45.113490  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.73214 (* 1 = 1.73214 loss)
I0825 11:20:45.113517  2068 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0825 11:22:46.422711  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:23:03.049510  2068 solver.cpp:357] Iteration 400 (0.724986 iter/s, 137.934s/100 iters), loss = 1.56136
I0825 11:23:03.049646  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.3565 (* 1 = 1.3565 loss)
I0825 11:23:03.049674  2068 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0825 11:25:12.084048  2068 solver.cpp:514] Iteration 500, Testing net (#0)
I0825 11:26:37.644945  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:26:38.016412  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1459
I0825 11:26:38.016527  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.66098 (* 1 = 2.66098 loss)
I0825 11:26:39.095495  2068 solver.cpp:357] Iteration 500 (0.46287 iter/s, 216.043s/100 iters), loss = 1.40784
I0825 11:26:39.095669  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.41899 (* 1 = 1.41899 loss)
I0825 11:26:39.095712  2068 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0825 11:28:58.157747  2068 solver.cpp:357] Iteration 600 (0.719126 iter/s, 139.058s/100 iters), loss = 1.27461
I0825 11:28:58.157948  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.46975 (* 1 = 1.46975 loss)
I0825 11:28:58.157990  2068 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0825 11:31:12.865005  2068 solver.cpp:357] Iteration 700 (0.742382 iter/s, 134.702s/100 iters), loss = 1.14843
I0825 11:31:12.865223  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.13249 (* 1 = 1.13249 loss)
I0825 11:31:12.865269  2068 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0825 11:32:52.259006  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:33:22.330086  2068 solver.cpp:357] Iteration 800 (0.772527 iter/s, 129.445s/100 iters), loss = 1.15298
I0825 11:33:22.330319  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.11946 (* 1 = 1.11946 loss)
I0825 11:33:22.330370  2068 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0825 11:35:35.141542  2068 solver.cpp:357] Iteration 900 (0.753036 iter/s, 132.796s/100 iters), loss = 1.11554
I0825 11:35:35.142102  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.13322 (* 1 = 1.13322 loss)
I0825 11:35:35.142278  2068 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0825 11:37:52.953073  2068 solver.cpp:514] Iteration 1000, Testing net (#0)
I0825 11:39:12.639267  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:39:12.947645  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.129899
I0825 11:39:12.948004  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.74738 (* 1 = 2.74738 loss)
I0825 11:39:13.995893  2068 solver.cpp:357] Iteration 1000 (0.456951 iter/s, 218.842s/100 iters), loss = 0.914959
I0825 11:39:13.996299  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.757728 (* 1 = 0.757728 loss)
I0825 11:39:13.996480  2068 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0825 11:41:28.382119  2068 solver.cpp:357] Iteration 1100 (0.744185 iter/s, 134.375s/100 iters), loss = 0.868573
I0825 11:41:28.382297  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.76839 (* 1 = 0.76839 loss)
I0825 11:41:28.382324  2068 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0825 11:42:57.039880  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:43:36.863307  2068 solver.cpp:357] Iteration 1200 (0.778386 iter/s, 128.471s/100 iters), loss = 0.900466
I0825 11:43:36.863574  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.87705 (* 1 = 0.87705 loss)
I0825 11:43:36.863601  2068 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0825 11:45:56.499436  2068 solver.cpp:357] Iteration 1300 (0.716177 iter/s, 139.63s/100 iters), loss = 0.814356
I0825 11:45:56.499675  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.862074 (* 1 = 0.862074 loss)
I0825 11:45:56.499706  2068 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0825 11:48:06.108965  2068 solver.cpp:357] Iteration 1400 (0.771551 iter/s, 129.609s/100 iters), loss = 0.739441
I0825 11:48:06.110605  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.656104 (* 1 = 0.656104 loss)
I0825 11:48:06.110635  2068 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0825 11:50:23.961624  2068 solver.cpp:514] Iteration 1500, Testing net (#0)
I0825 11:51:48.556200  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:51:48.761369  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1218
I0825 11:51:48.761525  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.94974 (* 1 = 2.94974 loss)
I0825 11:51:49.810667  2068 solver.cpp:357] Iteration 1500 (0.446945 iter/s, 223.741s/100 iters), loss = 0.688976
I0825 11:51:49.810832  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.803412 (* 1 = 0.803412 loss)
I0825 11:51:49.810878  2068 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0825 11:53:01.263718  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:53:56.677386  2068 solver.cpp:357] Iteration 1600 (0.788163 iter/s, 126.877s/100 iters), loss = 0.720962
I0825 11:53:56.677590  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.760098 (* 1 = 0.760098 loss)
I0825 11:53:56.677636  2068 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0825 11:56:12.921553  2068 solver.cpp:357] Iteration 1700 (0.733957 iter/s, 136.248s/100 iters), loss = 0.92017
I0825 11:56:12.921780  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.839984 (* 1 = 0.839984 loss)
I0825 11:56:12.921825  2068 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0825 11:58:25.116597  2068 solver.cpp:357] Iteration 1800 (0.756445 iter/s, 132.197s/100 iters), loss = 0.812531
I0825 11:58:25.118432  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.783451 (* 1 = 0.783451 loss)
I0825 11:58:25.118465  2068 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0825 12:00:45.525701  2068 solver.cpp:357] Iteration 1900 (0.712201 iter/s, 140.41s/100 iters), loss = 0.726959
I0825 12:00:45.525880  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.680435 (* 1 = 0.680435 loss)
I0825 12:00:45.525913  2068 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0825 12:01:48.603163  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:02:51.986228  2068 solver.cpp:514] Iteration 2000, Testing net (#0)
I0825 12:04:16.515588  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:04:16.963799  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1755
I0825 12:04:16.963963  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.69011 (* 1 = 2.69011 loss)
I0825 12:04:18.047698  2068 solver.cpp:357] Iteration 2000 (0.470536 iter/s, 212.523s/100 iters), loss = 0.652068
I0825 12:04:18.047875  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.59182 (* 1 = 0.59182 loss)
I0825 12:04:18.047921  2068 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0825 12:06:32.668601  2068 solver.cpp:357] Iteration 2100 (0.742847 iter/s, 134.617s/100 iters), loss = 0.741581
I0825 12:06:32.668846  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.752998 (* 1 = 0.752998 loss)
I0825 12:06:32.668892  2068 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0825 12:08:50.570175  2068 solver.cpp:357] Iteration 2200 (0.725164 iter/s, 137.9s/100 iters), loss = 0.701608
I0825 12:08:50.570370  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.63539 (* 1 = 0.63539 loss)
I0825 12:08:50.570399  2068 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0825 12:11:01.730051  2068 solver.cpp:357] Iteration 2300 (0.762448 iter/s, 131.156s/100 iters), loss = 0.623749
I0825 12:11:01.730343  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53271 (* 1 = 0.53271 loss)
I0825 12:11:01.730396  2068 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0825 12:11:59.359028  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:13:17.121047  2068 solver.cpp:357] Iteration 2400 (0.738619 iter/s, 135.388s/100 iters), loss = 0.739711
I0825 12:13:17.121371  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.836962 (* 1 = 0.836962 loss)
I0825 12:13:17.121464  2068 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0825 12:15:29.112637  2068 solver.cpp:514] Iteration 2500, Testing net (#0)
I0825 12:16:53.078900  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:16:53.353307  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1762
I0825 12:16:53.353472  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.79273 (* 1 = 2.79273 loss)
I0825 12:16:54.398404  2068 solver.cpp:357] Iteration 2500 (0.46025 iter/s, 217.273s/100 iters), loss = 0.689862
I0825 12:16:54.398583  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.724575 (* 1 = 0.724575 loss)
I0825 12:16:54.398628  2068 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0825 12:19:12.806136  2068 solver.cpp:357] Iteration 2600 (0.722517 iter/s, 138.405s/100 iters), loss = 0.735576
I0825 12:19:12.806340  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.63903 (* 1 = 0.63903 loss)
I0825 12:19:12.806387  2068 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0825 12:21:27.988301  2068 solver.cpp:357] Iteration 2700 (0.739757 iter/s, 135.179s/100 iters), loss = 0.867851
I0825 12:21:27.988500  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.87921 (* 1 = 0.87921 loss)
I0825 12:21:27.988546  2068 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0825 12:22:11.363241  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:23:43.528772  2068 solver.cpp:357] Iteration 2800 (0.737801 iter/s, 135.538s/100 iters), loss = 0.573712
I0825 12:23:43.534166  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.613895 (* 1 = 0.613895 loss)
I0825 12:23:43.534265  2068 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0825 12:25:54.698171  2068 solver.cpp:357] Iteration 2900 (0.762399 iter/s, 131.165s/100 iters), loss = 0.611265
I0825 12:25:54.698402  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.595747 (* 1 = 0.595747 loss)
I0825 12:25:54.698447  2068 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0825 12:28:13.208897  2068 solver.cpp:514] Iteration 3000, Testing net (#0)
I0825 12:29:33.324141  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:29:33.676578  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1717
I0825 12:29:33.676734  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.08045 (* 1 = 3.08045 loss)
I0825 12:29:34.748107  2068 solver.cpp:357] Iteration 3000 (0.454445 iter/s, 220.049s/100 iters), loss = 0.624418
I0825 12:29:34.748262  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.722697 (* 1 = 0.722697 loss)
I0825 12:29:34.748306  2068 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0825 12:31:51.842581  2068 solver.cpp:357] Iteration 3100 (0.729415 iter/s, 137.096s/100 iters), loss = 0.701732
I0825 12:31:51.842825  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.875603 (* 1 = 0.875603 loss)
I0825 12:31:51.842886  2068 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0825 12:32:20.991693  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:33:56.084931  2068 solver.cpp:357] Iteration 3200 (0.804895 iter/s, 124.24s/100 iters), loss = 0.580747
I0825 12:33:56.085170  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.459102 (* 1 = 0.459102 loss)
I0825 12:33:56.085216  2068 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0825 12:36:15.561290  2068 solver.cpp:357] Iteration 3300 (0.71699 iter/s, 139.472s/100 iters), loss = 0.546944
I0825 12:36:15.561961  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.60599 (* 1 = 0.60599 loss)
I0825 12:36:15.562145  2068 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0825 12:38:28.861723  2068 solver.cpp:357] Iteration 3400 (0.750199 iter/s, 133.298s/100 iters), loss = 0.461698
I0825 12:38:28.861968  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373042 (* 1 = 0.373042 loss)
I0825 12:38:28.862013  2068 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0825 12:40:43.649929  2068 solver.cpp:514] Iteration 3500, Testing net (#0)
I0825 12:42:08.914093  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:42:09.228799  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2498
I0825 12:42:09.228950  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.21276 (* 1 = 2.21276 loss)
I0825 12:42:10.233402  2068 solver.cpp:357] Iteration 3500 (0.451699 iter/s, 221.386s/100 iters), loss = 0.539774
I0825 12:42:10.233551  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.610107 (* 1 = 0.610107 loss)
I0825 12:42:10.233595  2068 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0825 12:42:26.622676  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:44:21.507292  2068 solver.cpp:357] Iteration 3600 (0.7617 iter/s, 131.285s/100 iters), loss = 0.508753
I0825 12:44:21.507452  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.613977 (* 1 = 0.613977 loss)
I0825 12:44:21.507479  2068 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0825 12:46:42.031148  2068 solver.cpp:357] Iteration 3700 (0.711591 iter/s, 140.53s/100 iters), loss = 0.496272
I0825 12:46:42.031618  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.556233 (* 1 = 0.556233 loss)
I0825 12:46:42.031797  2068 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0825 12:48:45.442925  2068 solver.cpp:357] Iteration 3800 (0.810268 iter/s, 123.416s/100 iters), loss = 0.448556
I0825 12:48:45.443156  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.592648 (* 1 = 0.592648 loss)
I0825 12:48:45.443202  2068 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0825 12:51:05.567984  2068 solver.cpp:357] Iteration 3900 (0.713629 iter/s, 140.129s/100 iters), loss = 0.665464
I0825 12:51:05.568171  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.869524 (* 1 = 0.869524 loss)
I0825 12:51:05.568197  2068 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0825 12:51:10.735018  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:53:11.730618  2068 solver.cpp:514] Iteration 4000, Testing net (#0)
I0825 12:54:34.171838  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:54:34.443356  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5466
I0825 12:54:34.443723  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32232 (* 1 = 1.32232 loss)
I0825 12:54:35.526950  2068 solver.cpp:357] Iteration 4000 (0.476274 iter/s, 209.963s/100 iters), loss = 0.629856
I0825 12:54:35.527346  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.532903 (* 1 = 0.532903 loss)
I0825 12:54:35.527520  2068 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0825 12:56:46.326040  2068 solver.cpp:357] Iteration 4100 (0.764522 iter/s, 130.801s/100 iters), loss = 0.555103
I0825 12:56:46.326277  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.67643 (* 1 = 0.67643 loss)
I0825 12:56:46.326323  2068 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0825 12:59:06.335615  2068 solver.cpp:357] Iteration 4200 (0.71424 iter/s, 140.009s/100 iters), loss = 0.510163
I0825 12:59:06.336000  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44741 (* 1 = 0.44741 loss)
I0825 12:59:06.336092  2068 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0825 13:01:10.942714  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:01:19.176616  2068 solver.cpp:357] Iteration 4300 (0.752774 iter/s, 132.842s/100 iters), loss = 0.575145
I0825 13:01:19.176795  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456171 (* 1 = 0.456171 loss)
I0825 13:01:19.176841  2068 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0825 13:03:31.155303  2068 solver.cpp:357] Iteration 4400 (0.757706 iter/s, 131.977s/100 iters), loss = 0.491397
I0825 13:03:31.155489  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.521254 (* 1 = 0.521254 loss)
I0825 13:03:31.155516  2068 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0825 13:05:44.399518  2068 solver.cpp:514] Iteration 4500, Testing net (#0)
I0825 13:07:09.318192  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:07:09.761878  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6068
I0825 13:07:09.762032  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.14853 (* 1 = 1.14853 loss)
I0825 13:07:10.834831  2068 solver.cpp:357] Iteration 4500 (0.455201 iter/s, 219.683s/100 iters), loss = 0.601577
I0825 13:07:10.834996  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387253 (* 1 = 0.387253 loss)
I0825 13:07:10.835042  2068 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0825 13:09:29.938671  2068 solver.cpp:357] Iteration 4600 (0.718874 iter/s, 139.106s/100 iters), loss = 0.457785
I0825 13:09:29.938920  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429979 (* 1 = 0.429979 loss)
I0825 13:09:29.938966  2068 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0825 13:11:14.695762  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:11:35.321964  2068 solver.cpp:357] Iteration 4700 (0.797541 iter/s, 125.385s/100 iters), loss = 0.458873
I0825 13:11:35.322158  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410171 (* 1 = 0.410171 loss)
I0825 13:11:35.322204  2068 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0825 13:13:50.688678  2068 solver.cpp:357] Iteration 4800 (0.738748 iter/s, 135.364s/100 iters), loss = 0.450867
I0825 13:13:50.690260  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430012 (* 1 = 0.430012 loss)
I0825 13:13:50.690310  2068 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0825 13:16:03.988461  2068 solver.cpp:357] Iteration 4900 (0.750305 iter/s, 133.279s/100 iters), loss = 0.542745
I0825 13:16:03.988854  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.511986 (* 1 = 0.511986 loss)
I0825 13:16:03.988948  2068 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0825 13:18:18.135536  2068 solver.cpp:514] Iteration 5000, Testing net (#0)
I0825 13:19:37.587469  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:19:37.940523  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.639
I0825 13:19:37.940646  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08239 (* 1 = 1.08239 loss)
I0825 13:19:39.043890  2068 solver.cpp:357] Iteration 5000 (0.465035 iter/s, 215.038s/100 iters), loss = 0.432817
I0825 13:19:39.044029  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416489 (* 1 = 0.416489 loss)
I0825 13:19:39.044061  2068 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0825 13:21:24.626694  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:21:58.123986  2068 solver.cpp:357] Iteration 5100 (0.719066 iter/s, 139.069s/100 iters), loss = 0.537513
I0825 13:21:58.124394  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393003 (* 1 = 0.393003 loss)
I0825 13:21:58.124488  2068 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0825 13:24:08.022593  2068 solver.cpp:357] Iteration 5200 (0.769895 iter/s, 129.888s/100 iters), loss = 0.597818
I0825 13:24:08.022814  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.673526 (* 1 = 0.673526 loss)
I0825 13:24:08.022859  2068 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0825 13:26:21.677269  2068 solver.cpp:357] Iteration 5300 (0.748238 iter/s, 133.647s/100 iters), loss = 0.617535
I0825 13:26:21.677479  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.746274 (* 1 = 0.746274 loss)
I0825 13:26:21.677526  2068 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0825 13:28:36.769470  2068 solver.cpp:357] Iteration 5400 (0.74028 iter/s, 135.084s/100 iters), loss = 0.334281
I0825 13:28:36.769757  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341915 (* 1 = 0.341915 loss)
I0825 13:28:36.769804  2068 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0825 13:30:08.432802  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:30:54.560225  2068 solver.cpp:514] Iteration 5500, Testing net (#0)
I0825 13:32:17.352988  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:32:17.662943  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.602601
I0825 13:32:17.663110  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.26749 (* 1 = 1.26749 loss)
I0825 13:32:18.704543  2068 solver.cpp:357] Iteration 5500 (0.450598 iter/s, 221.927s/100 iters), loss = 0.390219
I0825 13:32:18.704710  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456034 (* 1 = 0.456034 loss)
I0825 13:32:18.704756  2068 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0825 13:34:23.894137  2068 solver.cpp:357] Iteration 5600 (0.798828 iter/s, 125.183s/100 iters), loss = 0.460262
I0825 13:34:23.894367  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.268342 (* 1 = 0.268342 loss)
I0825 13:34:23.894414  2068 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0825 13:36:43.076095  2068 solver.cpp:357] Iteration 5700 (0.718515 iter/s, 139.176s/100 iters), loss = 0.532368
I0825 13:36:43.076527  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402998 (* 1 = 0.402998 loss)
I0825 13:36:43.076642  2068 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0825 13:38:56.656066  2068 solver.cpp:357] Iteration 5800 (0.748635 iter/s, 133.576s/100 iters), loss = 0.338942
I0825 13:38:56.656246  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449021 (* 1 = 0.449021 loss)
I0825 13:38:56.656280  2068 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0825 13:40:10.359374  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:41:10.175197  2068 solver.cpp:357] Iteration 5900 (0.748975 iter/s, 133.516s/100 iters), loss = 0.541228
I0825 13:41:10.175428  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474354 (* 1 = 0.474354 loss)
I0825 13:41:10.175474  2068 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0825 13:43:20.288058  2068 solver.cpp:514] Iteration 6000, Testing net (#0)
I0825 13:44:42.897347  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:44:43.198571  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.639401
I0825 13:44:43.198938  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.04065 (* 1 = 1.04065 loss)
I0825 13:44:44.314690  2068 solver.cpp:357] Iteration 6000 (0.46699 iter/s, 214.137s/100 iters), loss = 0.485105
I0825 13:44:44.315105  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.433347 (* 1 = 0.433347 loss)
I0825 13:44:44.315280  2068 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0825 13:46:55.797155  2068 solver.cpp:357] Iteration 6100 (0.760562 iter/s, 131.482s/100 iters), loss = 0.495062
I0825 13:46:55.798789  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.523757 (* 1 = 0.523757 loss)
I0825 13:46:55.798837  2068 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0825 13:49:10.417649  2068 solver.cpp:357] Iteration 6200 (0.742828 iter/s, 134.621s/100 iters), loss = 0.449745
I0825 13:49:10.417822  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.272275 (* 1 = 0.272275 loss)
I0825 13:49:10.417851  2068 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0825 13:50:16.031256  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:51:25.117987  2068 solver.cpp:357] Iteration 6300 (0.742368 iter/s, 134.704s/100 iters), loss = 0.424593
I0825 13:51:25.118393  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448132 (* 1 = 0.448132 loss)
I0825 13:51:25.118505  2068 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0825 13:53:41.138427  2068 solver.cpp:357] Iteration 6400 (0.735158 iter/s, 136.025s/100 iters), loss = 0.527384
I0825 13:53:41.138737  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.623296 (* 1 = 0.623296 loss)
I0825 13:53:41.138785  2068 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0825 13:55:48.278698  2068 solver.cpp:514] Iteration 6500, Testing net (#0)
I0825 13:57:12.203691  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:57:12.636517  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6586
I0825 13:57:12.636622  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08108 (* 1 = 1.08108 loss)
I0825 13:57:13.757005  2068 solver.cpp:357] Iteration 6500 (0.470315 iter/s, 212.624s/100 iters), loss = 0.346799
I0825 13:57:13.757163  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413848 (* 1 = 0.413848 loss)
I0825 13:57:13.757206  2068 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0825 13:59:33.248311  2068 solver.cpp:357] Iteration 6600 (0.716875 iter/s, 139.494s/100 iters), loss = 0.451423
I0825 13:59:33.248735  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417246 (* 1 = 0.417246 loss)
I0825 13:59:33.248848  2068 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0825 14:00:23.186686  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:01:47.843819  2068 solver.cpp:357] Iteration 6700 (0.742953 iter/s, 134.598s/100 iters), loss = 0.513921
I0825 14:01:47.843991  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.54141 (* 1 = 0.54141 loss)
I0825 14:01:47.844022  2068 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0825 14:03:55.825407  2068 solver.cpp:357] Iteration 6800 (0.781351 iter/s, 127.983s/100 iters), loss = 0.448951
I0825 14:03:55.825656  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471789 (* 1 = 0.471789 loss)
I0825 14:03:55.825685  2068 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0825 14:06:08.739593  2068 solver.cpp:357] Iteration 6900 (0.752379 iter/s, 132.912s/100 iters), loss = 0.509371
I0825 14:06:08.740006  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423394 (* 1 = 0.423394 loss)
I0825 14:06:08.740100  2068 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0825 14:08:26.491708  2068 solver.cpp:514] Iteration 7000, Testing net (#0)
I0825 14:09:45.033989  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:09:45.396245  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.756
I0825 14:09:45.396411  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.721997 (* 1 = 0.721997 loss)
I0825 14:09:46.438561  2068 solver.cpp:357] Iteration 7000 (0.459342 iter/s, 217.703s/100 iters), loss = 0.511267
I0825 14:09:46.438726  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.705451 (* 1 = 0.705451 loss)
I0825 14:09:46.438771  2068 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0825 14:10:23.492799  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:12:03.068064  2068 solver.cpp:357] Iteration 7100 (0.731921 iter/s, 136.627s/100 iters), loss = 0.538255
I0825 14:12:03.068279  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.596053 (* 1 = 0.596053 loss)
I0825 14:12:03.068325  2068 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0825 14:14:12.453619  2068 solver.cpp:357] Iteration 7200 (0.772902 iter/s, 129.382s/100 iters), loss = 0.545204
I0825 14:14:12.454077  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441075 (* 1 = 0.441075 loss)
I0825 14:14:12.454282  2068 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0825 14:16:31.848017  2068 solver.cpp:357] Iteration 7300 (0.717382 iter/s, 139.396s/100 iters), loss = 0.442489
I0825 14:16:31.848218  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368981 (* 1 = 0.368981 loss)
I0825 14:16:31.848248  2068 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0825 14:18:41.815773  2068 solver.cpp:357] Iteration 7400 (0.769428 iter/s, 129.967s/100 iters), loss = 0.490289
I0825 14:18:41.816077  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401398 (* 1 = 0.401398 loss)
I0825 14:18:41.816125  2068 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0825 14:19:08.139372  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:20:57.706370  2068 solver.cpp:514] Iteration 7500, Testing net (#0)
I0825 14:22:23.157629  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:22:23.448982  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6534
I0825 14:22:23.449136  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00193 (* 1 = 1.00193 loss)
I0825 14:22:24.550129  2068 solver.cpp:357] Iteration 7500 (0.448959 iter/s, 222.737s/100 iters), loss = 0.387648
I0825 14:22:24.550297  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40495 (* 1 = 0.40495 loss)
I0825 14:22:24.550344  2068 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0825 14:24:31.836841  2068 solver.cpp:357] Iteration 7600 (0.785629 iter/s, 127.287s/100 iters), loss = 0.41894
I0825 14:24:31.837074  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360665 (* 1 = 0.360665 loss)
I0825 14:24:31.837119  2068 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0825 14:26:46.592098  2068 solver.cpp:357] Iteration 7700 (0.742094 iter/s, 134.754s/100 iters), loss = 0.46813
I0825 14:26:46.592319  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.575573 (* 1 = 0.575573 loss)
I0825 14:26:46.592365  2068 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0825 14:28:59.057531  2068 solver.cpp:357] Iteration 7800 (0.754935 iter/s, 132.462s/100 iters), loss = 0.405128
I0825 14:28:59.057698  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495527 (* 1 = 0.495527 loss)
I0825 14:28:59.057730  2068 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0825 14:29:12.851773  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:31:19.009853  2068 solver.cpp:357] Iteration 7900 (0.714525 iter/s, 139.953s/100 iters), loss = 0.40597
I0825 14:31:19.010270  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397571 (* 1 = 0.397571 loss)
I0825 14:31:19.010402  2068 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0825 14:33:25.296780  2068 solver.cpp:514] Iteration 8000, Testing net (#0)
I0825 14:34:47.218641  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:34:47.501441  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7012
I0825 14:34:47.501550  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.9199 (* 1 = 0.9199 loss)
I0825 14:34:48.618248  2068 solver.cpp:357] Iteration 8000 (0.477079 iter/s, 209.609s/100 iters), loss = 0.376537
I0825 14:34:48.618433  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.336364 (* 1 = 0.336364 loss)
I0825 14:34:48.618479  2068 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0825 14:37:02.182746  2068 solver.cpp:357] Iteration 8100 (0.74871 iter/s, 133.563s/100 iters), loss = 0.425699
I0825 14:37:02.182967  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341908 (* 1 = 0.341908 loss)
I0825 14:37:02.183013  2068 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0825 14:39:20.545315  2068 solver.cpp:357] Iteration 8200 (0.722734 iter/s, 138.363s/100 iters), loss = 0.559871
I0825 14:39:20.545538  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.598104 (* 1 = 0.598104 loss)
I0825 14:39:20.545584  2068 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0825 14:39:21.384045  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:41:28.959859  2068 solver.cpp:357] Iteration 8300 (0.778724 iter/s, 128.415s/100 iters), loss = 0.514988
I0825 14:41:28.961446  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.56746 (* 1 = 0.56746 loss)
I0825 14:41:28.961495  2068 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0825 14:43:44.518797  2068 solver.cpp:357] Iteration 8400 (0.737682 iter/s, 135.56s/100 iters), loss = 0.364331
I0825 14:43:44.519426  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.350595 (* 1 = 0.350595 loss)
I0825 14:43:44.519604  2068 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0825 14:45:57.132112  2068 solver.cpp:514] Iteration 8500, Testing net (#0)
I0825 14:47:17.858999  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:47:18.157336  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.749
I0825 14:47:18.157729  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.750906 (* 1 = 0.750906 loss)
I0825 14:47:19.086009  2068 solver.cpp:357] Iteration 8500 (0.466045 iter/s, 214.571s/100 iters), loss = 0.423369
I0825 14:47:19.086431  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.51757 (* 1 = 0.51757 loss)
I0825 14:47:19.086611  2068 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0825 14:49:23.035009  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:49:35.173542  2068 solver.cpp:357] Iteration 8600 (0.734828 iter/s, 136.086s/100 iters), loss = 0.504652
I0825 14:49:35.173728  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499066 (* 1 = 0.499066 loss)
I0825 14:49:35.173774  2068 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0825 14:51:48.733616  2068 solver.cpp:357] Iteration 8700 (0.748734 iter/s, 133.559s/100 iters), loss = 0.36767
I0825 14:51:48.733808  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412689 (* 1 = 0.412689 loss)
I0825 14:51:48.733846  2068 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0825 14:54:02.548419  2068 solver.cpp:357] Iteration 8800 (0.747309 iter/s, 133.813s/100 iters), loss = 0.500371
I0825 14:54:02.548643  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509501 (* 1 = 0.509501 loss)
I0825 14:54:02.548687  2068 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0825 14:56:05.469811  2068 solver.cpp:357] Iteration 8900 (0.813531 iter/s, 122.921s/100 iters), loss = 0.504248
I0825 14:56:05.470353  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.535304 (* 1 = 0.535304 loss)
I0825 14:56:05.470536  2068 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0825 14:57:58.427338  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:58:22.775264  2068 solver.cpp:514] Iteration 9000, Testing net (#0)
I0825 14:59:43.883658  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:59:44.213896  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7239
I0825 14:59:44.214243  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.886645 (* 1 = 0.886645 loss)
I0825 14:59:45.157723  2068 solver.cpp:357] Iteration 9000 (0.455131 iter/s, 219.717s/100 iters), loss = 0.26605
I0825 14:59:45.158146  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.286332 (* 1 = 0.286332 loss)
I0825 14:59:45.158327  2068 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0825 15:02:01.928975  2068 solver.cpp:357] Iteration 9100 (0.731066 iter/s, 136.787s/100 iters), loss = 0.389499
I0825 15:02:01.929359  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327372 (* 1 = 0.327372 loss)
I0825 15:02:01.929451  2068 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0825 15:04:09.142814  2068 solver.cpp:357] Iteration 9200 (0.786005 iter/s, 127.226s/100 iters), loss = 0.45641
I0825 15:04:09.143044  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480451 (* 1 = 0.480451 loss)
I0825 15:04:09.143090  2068 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0825 15:06:24.295193  2068 solver.cpp:357] Iteration 9300 (0.739846 iter/s, 135.163s/100 iters), loss = 0.381678
I0825 15:06:24.295390  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471062 (* 1 = 0.471062 loss)
I0825 15:06:24.295416  2068 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0825 15:08:04.823222  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:08:40.742390  2068 solver.cpp:357] Iteration 9400 (0.732845 iter/s, 136.454s/100 iters), loss = 0.584589
I0825 15:08:40.742691  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380467 (* 1 = 0.380467 loss)
I0825 15:08:40.742738  2068 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0825 15:10:50.380872  2068 solver.cpp:514] Iteration 9500, Testing net (#0)
I0825 15:12:16.090021  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:12:16.452123  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.739
I0825 15:12:16.452284  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.748139 (* 1 = 0.748139 loss)
I0825 15:12:17.550472  2068 solver.cpp:357] Iteration 9500 (0.461212 iter/s, 216.82s/100 iters), loss = 0.459811
I0825 15:12:17.550642  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.489326 (* 1 = 0.489326 loss)
I0825 15:12:17.550686  2068 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0825 15:14:25.887526  2068 solver.cpp:357] Iteration 9600 (0.779172 iter/s, 128.341s/100 iters), loss = 0.513227
I0825 15:14:25.887729  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.602436 (* 1 = 0.602436 loss)
I0825 15:14:25.887774  2068 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0825 15:16:44.059962  2068 solver.cpp:357] Iteration 9700 (0.72371 iter/s, 138.177s/100 iters), loss = 0.377754
I0825 15:16:44.060452  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429144 (* 1 = 0.429144 loss)
I0825 15:16:44.060629  2068 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0825 15:18:01.258684  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:18:51.745676  2068 solver.cpp:357] Iteration 9800 (0.783165 iter/s, 127.687s/100 iters), loss = 0.405485
I0825 15:18:51.746049  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.508087 (* 1 = 0.508087 loss)
I0825 15:18:51.746141  2068 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0825 15:21:10.640516  2068 solver.cpp:357] Iteration 9900 (0.71994 iter/s, 138.901s/100 iters), loss = 0.365377
I0825 15:21:10.640700  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425536 (* 1 = 0.425536 loss)
I0825 15:21:10.640727  2068 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0825 15:23:21.917258  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_10000.caffemodel
I0825 15:23:21.988881  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_10000.solverstate
I0825 15:23:21.999153  2068 solver.cpp:514] Iteration 10000, Testing net (#0)
I0825 15:24:42.592448  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:24:42.948770  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.746401
I0825 15:24:42.948938  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.746471 (* 1 = 0.746471 loss)
I0825 15:24:43.973522  2068 solver.cpp:357] Iteration 10000 (0.468734 iter/s, 213.34s/100 iters), loss = 0.442566
I0825 15:24:43.973695  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394713 (* 1 = 0.394713 loss)
I0825 15:24:43.973740  2068 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0825 15:26:56.510001  2068 solver.cpp:357] Iteration 10100 (0.754506 iter/s, 132.537s/100 iters), loss = 0.401163
I0825 15:26:56.510219  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435765 (* 1 = 0.435765 loss)
I0825 15:26:56.510267  2068 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0825 15:28:08.219020  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:29:12.276182  2068 solver.cpp:357] Iteration 10200 (0.736534 iter/s, 135.771s/100 iters), loss = 0.449402
I0825 15:29:12.276458  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52983 (* 1 = 0.52983 loss)
I0825 15:29:12.276504  2068 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0825 15:31:29.530025  2068 solver.cpp:357] Iteration 10300 (0.728574 iter/s, 137.254s/100 iters), loss = 0.459211
I0825 15:31:29.530232  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.553215 (* 1 = 0.553215 loss)
I0825 15:31:29.530279  2068 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0825 15:33:37.203902  2068 solver.cpp:357] Iteration 10400 (0.783245 iter/s, 127.674s/100 iters), loss = 0.412694
I0825 15:33:37.204207  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480625 (* 1 = 0.480625 loss)
I0825 15:33:37.204253  2068 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0825 15:35:52.001603  2068 solver.cpp:514] Iteration 10500, Testing net (#0)
I0825 15:37:12.099719  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:37:12.387853  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6818
I0825 15:37:12.388098  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.04356 (* 1 = 1.04356 loss)
I0825 15:37:13.514924  2068 solver.cpp:357] Iteration 10500 (0.462283 iter/s, 216.318s/100 iters), loss = 0.471734
I0825 15:37:13.515048  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428127 (* 1 = 0.428127 loss)
I0825 15:37:13.515076  2068 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0825 15:38:15.247392  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:39:30.997239  2068 solver.cpp:357] Iteration 10600 (0.727364 iter/s, 137.483s/100 iters), loss = 0.567963
I0825 15:39:30.997774  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422893 (* 1 = 0.422893 loss)
I0825 15:39:30.997951  2068 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0825 15:41:40.278367  2068 solver.cpp:357] Iteration 10700 (0.773508 iter/s, 129.281s/100 iters), loss = 0.283909
I0825 15:41:40.278707  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.272592 (* 1 = 0.272592 loss)
I0825 15:41:40.278800  2068 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0825 15:43:57.482568  2068 solver.cpp:357] Iteration 10800 (0.728813 iter/s, 137.21s/100 iters), loss = 0.358249
I0825 15:43:57.482759  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.47319 (* 1 = 0.47319 loss)
I0825 15:43:57.482786  2068 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0825 15:46:05.475502  2068 solver.cpp:357] Iteration 10900 (0.781281 iter/s, 127.995s/100 iters), loss = 0.402742
I0825 15:46:05.475694  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478232 (* 1 = 0.478232 loss)
I0825 15:46:05.475721  2068 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0825 15:46:53.159680  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:48:18.483013  2068 solver.cpp:514] Iteration 11000, Testing net (#0)
I0825 15:49:42.375015  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:49:42.720397  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7795
I0825 15:49:42.720548  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.65576 (* 1 = 0.65576 loss)
I0825 15:49:43.689307  2068 solver.cpp:357] Iteration 11000 (0.458414 iter/s, 218.143s/100 iters), loss = 0.3615
I0825 15:49:43.689476  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.354216 (* 1 = 0.354216 loss)
I0825 15:49:43.689522  2068 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0825 15:51:58.011008  2068 solver.cpp:357] Iteration 11100 (0.744708 iter/s, 134.281s/100 iters), loss = 0.368087
I0825 15:51:58.011212  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389178 (* 1 = 0.389178 loss)
I0825 15:51:58.011243  2068 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0825 15:54:11.907665  2068 solver.cpp:357] Iteration 11200 (0.74701 iter/s, 133.867s/100 iters), loss = 0.368917
I0825 15:54:11.907865  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.307262 (* 1 = 0.307262 loss)
I0825 15:54:11.907924  2068 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0825 15:56:17.552913  2068 solver.cpp:357] Iteration 11300 (0.796025 iter/s, 125.624s/100 iters), loss = 0.468631
I0825 15:56:17.553108  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461286 (* 1 = 0.461286 loss)
I0825 15:56:17.553154  2068 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0825 15:56:53.758674  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:58:36.403213  2068 solver.cpp:357] Iteration 11400 (0.720272 iter/s, 138.836s/100 iters), loss = 0.478418
I0825 15:58:36.403441  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394066 (* 1 = 0.394066 loss)
I0825 15:58:36.403486  2068 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0825 16:00:49.050011  2068 solver.cpp:514] Iteration 11500, Testing net (#0)
I0825 16:02:13.021575  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:02:13.375571  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7519
I0825 16:02:13.375751  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.727864 (* 1 = 0.727864 loss)
I0825 16:02:14.390591  2068 solver.cpp:357] Iteration 11500 (0.458778 iter/s, 217.97s/100 iters), loss = 0.450783
I0825 16:02:14.390774  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463096 (* 1 = 0.463096 loss)
I0825 16:02:14.390822  2068 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0825 16:04:22.580688  2068 solver.cpp:357] Iteration 11600 (0.780155 iter/s, 128.18s/100 iters), loss = 0.4063
I0825 16:04:22.581084  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403097 (* 1 = 0.403097 loss)
I0825 16:04:22.581178  2068 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0825 16:06:40.892091  2068 solver.cpp:357] Iteration 11700 (0.722893 iter/s, 138.333s/100 iters), loss = 0.492391
I0825 16:06:40.892623  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.277848 (* 1 = 0.277848 loss)
I0825 16:06:40.892802  2068 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0825 16:07:02.651046  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:08:53.146704  2068 solver.cpp:357] Iteration 11800 (0.756059 iter/s, 132.265s/100 iters), loss = 0.612275
I0825 16:08:53.146952  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544751 (* 1 = 0.544751 loss)
I0825 16:08:53.146999  2068 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0825 16:11:08.432607  2068 solver.cpp:357] Iteration 11900 (0.739162 iter/s, 135.288s/100 iters), loss = 0.453646
I0825 16:11:08.432808  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.46671 (* 1 = 0.46671 loss)
I0825 16:11:08.432840  2068 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0825 16:13:20.075074  2068 solver.cpp:514] Iteration 12000, Testing net (#0)
I0825 16:14:42.268290  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:14:42.588351  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7165
I0825 16:14:42.588708  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.894446 (* 1 = 0.894446 loss)
I0825 16:14:43.686668  2068 solver.cpp:357] Iteration 12000 (0.464562 iter/s, 215.257s/100 iters), loss = 0.475991
I0825 16:14:43.687041  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450945 (* 1 = 0.450945 loss)
I0825 16:14:43.687211  2068 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0825 16:17:02.482918  2068 solver.cpp:357] Iteration 12100 (0.720472 iter/s, 138.798s/100 iters), loss = 0.421979
I0825 16:17:02.483147  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445703 (* 1 = 0.445703 loss)
I0825 16:17:02.483196  2068 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0825 16:17:09.643651  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:19:05.506929  2068 solver.cpp:357] Iteration 12200 (0.812871 iter/s, 123.021s/100 iters), loss = 0.411097
I0825 16:19:05.507129  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496002 (* 1 = 0.496002 loss)
I0825 16:19:05.507172  2068 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0825 16:21:25.686825  2068 solver.cpp:357] Iteration 12300 (0.713399 iter/s, 140.174s/100 iters), loss = 0.352929
I0825 16:21:25.687237  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.309623 (* 1 = 0.309623 loss)
I0825 16:21:25.687331  2068 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0825 16:23:37.817030  2068 solver.cpp:357] Iteration 12400 (0.75686 iter/s, 132.125s/100 iters), loss = 0.465578
I0825 16:23:37.817284  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474952 (* 1 = 0.474952 loss)
I0825 16:23:37.817318  2068 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0825 16:25:44.295680  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:25:46.941568  2068 solver.cpp:514] Iteration 12500, Testing net (#0)
I0825 16:27:06.823442  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:27:07.217908  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6849
I0825 16:27:07.218015  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.01728 (* 1 = 1.01728 loss)
I0825 16:27:08.271015  2068 solver.cpp:357] Iteration 12500 (0.475167 iter/s, 210.452s/100 iters), loss = 0.568982
I0825 16:27:08.271198  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.615413 (* 1 = 0.615413 loss)
I0825 16:27:08.271245  2068 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0825 16:29:26.793061  2068 solver.cpp:357] Iteration 12600 (0.721927 iter/s, 138.518s/100 iters), loss = 0.526543
I0825 16:29:26.793287  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373538 (* 1 = 0.373538 loss)
I0825 16:29:26.793315  2068 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0825 16:31:41.610790  2068 solver.cpp:357] Iteration 12700 (0.741776 iter/s, 134.812s/100 iters), loss = 0.50183
I0825 16:31:41.610983  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437425 (* 1 = 0.437425 loss)
I0825 16:31:41.611011  2068 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0825 16:33:50.822799  2068 solver.cpp:357] Iteration 12800 (0.773959 iter/s, 129.206s/100 iters), loss = 0.410792
I0825 16:33:50.823014  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472472 (* 1 = 0.472472 loss)
I0825 16:33:50.823060  2068 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0825 16:35:47.158674  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:36:02.823935  2068 solver.cpp:357] Iteration 12900 (0.757593 iter/s, 131.997s/100 iters), loss = 0.52559
I0825 16:36:02.824113  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397159 (* 1 = 0.397159 loss)
I0825 16:36:02.824158  2068 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0825 16:38:19.962301  2068 solver.cpp:514] Iteration 13000, Testing net (#0)
I0825 16:39:43.862112  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:39:44.193585  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7056
I0825 16:39:44.193703  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.977454 (* 1 = 0.977454 loss)
I0825 16:39:45.259536  2068 solver.cpp:357] Iteration 13000 (0.449577 iter/s, 222.432s/100 iters), loss = 0.32121
I0825 16:39:45.259706  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366979 (* 1 = 0.366979 loss)
I0825 16:39:45.259752  2068 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0825 16:41:57.097463  2068 solver.cpp:357] Iteration 13100 (0.758531 iter/s, 131.834s/100 iters), loss = 0.392917
I0825 16:41:57.097651  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458985 (* 1 = 0.458985 loss)
I0825 16:41:57.097677  2068 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0825 16:44:11.472801  2068 solver.cpp:357] Iteration 13200 (0.744196 iter/s, 134.373s/100 iters), loss = 0.337555
I0825 16:44:11.473033  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379819 (* 1 = 0.379819 loss)
I0825 16:44:11.473079  2068 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0825 16:45:54.375044  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:46:24.589685  2068 solver.cpp:357] Iteration 13300 (0.751244 iter/s, 133.113s/100 iters), loss = 0.460169
I0825 16:46:24.589905  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417512 (* 1 = 0.417512 loss)
I0825 16:46:24.589951  2068 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0825 16:48:40.238363  2068 solver.cpp:357] Iteration 13400 (0.737233 iter/s, 135.642s/100 iters), loss = 0.516089
I0825 16:48:40.238672  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404674 (* 1 = 0.404674 loss)
I0825 16:48:40.238719  2068 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0825 16:50:53.770848  2068 solver.cpp:514] Iteration 13500, Testing net (#0)
I0825 16:52:18.565187  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:52:18.805480  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5874
I0825 16:52:18.805608  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.46332 (* 1 = 1.46332 loss)
I0825 16:52:19.924104  2068 solver.cpp:357] Iteration 13500 (0.455209 iter/s, 219.679s/100 iters), loss = 0.492379
I0825 16:52:19.924283  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390101 (* 1 = 0.390101 loss)
I0825 16:52:19.924329  2068 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0825 16:54:31.330765  2068 solver.cpp:357] Iteration 13600 (0.761021 iter/s, 131.402s/100 iters), loss = 0.367026
I0825 16:54:31.332649  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.307274 (* 1 = 0.307274 loss)
I0825 16:54:31.332753  2068 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0825 16:55:59.585052  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:56:41.667860  2068 solver.cpp:357] Iteration 13700 (0.767193 iter/s, 130.345s/100 iters), loss = 0.515338
I0825 16:56:41.668066  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475419 (* 1 = 0.475419 loss)
I0825 16:56:41.668110  2068 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0825 16:58:56.357163  2068 solver.cpp:357] Iteration 13800 (0.74236 iter/s, 134.706s/100 iters), loss = 0.352052
I0825 16:58:56.357393  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419379 (* 1 = 0.419379 loss)
I0825 16:58:56.357439  2068 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0825 17:01:14.726003  2068 solver.cpp:357] Iteration 13900 (0.72262 iter/s, 138.385s/100 iters), loss = 0.265426
I0825 17:01:14.726503  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.188901 (* 1 = 0.188901 loss)
I0825 17:01:14.726552  2068 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0825 17:03:23.214521  2068 solver.cpp:514] Iteration 14000, Testing net (#0)
I0825 17:04:44.931356  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:04:45.205837  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5926
I0825 17:04:45.205991  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.43173 (* 1 = 1.43173 loss)
I0825 17:04:46.268802  2068 solver.cpp:357] Iteration 14000 (0.472683 iter/s, 211.558s/100 iters), loss = 0.391328
I0825 17:04:46.268982  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491193 (* 1 = 0.491193 loss)
I0825 17:04:46.269027  2068 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0825 17:06:09.203667  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:07:05.276686  2068 solver.cpp:357] Iteration 14100 (0.71936 iter/s, 139.012s/100 iters), loss = 0.333415
I0825 17:07:05.278975  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.311892 (* 1 = 0.311892 loss)
I0825 17:07:05.279156  2068 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0825 17:09:18.272835  2068 solver.cpp:357] Iteration 14200 (0.751865 iter/s, 133.003s/100 iters), loss = 0.426323
I0825 17:09:18.273048  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394578 (* 1 = 0.394578 loss)
I0825 17:09:18.273093  2068 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0825 17:11:34.198554  2068 solver.cpp:357] Iteration 14300 (0.735678 iter/s, 135.929s/100 iters), loss = 0.412785
I0825 17:11:34.198756  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396942 (* 1 = 0.396942 loss)
I0825 17:11:34.198782  2068 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0825 17:13:49.033634  2068 solver.cpp:357] Iteration 14400 (0.741634 iter/s, 134.837s/100 iters), loss = 0.306203
I0825 17:13:49.033876  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.245982 (* 1 = 0.245982 loss)
I0825 17:13:49.033923  2068 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0825 17:14:54.094696  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:16:02.052145  2068 solver.cpp:514] Iteration 14500, Testing net (#0)
I0825 17:17:19.896610  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:17:20.243662  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7334
I0825 17:17:20.243821  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.858322 (* 1 = 0.858322 loss)
I0825 17:17:21.255743  2068 solver.cpp:357] Iteration 14500 (0.471199 iter/s, 212.225s/100 iters), loss = 0.357358
I0825 17:17:21.255911  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.26217 (* 1 = 0.26217 loss)
I0825 17:17:21.255955  2068 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0825 17:19:38.951066  2068 solver.cpp:357] Iteration 14600 (0.726226 iter/s, 137.698s/100 iters), loss = 0.3998
I0825 17:19:38.951593  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409984 (* 1 = 0.409984 loss)
I0825 17:19:38.951771  2068 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0825 17:21:51.008150  2068 solver.cpp:357] Iteration 14700 (0.757248 iter/s, 132.057s/100 iters), loss = 0.43357
I0825 17:21:51.008358  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38812 (* 1 = 0.38812 loss)
I0825 17:21:51.008405  2068 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0825 17:24:10.786586  2068 solver.cpp:357] Iteration 14800 (0.715418 iter/s, 139.778s/100 iters), loss = 0.344773
I0825 17:24:10.786797  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.263855 (* 1 = 0.263855 loss)
I0825 17:24:10.786830  2068 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0825 17:24:58.907047  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:26:14.815443  2068 solver.cpp:357] Iteration 14900 (0.80626 iter/s, 124.029s/100 iters), loss = 0.277108
I0825 17:26:14.815682  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358673 (* 1 = 0.358673 loss)
I0825 17:26:14.815727  2068 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0825 17:28:31.772699  2068 solver.cpp:514] Iteration 15000, Testing net (#0)
I0825 17:29:56.358870  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:29:56.655068  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.752899
I0825 17:29:56.655597  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.701615 (* 1 = 0.701615 loss)
I0825 17:29:57.526064  2068 solver.cpp:357] Iteration 15000 (0.449009 iter/s, 222.712s/100 iters), loss = 0.437428
I0825 17:29:57.526484  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.34374 (* 1 = 0.34374 loss)
I0825 17:29:57.526661  2068 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0825 17:32:09.702488  2068 solver.cpp:357] Iteration 15100 (0.756603 iter/s, 132.17s/100 iters), loss = 0.422028
I0825 17:32:09.702695  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390248 (* 1 = 0.390248 loss)
I0825 17:32:09.702726  2068 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0825 17:34:25.028627  2068 solver.cpp:357] Iteration 15200 (0.739017 iter/s, 135.315s/100 iters), loss = 0.502798
I0825 17:34:25.028851  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.427079 (* 1 = 0.427079 loss)
I0825 17:34:25.028897  2068 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0825 17:35:00.138005  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:36:32.716320  2068 solver.cpp:357] Iteration 15300 (0.783233 iter/s, 127.676s/100 iters), loss = 0.368302
I0825 17:36:32.716693  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390481 (* 1 = 0.390481 loss)
I0825 17:36:32.716785  2068 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0825 17:38:50.332645  2068 solver.cpp:357] Iteration 15400 (0.726707 iter/s, 137.607s/100 iters), loss = 0.446335
I0825 17:38:50.332803  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438928 (* 1 = 0.438928 loss)
I0825 17:38:50.332847  2068 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0825 17:40:58.146765  2068 solver.cpp:514] Iteration 15500, Testing net (#0)
I0825 17:42:23.533133  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:42:23.796279  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6782
I0825 17:42:23.796391  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.989587 (* 1 = 0.989587 loss)
I0825 17:42:24.957218  2068 solver.cpp:357] Iteration 15500 (0.46595 iter/s, 214.615s/100 iters), loss = 0.357271
I0825 17:42:24.957388  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389343 (* 1 = 0.389343 loss)
I0825 17:42:24.957432  2068 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0825 17:44:35.952101  2068 solver.cpp:357] Iteration 15600 (0.763425 iter/s, 130.989s/100 iters), loss = 0.43384
I0825 17:44:35.952316  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.541018 (* 1 = 0.541018 loss)
I0825 17:44:35.952361  2068 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0825 17:45:06.079679  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:46:54.162704  2068 solver.cpp:357] Iteration 15700 (0.723574 iter/s, 138.203s/100 iters), loss = 0.496765
I0825 17:46:54.162907  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32835 (* 1 = 0.32835 loss)
I0825 17:46:54.162953  2068 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0825 17:49:03.408344  2068 solver.cpp:357] Iteration 15800 (0.773751 iter/s, 129.241s/100 iters), loss = 0.421716
I0825 17:49:03.408694  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.542411 (* 1 = 0.542411 loss)
I0825 17:49:03.408784  2068 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0825 17:51:21.784497  2068 solver.cpp:357] Iteration 15900 (0.722672 iter/s, 138.375s/100 iters), loss = 0.330548
I0825 17:51:21.784742  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330161 (* 1 = 0.330161 loss)
I0825 17:51:21.784788  2068 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0825 17:53:35.473516  2068 solver.cpp:514] Iteration 16000, Testing net (#0)
I0825 17:54:56.655738  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:54:56.981106  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6073
I0825 17:54:56.981246  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.24885 (* 1 = 1.24885 loss)
I0825 17:54:57.982439  2068 solver.cpp:357] Iteration 16000 (0.462548 iter/s, 216.194s/100 iters), loss = 0.465747
I0825 17:54:57.982669  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.492118 (* 1 = 0.492118 loss)
I0825 17:54:57.982717  2068 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0825 17:55:14.842716  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:57:14.589123  2068 solver.cpp:357] Iteration 16100 (0.732029 iter/s, 136.607s/100 iters), loss = 0.475266
I0825 17:57:14.589329  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496796 (* 1 = 0.496796 loss)
I0825 17:57:14.589375  2068 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0825 17:59:29.288451  2068 solver.cpp:357] Iteration 16200 (0.742404 iter/s, 134.698s/100 iters), loss = 0.378206
I0825 17:59:29.288601  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335108 (* 1 = 0.335108 loss)
I0825 17:59:29.288630  2068 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0825 18:01:47.906602  2068 solver.cpp:357] Iteration 16300 (0.721426 iter/s, 138.614s/100 iters), loss = 0.299724
I0825 18:01:47.907001  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410168 (* 1 = 0.410168 loss)
I0825 18:01:47.907124  2068 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0825 18:03:58.233130  2068 solver.cpp:357] Iteration 16400 (0.767313 iter/s, 130.325s/100 iters), loss = 0.383459
I0825 18:03:58.233399  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483361 (* 1 = 0.483361 loss)
I0825 18:03:58.233433  2068 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0825 18:04:03.443022  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:06:11.174823  2068 solver.cpp:514] Iteration 16500, Testing net (#0)
I0825 18:07:30.418401  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:07:30.760605  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7682
I0825 18:07:30.760767  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.682434 (* 1 = 0.682434 loss)
I0825 18:07:31.875000  2068 solver.cpp:357] Iteration 16500 (0.468077 iter/s, 213.64s/100 iters), loss = 0.395181
I0825 18:07:31.875167  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352534 (* 1 = 0.352534 loss)
I0825 18:07:31.875212  2068 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0825 18:09:50.065815  2068 solver.cpp:357] Iteration 16600 (0.723639 iter/s, 138.19s/100 iters), loss = 0.373366
I0825 18:09:50.066056  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.411659 (* 1 = 0.411659 loss)
I0825 18:09:50.066102  2068 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0825 18:12:01.377164  2068 solver.cpp:357] Iteration 16700 (0.761565 iter/s, 131.309s/100 iters), loss = 0.404019
I0825 18:12:01.377389  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.287512 (* 1 = 0.287512 loss)
I0825 18:12:01.377435  2068 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0825 18:14:12.177995  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:14:19.815656  2068 solver.cpp:357] Iteration 16800 (0.722368 iter/s, 138.434s/100 iters), loss = 0.435242
I0825 18:14:19.815837  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398511 (* 1 = 0.398511 loss)
I0825 18:14:19.815883  2068 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0825 18:16:30.208837  2068 solver.cpp:357] Iteration 16900 (0.766941 iter/s, 130.388s/100 iters), loss = 0.420674
I0825 18:16:30.209059  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415727 (* 1 = 0.415727 loss)
I0825 18:16:30.209105  2068 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0825 18:18:43.461110  2068 solver.cpp:514] Iteration 17000, Testing net (#0)
I0825 18:20:08.257448  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:20:08.557668  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7027
I0825 18:20:08.557824  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.927535 (* 1 = 0.927535 loss)
I0825 18:20:09.553951  2068 solver.cpp:357] Iteration 17000 (0.455903 iter/s, 219.345s/100 iters), loss = 0.40575
I0825 18:20:09.554128  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260034 (* 1 = 0.260034 loss)
I0825 18:20:09.554172  2068 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0825 18:22:24.111443  2068 solver.cpp:357] Iteration 17100 (0.743205 iter/s, 134.552s/100 iters), loss = 0.437533
I0825 18:22:24.111831  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412998 (* 1 = 0.412998 loss)
I0825 18:22:24.111950  2068 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0825 18:24:21.751672  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:24:42.032642  2068 solver.cpp:357] Iteration 17200 (0.725048 iter/s, 137.922s/100 iters), loss = 0.317364
I0825 18:24:42.032814  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340162 (* 1 = 0.340162 loss)
I0825 18:24:42.032860  2068 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0825 18:26:50.321789  2068 solver.cpp:357] Iteration 17300 (0.779508 iter/s, 128.286s/100 iters), loss = 0.320095
I0825 18:26:50.321995  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3101 (* 1 = 0.3101 loss)
I0825 18:26:50.322055  2068 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0825 18:29:08.643568  2068 solver.cpp:357] Iteration 17400 (0.722968 iter/s, 138.319s/100 iters), loss = 0.34322
I0825 18:29:08.643808  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.223095 (* 1 = 0.223095 loss)
I0825 18:29:08.643856  2068 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0825 18:31:21.678823  2068 solver.cpp:514] Iteration 17500, Testing net (#0)
I0825 18:32:44.515161  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:32:44.859027  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5667
I0825 18:32:44.859192  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.57563 (* 1 = 1.57563 loss)
I0825 18:32:45.879817  2068 solver.cpp:357] Iteration 17500 (0.460333 iter/s, 217.234s/100 iters), loss = 0.254994
I0825 18:32:45.879956  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.244529 (* 1 = 0.244529 loss)
I0825 18:32:45.879995  2068 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0825 18:34:25.954113  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:34:55.924850  2068 solver.cpp:357] Iteration 17600 (0.768996 iter/s, 130.04s/100 iters), loss = 0.349347
I0825 18:34:55.924998  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.182738 (* 1 = 0.182738 loss)
I0825 18:34:55.925024  2068 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0825 18:37:13.571224  2068 solver.cpp:357] Iteration 17700 (0.726516 iter/s, 137.643s/100 iters), loss = 0.426043
I0825 18:37:13.571449  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.501409 (* 1 = 0.501409 loss)
I0825 18:37:13.571493  2068 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0825 18:39:25.993417  2068 solver.cpp:357] Iteration 17800 (0.755167 iter/s, 132.421s/100 iters), loss = 0.396591
I0825 18:39:25.993597  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38816 (* 1 = 0.38816 loss)
I0825 18:39:25.993624  2068 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0825 18:41:39.762140  2068 solver.cpp:357] Iteration 17900 (0.747489 iter/s, 133.781s/100 iters), loss = 0.240196
I0825 18:41:39.762552  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.255873 (* 1 = 0.255873 loss)
I0825 18:41:39.762645  2068 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0825 18:43:08.957365  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:43:50.989491  2068 solver.cpp:514] Iteration 18000, Testing net (#0)
I0825 18:45:13.513772  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:45:13.792544  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.544
I0825 18:45:13.792723  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.97811 (* 1 = 1.97811 loss)
I0825 18:45:14.850124  2068 solver.cpp:357] Iteration 18000 (0.464888 iter/s, 215.105s/100 iters), loss = 0.39569
I0825 18:45:14.850294  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.473647 (* 1 = 0.473647 loss)
I0825 18:45:14.850344  2068 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0825 18:47:28.446305  2068 solver.cpp:357] Iteration 18100 (0.748502 iter/s, 133.6s/100 iters), loss = 0.410613
I0825 18:47:28.446789  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334766 (* 1 = 0.334766 loss)
I0825 18:47:28.446969  2068 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0825 18:49:40.378866  2068 solver.cpp:357] Iteration 18200 (0.757934 iter/s, 131.938s/100 iters), loss = 0.367715
I0825 18:49:40.379045  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.273937 (* 1 = 0.273937 loss)
I0825 18:49:40.379072  2068 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0825 18:51:58.813705  2068 solver.cpp:357] Iteration 18300 (0.722355 iter/s, 138.436s/100 iters), loss = 0.3298
I0825 18:51:58.813938  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394008 (* 1 = 0.394008 loss)
I0825 18:51:58.813984  2068 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0825 18:53:11.720747  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:54:12.328397  2068 solver.cpp:357] Iteration 18400 (0.748969 iter/s, 133.517s/100 iters), loss = 0.527165
I0825 18:54:12.328919  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334367 (* 1 = 0.334367 loss)
I0825 18:54:12.329095  2068 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0825 18:56:19.778781  2068 solver.cpp:514] Iteration 18500, Testing net (#0)
I0825 18:57:39.492560  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:57:39.826638  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7652
I0825 18:57:39.826807  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.725799 (* 1 = 0.725799 loss)
I0825 18:57:40.888134  2068 solver.cpp:357] Iteration 18500 (0.479469 iter/s, 208.564s/100 iters), loss = 0.383995
I0825 18:57:40.888305  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.322087 (* 1 = 0.322087 loss)
I0825 18:57:40.888350  2068 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0825 18:59:59.730674  2068 solver.cpp:357] Iteration 18600 (0.720237 iter/s, 138.843s/100 iters), loss = 0.428492
I0825 18:59:59.730912  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.460341 (* 1 = 0.460341 loss)
I0825 18:59:59.730958  2068 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0825 19:02:11.796346  2068 solver.cpp:357] Iteration 18700 (0.757196 iter/s, 132.066s/100 iters), loss = 0.324441
I0825 19:02:11.796871  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.275867 (* 1 = 0.275867 loss)
I0825 19:02:11.797049  2068 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0825 19:03:14.495676  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:04:27.511468  2068 solver.cpp:357] Iteration 18800 (0.736827 iter/s, 135.717s/100 iters), loss = 0.309667
I0825 19:04:27.511657  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.316452 (* 1 = 0.316452 loss)
I0825 19:04:27.511689  2068 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0825 19:06:33.128760  2068 solver.cpp:357] Iteration 18900 (0.796075 iter/s, 125.616s/100 iters), loss = 0.541849
I0825 19:06:33.128985  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.723591 (* 1 = 0.723591 loss)
I0825 19:06:33.129031  2068 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0825 19:08:49.233883  2068 solver.cpp:514] Iteration 19000, Testing net (#0)
I0825 19:10:11.849081  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:10:12.079497  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.789601
I0825 19:10:12.079620  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.648439 (* 1 = 0.648439 loss)
I0825 19:10:13.040444  2068 solver.cpp:357] Iteration 19000 (0.454724 iter/s, 219.913s/100 iters), loss = 0.401074
I0825 19:10:13.040603  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428246 (* 1 = 0.428246 loss)
I0825 19:10:13.040648  2068 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0825 19:12:26.761831  2068 solver.cpp:357] Iteration 19100 (0.747805 iter/s, 133.725s/100 iters), loss = 0.409202
I0825 19:12:26.762233  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.329431 (* 1 = 0.329431 loss)
I0825 19:12:26.762326  2068 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0825 19:13:20.418699  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:14:45.443939  2068 solver.cpp:357] Iteration 19200 (0.721111 iter/s, 138.675s/100 iters), loss = 0.414569
I0825 19:14:45.444136  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.43579 (* 1 = 0.43579 loss)
I0825 19:14:45.444162  2068 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0825 19:16:50.619827  2068 solver.cpp:357] Iteration 19300 (0.798938 iter/s, 125.166s/100 iters), loss = 0.336534
I0825 19:16:50.621490  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389498 (* 1 = 0.389498 loss)
I0825 19:16:50.621538  2068 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0825 19:19:03.821656  2068 solver.cpp:357] Iteration 19400 (0.750801 iter/s, 133.191s/100 iters), loss = 0.467049
I0825 19:19:03.821835  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388615 (* 1 = 0.388615 loss)
I0825 19:19:03.821861  2068 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0825 19:21:14.784337  2068 solver.cpp:514] Iteration 19500, Testing net (#0)
I0825 19:22:39.698259  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:22:40.030253  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.734201
I0825 19:22:40.030426  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.79015 (* 1 = 0.79015 loss)
I0825 19:22:41.177861  2068 solver.cpp:357] Iteration 19500 (0.460097 iter/s, 217.345s/100 iters), loss = 0.401106
I0825 19:22:41.178036  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.506192 (* 1 = 0.506192 loss)
I0825 19:22:41.178082  2068 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0825 19:23:21.439046  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:24:49.655791  2068 solver.cpp:357] Iteration 19600 (0.778385 iter/s, 128.471s/100 iters), loss = 0.382784
I0825 19:24:49.656317  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401148 (* 1 = 0.401148 loss)
I0825 19:24:49.656500  2068 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0825 19:27:02.165886  2068 solver.cpp:357] Iteration 19700 (0.754694 iter/s, 132.504s/100 iters), loss = 0.48308
I0825 19:27:02.166115  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.455345 (* 1 = 0.455345 loss)
I0825 19:27:02.166160  2068 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0825 19:29:14.048620  2068 solver.cpp:357] Iteration 19800 (0.758281 iter/s, 131.877s/100 iters), loss = 0.309704
I0825 19:29:14.050226  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360898 (* 1 = 0.360898 loss)
I0825 19:29:14.050274  2068 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0825 19:31:32.515394  2068 solver.cpp:357] Iteration 19900 (0.722232 iter/s, 138.46s/100 iters), loss = 0.447452
I0825 19:31:32.515833  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487658 (* 1 = 0.487658 loss)
I0825 19:31:32.516007  2068 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0825 19:31:59.319416  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:33:40.065690  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_20000.caffemodel
I0825 19:33:40.125593  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_20000.solverstate
I0825 19:33:40.136288  2068 solver.cpp:514] Iteration 20000, Testing net (#0)
I0825 19:35:03.320403  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:35:03.661778  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7645
I0825 19:35:03.661942  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.713875 (* 1 = 0.713875 loss)
I0825 19:35:04.661644  2068 solver.cpp:357] Iteration 20000 (0.471374 iter/s, 212.146s/100 iters), loss = 0.382728
I0825 19:35:04.661825  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400694 (* 1 = 0.400694 loss)
I0825 19:35:04.661872  2068 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0825 19:37:19.916646  2068 solver.cpp:357] Iteration 20100 (0.739356 iter/s, 135.253s/100 iters), loss = 0.303406
I0825 19:37:19.916872  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.263892 (* 1 = 0.263892 loss)
I0825 19:37:19.916918  2068 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0825 19:39:34.228088  2068 solver.cpp:357] Iteration 20200 (0.744549 iter/s, 134.31s/100 iters), loss = 0.369564
I0825 19:39:34.229640  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471752 (* 1 = 0.471752 loss)
I0825 19:39:34.229666  2068 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0825 19:41:48.137423  2068 solver.cpp:357] Iteration 20300 (0.746787 iter/s, 133.907s/100 iters), loss = 0.354761
I0825 19:41:48.137940  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4151 (* 1 = 0.4151 loss)
I0825 19:41:48.138146  2068 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0825 19:42:01.791676  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:44:02.785868  2068 solver.cpp:357] Iteration 20400 (0.742695 iter/s, 134.645s/100 iters), loss = 0.436638
I0825 19:44:02.786097  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487846 (* 1 = 0.487846 loss)
I0825 19:44:02.786144  2068 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0825 19:46:15.660631  2068 solver.cpp:514] Iteration 20500, Testing net (#0)
I0825 19:47:33.900382  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:47:34.186075  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7298
I0825 19:47:34.186233  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.931899 (* 1 = 0.931899 loss)
I0825 19:47:35.247022  2068 solver.cpp:357] Iteration 20500 (0.470686 iter/s, 212.456s/100 iters), loss = 0.440513
I0825 19:47:35.247200  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.47116 (* 1 = 0.47116 loss)
I0825 19:47:35.247246  2068 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0825 19:49:50.557718  2068 solver.cpp:357] Iteration 20600 (0.739024 iter/s, 135.314s/100 iters), loss = 0.418491
I0825 19:49:50.557961  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490569 (* 1 = 0.490569 loss)
I0825 19:49:50.557993  2068 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0825 19:52:05.517220  2068 solver.cpp:357] Iteration 20700 (0.74094 iter/s, 134.964s/100 iters), loss = 0.417937
I0825 19:52:05.517431  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477819 (* 1 = 0.477819 loss)
I0825 19:52:05.517477  2068 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0825 19:52:06.462685  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:54:24.461632  2068 solver.cpp:357] Iteration 20800 (0.719709 iter/s, 138.945s/100 iters), loss = 0.354686
I0825 19:54:24.461827  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398873 (* 1 = 0.398873 loss)
I0825 19:54:24.461859  2068 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0825 19:56:29.429394  2068 solver.cpp:357] Iteration 20900 (0.8002 iter/s, 124.969s/100 iters), loss = 0.35309
I0825 19:56:29.430958  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461061 (* 1 = 0.461061 loss)
I0825 19:56:29.431007  2068 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0825 19:58:47.579272  2068 solver.cpp:514] Iteration 21000, Testing net (#0)
I0825 20:00:12.749585  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:00:13.114540  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7404
I0825 20:00:13.114697  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.820773 (* 1 = 0.820773 loss)
I0825 20:00:14.206780  2068 solver.cpp:357] Iteration 21000 (0.444873 iter/s, 224.783s/100 iters), loss = 0.34391
I0825 20:00:14.207196  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430131 (* 1 = 0.430131 loss)
I0825 20:00:14.207373  2068 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0825 20:02:13.103024  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:02:24.604032  2068 solver.cpp:357] Iteration 21100 (0.76689 iter/s, 130.397s/100 iters), loss = 0.356144
I0825 20:02:24.604203  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40624 (* 1 = 0.40624 loss)
I0825 20:02:24.604250  2068 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0825 20:04:41.846747  2068 solver.cpp:357] Iteration 21200 (0.728629 iter/s, 137.244s/100 iters), loss = 0.368714
I0825 20:04:41.846945  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444984 (* 1 = 0.444984 loss)
I0825 20:04:41.846978  2068 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0825 20:06:48.230836  2068 solver.cpp:357] Iteration 21300 (0.79126 iter/s, 126.381s/100 iters), loss = 0.343439
I0825 20:06:48.231071  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313179 (* 1 = 0.313179 loss)
I0825 20:06:48.231134  2068 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0825 20:09:07.198673  2068 solver.cpp:357] Iteration 21400 (0.719586 iter/s, 138.969s/100 iters), loss = 0.497537
I0825 20:09:07.202493  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.517238 (* 1 = 0.517238 loss)
I0825 20:09:07.202525  2068 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0825 20:10:51.339341  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:11:15.829759  2068 solver.cpp:514] Iteration 21500, Testing net (#0)
I0825 20:12:41.194995  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:12:41.474522  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7671
I0825 20:12:41.474643  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.681083 (* 1 = 0.681083 loss)
I0825 20:12:42.622371  2068 solver.cpp:357] Iteration 21500 (0.464201 iter/s, 215.424s/100 iters), loss = 0.250543
I0825 20:12:42.622550  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29342 (* 1 = 0.29342 loss)
I0825 20:12:42.622598  2068 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0825 20:14:56.993588  2068 solver.cpp:357] Iteration 21600 (0.744217 iter/s, 134.369s/100 iters), loss = 0.439377
I0825 20:14:56.993811  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464778 (* 1 = 0.464778 loss)
I0825 20:14:56.993856  2068 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0825 20:17:09.898396  2068 solver.cpp:357] Iteration 21700 (0.752413 iter/s, 132.906s/100 iters), loss = 0.472197
I0825 20:17:09.898560  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41369 (* 1 = 0.41369 loss)
I0825 20:17:09.898586  2068 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0825 20:19:20.988385  2068 solver.cpp:357] Iteration 21800 (0.762857 iter/s, 131.086s/100 iters), loss = 0.27511
I0825 20:19:20.988610  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366488 (* 1 = 0.366488 loss)
I0825 20:19:20.988654  2068 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0825 20:21:01.891733  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:21:41.207891  2068 solver.cpp:357] Iteration 21900 (0.713188 iter/s, 140.215s/100 iters), loss = 0.481978
I0825 20:21:41.208109  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313776 (* 1 = 0.313776 loss)
I0825 20:21:41.208154  2068 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0825 20:23:52.927724  2068 solver.cpp:514] Iteration 22000, Testing net (#0)
I0825 20:25:13.827252  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:25:14.070248  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5974
I0825 20:25:14.070629  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32772 (* 1 = 1.32772 loss)
I0825 20:25:14.957113  2068 solver.cpp:357] Iteration 22000 (0.467823 iter/s, 213.756s/100 iters), loss = 0.375305
I0825 20:25:14.957478  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486775 (* 1 = 0.486775 loss)
I0825 20:25:14.957649  2068 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0825 20:27:27.809276  2068 solver.cpp:357] Iteration 22100 (0.752684 iter/s, 132.858s/100 iters), loss = 0.43173
I0825 20:27:27.810843  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.609829 (* 1 = 0.609829 loss)
I0825 20:27:27.810876  2068 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0825 20:29:42.408143  2068 solver.cpp:357] Iteration 22200 (0.742931 iter/s, 134.602s/100 iters), loss = 0.371952
I0825 20:29:42.408295  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.406711 (* 1 = 0.406711 loss)
I0825 20:29:42.408321  2068 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0825 20:31:09.958681  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:32:00.363862  2068 solver.cpp:357] Iteration 22300 (0.724867 iter/s, 137.956s/100 iters), loss = 0.396956
I0825 20:32:00.364073  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.506133 (* 1 = 0.506133 loss)
I0825 20:32:00.364135  2068 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0825 20:34:10.999410  2068 solver.cpp:357] Iteration 22400 (0.765489 iter/s, 130.636s/100 iters), loss = 0.30079
I0825 20:34:10.999624  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399542 (* 1 = 0.399542 loss)
I0825 20:34:10.999656  2068 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0825 20:36:24.399175  2068 solver.cpp:514] Iteration 22500, Testing net (#0)
I0825 20:37:44.026783  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:37:44.408695  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7143
I0825 20:37:44.408844  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.915017 (* 1 = 0.915017 loss)
I0825 20:37:45.465440  2068 solver.cpp:357] Iteration 22500 (0.46627 iter/s, 214.468s/100 iters), loss = 0.304565
I0825 20:37:45.465629  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.311475 (* 1 = 0.311475 loss)
I0825 20:37:45.465674  2068 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0825 20:40:03.177804  2068 solver.cpp:357] Iteration 22600 (0.726165 iter/s, 137.71s/100 iters), loss = 0.577143
I0825 20:40:03.178351  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48604 (* 1 = 0.48604 loss)
I0825 20:40:03.178548  2068 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0825 20:41:14.463587  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:42:11.704952  2068 solver.cpp:357] Iteration 22700 (0.778063 iter/s, 128.524s/100 iters), loss = 0.352277
I0825 20:42:11.705159  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.314081 (* 1 = 0.314081 loss)
I0825 20:42:11.705204  2068 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0825 20:44:31.397999  2068 solver.cpp:357] Iteration 22800 (0.715851 iter/s, 139.694s/100 iters), loss = 0.281015
I0825 20:44:31.398200  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353389 (* 1 = 0.353389 loss)
I0825 20:44:31.398244  2068 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0825 20:46:40.452976  2068 solver.cpp:357] Iteration 22900 (0.77487 iter/s, 129.054s/100 iters), loss = 0.432554
I0825 20:46:40.453105  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.524039 (* 1 = 0.524039 loss)
I0825 20:46:40.453119  2068 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0825 20:48:19.257797  2068 solver.cpp:514] Iteration 23000, Testing net (#0)
I0825 20:49:27.071218  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:49:27.423507  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.382399
I0825 20:49:27.423568  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.83561 (* 1 = 2.83561 loss)
I0825 20:49:28.236809  2068 solver.cpp:357] Iteration 23000 (0.596006 iter/s, 167.784s/100 iters), loss = 0.488309
I0825 20:49:28.236882  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488041 (* 1 = 0.488041 loss)
I0825 20:49:28.236894  2068 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0825 20:50:15.583541  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:51:15.258330  2068 solver.cpp:357] Iteration 23100 (0.934388 iter/s, 107.022s/100 iters), loss = 0.469446
I0825 20:51:15.258481  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458084 (* 1 = 0.458084 loss)
I0825 20:51:15.258493  2068 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0825 20:53:01.325873  2068 solver.cpp:357] Iteration 23200 (0.942792 iter/s, 106.068s/100 iters), loss = 0.332437
I0825 20:53:01.326059  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344138 (* 1 = 0.344138 loss)
I0825 20:53:01.326087  2068 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0825 20:54:36.584398  2068 solver.cpp:357] Iteration 23300 (1.04977 iter/s, 95.2586s/100 iters), loss = 0.305598
I0825 20:54:36.584568  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.269861 (* 1 = 0.269861 loss)
I0825 20:54:36.584580  2068 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0825 20:56:23.578619  2068 solver.cpp:357] Iteration 23400 (0.934631 iter/s, 106.994s/100 iters), loss = 0.369536
I0825 20:56:23.578775  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.485916 (* 1 = 0.485916 loss)
I0825 20:56:23.578788  2068 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0825 20:57:00.608681  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:58:10.082168  2068 solver.cpp:514] Iteration 23500, Testing net (#0)
I0825 20:59:12.219492  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:59:12.483227  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6687
I0825 20:59:12.483281  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20294 (* 1 = 1.20294 loss)
I0825 20:59:13.230958  2068 solver.cpp:357] Iteration 23500 (0.589392 iter/s, 169.666s/100 iters), loss = 0.298814
I0825 20:59:13.231029  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.30053 (* 1 = 0.30053 loss)
I0825 20:59:13.231041  2068 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0825 21:00:57.780702  2068 solver.cpp:357] Iteration 23600 (0.956445 iter/s, 104.554s/100 iters), loss = 0.293763
I0825 21:00:57.780844  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340131 (* 1 = 0.340131 loss)
I0825 21:00:57.780856  2068 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0825 21:02:40.217072  2068 solver.cpp:357] Iteration 23700 (0.976153 iter/s, 102.443s/100 iters), loss = 0.272157
I0825 21:02:40.217214  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.252152 (* 1 = 0.252152 loss)
I0825 21:02:40.217226  2068 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0825 21:04:27.760699  2068 solver.cpp:357] Iteration 23800 (0.929814 iter/s, 107.548s/100 iters), loss = 0.370948
I0825 21:04:27.760856  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408161 (* 1 = 0.408161 loss)
I0825 21:04:27.760869  2068 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0825 21:04:51.350620  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:06:07.608155  2068 solver.cpp:357] Iteration 23900 (1.00151 iter/s, 99.849s/100 iters), loss = 0.308293
I0825 21:06:07.608278  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.257783 (* 1 = 0.257783 loss)
I0825 21:06:07.608290  2068 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0825 21:07:54.074506  2068 solver.cpp:514] Iteration 24000, Testing net (#0)
I0825 21:08:59.149829  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:08:59.430861  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5885
I0825 21:08:59.430927  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.80649 (* 1 = 1.80649 loss)
I0825 21:09:00.179914  2068 solver.cpp:357] Iteration 24000 (0.579439 iter/s, 172.581s/100 iters), loss = 0.46556
I0825 21:09:00.179989  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.506199 (* 1 = 0.506199 loss)
I0825 21:09:00.179999  2068 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0825 21:10:39.184449  2068 solver.cpp:357] Iteration 24100 (1.01003 iter/s, 99.0071s/100 iters), loss = 0.381232
I0825 21:10:39.184573  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393835 (* 1 = 0.393835 loss)
I0825 21:10:39.184587  2068 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0825 21:12:23.427350  2068 solver.cpp:357] Iteration 24200 (0.959294 iter/s, 104.243s/100 iters), loss = 0.297864
I0825 21:12:23.427487  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.212712 (* 1 = 0.212712 loss)
I0825 21:12:23.427500  2068 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0825 21:12:40.867223  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:14:10.848196  2068 solver.cpp:357] Iteration 24300 (0.930897 iter/s, 107.423s/100 iters), loss = 0.594771
I0825 21:14:10.848353  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.607333 (* 1 = 0.607333 loss)
I0825 21:14:10.848366  2068 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0825 21:15:58.453289  2068 solver.cpp:357] Iteration 24400 (0.929323 iter/s, 107.605s/100 iters), loss = 0.413876
I0825 21:15:58.453430  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463544 (* 1 = 0.463544 loss)
I0825 21:15:58.453442  2068 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0825 21:17:31.405298  2068 solver.cpp:514] Iteration 24500, Testing net (#0)
I0825 21:18:43.807365  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:18:44.127640  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7098
I0825 21:18:44.127703  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.952276 (* 1 = 0.952276 loss)
I0825 21:18:44.979040  2068 solver.cpp:357] Iteration 24500 (0.600492 iter/s, 166.53s/100 iters), loss = 0.427068
I0825 21:18:44.979118  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364227 (* 1 = 0.364227 loss)
I0825 21:18:44.979131  2068 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0825 21:20:32.135027  2068 solver.cpp:357] Iteration 24600 (0.933221 iter/s, 107.156s/100 iters), loss = 0.532644
I0825 21:20:32.135236  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.610409 (* 1 = 0.610409 loss)
I0825 21:20:32.135251  2068 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0825 21:20:39.174505  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:22:21.712070  2068 solver.cpp:357] Iteration 24700 (0.912602 iter/s, 109.577s/100 iters), loss = 0.293628
I0825 21:22:21.712287  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.275923 (* 1 = 0.275923 loss)
I0825 21:22:21.712334  2068 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0825 21:24:07.310061  2068 solver.cpp:357] Iteration 24800 (0.946974 iter/s, 105.6s/100 iters), loss = 0.411807
I0825 21:24:07.310428  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331549 (* 1 = 0.331549 loss)
I0825 21:24:07.310458  2068 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0825 21:25:48.430289  2068 solver.cpp:357] Iteration 24900 (0.988908 iter/s, 101.122s/100 iters), loss = 0.478934
I0825 21:25:48.430450  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423965 (* 1 = 0.423965 loss)
I0825 21:25:48.430480  2068 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0825 21:27:32.574663  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:27:34.698297  2068 solver.cpp:514] Iteration 25000, Testing net (#0)
I0825 21:28:35.115201  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:28:35.352491  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7016
I0825 21:28:35.352556  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.04482 (* 1 = 1.04482 loss)
I0825 21:28:36.213382  2068 solver.cpp:357] Iteration 25000 (0.595995 iter/s, 167.787s/100 iters), loss = 0.335231
I0825 21:28:36.213449  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340691 (* 1 = 0.340691 loss)
I0825 21:28:36.213460  2068 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0825 21:30:23.452503  2068 solver.cpp:357] Iteration 25100 (0.932482 iter/s, 107.241s/100 iters), loss = 0.5001
I0825 21:30:23.452651  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544702 (* 1 = 0.544702 loss)
I0825 21:30:23.452661  2068 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0825 21:32:10.750102  2068 solver.cpp:357] Iteration 25200 (0.932155 iter/s, 107.278s/100 iters), loss = 0.516488
I0825 21:32:10.750427  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477723 (* 1 = 0.477723 loss)
I0825 21:32:10.750491  2068 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0825 21:33:49.823565  2068 solver.cpp:357] Iteration 25300 (1.00951 iter/s, 99.0583s/100 iters), loss = 0.397663
I0825 21:33:49.823693  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490215 (* 1 = 0.490215 loss)
I0825 21:33:49.823707  2068 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0825 21:35:18.987175  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:35:31.682991  2068 solver.cpp:357] Iteration 25400 (0.981871 iter/s, 101.846s/100 iters), loss = 0.407137
I0825 21:35:31.683069  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.235079 (* 1 = 0.235079 loss)
I0825 21:35:31.683081  2068 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0825 21:37:18.070449  2068 solver.cpp:514] Iteration 25500, Testing net (#0)
I0825 21:38:25.791280  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:38:26.050969  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6985
I0825 21:38:26.051028  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05771 (* 1 = 1.05771 loss)
I0825 21:38:26.909135  2068 solver.cpp:357] Iteration 25500 (0.570745 iter/s, 175.21s/100 iters), loss = 0.354675
I0825 21:38:26.909209  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.354028 (* 1 = 0.354028 loss)
I0825 21:38:26.909219  2068 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0825 21:40:06.679895  2068 solver.cpp:357] Iteration 25600 (1.0024 iter/s, 99.7602s/100 iters), loss = 0.336092
I0825 21:40:06.680166  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.285841 (* 1 = 0.285841 loss)
I0825 21:40:06.680197  2068 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0825 21:41:47.607702  2068 solver.cpp:357] Iteration 25700 (0.990881 iter/s, 100.92s/100 iters), loss = 0.315336
I0825 21:41:47.607856  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335063 (* 1 = 0.335063 loss)
I0825 21:41:47.607867  2068 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0825 21:43:11.602799  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:43:34.471290  2068 solver.cpp:357] Iteration 25800 (0.935814 iter/s, 106.859s/100 iters), loss = 0.38919
I0825 21:43:34.471362  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331004 (* 1 = 0.331004 loss)
I0825 21:43:34.471375  2068 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0825 21:45:21.327435  2068 solver.cpp:357] Iteration 25900 (0.935908 iter/s, 106.848s/100 iters), loss = 0.410615
I0825 21:45:21.327539  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341722 (* 1 = 0.341722 loss)
I0825 21:45:21.327551  2068 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0825 21:47:01.055511  2068 solver.cpp:514] Iteration 26000, Testing net (#0)
I0825 21:48:06.294091  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:48:06.521134  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6199
I0825 21:48:06.521235  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.45544 (* 1 = 1.45544 loss)
I0825 21:48:07.303434  2068 solver.cpp:357] Iteration 26000 (0.602513 iter/s, 165.971s/100 iters), loss = 0.404707
I0825 21:48:07.303506  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387114 (* 1 = 0.387114 loss)
I0825 21:48:07.303519  2068 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0825 21:49:51.095942  2068 solver.cpp:357] Iteration 26100 (0.963501 iter/s, 103.788s/100 iters), loss = 0.344957
I0825 21:49:51.096079  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.276261 (* 1 = 0.276261 loss)
I0825 21:49:51.096091  2068 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0825 21:51:04.795330  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:51:33.130517  2068 solver.cpp:357] Iteration 26200 (0.980098 iter/s, 102.031s/100 iters), loss = 0.435354
I0825 21:51:33.130637  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385514 (* 1 = 0.385514 loss)
I0825 21:51:33.130662  2068 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0825 21:53:17.842362  2068 solver.cpp:357] Iteration 26300 (0.955018 iter/s, 104.71s/100 iters), loss = 0.271913
I0825 21:53:17.842517  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.228178 (* 1 = 0.228178 loss)
I0825 21:53:17.842531  2068 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0825 21:55:05.099597  2068 solver.cpp:357] Iteration 26400 (0.932386 iter/s, 107.252s/100 iters), loss = 0.373435
I0825 21:55:05.099740  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393861 (* 1 = 0.393861 loss)
I0825 21:55:05.099750  2068 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0825 21:56:45.580024  2068 solver.cpp:514] Iteration 26500, Testing net (#0)
I0825 21:57:46.018112  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:57:46.230605  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6936
I0825 21:57:46.230721  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07075 (* 1 = 1.07075 loss)
I0825 21:57:47.141567  2068 solver.cpp:357] Iteration 26500 (0.61713 iter/s, 162.04s/100 iters), loss = 0.298675
I0825 21:57:47.141641  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.286251 (* 1 = 0.286251 loss)
I0825 21:57:47.141652  2068 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0825 21:58:50.855165  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:59:34.273463  2068 solver.cpp:357] Iteration 26600 (0.933455 iter/s, 107.129s/100 iters), loss = 0.276147
I0825 21:59:34.273780  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.205492 (* 1 = 0.205492 loss)
I0825 21:59:34.273844  2068 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0825 22:01:21.544284  2068 solver.cpp:357] Iteration 26700 (0.932245 iter/s, 107.268s/100 iters), loss = 0.52705
I0825 22:01:21.544446  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.505507 (* 1 = 0.505507 loss)
I0825 22:01:21.544461  2068 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0825 22:03:05.143723  2068 solver.cpp:357] Iteration 26800 (0.965281 iter/s, 103.597s/100 iters), loss = 0.441285
I0825 22:03:05.143860  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363524 (* 1 = 0.363524 loss)
I0825 22:03:05.143875  2068 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0825 22:04:42.684216  2068 solver.cpp:357] Iteration 26900 (1.0252 iter/s, 97.5423s/100 iters), loss = 0.339247
I0825 22:04:42.684432  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281805 (* 1 = 0.281805 loss)
I0825 22:04:42.684460  2068 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0825 22:05:37.132133  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:06:29.164415  2068 solver.cpp:514] Iteration 27000, Testing net (#0)
I0825 22:07:37.461254  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:07:37.797003  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7618
I0825 22:07:37.797063  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.785635 (* 1 = 0.785635 loss)
I0825 22:07:38.654353  2068 solver.cpp:357] Iteration 27000 (0.568184 iter/s, 175.999s/100 iters), loss = 0.387561
I0825 22:07:38.654489  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373927 (* 1 = 0.373927 loss)
I0825 22:07:38.654518  2068 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0825 22:09:18.436314  2068 solver.cpp:357] Iteration 27100 (1.00209 iter/s, 99.7918s/100 iters), loss = 0.469167
I0825 22:09:18.436481  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449492 (* 1 = 0.449492 loss)
I0825 22:09:18.436493  2068 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0825 22:11:05.611642  2068 solver.cpp:357] Iteration 27200 (0.932976 iter/s, 107.184s/100 iters), loss = 0.347218
I0825 22:11:05.611771  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.270489 (* 1 = 0.270489 loss)
I0825 22:11:05.611783  2068 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0825 22:12:47.025403  2068 solver.cpp:357] Iteration 27300 (0.986 iter/s, 101.42s/100 iters), loss = 0.282733
I0825 22:12:47.025545  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.210499 (* 1 = 0.210499 loss)
I0825 22:12:47.025558  2068 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0825 22:13:31.281446  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:14:31.836546  2068 solver.cpp:357] Iteration 27400 (0.954052 iter/s, 104.816s/100 iters), loss = 0.286139
I0825 22:14:31.836665  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.254731 (* 1 = 0.254731 loss)
I0825 22:14:31.836678  2068 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0825 22:16:13.431064  2068 solver.cpp:514] Iteration 27500, Testing net (#0)
I0825 22:17:21.468510  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:17:21.658648  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7146
I0825 22:17:21.658704  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.0067 (* 1 = 1.0067 loss)
I0825 22:17:22.510010  2068 solver.cpp:357] Iteration 27500 (0.585872 iter/s, 170.686s/100 iters), loss = 0.350291
I0825 22:17:22.510090  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.264269 (* 1 = 0.264269 loss)
I0825 22:17:22.510102  2068 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0825 22:19:08.182653  2068 solver.cpp:357] Iteration 27600 (0.9463 iter/s, 105.675s/100 iters), loss = 0.320326
I0825 22:19:08.182828  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.319243 (* 1 = 0.319243 loss)
I0825 22:19:08.182842  2068 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0825 22:20:43.987576  2068 solver.cpp:357] Iteration 27700 (1.04373 iter/s, 95.8098s/100 iters), loss = 0.569094
I0825 22:20:43.987756  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503369 (* 1 = 0.503369 loss)
I0825 22:20:43.987789  2068 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0825 22:21:17.469764  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:22:30.728026  2068 solver.cpp:357] Iteration 27800 (0.936826 iter/s, 106.743s/100 iters), loss = 0.311601
I0825 22:22:30.728199  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312987 (* 1 = 0.312987 loss)
I0825 22:22:30.728211  2068 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0825 22:24:17.819607  2068 solver.cpp:357] Iteration 27900 (0.933758 iter/s, 107.094s/100 iters), loss = 0.418632
I0825 22:24:17.819784  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.308372 (* 1 = 0.308372 loss)
I0825 22:24:17.819797  2068 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0825 22:26:03.305366  2068 solver.cpp:514] Iteration 28000, Testing net (#0)
I0825 22:27:01.555817  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:27:01.776440  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.760401
I0825 22:27:01.776561  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.744969 (* 1 = 0.744969 loss)
I0825 22:27:02.592871  2068 solver.cpp:357] Iteration 28000 (0.606879 iter/s, 164.777s/100 iters), loss = 0.368252
I0825 22:27:02.592943  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399349 (* 1 = 0.399349 loss)
I0825 22:27:02.592957  2068 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0825 22:28:46.887913  2068 solver.cpp:357] Iteration 28100 (0.958824 iter/s, 104.294s/100 iters), loss = 0.42445
I0825 22:28:46.888052  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449232 (* 1 = 0.449232 loss)
I0825 22:28:46.888064  2068 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0825 22:29:10.654356  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:30:33.976064  2068 solver.cpp:357] Iteration 28200 (0.933799 iter/s, 107.089s/100 iters), loss = 0.481662
I0825 22:30:33.976205  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312228 (* 1 = 0.312228 loss)
I0825 22:30:33.976218  2068 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0825 22:32:15.790192  2068 solver.cpp:357] Iteration 28300 (0.982173 iter/s, 101.815s/100 iters), loss = 0.409495
I0825 22:32:15.790436  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396901 (* 1 = 0.396901 loss)
I0825 22:32:15.790462  2068 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0825 22:34:01.801735  2068 solver.cpp:357] Iteration 28400 (0.943276 iter/s, 106.013s/100 iters), loss = 0.281098
I0825 22:34:01.801869  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.242893 (* 1 = 0.242893 loss)
I0825 22:34:01.801882  2068 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0825 22:35:42.036336  2068 solver.cpp:514] Iteration 28500, Testing net (#0)
I0825 22:36:50.225786  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:36:50.450232  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7448
I0825 22:36:50.450289  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.795466 (* 1 = 0.795466 loss)
I0825 22:36:51.288242  2068 solver.cpp:357] Iteration 28500 (0.590009 iter/s, 169.489s/100 iters), loss = 0.316006
I0825 22:36:51.288321  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326188 (* 1 = 0.326188 loss)
I0825 22:36:51.288332  2068 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0825 22:37:05.492815  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:38:31.451370  2068 solver.cpp:357] Iteration 28600 (0.998367 iter/s, 100.164s/100 iters), loss = 0.397143
I0825 22:38:31.451750  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496493 (* 1 = 0.496493 loss)
I0825 22:38:31.451814  2068 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0825 22:40:18.626907  2068 solver.cpp:357] Iteration 28700 (0.933047 iter/s, 107.176s/100 iters), loss = 0.242965
I0825 22:40:18.627035  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.222185 (* 1 = 0.222185 loss)
I0825 22:40:18.627049  2068 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0825 22:42:05.969830  2068 solver.cpp:357] Iteration 28800 (0.931611 iter/s, 107.341s/100 iters), loss = 0.327888
I0825 22:42:05.969972  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414456 (* 1 = 0.414456 loss)
I0825 22:42:05.969983  2068 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0825 22:43:42.884069  2068 solver.cpp:357] Iteration 28900 (1.03184 iter/s, 96.9141s/100 iters), loss = 0.474769
I0825 22:43:42.884246  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.521124 (* 1 = 0.521124 loss)
I0825 22:43:42.884274  2068 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0825 22:43:46.187994  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:45:26.765843  2068 solver.cpp:514] Iteration 29000, Testing net (#0)
I0825 22:46:34.736443  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:46:35.084545  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4542
I0825 22:46:35.084607  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.1622 (* 1 = 2.1622 loss)
I0825 22:46:35.931391  2068 solver.cpp:357] Iteration 29000 (0.577865 iter/s, 173.051s/100 iters), loss = 0.359439
I0825 22:46:35.931457  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353366 (* 1 = 0.353366 loss)
I0825 22:46:35.931469  2068 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0825 22:48:22.960362  2068 solver.cpp:357] Iteration 29100 (0.934325 iter/s, 107.029s/100 iters), loss = 0.314267
I0825 22:48:22.960582  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.308256 (* 1 = 0.308256 loss)
I0825 22:48:22.960611  2068 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0825 22:50:02.022262  2068 solver.cpp:357] Iteration 29200 (1.00949 iter/s, 99.0597s/100 iters), loss = 0.343633
I0825 22:50:02.022475  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.187447 (* 1 = 0.187447 loss)
I0825 22:50:02.022503  2068 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0825 22:51:37.842310  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:51:44.063508  2068 solver.cpp:357] Iteration 29300 (0.980015 iter/s, 102.039s/100 iters), loss = 0.307074
I0825 22:51:44.063575  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335353 (* 1 = 0.335353 loss)
I0825 22:51:44.063588  2068 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0825 22:53:31.146872  2068 solver.cpp:357] Iteration 29400 (0.93385 iter/s, 107.084s/100 iters), loss = 0.299255
I0825 22:53:31.147028  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.235875 (* 1 = 0.235875 loss)
I0825 22:53:31.147040  2068 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0825 22:55:14.233814  2068 solver.cpp:514] Iteration 29500, Testing net (#0)
I0825 22:56:17.724133  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:56:18.078238  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.536501
I0825 22:56:18.078296  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.54181 (* 1 = 1.54181 loss)
I0825 22:56:18.926059  2068 solver.cpp:357] Iteration 29500 (0.596016 iter/s, 167.781s/100 iters), loss = 0.439391
I0825 22:56:18.926134  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.320583 (* 1 = 0.320583 loss)
I0825 22:56:18.926147  2068 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0825 22:58:03.569727  2068 solver.cpp:357] Iteration 29600 (0.955642 iter/s, 104.642s/100 iters), loss = 0.336527
I0825 22:58:03.569963  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.320367 (* 1 = 0.320367 loss)
I0825 22:58:03.569975  2068 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0825 22:59:30.601547  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:59:47.306584  2068 solver.cpp:357] Iteration 29700 (0.963958 iter/s, 103.739s/100 iters), loss = 0.33062
I0825 22:59:47.306648  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.250309 (* 1 = 0.250309 loss)
I0825 22:59:47.306658  2068 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0825 23:01:27.030680  2068 solver.cpp:357] Iteration 29800 (1.00274 iter/s, 99.7263s/100 iters), loss = 0.365264
I0825 23:01:27.030845  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335345 (* 1 = 0.335345 loss)
I0825 23:01:27.030877  2068 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0825 23:03:14.050838  2068 solver.cpp:357] Iteration 29900 (0.934384 iter/s, 107.022s/100 iters), loss = 0.345086
I0825 23:03:14.051003  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312042 (* 1 = 0.312042 loss)
I0825 23:03:14.051015  2068 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0825 23:05:00.425006  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_30000.caffemodel
I0825 23:05:00.468155  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_30000.solverstate
I0825 23:05:00.478320  2068 solver.cpp:514] Iteration 30000, Testing net (#0)
I0825 23:06:04.475008  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:06:04.673756  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.564001
I0825 23:06:04.673817  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.57077 (* 1 = 1.57077 loss)
I0825 23:06:05.475481  2068 solver.cpp:357] Iteration 30000 (0.583341 iter/s, 171.426s/100 iters), loss = 0.25369
I0825 23:06:05.475549  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296238 (* 1 = 0.296238 loss)
I0825 23:06:05.475561  2068 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0825 23:07:16.901990  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:07:42.921391  2068 solver.cpp:357] Iteration 30100 (1.02623 iter/s, 97.4438s/100 iters), loss = 0.395903
I0825 23:07:42.921464  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.251505 (* 1 = 0.251505 loss)
I0825 23:07:42.921478  2068 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0825 23:09:30.200981  2068 solver.cpp:357] Iteration 30200 (0.93216 iter/s, 107.278s/100 iters), loss = 0.462196
I0825 23:09:30.201114  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432783 (* 1 = 0.432783 loss)
I0825 23:09:30.201126  2068 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0825 23:11:17.519634  2068 solver.cpp:357] Iteration 30300 (0.931802 iter/s, 107.319s/100 iters), loss = 0.449694
I0825 23:11:17.519950  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403809 (* 1 = 0.403809 loss)
I0825 23:11:17.520015  2068 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0825 23:12:58.877429  2068 solver.cpp:357] Iteration 30400 (0.986624 iter/s, 101.356s/100 iters), loss = 0.276039
I0825 23:12:58.877562  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.272897 (* 1 = 0.272897 loss)
I0825 23:12:58.877575  2068 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0825 23:14:03.538110  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:14:38.000267  2068 solver.cpp:514] Iteration 30500, Testing net (#0)
I0825 23:15:46.009480  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:15:46.298677  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7072
I0825 23:15:46.298740  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07278 (* 1 = 1.07278 loss)
I0825 23:15:47.146457  2068 solver.cpp:357] Iteration 30500 (0.594309 iter/s, 168.263s/100 iters), loss = 0.327604
I0825 23:15:47.146518  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338864 (* 1 = 0.338864 loss)
I0825 23:15:47.146528  2068 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0825 23:17:34.622417  2068 solver.cpp:357] Iteration 30600 (0.930453 iter/s, 107.475s/100 iters), loss = 0.360334
I0825 23:17:34.622622  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.317147 (* 1 = 0.317147 loss)
I0825 23:17:34.622634  2068 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0825 23:19:14.694566  2068 solver.cpp:357] Iteration 30700 (0.999309 iter/s, 100.069s/100 iters), loss = 0.440716
I0825 23:19:14.694710  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.270938 (* 1 = 0.270938 loss)
I0825 23:19:14.694721  2068 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0825 23:21:01.882098  2068 solver.cpp:357] Iteration 30800 (0.932967 iter/s, 107.185s/100 iters), loss = 0.236297
I0825 23:21:01.882251  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.316948 (* 1 = 0.316948 loss)
I0825 23:21:01.882262  2068 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0825 23:21:56.790285  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:22:43.371243  2068 solver.cpp:357] Iteration 30900 (0.985348 iter/s, 101.487s/100 iters), loss = 0.544658
I0825 23:22:43.371376  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488039 (* 1 = 0.488039 loss)
I0825 23:22:43.371388  2068 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0825 23:24:24.835625  2068 solver.cpp:514] Iteration 31000, Testing net (#0)
I0825 23:25:30.725026  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:25:30.967051  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.804401
I0825 23:25:30.967103  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.586698 (* 1 = 0.586698 loss)
I0825 23:25:31.847488  2068 solver.cpp:357] Iteration 31000 (0.593561 iter/s, 168.475s/100 iters), loss = 0.371084
I0825 23:25:31.847563  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302919 (* 1 = 0.302919 loss)
I0825 23:25:31.847573  2068 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0825 23:27:19.278726  2068 solver.cpp:357] Iteration 31100 (0.930841 iter/s, 107.43s/100 iters), loss = 0.408469
I0825 23:27:19.278928  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42414 (* 1 = 0.42414 loss)
I0825 23:27:19.278959  2068 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0825 23:29:05.099611  2068 solver.cpp:357] Iteration 31200 (0.945005 iter/s, 105.82s/100 iters), loss = 0.38831
I0825 23:29:05.099756  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360003 (* 1 = 0.360003 loss)
I0825 23:29:05.099769  2068 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0825 23:29:49.393609  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:30:40.613811  2068 solver.cpp:357] Iteration 31300 (1.04698 iter/s, 95.5129s/100 iters), loss = 0.311647
I0825 23:30:40.613937  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300336 (* 1 = 0.300336 loss)
I0825 23:30:40.613950  2068 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0825 23:32:28.038036  2068 solver.cpp:357] Iteration 31400 (0.930917 iter/s, 107.421s/100 iters), loss = 0.356414
I0825 23:32:28.038144  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.251262 (* 1 = 0.251262 loss)
I0825 23:32:28.038156  2068 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0825 23:34:14.141894  2068 solver.cpp:514] Iteration 31500, Testing net (#0)
I0825 23:35:22.122886  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:35:22.417217  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7187
I0825 23:35:22.417273  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.9362 (* 1 = 0.9362 loss)
I0825 23:35:23.244593  2068 solver.cpp:357] Iteration 31500 (0.570756 iter/s, 175.206s/100 iters), loss = 0.25253
I0825 23:35:23.244668  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3086 (* 1 = 0.3086 loss)
I0825 23:35:23.244680  2068 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0825 23:37:00.387198  2068 solver.cpp:357] Iteration 31600 (1.02943 iter/s, 97.1412s/100 iters), loss = 0.346561
I0825 23:37:00.387468  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330216 (* 1 = 0.330216 loss)
I0825 23:37:00.387497  2068 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0825 23:37:38.524379  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:38:44.957965  2068 solver.cpp:357] Iteration 31700 (0.956298 iter/s, 104.57s/100 iters), loss = 0.446148
I0825 23:38:44.958135  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.524807 (* 1 = 0.524807 loss)
I0825 23:38:44.958148  2068 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0825 23:40:32.418803  2068 solver.cpp:357] Iteration 31800 (0.930579 iter/s, 107.46s/100 iters), loss = 0.275068
I0825 23:40:32.418925  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.303279 (* 1 = 0.303279 loss)
I0825 23:40:32.418936  2068 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0825 23:42:12.665242  2068 solver.cpp:357] Iteration 31900 (0.997551 iter/s, 100.246s/100 iters), loss = 0.401459
I0825 23:42:12.665370  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344286 (* 1 = 0.344286 loss)
I0825 23:42:12.665383  2068 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0825 23:43:59.027271  2068 solver.cpp:514] Iteration 32000, Testing net (#0)
I0825 23:45:01.849251  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:45:02.088165  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7535
I0825 23:45:02.088310  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.780577 (* 1 = 0.780577 loss)
I0825 23:45:02.840716  2068 solver.cpp:357] Iteration 32000 (0.587635 iter/s, 170.174s/100 iters), loss = 0.351076
I0825 23:45:02.840785  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477892 (* 1 = 0.477892 loss)
I0825 23:45:02.840795  2068 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0825 23:45:02.840801  2068 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0825 23:45:32.218010  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:46:48.851150  2068 solver.cpp:357] Iteration 32100 (0.94331 iter/s, 106.01s/100 iters), loss = 0.329009
I0825 23:46:48.851307  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394427 (* 1 = 0.394427 loss)
I0825 23:46:48.851318  2068 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0825 23:48:29.414417  2068 solver.cpp:357] Iteration 32200 (0.994403 iter/s, 100.563s/100 iters), loss = 0.33894
I0825 23:48:29.414580  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.324041 (* 1 = 0.324041 loss)
I0825 23:48:29.414593  2068 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0825 23:50:16.801801  2068 solver.cpp:357] Iteration 32300 (0.931228 iter/s, 107.385s/100 iters), loss = 0.18795
I0825 23:50:16.802011  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.175158 (* 1 = 0.175158 loss)
I0825 23:50:16.802042  2068 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0825 23:52:04.081089  2068 solver.cpp:357] Iteration 32400 (0.932167 iter/s, 107.277s/100 iters), loss = 0.193665
I0825 23:52:04.081203  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.151028 (* 1 = 0.151028 loss)
I0825 23:52:04.081215  2068 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0825 23:52:23.548329  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:53:37.113374  2068 solver.cpp:514] Iteration 32500, Testing net (#0)
I0825 23:54:45.120385  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:54:45.406590  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.898002
I0825 23:54:45.406756  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.310558 (* 1 = 0.310558 loss)
I0825 23:54:46.254065  2068 solver.cpp:357] Iteration 32500 (0.616623 iter/s, 162.174s/100 iters), loss = 0.169834
I0825 23:54:46.254144  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170806 (* 1 = 0.170806 loss)
I0825 23:54:46.254158  2068 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0825 23:56:33.594460  2068 solver.cpp:357] Iteration 32600 (0.931637 iter/s, 107.338s/100 iters), loss = 0.177696
I0825 23:56:33.594667  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144996 (* 1 = 0.144996 loss)
I0825 23:56:33.594681  2068 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0825 23:58:20.462452  2068 solver.cpp:357] Iteration 32700 (0.935738 iter/s, 106.868s/100 iters), loss = 0.18045
I0825 23:58:20.462617  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.176069 (* 1 = 0.176069 loss)
I0825 23:58:20.462630  2068 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0825 23:59:59.753269  2068 solver.cpp:357] Iteration 32800 (1.00717 iter/s, 99.2882s/100 iters), loss = 0.186946
I0825 23:59:59.753413  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.178667 (* 1 = 0.178667 loss)
I0825 23:59:59.753427  2068 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0826 00:00:09.004735  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:01:41.912683  2068 solver.cpp:357] Iteration 32900 (0.978867 iter/s, 102.159s/100 iters), loss = 0.0994216
I0826 00:01:41.912791  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0935776 (* 1 = 0.0935776 loss)
I0826 00:01:41.912802  2068 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0826 00:03:28.359181  2068 solver.cpp:514] Iteration 33000, Testing net (#0)
I0826 00:04:35.044914  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:04:35.259438  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.885502
I0826 00:04:35.259500  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.34727 (* 1 = 0.34727 loss)
I0826 00:04:36.047811  2068 solver.cpp:357] Iteration 33000 (0.574258 iter/s, 174.138s/100 iters), loss = 0.110689
I0826 00:04:36.047876  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0962323 (* 1 = 0.0962323 loss)
I0826 00:04:36.047888  2068 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0826 00:06:18.030203  2068 solver.cpp:357] Iteration 33100 (0.980547 iter/s, 101.984s/100 iters), loss = 0.175693
I0826 00:06:18.030314  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170609 (* 1 = 0.170609 loss)
I0826 00:06:18.030326  2068 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0826 00:08:03.016222  2068 solver.cpp:357] Iteration 33200 (0.952513 iter/s, 104.985s/100 iters), loss = 0.123881
I0826 00:08:03.016350  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10967 (* 1 = 0.10967 loss)
I0826 00:08:03.016362  2068 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0826 00:08:03.581910  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:09:46.089633  2068 solver.cpp:357] Iteration 33300 (0.970188 iter/s, 103.073s/100 iters), loss = 0.155853
I0826 00:09:46.089781  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13648 (* 1 = 0.13648 loss)
I0826 00:09:46.089792  2068 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0826 00:11:26.340591  2068 solver.cpp:357] Iteration 33400 (0.997524 iter/s, 100.248s/100 iters), loss = 0.109985
I0826 00:11:26.340837  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0903564 (* 1 = 0.0903564 loss)
I0826 00:11:26.340896  2068 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0826 00:13:12.502379  2068 solver.cpp:514] Iteration 33500, Testing net (#0)
I0826 00:14:20.631439  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:14:20.759295  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.902603
I0826 00:14:20.759363  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.304488 (* 1 = 0.304488 loss)
I0826 00:14:21.661080  2068 solver.cpp:357] Iteration 33500 (0.570382 iter/s, 175.321s/100 iters), loss = 0.144757
I0826 00:14:21.661157  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.226864 (* 1 = 0.226864 loss)
I0826 00:14:21.661168  2068 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0826 00:15:56.083645  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:16:04.516986  2068 solver.cpp:357] Iteration 33600 (0.97224 iter/s, 102.855s/100 iters), loss = 0.179826
I0826 00:16:04.517057  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.198462 (* 1 = 0.198462 loss)
I0826 00:16:04.517069  2068 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0826 00:17:42.711835  2068 solver.cpp:357] Iteration 33700 (1.01837 iter/s, 98.1962s/100 iters), loss = 0.147085
I0826 00:17:42.711989  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.171753 (* 1 = 0.171753 loss)
I0826 00:17:42.712000  2068 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0826 00:19:29.848453  2068 solver.cpp:357] Iteration 33800 (0.933393 iter/s, 107.136s/100 iters), loss = 0.147096
I0826 00:19:29.848590  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0818193 (* 1 = 0.0818193 loss)
I0826 00:19:29.848603  2068 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0826 00:21:17.097656  2068 solver.cpp:357] Iteration 33900 (0.932413 iter/s, 107.249s/100 iters), loss = 0.146954
I0826 00:21:17.097798  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.18168 (* 1 = 0.18168 loss)
I0826 00:21:17.097811  2068 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0826 00:22:37.909611  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:22:56.342775  2068 solver.cpp:514] Iteration 34000, Testing net (#0)
I0826 00:23:58.236436  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:23:58.514276  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.860502
I0826 00:23:58.514346  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.439871 (* 1 = 0.439871 loss)
I0826 00:23:59.315604  2068 solver.cpp:357] Iteration 34000 (0.616481 iter/s, 162.211s/100 iters), loss = 0.080146
I0826 00:23:59.315685  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0645342 (* 1 = 0.0645342 loss)
I0826 00:23:59.315697  2068 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0826 00:25:45.723603  2068 solver.cpp:357] Iteration 34100 (0.939821 iter/s, 106.403s/100 iters), loss = 0.104344
I0826 00:25:45.723706  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158121 (* 1 = 0.158121 loss)
I0826 00:25:45.723716  2068 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0826 00:27:32.864385  2068 solver.cpp:357] Iteration 34200 (0.93337 iter/s, 107.139s/100 iters), loss = 0.113921
I0826 00:27:32.864490  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104365 (* 1 = 0.104365 loss)
I0826 00:27:32.864501  2068 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0826 00:29:13.018592  2068 solver.cpp:357] Iteration 34300 (0.998476 iter/s, 100.153s/100 iters), loss = 0.0732666
I0826 00:29:13.018738  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0922455 (* 1 = 0.0922455 loss)
I0826 00:29:13.018751  2068 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0826 00:30:30.364856  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:31:00.194589  2068 solver.cpp:357] Iteration 34400 (0.933073 iter/s, 107.173s/100 iters), loss = 0.21196
I0826 00:31:00.194654  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.203763 (* 1 = 0.203763 loss)
I0826 00:31:00.194665  2068 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0826 00:32:40.680018  2068 solver.cpp:514] Iteration 34500, Testing net (#0)
I0826 00:33:45.207566  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:33:45.434469  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.885702
I0826 00:33:45.434520  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.37118 (* 1 = 0.37118 loss)
I0826 00:33:46.178022  2068 solver.cpp:357] Iteration 34500 (0.602474 iter/s, 165.982s/100 iters), loss = 0.0586059
I0826 00:33:46.178097  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0552391 (* 1 = 0.0552391 loss)
I0826 00:33:46.178109  2068 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0826 00:35:29.873543  2068 solver.cpp:357] Iteration 34600 (0.964385 iter/s, 103.693s/100 iters), loss = 0.102244
I0826 00:35:29.873728  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117757 (* 1 = 0.117757 loss)
I0826 00:35:29.873739  2068 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0826 00:37:17.356060  2068 solver.cpp:357] Iteration 34700 (0.930404 iter/s, 107.48s/100 iters), loss = 0.114976
I0826 00:37:17.356187  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114519 (* 1 = 0.114519 loss)
I0826 00:37:17.356200  2068 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0826 00:38:25.167799  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:39:03.426633  2068 solver.cpp:357] Iteration 34800 (0.942807 iter/s, 106.066s/100 iters), loss = 0.0626414
I0826 00:39:03.426780  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0782472 (* 1 = 0.0782472 loss)
I0826 00:39:03.426793  2068 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0826 00:40:38.671736  2068 solver.cpp:357] Iteration 34900 (1.04995 iter/s, 95.243s/100 iters), loss = 0.116424
I0826 00:40:38.671880  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.153283 (* 1 = 0.153283 loss)
I0826 00:40:38.671891  2068 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0826 00:42:24.773890  2068 solver.cpp:514] Iteration 35000, Testing net (#0)
I0826 00:43:32.854897  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:43:33.145633  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.911402
I0826 00:43:33.145706  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.27505 (* 1 = 0.27505 loss)
I0826 00:43:34.040344  2068 solver.cpp:357] Iteration 35000 (0.570233 iter/s, 175.367s/100 iters), loss = 0.0544756
I0826 00:43:34.040417  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0622851 (* 1 = 0.0622851 loss)
I0826 00:43:34.040431  2068 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0826 00:45:18.676741  2068 solver.cpp:357] Iteration 35100 (0.955707 iter/s, 104.635s/100 iters), loss = 0.0911045
I0826 00:45:18.676897  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107004 (* 1 = 0.107004 loss)
I0826 00:45:18.676923  2068 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0826 00:46:11.194021  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:46:58.059466  2068 solver.cpp:357] Iteration 35200 (1.00621 iter/s, 99.383s/100 iters), loss = 0.0873951
I0826 00:46:58.059571  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0746828 (* 1 = 0.0746828 loss)
I0826 00:46:58.059581  2068 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0826 00:48:42.375298  2068 solver.cpp:357] Iteration 35300 (0.958624 iter/s, 104.316s/100 iters), loss = 0.118159
I0826 00:48:42.375609  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.187108 (* 1 = 0.187108 loss)
I0826 00:48:42.375674  2068 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0826 00:50:29.903388  2068 solver.cpp:357] Iteration 35400 (0.930004 iter/s, 107.526s/100 iters), loss = 0.064168
I0826 00:50:29.903527  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0679522 (* 1 = 0.0679522 loss)
I0826 00:50:29.903538  2068 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0826 00:52:08.807027  2068 solver.cpp:514] Iteration 35500, Testing net (#0)
I0826 00:53:16.747907  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:53:16.996098  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.901402
I0826 00:53:16.996165  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.326533 (* 1 = 0.326533 loss)
I0826 00:53:17.858981  2068 solver.cpp:357] Iteration 35500 (0.5954 iter/s, 167.954s/100 iters), loss = 0.172913
I0826 00:53:17.859057  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103904 (* 1 = 0.103904 loss)
I0826 00:53:17.859069  2068 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0826 00:54:05.224486  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:55:00.401042  2068 solver.cpp:357] Iteration 35600 (0.975245 iter/s, 102.538s/100 iters), loss = 0.167679
I0826 00:55:00.401204  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110513 (* 1 = 0.110513 loss)
I0826 00:55:00.401217  2068 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0826 00:56:44.772145  2068 solver.cpp:357] Iteration 35700 (0.958126 iter/s, 104.37s/100 iters), loss = 0.0860069
I0826 00:56:44.772320  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0712072 (* 1 = 0.0712072 loss)
I0826 00:56:44.772347  2068 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0826 00:58:25.840518  2068 solver.cpp:357] Iteration 35800 (0.989387 iter/s, 101.073s/100 iters), loss = 0.169218
I0826 00:58:25.840627  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260528 (* 1 = 0.260528 loss)
I0826 00:58:25.840639  2068 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0826 01:00:13.256361  2068 solver.cpp:357] Iteration 35900 (0.930945 iter/s, 107.418s/100 iters), loss = 0.123306
I0826 01:00:13.256494  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122577 (* 1 = 0.122577 loss)
I0826 01:00:13.256506  2068 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0826 01:00:50.587605  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:01:59.606282  2068 solver.cpp:514] Iteration 36000, Testing net (#0)
I0826 01:02:55.409998  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:02:55.618397  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880602
I0826 01:02:55.618448  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.378608 (* 1 = 0.378608 loss)
I0826 01:02:56.302039  2068 solver.cpp:357] Iteration 36000 (0.613313 iter/s, 163.049s/100 iters), loss = 0.110769
I0826 01:02:56.302116  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118788 (* 1 = 0.118788 loss)
I0826 01:02:56.302129  2068 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0826 01:04:42.019840  2068 solver.cpp:357] Iteration 36100 (0.94591 iter/s, 105.718s/100 iters), loss = 0.0827612
I0826 01:04:42.020004  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0693121 (* 1 = 0.0693121 loss)
I0826 01:04:42.020015  2068 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0826 01:06:29.248957  2068 solver.cpp:357] Iteration 36200 (0.932579 iter/s, 107.23s/100 iters), loss = 0.0401849
I0826 01:06:29.249119  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0433609 (* 1 = 0.0433609 loss)
I0826 01:06:29.249146  2068 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0826 01:08:16.373550  2068 solver.cpp:357] Iteration 36300 (0.933473 iter/s, 107.127s/100 iters), loss = 0.128308
I0826 01:08:16.373703  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0998993 (* 1 = 0.0998993 loss)
I0826 01:08:16.373731  2068 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0826 01:08:39.939723  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:09:55.997308  2068 solver.cpp:357] Iteration 36400 (1.00376 iter/s, 99.6255s/100 iters), loss = 0.063366
I0826 01:09:55.997508  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0534628 (* 1 = 0.0534628 loss)
I0826 01:09:55.997537  2068 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0826 01:11:36.733641  2068 solver.cpp:514] Iteration 36500, Testing net (#0)
I0826 01:12:44.919307  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:12:45.101514  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.900602
I0826 01:12:45.101577  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.334919 (* 1 = 0.334919 loss)
I0826 01:12:45.968482  2068 solver.cpp:357] Iteration 36500 (0.588337 iter/s, 169.971s/100 iters), loss = 0.0734406
I0826 01:12:45.968555  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0584977 (* 1 = 0.0584977 loss)
I0826 01:12:45.968566  2068 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0826 01:14:28.832777  2068 solver.cpp:357] Iteration 36600 (0.972159 iter/s, 102.864s/100 iters), loss = 0.0640309
I0826 01:14:28.833015  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.088157 (* 1 = 0.088157 loss)
I0826 01:14:28.833043  2068 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0826 01:16:13.253379  2068 solver.cpp:357] Iteration 36700 (0.95767 iter/s, 104.42s/100 iters), loss = 0.0871552
I0826 01:16:13.253600  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0550774 (* 1 = 0.0550774 loss)
I0826 01:16:13.253629  2068 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0826 01:16:30.657444  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:17:58.531193  2068 solver.cpp:357] Iteration 36800 (0.949892 iter/s, 105.275s/100 iters), loss = 0.0927695
I0826 01:17:58.531419  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103218 (* 1 = 0.103218 loss)
I0826 01:17:58.531451  2068 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0826 01:19:41.453721  2068 solver.cpp:357] Iteration 36900 (0.971611 iter/s, 102.922s/100 iters), loss = 0.109855
I0826 01:19:41.453866  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.177317 (* 1 = 0.177317 loss)
I0826 01:19:41.453877  2068 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0826 01:21:20.487390  2068 solver.cpp:514] Iteration 37000, Testing net (#0)
I0826 01:22:28.229125  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:22:28.471477  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888101
I0826 01:22:28.471541  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.380771 (* 1 = 0.380771 loss)
I0826 01:22:29.316910  2068 solver.cpp:357] Iteration 37000 (0.595723 iter/s, 167.863s/100 iters), loss = 0.0924456
I0826 01:22:29.316982  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0822582 (* 1 = 0.0822582 loss)
I0826 01:22:29.316993  2068 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0826 01:24:16.548545  2068 solver.cpp:357] Iteration 37100 (0.932549 iter/s, 107.233s/100 iters), loss = 0.0572881
I0826 01:24:16.548704  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0712176 (* 1 = 0.0712176 loss)
I0826 01:24:16.548717  2068 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0826 01:24:23.736656  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:25:57.100639  2068 solver.cpp:357] Iteration 37200 (0.994539 iter/s, 100.549s/100 iters), loss = 0.099352
I0826 01:25:57.100773  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0655297 (* 1 = 0.0655297 loss)
I0826 01:25:57.100787  2068 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0826 01:27:37.709256  2068 solver.cpp:357] Iteration 37300 (0.993961 iter/s, 100.608s/100 iters), loss = 0.0724717
I0826 01:27:37.709378  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.076345 (* 1 = 0.076345 loss)
I0826 01:27:37.709388  2068 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0826 01:29:24.942381  2068 solver.cpp:357] Iteration 37400 (0.932555 iter/s, 107.232s/100 iters), loss = 0.132799
I0826 01:29:24.942515  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119943 (* 1 = 0.119943 loss)
I0826 01:29:24.942528  2068 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0826 01:31:09.397333  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:31:11.518057  2068 solver.cpp:514] Iteration 37500, Testing net (#0)
I0826 01:32:12.122298  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:32:12.439872  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.907902
I0826 01:32:12.439929  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.323826 (* 1 = 0.323826 loss)
I0826 01:32:13.264901  2068 solver.cpp:357] Iteration 37500 (0.594062 iter/s, 168.333s/100 iters), loss = 0.0810432
I0826 01:32:13.264976  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.128824 (* 1 = 0.128824 loss)
I0826 01:32:13.264987  2068 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0826 01:33:55.535593  2068 solver.cpp:357] Iteration 37600 (0.97775 iter/s, 102.276s/100 iters), loss = 0.0910797
I0826 01:33:55.535869  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0984195 (* 1 = 0.0984195 loss)
I0826 01:33:55.535902  2068 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0826 01:35:41.784394  2068 solver.cpp:357] Iteration 37700 (0.941149 iter/s, 106.253s/100 iters), loss = 0.0898229
I0826 01:35:41.784523  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102429 (* 1 = 0.102429 loss)
I0826 01:35:41.784534  2068 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0826 01:37:27.066521  2068 solver.cpp:357] Iteration 37800 (0.949779 iter/s, 105.288s/100 iters), loss = 0.0612552
I0826 01:37:27.066639  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0958773 (* 1 = 0.0958773 loss)
I0826 01:37:27.066651  2068 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0826 01:38:56.844810  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:39:09.480940  2068 solver.cpp:357] Iteration 37900 (0.976379 iter/s, 102.419s/100 iters), loss = 0.122764
I0826 01:39:09.481024  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0815006 (* 1 = 0.0815006 loss)
I0826 01:39:09.481035  2068 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0826 01:40:55.931995  2068 solver.cpp:514] Iteration 38000, Testing net (#0)
I0826 01:41:57.176990  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:41:57.461299  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.889602
I0826 01:41:57.461360  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.367506 (* 1 = 0.367506 loss)
I0826 01:41:58.324800  2068 solver.cpp:357] Iteration 38000 (0.592245 iter/s, 168.849s/100 iters), loss = 0.0424493
I0826 01:41:58.324875  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0460527 (* 1 = 0.0460527 loss)
I0826 01:41:58.324888  2068 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0826 01:43:38.934814  2068 solver.cpp:357] Iteration 38100 (0.993941 iter/s, 100.61s/100 iters), loss = 0.126351
I0826 01:43:38.934960  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.141429 (* 1 = 0.141429 loss)
I0826 01:43:38.934973  2068 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0826 01:44:51.202841  2068 solver.cpp:357] Iteration 38200 (1.38373 iter/s, 72.2685s/100 iters), loss = 0.0315344
I0826 01:44:51.202949  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.038537 (* 1 = 0.038537 loss)
I0826 01:44:51.202960  2068 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0826 01:45:47.197470  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:46:02.469236  2068 solver.cpp:357] Iteration 38300 (1.40314 iter/s, 71.2688s/100 iters), loss = 0.0688196
I0826 01:46:02.469302  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0323135 (* 1 = 0.0323135 loss)
I0826 01:46:02.469312  2068 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0826 01:47:12.434993  2068 solver.cpp:357] Iteration 38400 (1.42922 iter/s, 69.968s/100 iters), loss = 0.0818991
I0826 01:47:12.435134  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0404888 (* 1 = 0.0404888 loss)
I0826 01:47:12.435147  2068 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0826 01:48:15.184823  2068 solver.cpp:514] Iteration 38500, Testing net (#0)
I0826 01:48:59.541771  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:48:59.757835  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894502
I0826 01:48:59.757899  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.369207 (* 1 = 0.369207 loss)
I0826 01:49:00.352214  2068 solver.cpp:357] Iteration 38500 (0.926608 iter/s, 107.921s/100 iters), loss = 0.0657854
I0826 01:49:00.352293  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101484 (* 1 = 0.101484 loss)
I0826 01:49:00.352306  2068 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0826 01:50:11.522343  2068 solver.cpp:357] Iteration 38600 (1.40508 iter/s, 71.1702s/100 iters), loss = 0.0273944
I0826 01:50:11.522549  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0258493 (* 1 = 0.0258493 loss)
I0826 01:50:11.522560  2068 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0826 01:51:01.041796  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:51:23.126260  2068 solver.cpp:357] Iteration 38700 (1.39653 iter/s, 71.6058s/100 iters), loss = 0.0857031
I0826 01:51:23.126325  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117877 (* 1 = 0.117877 loss)
I0826 01:51:23.126343  2068 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0826 01:52:31.731969  2068 solver.cpp:357] Iteration 38800 (1.45756 iter/s, 68.6076s/100 iters), loss = 0.0875522
I0826 01:52:31.732110  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.154278 (* 1 = 0.154278 loss)
I0826 01:52:31.732121  2068 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0826 01:53:36.829881  2068 solver.cpp:357] Iteration 38900 (1.53611 iter/s, 65.0996s/100 iters), loss = 0.0579941
I0826 01:53:36.830004  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0691708 (* 1 = 0.0691708 loss)
I0826 01:53:36.830016  2068 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0826 01:54:47.668951  2068 solver.cpp:514] Iteration 39000, Testing net (#0)
I0826 01:55:31.975960  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:55:32.136168  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894302
I0826 01:55:32.136229  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.384727 (* 1 = 0.384727 loss)
I0826 01:55:32.722057  2068 solver.cpp:357] Iteration 39000 (0.862863 iter/s, 115.893s/100 iters), loss = 0.0616023
I0826 01:55:32.722131  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10023 (* 1 = 0.10023 loss)
I0826 01:55:32.722142  2068 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0826 01:56:15.127909  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:56:44.027782  2068 solver.cpp:357] Iteration 39100 (1.40242 iter/s, 71.3055s/100 iters), loss = 0.059621
I0826 01:56:44.027859  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0480776 (* 1 = 0.0480776 loss)
I0826 01:56:44.027871  2068 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0826 01:57:50.908416  2068 solver.cpp:357] Iteration 39200 (1.49521 iter/s, 66.8803s/100 iters), loss = 0.125075
I0826 01:57:50.908561  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0898227 (* 1 = 0.0898227 loss)
I0826 01:57:50.908573  2068 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0826 01:58:57.281975  2068 solver.cpp:357] Iteration 39300 (1.50663 iter/s, 66.3732s/100 iters), loss = 0.0998375
I0826 01:58:57.282140  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.151466 (* 1 = 0.151466 loss)
I0826 01:58:57.282151  2068 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0826 02:00:08.526700  2068 solver.cpp:357] Iteration 39400 (1.40362 iter/s, 71.2445s/100 iters), loss = 0.0599594
I0826 02:00:08.526870  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.02939 (* 1 = 0.02939 loss)
I0826 02:00:08.526883  2068 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0826 02:00:44.578328  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:01:19.250720  2068 solver.cpp:514] Iteration 39500, Testing net (#0)
I0826 02:02:03.614429  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:02:03.824025  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.883302
I0826 02:02:03.824081  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.409421 (* 1 = 0.409421 loss)
I0826 02:02:04.401074  2068 solver.cpp:357] Iteration 39500 (0.862997 iter/s, 115.875s/100 iters), loss = 0.0927742
I0826 02:02:04.401149  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118371 (* 1 = 0.118371 loss)
I0826 02:02:04.401160  2068 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0826 02:03:10.246886  2068 solver.cpp:357] Iteration 39600 (1.51871 iter/s, 65.8454s/100 iters), loss = 0.0509632
I0826 02:03:10.247105  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0289636 (* 1 = 0.0289636 loss)
I0826 02:03:10.247119  2068 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0826 02:04:18.550721  2068 solver.cpp:357] Iteration 39700 (1.46407 iter/s, 68.3027s/100 iters), loss = 0.0960864
I0826 02:04:18.550897  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118072 (* 1 = 0.118072 loss)
I0826 02:04:18.550910  2068 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0826 02:05:29.724920  2068 solver.cpp:357] Iteration 39800 (1.40505 iter/s, 71.1718s/100 iters), loss = 0.0537836
I0826 02:05:29.725078  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0673402 (* 1 = 0.0673402 loss)
I0826 02:05:29.725091  2068 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0826 02:05:59.203667  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:06:41.192883  2068 solver.cpp:357] Iteration 39900 (1.39927 iter/s, 71.4658s/100 iters), loss = 0.0505818
I0826 02:06:41.193033  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0833998 (* 1 = 0.0833998 loss)
I0826 02:06:41.193047  2068 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0826 02:07:52.027864  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_40000.caffemodel
I0826 02:07:52.056690  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_40000.solverstate
I0826 02:07:52.066330  2068 solver.cpp:514] Iteration 40000, Testing net (#0)
I0826 02:08:29.195379  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:08:29.336120  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.877602
I0826 02:08:29.336174  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.443321 (* 1 = 0.443321 loss)
I0826 02:08:29.867018  2068 solver.cpp:357] Iteration 40000 (0.920197 iter/s, 108.672s/100 iters), loss = 0.0760581
I0826 02:08:29.867087  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0518395 (* 1 = 0.0518395 loss)
I0826 02:08:29.867100  2068 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0826 02:09:39.639317  2068 solver.cpp:357] Iteration 40100 (1.43323 iter/s, 69.7725s/100 iters), loss = 0.0689079
I0826 02:09:39.639464  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0545015 (* 1 = 0.0545015 loss)
I0826 02:09:39.639475  2068 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0826 02:10:51.000939  2068 solver.cpp:357] Iteration 40200 (1.40131 iter/s, 71.362s/100 iters), loss = 0.128812
I0826 02:10:51.001116  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0489108 (* 1 = 0.0489108 loss)
I0826 02:10:51.001128  2068 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0826 02:11:13.569700  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:12:02.464184  2068 solver.cpp:357] Iteration 40300 (1.39935 iter/s, 71.4617s/100 iters), loss = 0.0488601
I0826 02:12:02.464334  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0448674 (* 1 = 0.0448674 loss)
I0826 02:12:02.464345  2068 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0826 02:13:13.641641  2068 solver.cpp:357] Iteration 40400 (1.40497 iter/s, 71.1761s/100 iters), loss = 0.0864757
I0826 02:13:13.641810  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0510588 (* 1 = 0.0510588 loss)
I0826 02:13:13.641822  2068 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0826 02:14:15.349123  2068 solver.cpp:514] Iteration 40500, Testing net (#0)
I0826 02:14:59.810863  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:14:59.918923  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.889502
I0826 02:14:59.918973  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.392537 (* 1 = 0.392537 loss)
I0826 02:15:00.516928  2068 solver.cpp:357] Iteration 40500 (0.935677 iter/s, 106.875s/100 iters), loss = 0.0684102
I0826 02:15:00.517001  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110783 (* 1 = 0.110783 loss)
I0826 02:15:00.517012  2068 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0826 02:16:11.831280  2068 solver.cpp:357] Iteration 40600 (1.40227 iter/s, 71.3132s/100 iters), loss = 0.0795343
I0826 02:16:11.831496  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.091286 (* 1 = 0.091286 loss)
I0826 02:16:11.831508  2068 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0826 02:16:27.618255  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:17:23.273638  2068 solver.cpp:357] Iteration 40700 (1.39971 iter/s, 71.4432s/100 iters), loss = 0.0720912
I0826 02:17:23.273816  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0849635 (* 1 = 0.0849635 loss)
I0826 02:17:23.273828  2068 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0826 02:18:33.856793  2068 solver.cpp:357] Iteration 40800 (1.41679 iter/s, 70.5821s/100 iters), loss = 0.0494625
I0826 02:18:33.856891  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0705812 (* 1 = 0.0705812 loss)
I0826 02:18:33.856906  2068 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0826 02:19:36.573916  2068 solver.cpp:357] Iteration 40900 (1.59444 iter/s, 62.718s/100 iters), loss = 0.122453
I0826 02:19:36.574039  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100251 (* 1 = 0.100251 loss)
I0826 02:19:36.574050  2068 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0826 02:20:47.306644  2068 solver.cpp:514] Iteration 41000, Testing net (#0)
I0826 02:21:31.678390  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:21:31.868335  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.893801
I0826 02:21:31.868381  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.384186 (* 1 = 0.384186 loss)
I0826 02:21:32.487471  2068 solver.cpp:357] Iteration 41000 (0.862698 iter/s, 115.915s/100 iters), loss = 0.0961247
I0826 02:21:32.487543  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0912393 (* 1 = 0.0912393 loss)
I0826 02:21:32.487556  2068 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0826 02:21:41.821481  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:22:43.548230  2068 solver.cpp:357] Iteration 41100 (1.40726 iter/s, 71.0598s/100 iters), loss = 0.0577389
I0826 02:22:43.548352  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0508496 (* 1 = 0.0508496 loss)
I0826 02:22:43.548363  2068 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0826 02:23:52.727296  2068 solver.cpp:357] Iteration 41200 (1.44552 iter/s, 69.1792s/100 iters), loss = 0.062981
I0826 02:23:52.727443  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.047957 (* 1 = 0.047957 loss)
I0826 02:23:52.727460  2068 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0826 02:24:56.830752  2068 solver.cpp:357] Iteration 41300 (1.56 iter/s, 64.1025s/100 iters), loss = 0.0719692
I0826 02:24:56.830894  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0997977 (* 1 = 0.0997977 loss)
I0826 02:24:56.830906  2068 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0826 02:26:08.013783  2068 solver.cpp:357] Iteration 41400 (1.40481 iter/s, 71.1842s/100 iters), loss = 0.0824293
I0826 02:26:08.013952  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12349 (* 1 = 0.12349 loss)
I0826 02:26:08.013964  2068 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0826 02:26:10.556159  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:27:18.583251  2068 solver.cpp:514] Iteration 41500, Testing net (#0)
I0826 02:28:03.188182  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:28:03.376893  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880002
I0826 02:28:03.376940  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.438819 (* 1 = 0.438819 loss)
I0826 02:28:03.989665  2068 solver.cpp:357] Iteration 41500 (0.862247 iter/s, 115.976s/100 iters), loss = 0.092973
I0826 02:28:03.989737  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0784869 (* 1 = 0.0784869 loss)
I0826 02:28:03.989748  2068 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0826 02:29:11.931172  2068 solver.cpp:357] Iteration 41600 (1.47187 iter/s, 67.9407s/100 iters), loss = 0.06555
I0826 02:29:11.931373  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0397273 (* 1 = 0.0397273 loss)
I0826 02:29:11.931386  2068 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0826 02:30:17.926412  2068 solver.cpp:357] Iteration 41700 (1.51524 iter/s, 65.9963s/100 iters), loss = 0.0684567
I0826 02:30:17.926589  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0365744 (* 1 = 0.0365744 loss)
I0826 02:30:17.926602  2068 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0826 02:31:25.008261  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:31:29.180214  2068 solver.cpp:357] Iteration 41800 (1.40345 iter/s, 71.2531s/100 iters), loss = 0.125371
I0826 02:31:29.180291  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0701461 (* 1 = 0.0701461 loss)
I0826 02:31:29.180302  2068 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0826 02:32:40.669466  2068 solver.cpp:357] Iteration 41900 (1.39883 iter/s, 71.4885s/100 iters), loss = 0.0816981
I0826 02:32:40.669625  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0518193 (* 1 = 0.0518193 loss)
I0826 02:32:40.669637  2068 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0826 02:33:51.315403  2068 solver.cpp:514] Iteration 42000, Testing net (#0)
I0826 02:34:30.685479  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:34:30.822296  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.891802
I0826 02:34:30.822357  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.413015 (* 1 = 0.413015 loss)
I0826 02:34:31.352473  2068 solver.cpp:357] Iteration 42000 (0.90348 iter/s, 110.683s/100 iters), loss = 0.149175
I0826 02:34:31.352541  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103108 (* 1 = 0.103108 loss)
I0826 02:34:31.352553  2068 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0826 02:35:38.745807  2068 solver.cpp:357] Iteration 42100 (1.4838 iter/s, 67.3946s/100 iters), loss = 0.0303212
I0826 02:35:38.745935  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0227063 (* 1 = 0.0227063 loss)
I0826 02:35:38.745947  2068 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0826 02:36:39.010301  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:36:50.039070  2068 solver.cpp:357] Iteration 42200 (1.40267 iter/s, 71.2926s/100 iters), loss = 0.10324
I0826 02:36:50.039146  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0397916 (* 1 = 0.0397916 loss)
I0826 02:36:50.039158  2068 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0826 02:38:01.450947  2068 solver.cpp:357] Iteration 42300 (1.40034 iter/s, 71.4112s/100 iters), loss = 0.101143
I0826 02:38:01.451048  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14127 (* 1 = 0.14127 loss)
I0826 02:38:01.451061  2068 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0826 02:39:12.596108  2068 solver.cpp:357] Iteration 42400 (1.4055 iter/s, 71.1493s/100 iters), loss = 0.0511379
I0826 02:39:12.596274  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0106937 (* 1 = 0.0106937 loss)
I0826 02:39:12.596287  2068 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0826 02:40:14.250435  2068 solver.cpp:514] Iteration 42500, Testing net (#0)
I0826 02:40:58.600428  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:40:58.774164  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.887202
I0826 02:40:58.774224  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.415748 (* 1 = 0.415748 loss)
I0826 02:40:59.352190  2068 solver.cpp:357] Iteration 42500 (0.936682 iter/s, 106.76s/100 iters), loss = 0.0566223
I0826 02:40:59.352262  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0514535 (* 1 = 0.0514535 loss)
I0826 02:40:59.352272  2068 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0826 02:41:53.482905  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:42:11.018689  2068 solver.cpp:357] Iteration 42600 (1.39532 iter/s, 71.668s/100 iters), loss = 0.0653935
I0826 02:42:11.018752  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0209944 (* 1 = 0.0209944 loss)
I0826 02:42:11.018764  2068 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0826 02:43:22.364454  2068 solver.cpp:357] Iteration 42700 (1.40156 iter/s, 71.3491s/100 iters), loss = 0.195899
I0826 02:43:22.364635  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.187145 (* 1 = 0.187145 loss)
I0826 02:43:22.364647  2068 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0826 02:44:33.600824  2068 solver.cpp:357] Iteration 42800 (1.40376 iter/s, 71.2374s/100 iters), loss = 0.0765524
I0826 02:44:33.600971  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0672954 (* 1 = 0.0672954 loss)
I0826 02:44:33.600982  2068 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0826 02:45:35.967082  2068 solver.cpp:357] Iteration 42900 (1.60337 iter/s, 62.3685s/100 iters), loss = 0.0507482
I0826 02:45:35.967226  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0338443 (* 1 = 0.0338443 loss)
I0826 02:45:35.967236  2068 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0826 02:46:23.200065  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:46:46.617452  2068 solver.cpp:514] Iteration 43000, Testing net (#0)
I0826 02:47:31.123247  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:47:31.293871  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.878901
I0826 02:47:31.293931  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.458572 (* 1 = 0.458572 loss)
I0826 02:47:31.910768  2068 solver.cpp:357] Iteration 43000 (0.862454 iter/s, 115.948s/100 iters), loss = 0.0369744
I0826 02:47:31.910843  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0379088 (* 1 = 0.0379088 loss)
I0826 02:47:31.910853  2068 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0826 02:48:43.165865  2068 solver.cpp:357] Iteration 43100 (1.40336 iter/s, 71.2577s/100 iters), loss = 0.0606114
I0826 02:48:43.166034  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0451316 (* 1 = 0.0451316 loss)
I0826 02:48:43.166045  2068 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0826 02:49:54.438964  2068 solver.cpp:357] Iteration 43200 (1.40304 iter/s, 71.2736s/100 iters), loss = 0.113784
I0826 02:49:54.439113  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0614349 (* 1 = 0.0614349 loss)
I0826 02:49:54.439124  2068 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0826 02:50:57.215162  2068 solver.cpp:357] Iteration 43300 (1.59291 iter/s, 62.7783s/100 iters), loss = 0.0649131
I0826 02:50:57.215487  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0193424 (* 1 = 0.0193424 loss)
I0826 02:50:57.215553  2068 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0826 02:51:37.644464  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:52:08.662844  2068 solver.cpp:357] Iteration 43400 (1.39962 iter/s, 71.448s/100 iters), loss = 0.16482
I0826 02:52:08.663008  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0749654 (* 1 = 0.0749654 loss)
I0826 02:52:08.663022  2068 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0826 02:53:19.069032  2068 solver.cpp:514] Iteration 43500, Testing net (#0)
I0826 02:54:03.506789  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:54:03.714469  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.892302
I0826 02:54:03.714512  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.417554 (* 1 = 0.417554 loss)
I0826 02:54:04.312052  2068 solver.cpp:357] Iteration 43500 (0.864671 iter/s, 115.651s/100 iters), loss = 0.0521753
I0826 02:54:04.312126  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0272159 (* 1 = 0.0272159 loss)
I0826 02:54:04.312140  2068 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0826 02:55:14.216374  2068 solver.cpp:357] Iteration 43600 (1.43052 iter/s, 69.9044s/100 iters), loss = 0.115818
I0826 02:55:14.216711  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103871 (* 1 = 0.103871 loss)
I0826 02:55:14.216739  2068 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0826 02:56:17.545102  2068 solver.cpp:357] Iteration 43700 (1.57902 iter/s, 63.3304s/100 iters), loss = 0.0652424
I0826 02:56:17.545279  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0875731 (* 1 = 0.0875731 loss)
I0826 02:56:17.545290  2068 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0826 02:56:51.052996  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:57:28.622949  2068 solver.cpp:357] Iteration 43800 (1.40691 iter/s, 71.0779s/100 iters), loss = 0.0732091
I0826 02:57:28.623119  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0783347 (* 1 = 0.0783347 loss)
I0826 02:57:28.623131  2068 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0826 02:58:39.886082  2068 solver.cpp:357] Iteration 43900 (1.40325 iter/s, 71.2632s/100 iters), loss = 0.0592847
I0826 02:58:39.886245  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.056488 (* 1 = 0.056488 loss)
I0826 02:58:39.886256  2068 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0826 02:59:50.505571  2068 solver.cpp:514] Iteration 44000, Testing net (#0)
I0826 03:00:32.235365  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:00:32.368547  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.884902
I0826 03:00:32.368597  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.408796 (* 1 = 0.408796 loss)
I0826 03:00:32.900385  2068 solver.cpp:357] Iteration 44000 (0.884831 iter/s, 113.016s/100 iters), loss = 0.0201187
I0826 03:00:32.900454  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0246741 (* 1 = 0.0246741 loss)
I0826 03:00:32.900465  2068 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0826 03:01:38.156138  2068 solver.cpp:357] Iteration 44100 (1.53239 iter/s, 65.2576s/100 iters), loss = 0.0552233
I0826 03:01:38.156301  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0573242 (* 1 = 0.0573242 loss)
I0826 03:01:38.156314  2068 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0826 03:02:05.337983  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:02:49.448843  2068 solver.cpp:357] Iteration 44200 (1.40267 iter/s, 71.2926s/100 iters), loss = 0.080816
I0826 03:02:49.449017  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112141 (* 1 = 0.112141 loss)
I0826 03:02:49.449029  2068 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0826 03:04:00.770375  2068 solver.cpp:357] Iteration 44300 (1.4021 iter/s, 71.3215s/100 iters), loss = 0.0686153
I0826 03:04:00.770545  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0479833 (* 1 = 0.0479833 loss)
I0826 03:04:00.770557  2068 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0826 03:05:12.000018  2068 solver.cpp:357] Iteration 44400 (1.40391 iter/s, 71.2295s/100 iters), loss = 0.120922
I0826 03:05:12.000187  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.139075 (* 1 = 0.139075 loss)
I0826 03:05:12.000200  2068 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0826 03:06:14.726076  2068 solver.cpp:514] Iteration 44500, Testing net (#0)
I0826 03:06:58.184654  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:06:58.400874  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.881502
I0826 03:06:58.400930  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.458407 (* 1 = 0.458407 loss)
I0826 03:06:58.986116  2068 solver.cpp:357] Iteration 44500 (0.934693 iter/s, 106.987s/100 iters), loss = 0.0710116
I0826 03:06:58.986182  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107146 (* 1 = 0.107146 loss)
I0826 03:06:58.986193  2068 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0826 03:07:19.387028  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:08:10.207324  2068 solver.cpp:357] Iteration 44600 (1.40404 iter/s, 71.2232s/100 iters), loss = 0.0965896
I0826 03:08:10.207522  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126932 (* 1 = 0.126932 loss)
I0826 03:08:10.207535  2068 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0826 03:09:21.375777  2068 solver.cpp:357] Iteration 44700 (1.40508 iter/s, 71.1703s/100 iters), loss = 0.121728
I0826 03:09:21.375952  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103856 (* 1 = 0.103856 loss)
I0826 03:09:21.375965  2068 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0826 03:10:32.506917  2068 solver.cpp:357] Iteration 44800 (1.40586 iter/s, 71.131s/100 iters), loss = 0.0846851
I0826 03:10:32.507084  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0992304 (* 1 = 0.0992304 loss)
I0826 03:10:32.507097  2068 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0826 03:11:34.807986  2068 solver.cpp:357] Iteration 44900 (1.60512 iter/s, 62.3007s/100 iters), loss = 0.0754088
I0826 03:11:34.808105  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0811736 (* 1 = 0.0811736 loss)
I0826 03:11:34.808118  2068 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0826 03:11:48.529968  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:12:45.489508  2068 solver.cpp:514] Iteration 45000, Testing net (#0)
I0826 03:13:29.826953  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:13:30.040457  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8506
I0826 03:13:30.040518  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.559951 (* 1 = 0.559951 loss)
I0826 03:13:30.624861  2068 solver.cpp:357] Iteration 45000 (0.863403 iter/s, 115.821s/100 iters), loss = 0.109342
I0826 03:13:30.624934  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129947 (* 1 = 0.129947 loss)
I0826 03:13:30.624946  2068 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0826 03:14:41.799499  2068 solver.cpp:357] Iteration 45100 (1.40495 iter/s, 71.1767s/100 iters), loss = 0.0552155
I0826 03:14:41.799609  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0277975 (* 1 = 0.0277975 loss)
I0826 03:14:41.799620  2068 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0826 03:15:52.997031  2068 solver.cpp:357] Iteration 45200 (1.40447 iter/s, 71.2014s/100 iters), loss = 0.0691768
I0826 03:15:52.997200  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0530208 (* 1 = 0.0530208 loss)
I0826 03:15:52.997212  2068 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0826 03:16:55.506356  2068 solver.cpp:357] Iteration 45300 (1.59973 iter/s, 62.5105s/100 iters), loss = 0.0760829
I0826 03:16:55.506494  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0993786 (* 1 = 0.0993786 loss)
I0826 03:16:55.506505  2068 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0826 03:17:02.379035  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:18:07.103334  2068 solver.cpp:357] Iteration 45400 (1.39664 iter/s, 71.6004s/100 iters), loss = 0.0331237
I0826 03:18:07.103494  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0434085 (* 1 = 0.0434085 loss)
I0826 03:18:07.103507  2068 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0826 03:19:17.829855  2068 solver.cpp:514] Iteration 45500, Testing net (#0)
I0826 03:20:02.446738  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:20:02.541323  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.874902
I0826 03:20:02.541383  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.471577 (* 1 = 0.471577 loss)
I0826 03:20:03.165463  2068 solver.cpp:357] Iteration 45500 (0.861583 iter/s, 116.065s/100 iters), loss = 0.0670094
I0826 03:20:03.165536  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0571762 (* 1 = 0.0571762 loss)
I0826 03:20:03.165546  2068 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0826 03:21:14.453382  2068 solver.cpp:357] Iteration 45600 (1.4027 iter/s, 71.291s/100 iters), loss = 0.0804908
I0826 03:21:14.453562  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0871126 (* 1 = 0.0871126 loss)
I0826 03:21:14.453573  2068 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0826 03:22:16.282359  2068 solver.cpp:357] Iteration 45700 (1.6173 iter/s, 61.8314s/100 iters), loss = 0.135115
I0826 03:22:16.282522  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.149283 (* 1 = 0.149283 loss)
I0826 03:22:16.282536  2068 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0826 03:22:16.723647  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:23:27.536211  2068 solver.cpp:357] Iteration 45800 (1.40342 iter/s, 71.2547s/100 iters), loss = 0.0625833
I0826 03:23:27.536523  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0487606 (* 1 = 0.0487606 loss)
I0826 03:23:27.536587  2068 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0826 03:24:38.801558  2068 solver.cpp:357] Iteration 45900 (1.40319 iter/s, 71.266s/100 iters), loss = 0.0676106
I0826 03:24:38.801702  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0411096 (* 1 = 0.0411096 loss)
I0826 03:24:38.801718  2068 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0826 03:25:49.543040  2068 solver.cpp:514] Iteration 46000, Testing net (#0)
I0826 03:26:33.819190  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:26:33.948760  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.885002
I0826 03:26:33.948812  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.408469 (* 1 = 0.408469 loss)
I0826 03:26:34.474895  2068 solver.cpp:357] Iteration 46000 (0.864471 iter/s, 115.678s/100 iters), loss = 0.0657999
I0826 03:26:34.474964  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106361 (* 1 = 0.106361 loss)
I0826 03:26:34.474977  2068 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0826 03:27:30.742213  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:27:37.063324  2068 solver.cpp:357] Iteration 46100 (1.59769 iter/s, 62.5905s/100 iters), loss = 0.0953127
I0826 03:27:37.063397  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156049 (* 1 = 0.156049 loss)
I0826 03:27:37.063407  2068 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0826 03:28:48.244570  2068 solver.cpp:357] Iteration 46200 (1.40481 iter/s, 71.1838s/100 iters), loss = 0.0657849
I0826 03:28:48.244734  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106062 (* 1 = 0.106062 loss)
I0826 03:28:48.244746  2068 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0826 03:29:59.519027  2068 solver.cpp:357] Iteration 46300 (1.40302 iter/s, 71.2749s/100 iters), loss = 0.0709506
I0826 03:29:59.519150  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0799208 (* 1 = 0.0799208 loss)
I0826 03:29:59.519161  2068 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0826 03:31:10.734936  2068 solver.cpp:357] Iteration 46400 (1.40414 iter/s, 71.2181s/100 iters), loss = 0.0761632
I0826 03:31:10.735083  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0895498 (* 1 = 0.0895498 loss)
I0826 03:31:10.735095  2068 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0826 03:32:05.956182  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:32:16.053690  2068 solver.cpp:514] Iteration 46500, Testing net (#0)
I0826 03:32:57.403244  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:32:57.506168  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.865701
I0826 03:32:57.506219  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.478884 (* 1 = 0.478884 loss)
I0826 03:32:58.146452  2068 solver.cpp:357] Iteration 46500 (0.930969 iter/s, 107.415s/100 iters), loss = 0.0776982
I0826 03:32:58.146512  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0687322 (* 1 = 0.0687322 loss)
I0826 03:32:58.146523  2068 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0826 03:34:09.636528  2068 solver.cpp:357] Iteration 46600 (1.39875 iter/s, 71.4925s/100 iters), loss = 0.0702306
I0826 03:34:09.636773  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0278748 (* 1 = 0.0278748 loss)
I0826 03:34:09.636785  2068 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0826 03:35:20.908615  2068 solver.cpp:357] Iteration 46700 (1.40303 iter/s, 71.2743s/100 iters), loss = 0.0475139
I0826 03:35:20.908733  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0422526 (* 1 = 0.0422526 loss)
I0826 03:35:20.908744  2068 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0826 03:36:32.194100  2068 solver.cpp:357] Iteration 46800 (1.40281 iter/s, 71.2857s/100 iters), loss = 0.101338
I0826 03:36:32.194241  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.154589 (* 1 = 0.154589 loss)
I0826 03:36:32.194250  2068 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0826 03:37:20.012735  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:37:36.035384  2068 solver.cpp:357] Iteration 46900 (1.56634 iter/s, 63.8433s/100 iters), loss = 0.158574
I0826 03:37:36.035451  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0883826 (* 1 = 0.0883826 loss)
I0826 03:37:36.035462  2068 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0826 03:38:45.217838  2068 solver.cpp:514] Iteration 47000, Testing net (#0)
I0826 03:39:29.566445  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:39:29.783917  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.876302
I0826 03:39:29.783975  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.442682 (* 1 = 0.442682 loss)
I0826 03:39:30.385560  2068 solver.cpp:357] Iteration 47000 (0.874478 iter/s, 114.354s/100 iters), loss = 0.122607
I0826 03:39:30.385638  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126898 (* 1 = 0.126898 loss)
I0826 03:39:30.385651  2068 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0826 03:40:41.577280  2068 solver.cpp:357] Iteration 47100 (1.40465 iter/s, 71.1919s/100 iters), loss = 0.0904173
I0826 03:40:41.577442  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102747 (* 1 = 0.102747 loss)
I0826 03:40:41.577455  2068 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0826 03:41:52.771770  2068 solver.cpp:357] Iteration 47200 (1.4046 iter/s, 71.1947s/100 iters), loss = 0.043153
I0826 03:41:52.771917  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0387677 (* 1 = 0.0387677 loss)
I0826 03:41:52.771931  2068 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0826 03:42:33.957226  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:42:55.186465  2068 solver.cpp:357] Iteration 47300 (1.60219 iter/s, 62.4146s/100 iters), loss = 0.0962895
I0826 03:42:55.186530  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0969416 (* 1 = 0.0969416 loss)
I0826 03:42:55.186544  2068 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0826 03:44:06.570891  2068 solver.cpp:357] Iteration 47400 (1.40086 iter/s, 71.3846s/100 iters), loss = 0.0657798
I0826 03:44:06.571071  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.034541 (* 1 = 0.034541 loss)
I0826 03:44:06.571084  2068 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0826 03:45:17.110507  2068 solver.cpp:514] Iteration 47500, Testing net (#0)
I0826 03:46:01.698407  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:46:01.901471  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.863401
I0826 03:46:01.901517  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.563954 (* 1 = 0.563954 loss)
I0826 03:46:02.511456  2068 solver.cpp:357] Iteration 47500 (0.862499 iter/s, 115.942s/100 iters), loss = 0.0390683
I0826 03:46:02.511528  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0382517 (* 1 = 0.0382517 loss)
I0826 03:46:02.511541  2068 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0826 03:47:13.663156  2068 solver.cpp:357] Iteration 47600 (1.40561 iter/s, 71.1434s/100 iters), loss = 0.0627017
I0826 03:47:13.663372  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0366736 (* 1 = 0.0366736 loss)
I0826 03:47:13.663385  2068 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0826 03:47:48.045387  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:48:16.434084  2068 solver.cpp:357] Iteration 47700 (1.59332 iter/s, 62.7619s/100 iters), loss = 0.0332599
I0826 03:48:16.434160  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.010546 (* 1 = 0.010546 loss)
I0826 03:48:16.434172  2068 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0826 03:49:27.901607  2068 solver.cpp:357] Iteration 47800 (1.39941 iter/s, 71.4585s/100 iters), loss = 0.128313
I0826 03:49:27.901726  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.202922 (* 1 = 0.202922 loss)
I0826 03:49:27.901736  2068 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0826 03:50:39.558068  2068 solver.cpp:357] Iteration 47900 (1.39567 iter/s, 71.6504s/100 iters), loss = 0.0413956
I0826 03:50:39.558233  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0475934 (* 1 = 0.0475934 loss)
I0826 03:50:39.558244  2068 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0826 03:51:50.355512  2068 solver.cpp:514] Iteration 48000, Testing net (#0)
I0826 03:52:34.803324  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:52:34.920305  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.861101
I0826 03:52:34.920361  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.523054 (* 1 = 0.523054 loss)
I0826 03:52:35.519407  2068 solver.cpp:357] Iteration 48000 (0.86243 iter/s, 115.951s/100 iters), loss = 0.108143
I0826 03:52:35.519479  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121144 (* 1 = 0.121144 loss)
I0826 03:52:35.519490  2068 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0826 03:52:35.519496  2068 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0826 03:53:02.443426  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:53:38.033228  2068 solver.cpp:357] Iteration 48100 (1.59979 iter/s, 62.5082s/100 iters), loss = 0.05853
I0826 03:53:38.033396  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0626647 (* 1 = 0.0626647 loss)
I0826 03:53:38.033408  2068 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0826 03:54:49.154089  2068 solver.cpp:357] Iteration 48200 (1.40617 iter/s, 71.1153s/100 iters), loss = 0.064129
I0826 03:54:49.154204  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0438326 (* 1 = 0.0438326 loss)
I0826 03:54:49.154217  2068 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0826 03:56:00.348579  2068 solver.cpp:357] Iteration 48300 (1.4047 iter/s, 71.1894s/100 iters), loss = 0.0181619
I0826 03:56:00.348728  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0196612 (* 1 = 0.0196612 loss)
I0826 03:56:00.348739  2068 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0826 03:57:11.460693  2068 solver.cpp:357] Iteration 48400 (1.40628 iter/s, 71.1095s/100 iters), loss = 0.031088
I0826 03:57:11.460827  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0408664 (* 1 = 0.0408664 loss)
I0826 03:57:11.460840  2068 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0826 03:57:35.971563  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:58:18.218050  2068 solver.cpp:514] Iteration 48500, Testing net (#0)
I0826 03:58:57.500006  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:58:57.689754  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922203
I0826 03:58:57.689817  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.28358 (* 1 = 0.28358 loss)
I0826 03:58:58.277765  2068 solver.cpp:357] Iteration 48500 (0.936225 iter/s, 106.812s/100 iters), loss = 0.038578
I0826 03:58:58.277829  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0549757 (* 1 = 0.0549757 loss)
I0826 03:58:58.277843  2068 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0826 04:00:09.671516  2068 solver.cpp:357] Iteration 48600 (1.40076 iter/s, 71.39s/100 iters), loss = 0.0181966
I0826 04:00:09.671700  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288482 (* 1 = 0.0288482 loss)
I0826 04:00:09.671716  2068 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0826 04:01:21.182641  2068 solver.cpp:357] Iteration 48700 (1.39842 iter/s, 71.5094s/100 iters), loss = 0.0323497
I0826 04:01:21.182796  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0432161 (* 1 = 0.0432161 loss)
I0826 04:01:21.182808  2068 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0826 04:02:32.616389  2068 solver.cpp:357] Iteration 48800 (1.39992 iter/s, 71.4325s/100 iters), loss = 0.0344698
I0826 04:02:32.616523  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0432582 (* 1 = 0.0432582 loss)
I0826 04:02:32.616535  2068 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0826 04:02:50.793890  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:03:38.329139  2068 solver.cpp:357] Iteration 48900 (1.52185 iter/s, 65.7097s/100 iters), loss = 0.0273456
I0826 04:03:38.329295  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0374977 (* 1 = 0.0374977 loss)
I0826 04:03:38.329308  2068 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0826 04:04:45.615787  2068 solver.cpp:514] Iteration 49000, Testing net (#0)
I0826 04:05:29.967128  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:05:30.162508  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927002
I0826 04:05:30.162554  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273335 (* 1 = 0.273335 loss)
I0826 04:05:30.771348  2068 solver.cpp:357] Iteration 49000 (0.889356 iter/s, 112.441s/100 iters), loss = 0.0527363
I0826 04:05:30.771419  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0358631 (* 1 = 0.0358631 loss)
I0826 04:05:30.771431  2068 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0826 04:06:41.834388  2068 solver.cpp:357] Iteration 49100 (1.40725 iter/s, 71.0604s/100 iters), loss = 0.0292741
I0826 04:06:41.834559  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0559682 (* 1 = 0.0559682 loss)
I0826 04:06:41.834573  2068 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0826 04:07:53.012079  2068 solver.cpp:357] Iteration 49200 (1.40498 iter/s, 71.1751s/100 iters), loss = 0.0149332
I0826 04:07:53.012202  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.017185 (* 1 = 0.017185 loss)
I0826 04:07:53.012217  2068 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0826 04:08:04.613610  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:08:57.380807  2068 solver.cpp:357] Iteration 49300 (1.55356 iter/s, 64.3683s/100 iters), loss = 0.0487227
I0826 04:08:57.380960  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0846737 (* 1 = 0.0846737 loss)
I0826 04:08:57.380972  2068 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0826 04:10:06.571396  2068 solver.cpp:357] Iteration 49400 (1.44529 iter/s, 69.1902s/100 iters), loss = 0.0505719
I0826 04:10:06.571528  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0983956 (* 1 = 0.0983956 loss)
I0826 04:10:06.571542  2068 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0826 04:11:17.033473  2068 solver.cpp:514] Iteration 49500, Testing net (#0)
I0826 04:12:01.401031  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:12:01.515223  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929502
I0826 04:12:01.515283  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.276872 (* 1 = 0.276872 loss)
I0826 04:12:02.155566  2068 solver.cpp:357] Iteration 49500 (0.865188 iter/s, 115.582s/100 iters), loss = 0.0485174
I0826 04:12:02.155633  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0383389 (* 1 = 0.0383389 loss)
I0826 04:12:02.155643  2068 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0826 04:13:13.381234  2068 solver.cpp:357] Iteration 49600 (1.40399 iter/s, 71.2256s/100 iters), loss = 0.030707
I0826 04:13:13.381484  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0338803 (* 1 = 0.0338803 loss)
I0826 04:13:13.381495  2068 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0826 04:13:18.015259  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:14:16.583376  2068 solver.cpp:357] Iteration 49700 (1.58228 iter/s, 63.2s/100 iters), loss = 0.0161797
I0826 04:14:16.583519  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0101804 (* 1 = 0.0101804 loss)
I0826 04:14:16.583531  2068 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0826 04:15:27.203806  2068 solver.cpp:357] Iteration 49800 (1.41602 iter/s, 70.6204s/100 iters), loss = 0.0158643
I0826 04:15:27.203945  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0159886 (* 1 = 0.0159886 loss)
I0826 04:15:27.203956  2068 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0826 04:16:38.446755  2068 solver.cpp:357] Iteration 49900 (1.40365 iter/s, 71.2429s/100 iters), loss = 0.0676435
I0826 04:16:38.446878  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0483002 (* 1 = 0.0483002 loss)
I0826 04:16:38.446889  2068 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0826 04:17:47.624770  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:17:49.093550  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_50000.caffemodel
I0826 04:17:49.122524  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_50000.solverstate
I0826 04:17:49.132680  2068 solver.cpp:514] Iteration 50000, Testing net (#0)
I0826 04:18:33.643800  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:18:33.829843  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929303
I0826 04:18:33.829890  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273283 (* 1 = 0.273283 loss)
I0826 04:18:34.439945  2068 solver.cpp:357] Iteration 50000 (0.862118 iter/s, 115.993s/100 iters), loss = 0.0124576
I0826 04:18:34.440017  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0124446 (* 1 = 0.0124446 loss)
I0826 04:18:34.440029  2068 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0826 04:19:36.392431  2068 solver.cpp:357] Iteration 50100 (1.61419 iter/s, 61.9505s/100 iters), loss = 0.0237047
I0826 04:19:36.392545  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00953995 (* 1 = 0.00953995 loss)
I0826 04:19:36.392557  2068 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0826 04:20:47.775233  2068 solver.cpp:357] Iteration 50200 (1.40091 iter/s, 71.3821s/100 iters), loss = 0.0110201
I0826 04:20:47.775379  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0136895 (* 1 = 0.0136895 loss)
I0826 04:20:47.775390  2068 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0826 04:21:59.112270  2068 solver.cpp:357] Iteration 50300 (1.40174 iter/s, 71.3399s/100 iters), loss = 0.004621
I0826 04:21:59.112396  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00572316 (* 1 = 0.00572316 loss)
I0826 04:21:59.112409  2068 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0826 04:23:01.795136  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:23:10.251128  2068 solver.cpp:357] Iteration 50400 (1.40569 iter/s, 71.1394s/100 iters), loss = 0.0135579
I0826 04:23:10.251204  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0213493 (* 1 = 0.0213493 loss)
I0826 04:23:10.251216  2068 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0826 04:24:19.659482  2068 solver.cpp:514] Iteration 50500, Testing net (#0)
I0826 04:24:56.342648  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:24:56.558682  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930002
I0826 04:24:56.558732  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.280752 (* 1 = 0.280752 loss)
I0826 04:24:57.166244  2068 solver.cpp:357] Iteration 50500 (0.935309 iter/s, 106.917s/100 iters), loss = 0.0281385
I0826 04:24:57.166321  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00566909 (* 1 = 0.00566909 loss)
I0826 04:24:57.166342  2068 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0826 04:26:08.646073  2068 solver.cpp:357] Iteration 50600 (1.399 iter/s, 71.4798s/100 iters), loss = 0.00999797
I0826 04:26:08.646230  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0127731 (* 1 = 0.0127731 loss)
I0826 04:26:08.646241  2068 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0826 04:27:19.907049  2068 solver.cpp:357] Iteration 50700 (1.40326 iter/s, 71.2628s/100 iters), loss = 0.00841996
I0826 04:27:19.907225  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0100552 (* 1 = 0.0100552 loss)
I0826 04:27:19.907238  2068 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0826 04:28:15.730355  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:28:30.912714  2068 solver.cpp:357] Iteration 50800 (1.40835 iter/s, 71.0053s/100 iters), loss = 0.0350214
I0826 04:28:30.912786  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0141535 (* 1 = 0.0141535 loss)
I0826 04:28:30.912798  2068 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0826 04:29:39.421188  2068 solver.cpp:357] Iteration 50900 (1.45969 iter/s, 68.5079s/100 iters), loss = 0.00937381
I0826 04:29:39.421346  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00873788 (* 1 = 0.00873788 loss)
I0826 04:29:39.421360  2068 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0826 04:30:43.776558  2068 solver.cpp:514] Iteration 51000, Testing net (#0)
I0826 04:31:28.145591  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:31:28.262401  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930002
I0826 04:31:28.262459  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.28829 (* 1 = 0.28829 loss)
I0826 04:31:28.857919  2068 solver.cpp:357] Iteration 51000 (0.913768 iter/s, 109.437s/100 iters), loss = 0.0132013
I0826 04:31:28.858000  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00616073 (* 1 = 0.00616073 loss)
I0826 04:31:28.858011  2068 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0826 04:32:40.031390  2068 solver.cpp:357] Iteration 51100 (1.40503 iter/s, 71.1727s/100 iters), loss = 0.00404147
I0826 04:32:40.031663  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0042861 (* 1 = 0.0042861 loss)
I0826 04:32:40.031723  2068 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0826 04:33:29.390862  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:33:51.406703  2068 solver.cpp:357] Iteration 51200 (1.40102 iter/s, 71.3764s/100 iters), loss = 0.00418444
I0826 04:33:51.406769  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00551032 (* 1 = 0.00551032 loss)
I0826 04:33:51.406780  2068 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0826 04:34:58.244235  2068 solver.cpp:357] Iteration 51300 (1.49614 iter/s, 66.8386s/100 iters), loss = 0.0194222
I0826 04:34:58.244359  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0314384 (* 1 = 0.0314384 loss)
I0826 04:34:58.244374  2068 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0826 04:35:40.491096  2068 solver.cpp:357] Iteration 51400 (2.36701 iter/s, 42.2474s/100 iters), loss = 0.0114533
I0826 04:35:40.491240  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0166408 (* 1 = 0.0166408 loss)
I0826 04:35:40.491251  2068 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0826 04:36:14.486879  2068 solver.cpp:514] Iteration 51500, Testing net (#0)
I0826 04:36:22.644758  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:36:22.675976  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929902
I0826 04:36:22.676019  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.286919 (* 1 = 0.286919 loss)
I0826 04:36:23.014407  2068 solver.cpp:357] Iteration 51500 (2.35162 iter/s, 42.5239s/100 iters), loss = 0.00472111
I0826 04:36:23.014461  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00550928 (* 1 = 0.00550928 loss)
I0826 04:36:23.014472  2068 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0826 04:36:43.454324  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:36:57.350777  2068 solver.cpp:357] Iteration 51600 (2.91232 iter/s, 34.3369s/100 iters), loss = 0.0127735
I0826 04:36:57.350951  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00332376 (* 1 = 0.00332376 loss)
I0826 04:36:57.350966  2068 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0826 04:37:31.740520  2068 solver.cpp:357] Iteration 51700 (2.90781 iter/s, 34.3901s/100 iters), loss = 0.0172936
I0826 04:37:31.740667  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00919377 (* 1 = 0.00919377 loss)
I0826 04:37:31.740679  2068 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0826 04:38:06.113934  2068 solver.cpp:357] Iteration 51800 (2.90919 iter/s, 34.3738s/100 iters), loss = 0.0112467
I0826 04:38:06.114051  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00975007 (* 1 = 0.00975007 loss)
I0826 04:38:06.114066  2068 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0826 04:38:40.481276  2068 solver.cpp:357] Iteration 51900 (2.9097 iter/s, 34.3677s/100 iters), loss = 0.0089847
I0826 04:38:40.481375  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00634504 (* 1 = 0.00634504 loss)
I0826 04:38:40.481385  2068 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0826 04:38:57.829484  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:39:14.459944  2068 solver.cpp:514] Iteration 52000, Testing net (#0)
I0826 04:39:22.608747  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:39:22.640173  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930402
I0826 04:39:22.640215  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.287013 (* 1 = 0.287013 loss)
I0826 04:39:22.978426  2068 solver.cpp:357] Iteration 52000 (2.35307 iter/s, 42.4977s/100 iters), loss = 0.00700758
I0826 04:39:22.978478  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00540234 (* 1 = 0.00540234 loss)
I0826 04:39:22.978492  2068 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0826 04:39:57.319248  2068 solver.cpp:357] Iteration 52100 (2.91195 iter/s, 34.3413s/100 iters), loss = 0.0108352
I0826 04:39:57.319391  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00899772 (* 1 = 0.00899772 loss)
I0826 04:39:57.319402  2068 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0826 04:40:31.641285  2068 solver.cpp:357] Iteration 52200 (2.91355 iter/s, 34.3224s/100 iters), loss = 0.0197023
I0826 04:40:31.641400  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0033096 (* 1 = 0.0033096 loss)
I0826 04:40:31.641417  2068 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0826 04:41:05.991534  2068 solver.cpp:357] Iteration 52300 (2.91116 iter/s, 34.3506s/100 iters), loss = 0.00973986
I0826 04:41:05.991655  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00708872 (* 1 = 0.00708872 loss)
I0826 04:41:05.991665  2068 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0826 04:41:20.075726  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:41:40.318156  2068 solver.cpp:357] Iteration 52400 (2.91316 iter/s, 34.327s/100 iters), loss = 0.00628046
I0826 04:41:40.318295  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00852494 (* 1 = 0.00852494 loss)
I0826 04:41:40.318306  2068 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0826 04:42:14.327417  2068 solver.cpp:514] Iteration 52500, Testing net (#0)
I0826 04:42:22.498988  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:42:22.530357  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926302
I0826 04:42:22.530400  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294835 (* 1 = 0.294835 loss)
I0826 04:42:22.869249  2068 solver.cpp:357] Iteration 52500 (2.35009 iter/s, 42.5516s/100 iters), loss = 0.00987379
I0826 04:42:22.869303  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00968624 (* 1 = 0.00968624 loss)
I0826 04:42:22.869316  2068 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0826 04:42:57.211771  2068 solver.cpp:357] Iteration 52600 (2.91181 iter/s, 34.3429s/100 iters), loss = 0.0092219
I0826 04:42:57.211966  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0065604 (* 1 = 0.0065604 loss)
I0826 04:42:57.211978  2068 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0826 04:43:31.554164  2068 solver.cpp:357] Iteration 52700 (2.91183 iter/s, 34.3427s/100 iters), loss = 0.0140295
I0826 04:43:31.554311  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0210378 (* 1 = 0.0210378 loss)
I0826 04:43:31.554322  2068 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0826 04:43:42.372774  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:44:05.891331  2068 solver.cpp:357] Iteration 52800 (2.91227 iter/s, 34.3375s/100 iters), loss = 0.00581696
I0826 04:44:05.891427  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0072758 (* 1 = 0.0072758 loss)
I0826 04:44:05.891439  2068 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0826 04:44:40.221972  2068 solver.cpp:357] Iteration 52900 (2.91282 iter/s, 34.331s/100 iters), loss = 0.0107049
I0826 04:44:40.222115  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00360368 (* 1 = 0.00360368 loss)
I0826 04:44:40.222124  2068 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0826 04:45:14.210247  2068 solver.cpp:514] Iteration 53000, Testing net (#0)
I0826 04:45:22.355918  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:45:22.387367  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931403
I0826 04:45:22.387410  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.295342 (* 1 = 0.295342 loss)
I0826 04:45:22.724498  2068 solver.cpp:357] Iteration 53000 (2.35278 iter/s, 42.5029s/100 iters), loss = 0.00495274
I0826 04:45:22.724550  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00676479 (* 1 = 0.00676479 loss)
I0826 04:45:22.724563  2068 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0826 04:45:57.061738  2068 solver.cpp:357] Iteration 53100 (2.91226 iter/s, 34.3376s/100 iters), loss = 0.0141383
I0826 04:45:57.061879  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00623827 (* 1 = 0.00623827 loss)
I0826 04:45:57.061889  2068 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0826 04:46:04.625507  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:46:31.415257  2068 solver.cpp:357] Iteration 53200 (2.91088 iter/s, 34.3538s/100 iters), loss = 0.026908
I0826 04:46:31.415377  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0493311 (* 1 = 0.0493311 loss)
I0826 04:46:31.415388  2068 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0826 04:47:05.792397  2068 solver.cpp:357] Iteration 53300 (2.90888 iter/s, 34.3775s/100 iters), loss = 0.00648408
I0826 04:47:05.792491  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00785821 (* 1 = 0.00785821 loss)
I0826 04:47:05.792503  2068 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0826 04:47:40.147519  2068 solver.cpp:357] Iteration 53400 (2.91075 iter/s, 34.3555s/100 iters), loss = 0.0110566
I0826 04:47:40.147667  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0113083 (* 1 = 0.0113083 loss)
I0826 04:47:40.147678  2068 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0826 04:48:14.180791  2068 solver.cpp:514] Iteration 53500, Testing net (#0)
I0826 04:48:22.373849  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:48:22.405171  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930003
I0826 04:48:22.405213  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.293358 (* 1 = 0.293358 loss)
I0826 04:48:22.741333  2068 solver.cpp:357] Iteration 53500 (2.34774 iter/s, 42.5942s/100 iters), loss = 0.00928502
I0826 04:48:22.741387  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0116868 (* 1 = 0.0116868 loss)
I0826 04:48:22.741398  2068 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0826 04:48:27.212957  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:48:57.084592  2068 solver.cpp:357] Iteration 53600 (2.91175 iter/s, 34.3436s/100 iters), loss = 0.00803758
I0826 04:48:57.084789  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00798208 (* 1 = 0.00798208 loss)
I0826 04:48:57.084800  2068 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0826 04:49:31.417971  2068 solver.cpp:357] Iteration 53700 (2.9126 iter/s, 34.3336s/100 iters), loss = 0.00332033
I0826 04:49:31.418076  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00402516 (* 1 = 0.00402516 loss)
I0826 04:49:31.418088  2068 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0826 04:50:05.760396  2068 solver.cpp:357] Iteration 53800 (2.91182 iter/s, 34.3427s/100 iters), loss = 0.00406218
I0826 04:50:05.760490  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00352732 (* 1 = 0.00352732 loss)
I0826 04:50:05.760504  2068 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0826 04:50:40.058465  2068 solver.cpp:357] Iteration 53900 (2.91559 iter/s, 34.2984s/100 iters), loss = 0.00535902
I0826 04:50:40.058563  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00695813 (* 1 = 0.00695813 loss)
I0826 04:50:40.058573  2068 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0826 04:50:41.266366  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:51:14.044687  2068 solver.cpp:514] Iteration 54000, Testing net (#0)
I0826 04:51:22.166669  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:51:22.197963  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931602
I0826 04:51:22.198005  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.296854 (* 1 = 0.296854 loss)
I0826 04:51:22.538018  2068 solver.cpp:357] Iteration 54000 (2.35405 iter/s, 42.48s/100 iters), loss = 0.00300712
I0826 04:51:22.538074  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00102339 (* 1 = 0.00102339 loss)
I0826 04:51:22.538084  2068 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0826 04:51:56.930485  2068 solver.cpp:357] Iteration 54100 (2.90758 iter/s, 34.3928s/100 iters), loss = 0.00687644
I0826 04:51:56.930634  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0105727 (* 1 = 0.0105727 loss)
I0826 04:51:56.930647  2068 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0826 04:52:31.280726  2068 solver.cpp:357] Iteration 54200 (2.91117 iter/s, 34.3505s/100 iters), loss = 0.00226273
I0826 04:52:31.280869  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00195119 (* 1 = 0.00195119 loss)
I0826 04:52:31.280880  2068 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0826 04:53:03.558558  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:53:05.614115  2068 solver.cpp:357] Iteration 54300 (2.91259 iter/s, 34.3337s/100 iters), loss = 0.0125303
I0826 04:53:05.614177  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0164051 (* 1 = 0.0164051 loss)
I0826 04:53:05.614187  2068 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0826 04:53:39.921474  2068 solver.cpp:357] Iteration 54400 (2.9148 iter/s, 34.3077s/100 iters), loss = 0.0125517
I0826 04:53:39.921617  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00745018 (* 1 = 0.00745018 loss)
I0826 04:53:39.921628  2068 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0826 04:54:13.910001  2068 solver.cpp:514] Iteration 54500, Testing net (#0)
I0826 04:54:22.093168  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:54:22.124657  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930902
I0826 04:54:22.124701  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.299318 (* 1 = 0.299318 loss)
I0826 04:54:22.461714  2068 solver.cpp:357] Iteration 54500 (2.3507 iter/s, 42.5406s/100 iters), loss = 0.00587032
I0826 04:54:22.461766  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00423175 (* 1 = 0.00423175 loss)
I0826 04:54:22.461777  2068 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0826 04:54:56.767055  2068 solver.cpp:357] Iteration 54600 (2.915 iter/s, 34.3053s/100 iters), loss = 0.00826385
I0826 04:54:56.767252  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0115352 (* 1 = 0.0115352 loss)
I0826 04:54:56.767264  2068 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0826 04:55:25.824753  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:55:31.147140  2068 solver.cpp:357] Iteration 54700 (2.90869 iter/s, 34.3797s/100 iters), loss = 0.0159399
I0826 04:55:31.147330  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112423 (* 1 = 0.0112423 loss)
I0826 04:55:31.147341  2068 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0826 04:56:05.511852  2068 solver.cpp:357] Iteration 54800 (2.90999 iter/s, 34.3644s/100 iters), loss = 0.0249848
I0826 04:56:05.511947  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00287306 (* 1 = 0.00287306 loss)
I0826 04:56:05.511962  2068 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0826 04:56:39.860931  2068 solver.cpp:357] Iteration 54900 (2.91131 iter/s, 34.3488s/100 iters), loss = 0.00741754
I0826 04:56:39.861048  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00751427 (* 1 = 0.00751427 loss)
I0826 04:56:39.861063  2068 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0826 04:57:13.870887  2068 solver.cpp:514] Iteration 55000, Testing net (#0)
I0826 04:57:22.061810  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:57:22.093165  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929403
I0826 04:57:22.093210  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.305465 (* 1 = 0.305465 loss)
I0826 04:57:22.433904  2068 solver.cpp:357] Iteration 55000 (2.34892 iter/s, 42.5727s/100 iters), loss = 0.0125318
I0826 04:57:22.433959  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0192762 (* 1 = 0.0192762 loss)
I0826 04:57:22.433969  2068 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0826 04:57:48.373920  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:57:56.761620  2068 solver.cpp:357] Iteration 55100 (2.91311 iter/s, 34.3276s/100 iters), loss = 0.0116184
I0826 04:57:56.761682  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00211011 (* 1 = 0.00211011 loss)
I0826 04:57:56.761693  2068 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0826 04:58:31.083262  2068 solver.cpp:357] Iteration 55200 (2.91362 iter/s, 34.3215s/100 iters), loss = 0.00578438
I0826 04:58:31.083413  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00439317 (* 1 = 0.00439317 loss)
I0826 04:58:31.083425  2068 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0826 04:59:05.382618  2068 solver.cpp:357] Iteration 55300 (2.91552 iter/s, 34.2992s/100 iters), loss = 0.0144086
I0826 04:59:05.382756  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0100052 (* 1 = 0.0100052 loss)
I0826 04:59:05.382766  2068 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0826 04:59:39.681830  2068 solver.cpp:357] Iteration 55400 (2.91553 iter/s, 34.2991s/100 iters), loss = 0.00321966
I0826 04:59:39.681951  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0018394 (* 1 = 0.0018394 loss)
I0826 04:59:39.681964  2068 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0826 05:00:02.365788  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:00:13.701211  2068 solver.cpp:514] Iteration 55500, Testing net (#0)
I0826 05:00:21.897650  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:00:21.929082  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931802
I0826 05:00:21.929126  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.298319 (* 1 = 0.298319 loss)
I0826 05:00:22.265409  2068 solver.cpp:357] Iteration 55500 (2.34833 iter/s, 42.5835s/100 iters), loss = 0.00857586
I0826 05:00:22.265462  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00373975 (* 1 = 0.00373975 loss)
I0826 05:00:22.265475  2068 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0826 05:00:56.607580  2068 solver.cpp:357] Iteration 55600 (2.91187 iter/s, 34.3421s/100 iters), loss = 0.0214898
I0826 05:00:56.607776  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.034235 (* 1 = 0.034235 loss)
I0826 05:00:56.607787  2068 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0826 05:01:30.930724  2068 solver.cpp:357] Iteration 55700 (2.9135 iter/s, 34.323s/100 iters), loss = 0.00609699
I0826 05:01:30.930868  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0029787 (* 1 = 0.0029787 loss)
I0826 05:01:30.930878  2068 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0826 05:02:05.276866  2068 solver.cpp:357] Iteration 55800 (2.91154 iter/s, 34.3461s/100 iters), loss = 0.00808507
I0826 05:02:05.277007  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00597302 (* 1 = 0.00597302 loss)
I0826 05:02:05.277019  2068 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0826 05:02:24.691619  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:02:39.629905  2068 solver.cpp:357] Iteration 55900 (2.91096 iter/s, 34.353s/100 iters), loss = 0.00980078
I0826 05:02:39.630050  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00616557 (* 1 = 0.00616557 loss)
I0826 05:02:39.630064  2068 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0826 05:03:13.588006  2068 solver.cpp:514] Iteration 56000, Testing net (#0)
I0826 05:03:21.761174  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:03:21.792564  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933502
I0826 05:03:21.792608  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.293965 (* 1 = 0.293965 loss)
I0826 05:03:22.131122  2068 solver.cpp:357] Iteration 56000 (2.35288 iter/s, 42.5012s/100 iters), loss = 0.0426653
I0826 05:03:22.131175  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0732742 (* 1 = 0.0732742 loss)
I0826 05:03:22.131187  2068 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0826 05:03:56.448441  2068 solver.cpp:357] Iteration 56100 (2.91398 iter/s, 34.3174s/100 iters), loss = 0.00815263
I0826 05:03:56.448583  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0098202 (* 1 = 0.0098202 loss)
I0826 05:03:56.448595  2068 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0826 05:04:30.836426  2068 solver.cpp:357] Iteration 56200 (2.90799 iter/s, 34.388s/100 iters), loss = 0.00875901
I0826 05:04:30.836572  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0156472 (* 1 = 0.0156472 loss)
I0826 05:04:30.836583  2068 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0826 05:04:47.002164  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:05:05.193766  2068 solver.cpp:357] Iteration 56300 (2.91059 iter/s, 34.3573s/100 iters), loss = 0.00373725
I0826 05:05:05.193879  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00308992 (* 1 = 0.00308992 loss)
I0826 05:05:05.193895  2068 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0826 05:05:39.526103  2068 solver.cpp:357] Iteration 56400 (2.9127 iter/s, 34.3324s/100 iters), loss = 0.00955734
I0826 05:05:39.526221  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0108183 (* 1 = 0.0108183 loss)
I0826 05:05:39.526232  2068 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0826 05:06:13.559837  2068 solver.cpp:514] Iteration 56500, Testing net (#0)
I0826 05:06:21.700114  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:06:21.731519  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931503
I0826 05:06:21.731562  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.306508 (* 1 = 0.306508 loss)
I0826 05:06:22.071110  2068 solver.cpp:357] Iteration 56500 (2.35045 iter/s, 42.5451s/100 iters), loss = 0.00531171
I0826 05:06:22.071162  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00920827 (* 1 = 0.00920827 loss)
I0826 05:06:22.071177  2068 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0826 05:06:56.369956  2068 solver.cpp:357] Iteration 56600 (2.91554 iter/s, 34.2989s/100 iters), loss = 0.0136516
I0826 05:06:56.370148  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00425304 (* 1 = 0.00425304 loss)
I0826 05:06:56.370159  2068 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0826 05:07:09.413462  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:07:30.688589  2068 solver.cpp:357] Iteration 56700 (2.91387 iter/s, 34.3186s/100 iters), loss = 0.0077521
I0826 05:07:30.688707  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0132471 (* 1 = 0.0132471 loss)
I0826 05:07:30.688722  2068 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0826 05:08:05.012784  2068 solver.cpp:357] Iteration 56800 (2.91339 iter/s, 34.3242s/100 iters), loss = 0.00974976
I0826 05:08:05.012879  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00997713 (* 1 = 0.00997713 loss)
I0826 05:08:05.012890  2068 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0826 05:08:39.312520  2068 solver.cpp:357] Iteration 56900 (2.91547 iter/s, 34.2998s/100 iters), loss = 0.0495947
I0826 05:08:39.312661  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0364712 (* 1 = 0.0364712 loss)
I0826 05:08:39.312672  2068 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0826 05:09:13.286828  2068 solver.cpp:514] Iteration 57000, Testing net (#0)
I0826 05:09:21.441592  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:09:21.473175  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932003
I0826 05:09:21.473217  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.301818 (* 1 = 0.301818 loss)
I0826 05:09:21.811904  2068 solver.cpp:357] Iteration 57000 (2.35297 iter/s, 42.4995s/100 iters), loss = 0.006567
I0826 05:09:21.811956  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.005539 (* 1 = 0.005539 loss)
I0826 05:09:21.811969  2068 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0826 05:09:31.609019  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:09:56.164918  2068 solver.cpp:357] Iteration 57100 (2.91094 iter/s, 34.3532s/100 iters), loss = 0.00500855
I0826 05:09:56.165063  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00759733 (* 1 = 0.00759733 loss)
I0826 05:09:56.165074  2068 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0826 05:10:30.510277  2068 solver.cpp:357] Iteration 57200 (2.9116 iter/s, 34.3454s/100 iters), loss = 0.0351966
I0826 05:10:30.510426  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0277577 (* 1 = 0.0277577 loss)
I0826 05:10:30.510437  2068 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0826 05:11:04.874836  2068 solver.cpp:357] Iteration 57300 (2.90997 iter/s, 34.3646s/100 iters), loss = 0.00670009
I0826 05:11:04.874933  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0051315 (* 1 = 0.0051315 loss)
I0826 05:11:04.874943  2068 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0826 05:11:39.188637  2068 solver.cpp:357] Iteration 57400 (2.91427 iter/s, 34.3139s/100 iters), loss = 0.00512939
I0826 05:11:39.188773  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00769751 (* 1 = 0.00769751 loss)
I0826 05:11:39.188784  2068 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0826 05:11:45.713553  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:12:13.129045  2068 solver.cpp:514] Iteration 57500, Testing net (#0)
I0826 05:12:21.299438  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:12:21.330945  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931702
I0826 05:12:21.330987  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.299722 (* 1 = 0.299722 loss)
I0826 05:12:21.671347  2068 solver.cpp:357] Iteration 57500 (2.35389 iter/s, 42.4828s/100 iters), loss = 0.00325126
I0826 05:12:21.671398  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00266366 (* 1 = 0.00266366 loss)
I0826 05:12:21.671411  2068 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0826 05:12:56.019342  2068 solver.cpp:357] Iteration 57600 (2.91136 iter/s, 34.3482s/100 iters), loss = 0.0107672
I0826 05:12:56.019537  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00472124 (* 1 = 0.00472124 loss)
I0826 05:12:56.019548  2068 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0826 05:13:30.342231  2068 solver.cpp:357] Iteration 57700 (2.91351 iter/s, 34.3229s/100 iters), loss = 0.00810917
I0826 05:13:30.342365  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00320412 (* 1 = 0.00320412 loss)
I0826 05:13:30.342375  2068 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0826 05:14:04.632691  2068 solver.cpp:357] Iteration 57800 (2.91625 iter/s, 34.2906s/100 iters), loss = 0.0075286
I0826 05:14:04.632824  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00886193 (* 1 = 0.00886193 loss)
I0826 05:14:04.632834  2068 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0826 05:14:07.896456  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:14:38.950485  2068 solver.cpp:357] Iteration 57900 (2.91393 iter/s, 34.3179s/100 iters), loss = 0.0285536
I0826 05:14:38.950603  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00221924 (* 1 = 0.00221924 loss)
I0826 05:14:38.950618  2068 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0826 05:15:12.931960  2068 solver.cpp:514] Iteration 58000, Testing net (#0)
I0826 05:15:21.095734  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:15:21.127312  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0826 05:15:21.127357  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.303766 (* 1 = 0.303766 loss)
I0826 05:15:21.466626  2068 solver.cpp:357] Iteration 58000 (2.35204 iter/s, 42.5163s/100 iters), loss = 0.0048678
I0826 05:15:21.466681  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00618449 (* 1 = 0.00618449 loss)
I0826 05:15:21.466692  2068 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0826 05:15:55.796862  2068 solver.cpp:357] Iteration 58100 (2.91287 iter/s, 34.3304s/100 iters), loss = 0.00500276
I0826 05:15:55.797003  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00725186 (* 1 = 0.00725186 loss)
I0826 05:15:55.797016  2068 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0826 05:16:30.133239  2068 solver.cpp:357] Iteration 58200 (2.91235 iter/s, 34.3365s/100 iters), loss = 0.00903969
I0826 05:16:30.133337  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00728501 (* 1 = 0.00728501 loss)
I0826 05:16:30.133348  2068 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0826 05:16:30.312327  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:17:04.440098  2068 solver.cpp:357] Iteration 58300 (2.91486 iter/s, 34.307s/100 iters), loss = 0.0106709
I0826 05:17:04.440192  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0054004 (* 1 = 0.0054004 loss)
I0826 05:17:04.440204  2068 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0826 05:17:38.755522  2068 solver.cpp:357] Iteration 58400 (2.91413 iter/s, 34.3156s/100 iters), loss = 0.0049225
I0826 05:17:38.755663  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00616368 (* 1 = 0.00616368 loss)
I0826 05:17:38.755674  2068 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0826 05:18:12.745836  2068 solver.cpp:514] Iteration 58500, Testing net (#0)
I0826 05:18:20.913936  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:18:20.945153  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930102
I0826 05:18:20.945197  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.302097 (* 1 = 0.302097 loss)
I0826 05:18:21.283118  2068 solver.cpp:357] Iteration 58500 (2.3514 iter/s, 42.5278s/100 iters), loss = 0.0107001
I0826 05:18:21.283169  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00667039 (* 1 = 0.00667039 loss)
I0826 05:18:21.283180  2068 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0826 05:18:52.520834  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:18:55.605351  2068 solver.cpp:357] Iteration 58600 (2.91355 iter/s, 34.3224s/100 iters), loss = 0.0209405
I0826 05:18:55.605414  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0290654 (* 1 = 0.0290654 loss)
I0826 05:18:55.605427  2068 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0826 05:19:29.965804  2068 solver.cpp:357] Iteration 58700 (2.91031 iter/s, 34.3606s/100 iters), loss = 0.0132495
I0826 05:19:29.965953  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0125032 (* 1 = 0.0125032 loss)
I0826 05:19:29.965965  2068 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0826 05:20:04.292704  2068 solver.cpp:357] Iteration 58800 (2.91316 iter/s, 34.327s/100 iters), loss = 0.00759687
I0826 05:20:04.292793  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00946165 (* 1 = 0.00946165 loss)
I0826 05:20:04.292805  2068 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0826 05:20:38.620620  2068 solver.cpp:357] Iteration 58900 (2.91307 iter/s, 34.3281s/100 iters), loss = 0.00584038
I0826 05:20:38.620719  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00393664 (* 1 = 0.00393664 loss)
I0826 05:20:38.620731  2068 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0826 05:21:06.612982  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:21:12.621852  2068 solver.cpp:514] Iteration 59000, Testing net (#0)
I0826 05:21:20.807430  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:21:20.838799  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932002
I0826 05:21:20.838842  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.300488 (* 1 = 0.300488 loss)
I0826 05:21:21.178000  2068 solver.cpp:357] Iteration 59000 (2.34976 iter/s, 42.5576s/100 iters), loss = 0.00183953
I0826 05:21:21.178055  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00223908 (* 1 = 0.00223908 loss)
I0826 05:21:21.178066  2068 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0826 05:21:55.526535  2068 solver.cpp:357] Iteration 59100 (2.91131 iter/s, 34.3487s/100 iters), loss = 0.0038575
I0826 05:21:55.526679  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00467664 (* 1 = 0.00467664 loss)
I0826 05:21:55.526690  2068 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0826 05:22:29.918385  2068 solver.cpp:357] Iteration 59200 (2.90766 iter/s, 34.392s/100 iters), loss = 0.018502
I0826 05:22:29.918529  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0208556 (* 1 = 0.0208556 loss)
I0826 05:22:29.918540  2068 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0826 05:23:04.252616  2068 solver.cpp:357] Iteration 59300 (2.91253 iter/s, 34.3344s/100 iters), loss = 0.00343527
I0826 05:23:04.252715  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0028004 (* 1 = 0.0028004 loss)
I0826 05:23:04.252727  2068 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0826 05:23:28.993892  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:23:38.598875  2068 solver.cpp:357] Iteration 59400 (2.91151 iter/s, 34.3464s/100 iters), loss = 0.00574611
I0826 05:23:38.599014  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0075695 (* 1 = 0.0075695 loss)
I0826 05:23:38.599025  2068 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0826 05:24:12.573806  2068 solver.cpp:514] Iteration 59500, Testing net (#0)
I0826 05:24:20.772064  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:24:20.803707  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932802
I0826 05:24:20.803750  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.302284 (* 1 = 0.302284 loss)
I0826 05:24:21.145138  2068 solver.cpp:357] Iteration 59500 (2.35037 iter/s, 42.5465s/100 iters), loss = 0.00868387
I0826 05:24:21.145190  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00442437 (* 1 = 0.00442437 loss)
I0826 05:24:21.145206  2068 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0826 05:24:55.460885  2068 solver.cpp:357] Iteration 59600 (2.9141 iter/s, 34.316s/100 iters), loss = 0.00751492
I0826 05:24:55.461078  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00386717 (* 1 = 0.00386717 loss)
I0826 05:24:55.461089  2068 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0826 05:25:29.788447  2068 solver.cpp:357] Iteration 59700 (2.9131 iter/s, 34.3276s/100 iters), loss = 0.00772324
I0826 05:25:29.788591  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00787368 (* 1 = 0.00787368 loss)
I0826 05:25:29.788604  2068 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0826 05:25:51.421878  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:26:04.126122  2068 solver.cpp:357] Iteration 59800 (2.91224 iter/s, 34.3378s/100 iters), loss = 0.00576078
I0826 05:26:04.126233  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00513243 (* 1 = 0.00513243 loss)
I0826 05:26:04.126248  2068 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0826 05:26:38.467937  2068 solver.cpp:357] Iteration 59900 (2.91189 iter/s, 34.342s/100 iters), loss = 0.0043116
I0826 05:26:38.468073  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00672536 (* 1 = 0.00672536 loss)
I0826 05:26:38.468083  2068 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0826 05:27:12.447129  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_60000.caffemodel
I0826 05:27:12.471072  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_60000.solverstate
I0826 05:27:12.480470  2068 solver.cpp:514] Iteration 60000, Testing net (#0)
I0826 05:27:20.685000  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:27:20.728991  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932602
I0826 05:27:20.729035  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.307406 (* 1 = 0.307406 loss)
I0826 05:27:21.066198  2068 solver.cpp:357] Iteration 60000 (2.3475 iter/s, 42.5985s/100 iters), loss = 0.00666321
I0826 05:27:21.066252  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00125002 (* 1 = 0.00125002 loss)
I0826 05:27:21.066264  2068 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0826 05:27:55.365792  2068 solver.cpp:357] Iteration 60100 (2.91547 iter/s, 34.2998s/100 iters), loss = 0.00434515
I0826 05:27:55.365936  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00471392 (* 1 = 0.00471392 loss)
I0826 05:27:55.365947  2068 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0826 05:28:13.762925  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:28:29.734236  2068 solver.cpp:357] Iteration 60200 (2.90963 iter/s, 34.3686s/100 iters), loss = 0.00678858
I0826 05:28:29.734401  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0108878 (* 1 = 0.0108878 loss)
I0826 05:28:29.734414  2068 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0826 05:29:04.105039  2068 solver.cpp:357] Iteration 60300 (2.90928 iter/s, 34.3728s/100 iters), loss = 0.00434567
I0826 05:29:04.105136  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00713282 (* 1 = 0.00713282 loss)
I0826 05:29:04.105149  2068 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0826 05:29:38.437386  2068 solver.cpp:357] Iteration 60400 (2.91242 iter/s, 34.3357s/100 iters), loss = 0.00614503
I0826 05:29:38.437527  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.011425 (* 1 = 0.011425 loss)
I0826 05:29:38.437539  2068 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0826 05:30:12.365219  2068 solver.cpp:514] Iteration 60500, Testing net (#0)
I0826 05:30:20.514497  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:30:20.545811  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932302
I0826 05:30:20.545855  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.305706 (* 1 = 0.305706 loss)
I0826 05:30:20.885685  2068 solver.cpp:357] Iteration 60500 (2.35559 iter/s, 42.4522s/100 iters), loss = 0.00475469
I0826 05:30:20.885736  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.007054 (* 1 = 0.007054 loss)
I0826 05:30:20.885749  2068 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0826 05:30:35.974159  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:30:55.166842  2068 solver.cpp:357] Iteration 60600 (2.9168 iter/s, 34.2841s/100 iters), loss = 0.00724844
I0826 05:30:55.166997  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00792629 (* 1 = 0.00792629 loss)
I0826 05:30:55.167008  2068 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0826 05:31:29.459122  2068 solver.cpp:357] Iteration 60700 (2.91588 iter/s, 34.295s/100 iters), loss = 0.00644897
I0826 05:31:29.459269  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00150193 (* 1 = 0.00150193 loss)
I0826 05:31:29.459278  2068 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0826 05:32:03.795852  2068 solver.cpp:357] Iteration 60800 (2.91211 iter/s, 34.3393s/100 iters), loss = 0.00511782
I0826 05:32:03.795948  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00766548 (* 1 = 0.00766548 loss)
I0826 05:32:03.795958  2068 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0826 05:32:38.144428  2068 solver.cpp:357] Iteration 60900 (2.91112 iter/s, 34.3511s/100 iters), loss = 0.00213265
I0826 05:32:38.144520  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00185077 (* 1 = 0.00185077 loss)
I0826 05:32:38.144534  2068 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0826 05:32:49.987335  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:33:12.158835  2068 solver.cpp:514] Iteration 61000, Testing net (#0)
I0826 05:33:20.345508  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:33:20.377149  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932202
I0826 05:33:20.377194  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.305989 (* 1 = 0.305989 loss)
I0826 05:33:20.716326  2068 solver.cpp:357] Iteration 61000 (2.3488 iter/s, 42.5749s/100 iters), loss = 0.00294598
I0826 05:33:20.716379  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0024269 (* 1 = 0.0024269 loss)
I0826 05:33:20.716392  2068 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0826 05:33:55.075656  2068 solver.cpp:357] Iteration 61100 (2.91022 iter/s, 34.3616s/100 iters), loss = 0.00540172
I0826 05:33:55.075808  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00815264 (* 1 = 0.00815264 loss)
I0826 05:33:55.075819  2068 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0826 05:34:29.418941  2068 solver.cpp:357] Iteration 61200 (2.9116 iter/s, 34.3454s/100 iters), loss = 0.00354
I0826 05:34:29.419062  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00145002 (* 1 = 0.00145002 loss)
I0826 05:34:29.419072  2068 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0826 05:35:03.821516  2068 solver.cpp:357] Iteration 61300 (2.90659 iter/s, 34.4046s/100 iters), loss = 0.00724064
I0826 05:35:03.821612  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00591398 (* 1 = 0.00591398 loss)
I0826 05:35:03.821626  2068 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0826 05:35:12.585471  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:35:38.173296  2068 solver.cpp:357] Iteration 61400 (2.91089 iter/s, 34.3537s/100 iters), loss = 0.0121404
I0826 05:35:38.173444  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00346744 (* 1 = 0.00346744 loss)
I0826 05:35:38.173456  2068 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0826 05:36:12.116523  2068 solver.cpp:514] Iteration 61500, Testing net (#0)
I0826 05:36:20.239989  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:36:20.271458  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932602
I0826 05:36:20.271502  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.310055 (* 1 = 0.310055 loss)
I0826 05:36:20.610167  2068 solver.cpp:357] Iteration 61500 (2.35632 iter/s, 42.4391s/100 iters), loss = 0.0122333
I0826 05:36:20.610220  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157652 (* 1 = 0.0157652 loss)
I0826 05:36:20.610234  2068 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0826 05:36:54.967628  2068 solver.cpp:357] Iteration 61600 (2.91043 iter/s, 34.3592s/100 iters), loss = 0.0100782
I0826 05:36:54.967751  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00730607 (* 1 = 0.00730607 loss)
I0826 05:36:54.967767  2068 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0826 05:37:29.305913  2068 solver.cpp:357] Iteration 61700 (2.91206 iter/s, 34.3399s/100 iters), loss = 0.00107224
I0826 05:37:29.306054  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.000914394 (* 1 = 0.000914394 loss)
I0826 05:37:29.306064  2068 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0826 05:37:34.805399  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:38:03.556586  2068 solver.cpp:357] Iteration 61800 (2.91952 iter/s, 34.2522s/100 iters), loss = 0.00333739
I0826 05:38:03.556725  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00506303 (* 1 = 0.00506303 loss)
I0826 05:38:03.556735  2068 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0826 05:38:37.908341  2068 solver.cpp:357] Iteration 61900 (2.91093 iter/s, 34.3532s/100 iters), loss = 0.00328984
I0826 05:38:37.908478  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00397676 (* 1 = 0.00397676 loss)
I0826 05:38:37.908490  2068 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0826 05:39:11.927073  2068 solver.cpp:514] Iteration 62000, Testing net (#0)
I0826 05:39:20.050211  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:39:20.081487  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933902
I0826 05:39:20.081531  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.306377 (* 1 = 0.306377 loss)
I0826 05:39:20.419322  2068 solver.cpp:357] Iteration 62000 (2.35223 iter/s, 42.5128s/100 iters), loss = 0.00639123
I0826 05:39:20.419373  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0039546 (* 1 = 0.0039546 loss)
I0826 05:39:20.419386  2068 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0826 05:39:54.766814  2068 solver.cpp:357] Iteration 62100 (2.9113 iter/s, 34.3489s/100 iters), loss = 0.0037393
I0826 05:39:54.766957  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00648295 (* 1 = 0.00648295 loss)
I0826 05:39:54.766971  2068 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0826 05:39:57.001987  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:40:29.066474  2068 solver.cpp:357] Iteration 62200 (2.91537 iter/s, 34.3009s/100 iters), loss = 0.00660312
I0826 05:40:29.066571  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.003065 (* 1 = 0.003065 loss)
I0826 05:40:29.066584  2068 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0826 05:41:03.371846  2068 solver.cpp:357] Iteration 62300 (2.91489 iter/s, 34.3067s/100 iters), loss = 0.02849
I0826 05:41:03.371942  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00593852 (* 1 = 0.00593852 loss)
I0826 05:41:03.371954  2068 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0826 05:41:37.688536  2068 solver.cpp:357] Iteration 62400 (2.91393 iter/s, 34.3179s/100 iters), loss = 0.0102331
I0826 05:41:37.688676  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0151099 (* 1 = 0.0151099 loss)
I0826 05:41:37.688691  2068 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0826 05:42:11.017817  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:42:11.702913  2068 solver.cpp:514] Iteration 62500, Testing net (#0)
I0826 05:42:19.902824  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:42:19.934018  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933302
I0826 05:42:19.934060  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.308838 (* 1 = 0.308838 loss)
I0826 05:42:20.271283  2068 solver.cpp:357] Iteration 62500 (2.34829 iter/s, 42.5842s/100 iters), loss = 0.00375847
I0826 05:42:20.271335  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00465219 (* 1 = 0.00465219 loss)
I0826 05:42:20.271347  2068 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0826 05:42:54.636700  2068 solver.cpp:357] Iteration 62600 (2.9098 iter/s, 34.3666s/100 iters), loss = 0.0060679
I0826 05:42:54.636852  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00619356 (* 1 = 0.00619356 loss)
I0826 05:42:54.636868  2068 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0826 05:43:28.965883  2068 solver.cpp:357] Iteration 62700 (2.91288 iter/s, 34.3302s/100 iters), loss = 0.00306138
I0826 05:43:28.966018  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00243562 (* 1 = 0.00243562 loss)
I0826 05:43:28.966029  2068 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0826 05:44:03.294939  2068 solver.cpp:357] Iteration 62800 (2.9129 iter/s, 34.3301s/100 iters), loss = 0.00446037
I0826 05:44:03.295037  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00788847 (* 1 = 0.00788847 loss)
I0826 05:44:03.295047  2068 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0826 05:44:33.501653  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:44:37.621990  2068 solver.cpp:357] Iteration 62900 (2.91307 iter/s, 34.3281s/100 iters), loss = 0.0158381
I0826 05:44:37.622053  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0227596 (* 1 = 0.0227596 loss)
I0826 05:44:37.622066  2068 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0826 05:45:11.643105  2068 solver.cpp:514] Iteration 63000, Testing net (#0)
I0826 05:45:19.812407  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:45:19.843780  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932102
I0826 05:45:19.843823  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.307456 (* 1 = 0.307456 loss)
I0826 05:45:20.181504  2068 solver.cpp:357] Iteration 63000 (2.34958 iter/s, 42.5608s/100 iters), loss = 0.00240647
I0826 05:45:20.181555  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00236375 (* 1 = 0.00236375 loss)
I0826 05:45:20.181566  2068 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0826 05:45:54.538938  2068 solver.cpp:357] Iteration 63100 (2.91049 iter/s, 34.3584s/100 iters), loss = 0.00609715
I0826 05:45:54.539083  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00627882 (* 1 = 0.00627882 loss)
I0826 05:45:54.539094  2068 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0826 05:46:28.847832  2068 solver.cpp:357] Iteration 63200 (2.91462 iter/s, 34.3098s/100 iters), loss = 0.00254226
I0826 05:46:28.847975  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00217899 (* 1 = 0.00217899 loss)
I0826 05:46:28.847986  2068 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0826 05:46:55.852746  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:47:03.253007  2068 solver.cpp:357] Iteration 63300 (2.90647 iter/s, 34.406s/100 iters), loss = 0.00341791
I0826 05:47:03.253101  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00298499 (* 1 = 0.00298499 loss)
I0826 05:47:03.253113  2068 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0826 05:47:37.578658  2068 solver.cpp:357] Iteration 63400 (2.9132 iter/s, 34.3265s/100 iters), loss = 0.0153368
I0826 05:47:37.578791  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00338056 (* 1 = 0.00338056 loss)
I0826 05:47:37.578806  2068 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0826 05:48:11.602073  2068 solver.cpp:514] Iteration 63500, Testing net (#0)
I0826 05:48:19.741721  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:48:19.773120  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931302
I0826 05:48:19.773164  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.310692 (* 1 = 0.310692 loss)
I0826 05:48:20.112622  2068 solver.cpp:357] Iteration 63500 (2.351 iter/s, 42.535s/100 iters), loss = 0.0192058
I0826 05:48:20.112676  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0369444 (* 1 = 0.0369444 loss)
I0826 05:48:20.112689  2068 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0826 05:48:54.402952  2068 solver.cpp:357] Iteration 63600 (2.9162 iter/s, 34.2912s/100 iters), loss = 0.00936184
I0826 05:48:54.403075  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0060059 (* 1 = 0.0060059 loss)
I0826 05:48:54.403085  2068 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0826 05:49:18.106356  2082 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:49:28.733031  2068 solver.cpp:357] Iteration 63700 (2.91283 iter/s, 34.3309s/100 iters), loss = 0.00440613
I0826 05:49:28.733172  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00656009 (* 1 = 0.00656009 loss)
I0826 05:49:28.733183  2068 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0826 05:50:03.038230  2068 solver.cpp:357] Iteration 63800 (2.91494 iter/s, 34.306s/100 iters), loss = 0.00731929
I0826 05:50:03.038326  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00834818 (* 1 = 0.00834818 loss)
I0826 05:50:03.038364  2068 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0826 05:50:37.417347  2068 solver.cpp:357] Iteration 63900 (2.90868 iter/s, 34.3799s/100 iters), loss = 0.00358778
I0826 05:50:37.417450  2068 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00240867 (* 1 = 0.00240867 loss)
I0826 05:50:37.417464  2068 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0826 05:51:11.448463  2068 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_56_iter_64000.caffemodel
I0826 05:51:11.472373  2068 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_56_iter_64000.solverstate
I0826 05:51:11.528913  2068 solver.cpp:472] Iteration 64000, loss = 0.00502443
I0826 05:51:11.528961  2068 solver.cpp:514] Iteration 64000, Testing net (#0)
I0826 05:51:19.736275  2093 data_layer.cpp:73] Restarting data prefetching from start.
I0826 05:51:19.767539  2068 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931902
I0826 05:51:19.767581  2068 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.313959 (* 1 = 0.313959 loss)
I0826 05:51:19.767588  2068 solver.cpp:479] Optimization Done.
I0826 05:51:19.767593  2068 caffe.cpp:326] Optimization Done.
