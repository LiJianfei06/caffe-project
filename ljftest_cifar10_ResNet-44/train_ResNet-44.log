WARNING: Logging before InitGoogleLogging() is written to STDERR
I0818 10:49:14.335034  2412 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0818 10:49:14.335178  2412 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0818 10:49:14.335184  2412 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0818 10:49:14.335188  2412 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0818 10:49:14.335191  2412 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0818 10:49:14.335196  2412 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0818 10:49:14.335265  2412 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0818 10:49:14.335463  2412 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0818 10:49:14.337255  2412 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0818 10:49:14.337291  2412 caffe.cpp:269] Using GPUs 0
I0818 10:49:14.343519  2412 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0818 10:49:15.005426  2412 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0818 10:49:15.005475  2412 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0818 10:49:15.097095  2412 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_44.prototxt"
test_net: "./test_ResNet_44.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_44"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
type: "Nesterov"
I0818 10:49:15.097396  2412 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_44.prototxt
I0818 10:49:15.099038  2412 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_44.prototxt
I0818 10:49:15.099064  2412 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0818 10:49:15.099424  2412 net.cpp:390] layer_param.include_size():1
I0818 10:49:15.099439  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099448  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099453  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099457  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099462  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099465  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099469  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099473  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099478  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099481  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099485  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099491  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099495  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099499  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099503  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099508  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099510  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099514  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099519  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099524  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099527  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099531  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099535  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099540  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099545  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099548  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099552  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099557  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099561  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099565  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099570  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099575  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099578  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099582  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099586  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099591  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099594  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099598  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099634  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099638  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099642  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099647  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099650  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099654  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099658  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099661  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099665  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099670  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099673  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099678  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099681  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099685  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099689  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099692  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099696  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099700  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099704  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099709  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099712  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099716  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099720  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099723  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099727  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099731  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099735  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099740  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099743  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099747  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099750  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099755  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099758  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099762  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099766  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099771  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099774  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099777  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099781  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099786  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099788  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099793  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099797  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099800  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099804  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099808  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099812  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099815  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099819  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099823  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099828  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099831  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099834  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099838  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099843  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099846  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099849  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099853  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099858  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099869  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099874  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099879  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099882  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099886  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099889  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099894  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099897  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099901  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099905  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099910  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099912  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099916  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099920  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099925  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099927  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099932  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099936  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099939  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099943  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099947  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099951  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099954  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099958  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099962  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099967  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099970  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099973  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099977  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099982  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099985  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099989  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.099993  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.099997  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100000  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100004  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100008  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100013  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100016  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100021  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100025  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100028  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100033  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100037  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100041  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100045  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100049  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100054  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100059  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100061  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100065  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100070  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100075  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100078  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100082  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100085  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100090  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100093  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100106  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100109  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100114  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100119  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100123  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100126  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100131  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100136  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100138  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100142  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100147  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100150  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100154  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100157  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100162  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100167  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100169  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100173  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100178  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100181  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100185  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100188  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100193  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100198  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100201  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100204  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100208  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100212  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100216  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100220  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100224  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100229  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100232  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100236  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100240  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100244  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100248  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100252  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100256  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100261  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100265  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100268  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100272  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100278  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100282  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100286  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100289  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100296  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100299  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100302  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100306  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100311  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100316  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100319  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100323  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100327  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100330  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100334  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100340  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100350  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100355  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100358  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100363  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100366  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100370  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100374  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100378  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100381  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100385  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100390  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100394  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100399  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100402  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100405  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100409  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100414  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100417  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100421  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100425  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100428  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100432  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100436  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100440  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100443  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100447  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100450  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100455  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100458  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100462  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100466  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100469  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100473  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100477  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100481  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100484  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100488  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100492  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100495  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100499  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100503  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100507  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100510  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100514  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100518  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100522  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100525  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100529  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100533  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100538  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100540  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100544  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100548  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100551  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100555  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100559  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100564  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100567  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100577  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100581  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100585  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100589  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100594  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100597  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100601  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100605  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100608  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100612  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100616  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100620  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100623  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100627  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100631  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100636  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100638  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100642  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100646  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100649  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100653  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100657  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100661  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100664  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100668  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100672  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100675  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100679  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100683  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100687  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100690  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100694  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100698  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100703  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100705  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100709  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100713  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100716  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100720  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100724  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100728  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100733  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100735  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100739  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100744  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100746  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100750  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100754  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100759  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100761  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100765  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100769  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100772  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100776  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100780  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100785  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100787  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100791  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100795  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100805  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100809  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100813  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100817  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100821  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100824  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100828  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100832  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100836  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100839  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100843  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100847  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100850  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100854  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100858  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100862  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100865  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100869  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100873  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100878  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100880  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100884  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100888  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100891  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100895  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100899  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100903  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100906  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100910  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100914  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100919  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100921  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100925  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100929  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100934  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100936  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100940  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100944  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100949  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100951  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100956  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100960  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100963  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100967  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100970  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100975  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100978  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100981  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100986  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100989  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.100993  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.100997  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101001  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101004  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101008  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101012  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101016  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101019  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101029  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101033  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101037  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101042  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101045  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101048  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101052  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101056  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101060  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101063  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101068  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101071  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101075  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101078  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.101083  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:15.101086  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:15.102422  2412 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215684
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_para
I0818 10:49:15.103328  2412 layer_factory.hpp:77] Creating layer Data1
I0818 10:49:15.103513  2412 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0818 10:49:15.103574  2412 net.cpp:128] Creating Layer Data1
I0818 10:49:15.103585  2412 net.cpp:522] Data1 -> Data1
I0818 10:49:15.103621  2412 net.cpp:522] Data1 -> Data2
I0818 10:49:15.105422  2412 data_layer.cpp:45] output data size: 128,3,32,32
I0818 10:49:15.120543  2412 net.cpp:172] Setting up Data1
I0818 10:49:15.120602  2412 net.cpp:186] Top shape: 128 3 32 32 (393216)
I0818 10:49:15.120610  2412 net.cpp:186] Top shape: 128 (128)
I0818 10:49:15.120620  2412 net.cpp:194] Memory required for data: 1573376
I0818 10:49:15.120640  2412 layer_factory.hpp:77] Creating layer Convolution1
I0818 10:49:15.120676  2412 net.cpp:128] Creating Layer Convolution1
I0818 10:49:15.120685  2412 net.cpp:558] Convolution1 <- Data1
I0818 10:49:15.120708  2412 net.cpp:522] Convolution1 -> Convolution1
I0818 10:49:16.145515  2412 net.cpp:172] Setting up Convolution1
I0818 10:49:16.145572  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.145577  2412 net.cpp:194] Memory required for data: 9961984
I0818 10:49:16.145622  2412 layer_factory.hpp:77] Creating layer BatchNorm1
I0818 10:49:16.145640  2412 net.cpp:128] Creating Layer BatchNorm1
I0818 10:49:16.145699  2412 net.cpp:558] BatchNorm1 <- Convolution1
I0818 10:49:16.145723  2412 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0818 10:49:16.145983  2412 net.cpp:172] Setting up BatchNorm1
I0818 10:49:16.146013  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.146028  2412 net.cpp:194] Memory required for data: 18350592
I0818 10:49:16.146087  2412 layer_factory.hpp:77] Creating layer Scale1
I0818 10:49:16.146112  2412 net.cpp:128] Creating Layer Scale1
I0818 10:49:16.146134  2412 net.cpp:558] Scale1 <- Convolution1
I0818 10:49:16.146152  2412 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0818 10:49:16.146219  2412 layer_factory.hpp:77] Creating layer Scale1
I0818 10:49:16.146353  2412 net.cpp:172] Setting up Scale1
I0818 10:49:16.146381  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.146389  2412 net.cpp:194] Memory required for data: 26739200
I0818 10:49:16.146399  2412 layer_factory.hpp:77] Creating layer ReLU1
I0818 10:49:16.146410  2412 net.cpp:128] Creating Layer ReLU1
I0818 10:49:16.146417  2412 net.cpp:558] ReLU1 <- Convolution1
I0818 10:49:16.146423  2412 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0818 10:49:16.146658  2412 net.cpp:172] Setting up ReLU1
I0818 10:49:16.146672  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.146677  2412 net.cpp:194] Memory required for data: 35127808
I0818 10:49:16.146682  2412 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0818 10:49:16.146690  2412 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0818 10:49:16.146719  2412 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0818 10:49:16.146741  2412 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0818 10:49:16.146764  2412 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0818 10:49:16.146817  2412 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0818 10:49:16.146841  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.146863  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.146878  2412 net.cpp:194] Memory required for data: 51905024
I0818 10:49:16.146894  2412 layer_factory.hpp:77] Creating layer Convolution2
I0818 10:49:16.146919  2412 net.cpp:128] Creating Layer Convolution2
I0818 10:49:16.146937  2412 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0818 10:49:16.146957  2412 net.cpp:522] Convolution2 -> Convolution2
I0818 10:49:16.151629  2412 net.cpp:172] Setting up Convolution2
I0818 10:49:16.151657  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.151662  2412 net.cpp:194] Memory required for data: 60293632
I0818 10:49:16.151680  2412 layer_factory.hpp:77] Creating layer BatchNorm2
I0818 10:49:16.151690  2412 net.cpp:128] Creating Layer BatchNorm2
I0818 10:49:16.151700  2412 net.cpp:558] BatchNorm2 <- Convolution2
I0818 10:49:16.151707  2412 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0818 10:49:16.151926  2412 net.cpp:172] Setting up BatchNorm2
I0818 10:49:16.151937  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.151942  2412 net.cpp:194] Memory required for data: 68682240
I0818 10:49:16.151952  2412 layer_factory.hpp:77] Creating layer Scale2
I0818 10:49:16.151990  2412 net.cpp:128] Creating Layer Scale2
I0818 10:49:16.152009  2412 net.cpp:558] Scale2 <- Convolution2
I0818 10:49:16.152029  2412 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0818 10:49:16.152084  2412 layer_factory.hpp:77] Creating layer Scale2
I0818 10:49:16.152213  2412 net.cpp:172] Setting up Scale2
I0818 10:49:16.152223  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.152228  2412 net.cpp:194] Memory required for data: 77070848
I0818 10:49:16.152236  2412 layer_factory.hpp:77] Creating layer ReLU2
I0818 10:49:16.152245  2412 net.cpp:128] Creating Layer ReLU2
I0818 10:49:16.152248  2412 net.cpp:558] ReLU2 <- Convolution2
I0818 10:49:16.152254  2412 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0818 10:49:16.153692  2412 net.cpp:172] Setting up ReLU2
I0818 10:49:16.153708  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.153712  2412 net.cpp:194] Memory required for data: 85459456
I0818 10:49:16.153718  2412 layer_factory.hpp:77] Creating layer Convolution3
I0818 10:49:16.153734  2412 net.cpp:128] Creating Layer Convolution3
I0818 10:49:16.153739  2412 net.cpp:558] Convolution3 <- Convolution2
I0818 10:49:16.153792  2412 net.cpp:522] Convolution3 -> Convolution3
I0818 10:49:16.160336  2412 net.cpp:172] Setting up Convolution3
I0818 10:49:16.160363  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.160367  2412 net.cpp:194] Memory required for data: 93848064
I0818 10:49:16.160377  2412 layer_factory.hpp:77] Creating layer BatchNorm3
I0818 10:49:16.160387  2412 net.cpp:128] Creating Layer BatchNorm3
I0818 10:49:16.160392  2412 net.cpp:558] BatchNorm3 <- Convolution3
I0818 10:49:16.160400  2412 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0818 10:49:16.160627  2412 net.cpp:172] Setting up BatchNorm3
I0818 10:49:16.160640  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.160645  2412 net.cpp:194] Memory required for data: 102236672
I0818 10:49:16.160658  2412 layer_factory.hpp:77] Creating layer Scale3
I0818 10:49:16.160667  2412 net.cpp:128] Creating Layer Scale3
I0818 10:49:16.160671  2412 net.cpp:558] Scale3 <- Convolution3
I0818 10:49:16.160677  2412 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0818 10:49:16.160714  2412 layer_factory.hpp:77] Creating layer Scale3
I0818 10:49:16.160848  2412 net.cpp:172] Setting up Scale3
I0818 10:49:16.160859  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.160863  2412 net.cpp:194] Memory required for data: 110625280
I0818 10:49:16.160871  2412 layer_factory.hpp:77] Creating layer Eltwise1
I0818 10:49:16.160881  2412 net.cpp:128] Creating Layer Eltwise1
I0818 10:49:16.160886  2412 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0818 10:49:16.160890  2412 net.cpp:558] Eltwise1 <- Convolution3
I0818 10:49:16.160897  2412 net.cpp:522] Eltwise1 -> Eltwise1
I0818 10:49:16.160928  2412 net.cpp:172] Setting up Eltwise1
I0818 10:49:16.160939  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.160944  2412 net.cpp:194] Memory required for data: 119013888
I0818 10:49:16.160948  2412 layer_factory.hpp:77] Creating layer ReLU3
I0818 10:49:16.160954  2412 net.cpp:128] Creating Layer ReLU3
I0818 10:49:16.160959  2412 net.cpp:558] ReLU3 <- Eltwise1
I0818 10:49:16.160964  2412 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0818 10:49:16.162441  2412 net.cpp:172] Setting up ReLU3
I0818 10:49:16.162466  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.162470  2412 net.cpp:194] Memory required for data: 127402496
I0818 10:49:16.162475  2412 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0818 10:49:16.162485  2412 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0818 10:49:16.162490  2412 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0818 10:49:16.162497  2412 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0818 10:49:16.162510  2412 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0818 10:49:16.162557  2412 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0818 10:49:16.162565  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.162571  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.162575  2412 net.cpp:194] Memory required for data: 144179712
I0818 10:49:16.162580  2412 layer_factory.hpp:77] Creating layer Convolution4
I0818 10:49:16.162593  2412 net.cpp:128] Creating Layer Convolution4
I0818 10:49:16.162597  2412 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0818 10:49:16.162606  2412 net.cpp:522] Convolution4 -> Convolution4
I0818 10:49:16.169075  2412 net.cpp:172] Setting up Convolution4
I0818 10:49:16.169102  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.169106  2412 net.cpp:194] Memory required for data: 152568320
I0818 10:49:16.169116  2412 layer_factory.hpp:77] Creating layer BatchNorm4
I0818 10:49:16.169126  2412 net.cpp:128] Creating Layer BatchNorm4
I0818 10:49:16.169131  2412 net.cpp:558] BatchNorm4 <- Convolution4
I0818 10:49:16.169138  2412 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0818 10:49:16.169373  2412 net.cpp:172] Setting up BatchNorm4
I0818 10:49:16.169387  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.169391  2412 net.cpp:194] Memory required for data: 160956928
I0818 10:49:16.169418  2412 layer_factory.hpp:77] Creating layer Scale4
I0818 10:49:16.169426  2412 net.cpp:128] Creating Layer Scale4
I0818 10:49:16.169431  2412 net.cpp:558] Scale4 <- Convolution4
I0818 10:49:16.169437  2412 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0818 10:49:16.169477  2412 layer_factory.hpp:77] Creating layer Scale4
I0818 10:49:16.169610  2412 net.cpp:172] Setting up Scale4
I0818 10:49:16.169621  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.169625  2412 net.cpp:194] Memory required for data: 169345536
I0818 10:49:16.169633  2412 layer_factory.hpp:77] Creating layer ReLU4
I0818 10:49:16.169641  2412 net.cpp:128] Creating Layer ReLU4
I0818 10:49:16.169646  2412 net.cpp:558] ReLU4 <- Convolution4
I0818 10:49:16.169651  2412 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0818 10:49:16.171134  2412 net.cpp:172] Setting up ReLU4
I0818 10:49:16.171149  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.171154  2412 net.cpp:194] Memory required for data: 177734144
I0818 10:49:16.171157  2412 layer_factory.hpp:77] Creating layer Convolution5
I0818 10:49:16.171170  2412 net.cpp:128] Creating Layer Convolution5
I0818 10:49:16.171175  2412 net.cpp:558] Convolution5 <- Convolution4
I0818 10:49:16.171183  2412 net.cpp:522] Convolution5 -> Convolution5
I0818 10:49:16.177824  2412 net.cpp:172] Setting up Convolution5
I0818 10:49:16.177850  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.177855  2412 net.cpp:194] Memory required for data: 186122752
I0818 10:49:16.177865  2412 layer_factory.hpp:77] Creating layer BatchNorm5
I0818 10:49:16.177875  2412 net.cpp:128] Creating Layer BatchNorm5
I0818 10:49:16.177881  2412 net.cpp:558] BatchNorm5 <- Convolution5
I0818 10:49:16.177887  2412 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0818 10:49:16.178126  2412 net.cpp:172] Setting up BatchNorm5
I0818 10:49:16.178139  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.178143  2412 net.cpp:194] Memory required for data: 194511360
I0818 10:49:16.178156  2412 layer_factory.hpp:77] Creating layer Scale5
I0818 10:49:16.178165  2412 net.cpp:128] Creating Layer Scale5
I0818 10:49:16.178169  2412 net.cpp:558] Scale5 <- Convolution5
I0818 10:49:16.178177  2412 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0818 10:49:16.178215  2412 layer_factory.hpp:77] Creating layer Scale5
I0818 10:49:16.178349  2412 net.cpp:172] Setting up Scale5
I0818 10:49:16.178361  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.178365  2412 net.cpp:194] Memory required for data: 202899968
I0818 10:49:16.178373  2412 layer_factory.hpp:77] Creating layer Eltwise2
I0818 10:49:16.178381  2412 net.cpp:128] Creating Layer Eltwise2
I0818 10:49:16.178386  2412 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0818 10:49:16.178391  2412 net.cpp:558] Eltwise2 <- Convolution5
I0818 10:49:16.178397  2412 net.cpp:522] Eltwise2 -> Eltwise2
I0818 10:49:16.178422  2412 net.cpp:172] Setting up Eltwise2
I0818 10:49:16.178434  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.178438  2412 net.cpp:194] Memory required for data: 211288576
I0818 10:49:16.178443  2412 layer_factory.hpp:77] Creating layer ReLU5
I0818 10:49:16.178450  2412 net.cpp:128] Creating Layer ReLU5
I0818 10:49:16.178454  2412 net.cpp:558] ReLU5 <- Eltwise2
I0818 10:49:16.178460  2412 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0818 10:49:16.179879  2412 net.cpp:172] Setting up ReLU5
I0818 10:49:16.179893  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.179898  2412 net.cpp:194] Memory required for data: 219677184
I0818 10:49:16.179903  2412 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0818 10:49:16.179910  2412 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0818 10:49:16.179915  2412 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0818 10:49:16.179921  2412 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0818 10:49:16.179930  2412 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0818 10:49:16.179994  2412 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0818 10:49:16.180007  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.180014  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.180018  2412 net.cpp:194] Memory required for data: 236454400
I0818 10:49:16.180022  2412 layer_factory.hpp:77] Creating layer Convolution6
I0818 10:49:16.180034  2412 net.cpp:128] Creating Layer Convolution6
I0818 10:49:16.180039  2412 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0818 10:49:16.180047  2412 net.cpp:522] Convolution6 -> Convolution6
I0818 10:49:16.186588  2412 net.cpp:172] Setting up Convolution6
I0818 10:49:16.186614  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.186619  2412 net.cpp:194] Memory required for data: 244843008
I0818 10:49:16.186630  2412 layer_factory.hpp:77] Creating layer BatchNorm6
I0818 10:49:16.186638  2412 net.cpp:128] Creating Layer BatchNorm6
I0818 10:49:16.186643  2412 net.cpp:558] BatchNorm6 <- Convolution6
I0818 10:49:16.186661  2412 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0818 10:49:16.186900  2412 net.cpp:172] Setting up BatchNorm6
I0818 10:49:16.186911  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.186918  2412 net.cpp:194] Memory required for data: 253231616
I0818 10:49:16.186928  2412 layer_factory.hpp:77] Creating layer Scale6
I0818 10:49:16.186935  2412 net.cpp:128] Creating Layer Scale6
I0818 10:49:16.186939  2412 net.cpp:558] Scale6 <- Convolution6
I0818 10:49:16.186945  2412 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0818 10:49:16.186983  2412 layer_factory.hpp:77] Creating layer Scale6
I0818 10:49:16.187116  2412 net.cpp:172] Setting up Scale6
I0818 10:49:16.187127  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.187131  2412 net.cpp:194] Memory required for data: 261620224
I0818 10:49:16.187139  2412 layer_factory.hpp:77] Creating layer ReLU6
I0818 10:49:16.187146  2412 net.cpp:128] Creating Layer ReLU6
I0818 10:49:16.187150  2412 net.cpp:558] ReLU6 <- Convolution6
I0818 10:49:16.187156  2412 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0818 10:49:16.188681  2412 net.cpp:172] Setting up ReLU6
I0818 10:49:16.188706  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.188711  2412 net.cpp:194] Memory required for data: 270008832
I0818 10:49:16.188716  2412 layer_factory.hpp:77] Creating layer Convolution7
I0818 10:49:16.188730  2412 net.cpp:128] Creating Layer Convolution7
I0818 10:49:16.188735  2412 net.cpp:558] Convolution7 <- Convolution6
I0818 10:49:16.188747  2412 net.cpp:522] Convolution7 -> Convolution7
I0818 10:49:16.195405  2412 net.cpp:172] Setting up Convolution7
I0818 10:49:16.195431  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.195436  2412 net.cpp:194] Memory required for data: 278397440
I0818 10:49:16.195446  2412 layer_factory.hpp:77] Creating layer BatchNorm7
I0818 10:49:16.195456  2412 net.cpp:128] Creating Layer BatchNorm7
I0818 10:49:16.195461  2412 net.cpp:558] BatchNorm7 <- Convolution7
I0818 10:49:16.195468  2412 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0818 10:49:16.195719  2412 net.cpp:172] Setting up BatchNorm7
I0818 10:49:16.195729  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.195735  2412 net.cpp:194] Memory required for data: 286786048
I0818 10:49:16.195745  2412 layer_factory.hpp:77] Creating layer Scale7
I0818 10:49:16.195755  2412 net.cpp:128] Creating Layer Scale7
I0818 10:49:16.195760  2412 net.cpp:558] Scale7 <- Convolution7
I0818 10:49:16.195765  2412 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0818 10:49:16.195804  2412 layer_factory.hpp:77] Creating layer Scale7
I0818 10:49:16.195940  2412 net.cpp:172] Setting up Scale7
I0818 10:49:16.195951  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.195956  2412 net.cpp:194] Memory required for data: 295174656
I0818 10:49:16.195964  2412 layer_factory.hpp:77] Creating layer Eltwise3
I0818 10:49:16.195972  2412 net.cpp:128] Creating Layer Eltwise3
I0818 10:49:16.195977  2412 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0818 10:49:16.196000  2412 net.cpp:558] Eltwise3 <- Convolution7
I0818 10:49:16.196007  2412 net.cpp:522] Eltwise3 -> Eltwise3
I0818 10:49:16.196033  2412 net.cpp:172] Setting up Eltwise3
I0818 10:49:16.196048  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.196051  2412 net.cpp:194] Memory required for data: 303563264
I0818 10:49:16.196055  2412 layer_factory.hpp:77] Creating layer ReLU7
I0818 10:49:16.196063  2412 net.cpp:128] Creating Layer ReLU7
I0818 10:49:16.196066  2412 net.cpp:558] ReLU7 <- Eltwise3
I0818 10:49:16.196072  2412 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0818 10:49:16.197451  2412 net.cpp:172] Setting up ReLU7
I0818 10:49:16.197463  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.197470  2412 net.cpp:194] Memory required for data: 311951872
I0818 10:49:16.197474  2412 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0818 10:49:16.197481  2412 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0818 10:49:16.197485  2412 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0818 10:49:16.197494  2412 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0818 10:49:16.197501  2412 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0818 10:49:16.197546  2412 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0818 10:49:16.197554  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.197559  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.197563  2412 net.cpp:194] Memory required for data: 328729088
I0818 10:49:16.197567  2412 layer_factory.hpp:77] Creating layer Convolution8
I0818 10:49:16.197578  2412 net.cpp:128] Creating Layer Convolution8
I0818 10:49:16.197583  2412 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0818 10:49:16.197592  2412 net.cpp:522] Convolution8 -> Convolution8
I0818 10:49:16.204177  2412 net.cpp:172] Setting up Convolution8
I0818 10:49:16.204205  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.204208  2412 net.cpp:194] Memory required for data: 337117696
I0818 10:49:16.204218  2412 layer_factory.hpp:77] Creating layer BatchNorm8
I0818 10:49:16.204228  2412 net.cpp:128] Creating Layer BatchNorm8
I0818 10:49:16.204233  2412 net.cpp:558] BatchNorm8 <- Convolution8
I0818 10:49:16.204241  2412 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0818 10:49:16.204486  2412 net.cpp:172] Setting up BatchNorm8
I0818 10:49:16.204494  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.204499  2412 net.cpp:194] Memory required for data: 345506304
I0818 10:49:16.204509  2412 layer_factory.hpp:77] Creating layer Scale8
I0818 10:49:16.204515  2412 net.cpp:128] Creating Layer Scale8
I0818 10:49:16.204520  2412 net.cpp:558] Scale8 <- Convolution8
I0818 10:49:16.204526  2412 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0818 10:49:16.204566  2412 layer_factory.hpp:77] Creating layer Scale8
I0818 10:49:16.204696  2412 net.cpp:172] Setting up Scale8
I0818 10:49:16.204704  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.204708  2412 net.cpp:194] Memory required for data: 353894912
I0818 10:49:16.204716  2412 layer_factory.hpp:77] Creating layer ReLU8
I0818 10:49:16.204723  2412 net.cpp:128] Creating Layer ReLU8
I0818 10:49:16.204727  2412 net.cpp:558] ReLU8 <- Convolution8
I0818 10:49:16.204735  2412 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0818 10:49:16.206271  2412 net.cpp:172] Setting up ReLU8
I0818 10:49:16.206290  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.206293  2412 net.cpp:194] Memory required for data: 362283520
I0818 10:49:16.206298  2412 layer_factory.hpp:77] Creating layer Convolution9
I0818 10:49:16.206310  2412 net.cpp:128] Creating Layer Convolution9
I0818 10:49:16.206315  2412 net.cpp:558] Convolution9 <- Convolution8
I0818 10:49:16.206321  2412 net.cpp:522] Convolution9 -> Convolution9
I0818 10:49:16.212940  2412 net.cpp:172] Setting up Convolution9
I0818 10:49:16.212966  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.212990  2412 net.cpp:194] Memory required for data: 370672128
I0818 10:49:16.213001  2412 layer_factory.hpp:77] Creating layer BatchNorm9
I0818 10:49:16.213014  2412 net.cpp:128] Creating Layer BatchNorm9
I0818 10:49:16.213019  2412 net.cpp:558] BatchNorm9 <- Convolution9
I0818 10:49:16.213026  2412 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0818 10:49:16.213268  2412 net.cpp:172] Setting up BatchNorm9
I0818 10:49:16.213279  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.213284  2412 net.cpp:194] Memory required for data: 379060736
I0818 10:49:16.213294  2412 layer_factory.hpp:77] Creating layer Scale9
I0818 10:49:16.213302  2412 net.cpp:128] Creating Layer Scale9
I0818 10:49:16.213306  2412 net.cpp:558] Scale9 <- Convolution9
I0818 10:49:16.213312  2412 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0818 10:49:16.213351  2412 layer_factory.hpp:77] Creating layer Scale9
I0818 10:49:16.213484  2412 net.cpp:172] Setting up Scale9
I0818 10:49:16.213496  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.213500  2412 net.cpp:194] Memory required for data: 387449344
I0818 10:49:16.213508  2412 layer_factory.hpp:77] Creating layer Eltwise4
I0818 10:49:16.213516  2412 net.cpp:128] Creating Layer Eltwise4
I0818 10:49:16.213521  2412 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0818 10:49:16.213526  2412 net.cpp:558] Eltwise4 <- Convolution9
I0818 10:49:16.213531  2412 net.cpp:522] Eltwise4 -> Eltwise4
I0818 10:49:16.213557  2412 net.cpp:172] Setting up Eltwise4
I0818 10:49:16.213564  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.213568  2412 net.cpp:194] Memory required for data: 395837952
I0818 10:49:16.213572  2412 layer_factory.hpp:77] Creating layer ReLU9
I0818 10:49:16.213579  2412 net.cpp:128] Creating Layer ReLU9
I0818 10:49:16.213583  2412 net.cpp:558] ReLU9 <- Eltwise4
I0818 10:49:16.213589  2412 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0818 10:49:16.215028  2412 net.cpp:172] Setting up ReLU9
I0818 10:49:16.215042  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.215047  2412 net.cpp:194] Memory required for data: 404226560
I0818 10:49:16.215051  2412 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0818 10:49:16.215059  2412 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0818 10:49:16.215064  2412 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0818 10:49:16.215070  2412 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0818 10:49:16.215080  2412 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0818 10:49:16.215126  2412 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0818 10:49:16.215133  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.215139  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.215143  2412 net.cpp:194] Memory required for data: 421003776
I0818 10:49:16.215147  2412 layer_factory.hpp:77] Creating layer Convolution10
I0818 10:49:16.215158  2412 net.cpp:128] Creating Layer Convolution10
I0818 10:49:16.215162  2412 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0818 10:49:16.215170  2412 net.cpp:522] Convolution10 -> Convolution10
I0818 10:49:16.221709  2412 net.cpp:172] Setting up Convolution10
I0818 10:49:16.221735  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.221740  2412 net.cpp:194] Memory required for data: 429392384
I0818 10:49:16.221755  2412 layer_factory.hpp:77] Creating layer BatchNorm10
I0818 10:49:16.221765  2412 net.cpp:128] Creating Layer BatchNorm10
I0818 10:49:16.221771  2412 net.cpp:558] BatchNorm10 <- Convolution10
I0818 10:49:16.221782  2412 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0818 10:49:16.222026  2412 net.cpp:172] Setting up BatchNorm10
I0818 10:49:16.222039  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.222043  2412 net.cpp:194] Memory required for data: 437780992
I0818 10:49:16.222054  2412 layer_factory.hpp:77] Creating layer Scale10
I0818 10:49:16.222061  2412 net.cpp:128] Creating Layer Scale10
I0818 10:49:16.222065  2412 net.cpp:558] Scale10 <- Convolution10
I0818 10:49:16.222090  2412 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0818 10:49:16.222128  2412 layer_factory.hpp:77] Creating layer Scale10
I0818 10:49:16.222263  2412 net.cpp:172] Setting up Scale10
I0818 10:49:16.222272  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.222276  2412 net.cpp:194] Memory required for data: 446169600
I0818 10:49:16.222285  2412 layer_factory.hpp:77] Creating layer ReLU10
I0818 10:49:16.222291  2412 net.cpp:128] Creating Layer ReLU10
I0818 10:49:16.222295  2412 net.cpp:558] ReLU10 <- Convolution10
I0818 10:49:16.222301  2412 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0818 10:49:16.223798  2412 net.cpp:172] Setting up ReLU10
I0818 10:49:16.223824  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.223829  2412 net.cpp:194] Memory required for data: 454558208
I0818 10:49:16.223834  2412 layer_factory.hpp:77] Creating layer Convolution11
I0818 10:49:16.223847  2412 net.cpp:128] Creating Layer Convolution11
I0818 10:49:16.223853  2412 net.cpp:558] Convolution11 <- Convolution10
I0818 10:49:16.223861  2412 net.cpp:522] Convolution11 -> Convolution11
I0818 10:49:16.230556  2412 net.cpp:172] Setting up Convolution11
I0818 10:49:16.230584  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.230588  2412 net.cpp:194] Memory required for data: 462946816
I0818 10:49:16.230598  2412 layer_factory.hpp:77] Creating layer BatchNorm11
I0818 10:49:16.230607  2412 net.cpp:128] Creating Layer BatchNorm11
I0818 10:49:16.230612  2412 net.cpp:558] BatchNorm11 <- Convolution11
I0818 10:49:16.230621  2412 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0818 10:49:16.230888  2412 net.cpp:172] Setting up BatchNorm11
I0818 10:49:16.230901  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.230906  2412 net.cpp:194] Memory required for data: 471335424
I0818 10:49:16.230916  2412 layer_factory.hpp:77] Creating layer Scale11
I0818 10:49:16.230922  2412 net.cpp:128] Creating Layer Scale11
I0818 10:49:16.230927  2412 net.cpp:558] Scale11 <- Convolution11
I0818 10:49:16.230935  2412 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0818 10:49:16.230974  2412 layer_factory.hpp:77] Creating layer Scale11
I0818 10:49:16.231113  2412 net.cpp:172] Setting up Scale11
I0818 10:49:16.231122  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.231125  2412 net.cpp:194] Memory required for data: 479724032
I0818 10:49:16.231132  2412 layer_factory.hpp:77] Creating layer Eltwise5
I0818 10:49:16.231142  2412 net.cpp:128] Creating Layer Eltwise5
I0818 10:49:16.231148  2412 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0818 10:49:16.231153  2412 net.cpp:558] Eltwise5 <- Convolution11
I0818 10:49:16.231158  2412 net.cpp:522] Eltwise5 -> Eltwise5
I0818 10:49:16.231185  2412 net.cpp:172] Setting up Eltwise5
I0818 10:49:16.231192  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.231195  2412 net.cpp:194] Memory required for data: 488112640
I0818 10:49:16.231200  2412 layer_factory.hpp:77] Creating layer ReLU11
I0818 10:49:16.231206  2412 net.cpp:128] Creating Layer ReLU11
I0818 10:49:16.231210  2412 net.cpp:558] ReLU11 <- Eltwise5
I0818 10:49:16.231218  2412 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0818 10:49:16.232607  2412 net.cpp:172] Setting up ReLU11
I0818 10:49:16.232625  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.232630  2412 net.cpp:194] Memory required for data: 496501248
I0818 10:49:16.232633  2412 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0818 10:49:16.232641  2412 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0818 10:49:16.232646  2412 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0818 10:49:16.232656  2412 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0818 10:49:16.232664  2412 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0818 10:49:16.232714  2412 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0818 10:49:16.232722  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.232743  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.232748  2412 net.cpp:194] Memory required for data: 513278464
I0818 10:49:16.232751  2412 layer_factory.hpp:77] Creating layer Convolution12
I0818 10:49:16.232764  2412 net.cpp:128] Creating Layer Convolution12
I0818 10:49:16.232769  2412 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0818 10:49:16.232779  2412 net.cpp:522] Convolution12 -> Convolution12
I0818 10:49:16.239302  2412 net.cpp:172] Setting up Convolution12
I0818 10:49:16.239331  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.239336  2412 net.cpp:194] Memory required for data: 521667072
I0818 10:49:16.239346  2412 layer_factory.hpp:77] Creating layer BatchNorm12
I0818 10:49:16.239354  2412 net.cpp:128] Creating Layer BatchNorm12
I0818 10:49:16.239359  2412 net.cpp:558] BatchNorm12 <- Convolution12
I0818 10:49:16.239368  2412 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0818 10:49:16.239621  2412 net.cpp:172] Setting up BatchNorm12
I0818 10:49:16.239634  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.239637  2412 net.cpp:194] Memory required for data: 530055680
I0818 10:49:16.239647  2412 layer_factory.hpp:77] Creating layer Scale12
I0818 10:49:16.239655  2412 net.cpp:128] Creating Layer Scale12
I0818 10:49:16.239658  2412 net.cpp:558] Scale12 <- Convolution12
I0818 10:49:16.239666  2412 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0818 10:49:16.239704  2412 layer_factory.hpp:77] Creating layer Scale12
I0818 10:49:16.239846  2412 net.cpp:172] Setting up Scale12
I0818 10:49:16.239854  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.239858  2412 net.cpp:194] Memory required for data: 538444288
I0818 10:49:16.239866  2412 layer_factory.hpp:77] Creating layer ReLU12
I0818 10:49:16.239872  2412 net.cpp:128] Creating Layer ReLU12
I0818 10:49:16.239876  2412 net.cpp:558] ReLU12 <- Convolution12
I0818 10:49:16.239884  2412 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0818 10:49:16.241380  2412 net.cpp:172] Setting up ReLU12
I0818 10:49:16.241399  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.241402  2412 net.cpp:194] Memory required for data: 546832896
I0818 10:49:16.241406  2412 layer_factory.hpp:77] Creating layer Convolution13
I0818 10:49:16.241420  2412 net.cpp:128] Creating Layer Convolution13
I0818 10:49:16.241425  2412 net.cpp:558] Convolution13 <- Convolution12
I0818 10:49:16.241433  2412 net.cpp:522] Convolution13 -> Convolution13
I0818 10:49:16.248073  2412 net.cpp:172] Setting up Convolution13
I0818 10:49:16.248100  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.248105  2412 net.cpp:194] Memory required for data: 555221504
I0818 10:49:16.248114  2412 layer_factory.hpp:77] Creating layer BatchNorm13
I0818 10:49:16.248126  2412 net.cpp:128] Creating Layer BatchNorm13
I0818 10:49:16.248133  2412 net.cpp:558] BatchNorm13 <- Convolution13
I0818 10:49:16.248139  2412 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0818 10:49:16.248394  2412 net.cpp:172] Setting up BatchNorm13
I0818 10:49:16.248405  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.248409  2412 net.cpp:194] Memory required for data: 563610112
I0818 10:49:16.248420  2412 layer_factory.hpp:77] Creating layer Scale13
I0818 10:49:16.248427  2412 net.cpp:128] Creating Layer Scale13
I0818 10:49:16.248431  2412 net.cpp:558] Scale13 <- Convolution13
I0818 10:49:16.248436  2412 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0818 10:49:16.248478  2412 layer_factory.hpp:77] Creating layer Scale13
I0818 10:49:16.248620  2412 net.cpp:172] Setting up Scale13
I0818 10:49:16.248628  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.248632  2412 net.cpp:194] Memory required for data: 571998720
I0818 10:49:16.248639  2412 layer_factory.hpp:77] Creating layer Eltwise6
I0818 10:49:16.248649  2412 net.cpp:128] Creating Layer Eltwise6
I0818 10:49:16.248656  2412 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0818 10:49:16.248661  2412 net.cpp:558] Eltwise6 <- Convolution13
I0818 10:49:16.248682  2412 net.cpp:522] Eltwise6 -> Eltwise6
I0818 10:49:16.248714  2412 net.cpp:172] Setting up Eltwise6
I0818 10:49:16.248723  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.248726  2412 net.cpp:194] Memory required for data: 580387328
I0818 10:49:16.248731  2412 layer_factory.hpp:77] Creating layer ReLU13
I0818 10:49:16.248741  2412 net.cpp:128] Creating Layer ReLU13
I0818 10:49:16.248745  2412 net.cpp:558] ReLU13 <- Eltwise6
I0818 10:49:16.248751  2412 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0818 10:49:16.250149  2412 net.cpp:172] Setting up ReLU13
I0818 10:49:16.250174  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.250180  2412 net.cpp:194] Memory required for data: 588775936
I0818 10:49:16.250185  2412 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0818 10:49:16.250196  2412 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0818 10:49:16.250201  2412 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0818 10:49:16.250208  2412 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0818 10:49:16.250222  2412 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0818 10:49:16.250274  2412 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0818 10:49:16.250283  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.250288  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.250291  2412 net.cpp:194] Memory required for data: 605553152
I0818 10:49:16.250295  2412 layer_factory.hpp:77] Creating layer Convolution14
I0818 10:49:16.250309  2412 net.cpp:128] Creating Layer Convolution14
I0818 10:49:16.250314  2412 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0818 10:49:16.250321  2412 net.cpp:522] Convolution14 -> Convolution14
I0818 10:49:16.256884  2412 net.cpp:172] Setting up Convolution14
I0818 10:49:16.256911  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.256916  2412 net.cpp:194] Memory required for data: 613941760
I0818 10:49:16.256927  2412 layer_factory.hpp:77] Creating layer BatchNorm14
I0818 10:49:16.256937  2412 net.cpp:128] Creating Layer BatchNorm14
I0818 10:49:16.256942  2412 net.cpp:558] BatchNorm14 <- Convolution14
I0818 10:49:16.256949  2412 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0818 10:49:16.257206  2412 net.cpp:172] Setting up BatchNorm14
I0818 10:49:16.257218  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.257223  2412 net.cpp:194] Memory required for data: 622330368
I0818 10:49:16.257233  2412 layer_factory.hpp:77] Creating layer Scale14
I0818 10:49:16.257242  2412 net.cpp:128] Creating Layer Scale14
I0818 10:49:16.257247  2412 net.cpp:558] Scale14 <- Convolution14
I0818 10:49:16.257253  2412 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0818 10:49:16.257292  2412 layer_factory.hpp:77] Creating layer Scale14
I0818 10:49:16.257432  2412 net.cpp:172] Setting up Scale14
I0818 10:49:16.257443  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.257447  2412 net.cpp:194] Memory required for data: 630718976
I0818 10:49:16.257455  2412 layer_factory.hpp:77] Creating layer ReLU14
I0818 10:49:16.257462  2412 net.cpp:128] Creating Layer ReLU14
I0818 10:49:16.257467  2412 net.cpp:558] ReLU14 <- Convolution14
I0818 10:49:16.257472  2412 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0818 10:49:16.258919  2412 net.cpp:172] Setting up ReLU14
I0818 10:49:16.258934  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.258939  2412 net.cpp:194] Memory required for data: 639107584
I0818 10:49:16.258946  2412 layer_factory.hpp:77] Creating layer Convolution15
I0818 10:49:16.258958  2412 net.cpp:128] Creating Layer Convolution15
I0818 10:49:16.258963  2412 net.cpp:558] Convolution15 <- Convolution14
I0818 10:49:16.258975  2412 net.cpp:522] Convolution15 -> Convolution15
I0818 10:49:16.265655  2412 net.cpp:172] Setting up Convolution15
I0818 10:49:16.265681  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.265684  2412 net.cpp:194] Memory required for data: 647496192
I0818 10:49:16.265710  2412 layer_factory.hpp:77] Creating layer BatchNorm15
I0818 10:49:16.265722  2412 net.cpp:128] Creating Layer BatchNorm15
I0818 10:49:16.265729  2412 net.cpp:558] BatchNorm15 <- Convolution15
I0818 10:49:16.265738  2412 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0818 10:49:16.265996  2412 net.cpp:172] Setting up BatchNorm15
I0818 10:49:16.266010  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.266014  2412 net.cpp:194] Memory required for data: 655884800
I0818 10:49:16.266024  2412 layer_factory.hpp:77] Creating layer Scale15
I0818 10:49:16.266031  2412 net.cpp:128] Creating Layer Scale15
I0818 10:49:16.266036  2412 net.cpp:558] Scale15 <- Convolution15
I0818 10:49:16.266041  2412 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0818 10:49:16.266083  2412 layer_factory.hpp:77] Creating layer Scale15
I0818 10:49:16.266218  2412 net.cpp:172] Setting up Scale15
I0818 10:49:16.266227  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.266230  2412 net.cpp:194] Memory required for data: 664273408
I0818 10:49:16.266238  2412 layer_factory.hpp:77] Creating layer Eltwise7
I0818 10:49:16.266245  2412 net.cpp:128] Creating Layer Eltwise7
I0818 10:49:16.266252  2412 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0818 10:49:16.266258  2412 net.cpp:558] Eltwise7 <- Convolution15
I0818 10:49:16.266263  2412 net.cpp:522] Eltwise7 -> Eltwise7
I0818 10:49:16.266290  2412 net.cpp:172] Setting up Eltwise7
I0818 10:49:16.266297  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.266301  2412 net.cpp:194] Memory required for data: 672662016
I0818 10:49:16.266307  2412 layer_factory.hpp:77] Creating layer ReLU15
I0818 10:49:16.266314  2412 net.cpp:128] Creating Layer ReLU15
I0818 10:49:16.266317  2412 net.cpp:558] ReLU15 <- Eltwise7
I0818 10:49:16.266324  2412 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0818 10:49:16.267732  2412 net.cpp:172] Setting up ReLU15
I0818 10:49:16.267748  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.267752  2412 net.cpp:194] Memory required for data: 681050624
I0818 10:49:16.267757  2412 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0818 10:49:16.267766  2412 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0818 10:49:16.267771  2412 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0818 10:49:16.267781  2412 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0818 10:49:16.267788  2412 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0818 10:49:16.267840  2412 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0818 10:49:16.267850  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.267856  2412 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:49:16.267859  2412 net.cpp:194] Memory required for data: 697827840
I0818 10:49:16.267864  2412 layer_factory.hpp:77] Creating layer Convolution16
I0818 10:49:16.267875  2412 net.cpp:128] Creating Layer Convolution16
I0818 10:49:16.267880  2412 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0818 10:49:16.267889  2412 net.cpp:522] Convolution16 -> Convolution16
I0818 10:49:16.274881  2412 net.cpp:172] Setting up Convolution16
I0818 10:49:16.274907  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.274912  2412 net.cpp:194] Memory required for data: 702022144
I0818 10:49:16.274922  2412 layer_factory.hpp:77] Creating layer BatchNorm16
I0818 10:49:16.274931  2412 net.cpp:128] Creating Layer BatchNorm16
I0818 10:49:16.274936  2412 net.cpp:558] BatchNorm16 <- Convolution16
I0818 10:49:16.274947  2412 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0818 10:49:16.275212  2412 net.cpp:172] Setting up BatchNorm16
I0818 10:49:16.275223  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.275228  2412 net.cpp:194] Memory required for data: 706216448
I0818 10:49:16.275238  2412 layer_factory.hpp:77] Creating layer Scale16
I0818 10:49:16.275245  2412 net.cpp:128] Creating Layer Scale16
I0818 10:49:16.275249  2412 net.cpp:558] Scale16 <- Convolution16
I0818 10:49:16.275274  2412 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0818 10:49:16.275315  2412 layer_factory.hpp:77] Creating layer Scale16
I0818 10:49:16.275456  2412 net.cpp:172] Setting up Scale16
I0818 10:49:16.275467  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.275471  2412 net.cpp:194] Memory required for data: 710410752
I0818 10:49:16.275480  2412 layer_factory.hpp:77] Creating layer Convolution17
I0818 10:49:16.275493  2412 net.cpp:128] Creating Layer Convolution17
I0818 10:49:16.275498  2412 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_1
I0818 10:49:16.275506  2412 net.cpp:522] Convolution17 -> Convolution17
I0818 10:49:16.280663  2412 net.cpp:172] Setting up Convolution17
I0818 10:49:16.280689  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.280692  2412 net.cpp:194] Memory required for data: 714605056
I0818 10:49:16.280704  2412 layer_factory.hpp:77] Creating layer BatchNorm17
I0818 10:49:16.280714  2412 net.cpp:128] Creating Layer BatchNorm17
I0818 10:49:16.280719  2412 net.cpp:558] BatchNorm17 <- Convolution17
I0818 10:49:16.280732  2412 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0818 10:49:16.280975  2412 net.cpp:172] Setting up BatchNorm17
I0818 10:49:16.280987  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.280992  2412 net.cpp:194] Memory required for data: 718799360
I0818 10:49:16.281002  2412 layer_factory.hpp:77] Creating layer Scale17
I0818 10:49:16.281010  2412 net.cpp:128] Creating Layer Scale17
I0818 10:49:16.281015  2412 net.cpp:558] Scale17 <- Convolution17
I0818 10:49:16.281020  2412 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0818 10:49:16.281061  2412 layer_factory.hpp:77] Creating layer Scale17
I0818 10:49:16.281203  2412 net.cpp:172] Setting up Scale17
I0818 10:49:16.281211  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.281215  2412 net.cpp:194] Memory required for data: 722993664
I0818 10:49:16.281224  2412 layer_factory.hpp:77] Creating layer ReLU16
I0818 10:49:16.281229  2412 net.cpp:128] Creating Layer ReLU16
I0818 10:49:16.281234  2412 net.cpp:558] ReLU16 <- Convolution17
I0818 10:49:16.281241  2412 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0818 10:49:16.281709  2412 net.cpp:172] Setting up ReLU16
I0818 10:49:16.281730  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.281735  2412 net.cpp:194] Memory required for data: 727187968
I0818 10:49:16.281740  2412 layer_factory.hpp:77] Creating layer Convolution18
I0818 10:49:16.281754  2412 net.cpp:128] Creating Layer Convolution18
I0818 10:49:16.281759  2412 net.cpp:558] Convolution18 <- Convolution17
I0818 10:49:16.281770  2412 net.cpp:522] Convolution18 -> Convolution18
I0818 10:49:16.283221  2412 net.cpp:172] Setting up Convolution18
I0818 10:49:16.283246  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.283252  2412 net.cpp:194] Memory required for data: 731382272
I0818 10:49:16.283262  2412 layer_factory.hpp:77] Creating layer BatchNorm18
I0818 10:49:16.283272  2412 net.cpp:128] Creating Layer BatchNorm18
I0818 10:49:16.283278  2412 net.cpp:558] BatchNorm18 <- Convolution18
I0818 10:49:16.283285  2412 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0818 10:49:16.283535  2412 net.cpp:172] Setting up BatchNorm18
I0818 10:49:16.283547  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.283551  2412 net.cpp:194] Memory required for data: 735576576
I0818 10:49:16.283561  2412 layer_factory.hpp:77] Creating layer Scale18
I0818 10:49:16.283571  2412 net.cpp:128] Creating Layer Scale18
I0818 10:49:16.283576  2412 net.cpp:558] Scale18 <- Convolution18
I0818 10:49:16.283582  2412 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0818 10:49:16.283622  2412 layer_factory.hpp:77] Creating layer Scale18
I0818 10:49:16.283761  2412 net.cpp:172] Setting up Scale18
I0818 10:49:16.283771  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.283776  2412 net.cpp:194] Memory required for data: 739770880
I0818 10:49:16.283783  2412 layer_factory.hpp:77] Creating layer Eltwise8
I0818 10:49:16.283809  2412 net.cpp:128] Creating Layer Eltwise8
I0818 10:49:16.283814  2412 net.cpp:558] Eltwise8 <- Convolution16
I0818 10:49:16.283819  2412 net.cpp:558] Eltwise8 <- Convolution18
I0818 10:49:16.283825  2412 net.cpp:522] Eltwise8 -> Eltwise8
I0818 10:49:16.283850  2412 net.cpp:172] Setting up Eltwise8
I0818 10:49:16.283859  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.283862  2412 net.cpp:194] Memory required for data: 743965184
I0818 10:49:16.283866  2412 layer_factory.hpp:77] Creating layer ReLU17
I0818 10:49:16.283872  2412 net.cpp:128] Creating Layer ReLU17
I0818 10:49:16.283876  2412 net.cpp:558] ReLU17 <- Eltwise8
I0818 10:49:16.283885  2412 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0818 10:49:16.284116  2412 net.cpp:172] Setting up ReLU17
I0818 10:49:16.284130  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.284135  2412 net.cpp:194] Memory required for data: 748159488
I0818 10:49:16.284139  2412 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0818 10:49:16.284147  2412 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0818 10:49:16.284152  2412 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0818 10:49:16.284160  2412 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0818 10:49:16.284169  2412 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0818 10:49:16.284220  2412 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0818 10:49:16.284227  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.284234  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.284237  2412 net.cpp:194] Memory required for data: 756548096
I0818 10:49:16.284241  2412 layer_factory.hpp:77] Creating layer Convolution19
I0818 10:49:16.284253  2412 net.cpp:128] Creating Layer Convolution19
I0818 10:49:16.284258  2412 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0818 10:49:16.284271  2412 net.cpp:522] Convolution19 -> Convolution19
I0818 10:49:16.290824  2412 net.cpp:172] Setting up Convolution19
I0818 10:49:16.290853  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.290856  2412 net.cpp:194] Memory required for data: 760742400
I0818 10:49:16.290868  2412 layer_factory.hpp:77] Creating layer BatchNorm19
I0818 10:49:16.290880  2412 net.cpp:128] Creating Layer BatchNorm19
I0818 10:49:16.290887  2412 net.cpp:558] BatchNorm19 <- Convolution19
I0818 10:49:16.290897  2412 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0818 10:49:16.291148  2412 net.cpp:172] Setting up BatchNorm19
I0818 10:49:16.291159  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.291164  2412 net.cpp:194] Memory required for data: 764936704
I0818 10:49:16.291191  2412 layer_factory.hpp:77] Creating layer Scale19
I0818 10:49:16.291203  2412 net.cpp:128] Creating Layer Scale19
I0818 10:49:16.291208  2412 net.cpp:558] Scale19 <- Convolution19
I0818 10:49:16.291213  2412 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0818 10:49:16.291258  2412 layer_factory.hpp:77] Creating layer Scale19
I0818 10:49:16.291399  2412 net.cpp:172] Setting up Scale19
I0818 10:49:16.291406  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.291410  2412 net.cpp:194] Memory required for data: 769131008
I0818 10:49:16.291419  2412 layer_factory.hpp:77] Creating layer ReLU18
I0818 10:49:16.291427  2412 net.cpp:128] Creating Layer ReLU18
I0818 10:49:16.291431  2412 net.cpp:558] ReLU18 <- Convolution19
I0818 10:49:16.291437  2412 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0818 10:49:16.292871  2412 net.cpp:172] Setting up ReLU18
I0818 10:49:16.292888  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.292893  2412 net.cpp:194] Memory required for data: 773325312
I0818 10:49:16.292897  2412 layer_factory.hpp:77] Creating layer Convolution20
I0818 10:49:16.292917  2412 net.cpp:128] Creating Layer Convolution20
I0818 10:49:16.292922  2412 net.cpp:558] Convolution20 <- Convolution19
I0818 10:49:16.292932  2412 net.cpp:522] Convolution20 -> Convolution20
I0818 10:49:16.299634  2412 net.cpp:172] Setting up Convolution20
I0818 10:49:16.299679  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.299685  2412 net.cpp:194] Memory required for data: 777519616
I0818 10:49:16.299696  2412 layer_factory.hpp:77] Creating layer BatchNorm20
I0818 10:49:16.299707  2412 net.cpp:128] Creating Layer BatchNorm20
I0818 10:49:16.299713  2412 net.cpp:558] BatchNorm20 <- Convolution20
I0818 10:49:16.299720  2412 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0818 10:49:16.299968  2412 net.cpp:172] Setting up BatchNorm20
I0818 10:49:16.299985  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.299990  2412 net.cpp:194] Memory required for data: 781713920
I0818 10:49:16.299999  2412 layer_factory.hpp:77] Creating layer Scale20
I0818 10:49:16.300009  2412 net.cpp:128] Creating Layer Scale20
I0818 10:49:16.300014  2412 net.cpp:558] Scale20 <- Convolution20
I0818 10:49:16.300019  2412 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0818 10:49:16.300061  2412 layer_factory.hpp:77] Creating layer Scale20
I0818 10:49:16.300207  2412 net.cpp:172] Setting up Scale20
I0818 10:49:16.300215  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.300220  2412 net.cpp:194] Memory required for data: 785908224
I0818 10:49:16.300226  2412 layer_factory.hpp:77] Creating layer Eltwise9
I0818 10:49:16.300237  2412 net.cpp:128] Creating Layer Eltwise9
I0818 10:49:16.300242  2412 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0818 10:49:16.300247  2412 net.cpp:558] Eltwise9 <- Convolution20
I0818 10:49:16.300256  2412 net.cpp:522] Eltwise9 -> Eltwise9
I0818 10:49:16.300277  2412 net.cpp:172] Setting up Eltwise9
I0818 10:49:16.300284  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.300288  2412 net.cpp:194] Memory required for data: 790102528
I0818 10:49:16.300292  2412 layer_factory.hpp:77] Creating layer ReLU19
I0818 10:49:16.300298  2412 net.cpp:128] Creating Layer ReLU19
I0818 10:49:16.300303  2412 net.cpp:558] ReLU19 <- Eltwise9
I0818 10:49:16.300312  2412 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0818 10:49:16.301697  2412 net.cpp:172] Setting up ReLU19
I0818 10:49:16.301714  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.301719  2412 net.cpp:194] Memory required for data: 794296832
I0818 10:49:16.301723  2412 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0818 10:49:16.301731  2412 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0818 10:49:16.301736  2412 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0818 10:49:16.301744  2412 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0818 10:49:16.301753  2412 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0818 10:49:16.301807  2412 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0818 10:49:16.301815  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.301820  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.301825  2412 net.cpp:194] Memory required for data: 802685440
I0818 10:49:16.301828  2412 layer_factory.hpp:77] Creating layer Convolution21
I0818 10:49:16.301841  2412 net.cpp:128] Creating Layer Convolution21
I0818 10:49:16.301846  2412 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0818 10:49:16.301854  2412 net.cpp:522] Convolution21 -> Convolution21
I0818 10:49:16.308655  2412 net.cpp:172] Setting up Convolution21
I0818 10:49:16.308681  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.308686  2412 net.cpp:194] Memory required for data: 806879744
I0818 10:49:16.308696  2412 layer_factory.hpp:77] Creating layer BatchNorm21
I0818 10:49:16.308712  2412 net.cpp:128] Creating Layer BatchNorm21
I0818 10:49:16.308717  2412 net.cpp:558] BatchNorm21 <- Convolution21
I0818 10:49:16.308729  2412 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0818 10:49:16.308990  2412 net.cpp:172] Setting up BatchNorm21
I0818 10:49:16.308997  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.309001  2412 net.cpp:194] Memory required for data: 811074048
I0818 10:49:16.309011  2412 layer_factory.hpp:77] Creating layer Scale21
I0818 10:49:16.309034  2412 net.cpp:128] Creating Layer Scale21
I0818 10:49:16.309039  2412 net.cpp:558] Scale21 <- Convolution21
I0818 10:49:16.309047  2412 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0818 10:49:16.309088  2412 layer_factory.hpp:77] Creating layer Scale21
I0818 10:49:16.309233  2412 net.cpp:172] Setting up Scale21
I0818 10:49:16.309247  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.309252  2412 net.cpp:194] Memory required for data: 815268352
I0818 10:49:16.309259  2412 layer_factory.hpp:77] Creating layer ReLU20
I0818 10:49:16.309267  2412 net.cpp:128] Creating Layer ReLU20
I0818 10:49:16.309270  2412 net.cpp:558] ReLU20 <- Convolution21
I0818 10:49:16.309278  2412 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0818 10:49:16.310463  2412 net.cpp:172] Setting up ReLU20
I0818 10:49:16.310488  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.310492  2412 net.cpp:194] Memory required for data: 819462656
I0818 10:49:16.310497  2412 layer_factory.hpp:77] Creating layer Convolution22
I0818 10:49:16.310516  2412 net.cpp:128] Creating Layer Convolution22
I0818 10:49:16.310521  2412 net.cpp:558] Convolution22 <- Convolution21
I0818 10:49:16.310535  2412 net.cpp:522] Convolution22 -> Convolution22
I0818 10:49:16.317239  2412 net.cpp:172] Setting up Convolution22
I0818 10:49:16.317265  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.317270  2412 net.cpp:194] Memory required for data: 823656960
I0818 10:49:16.317281  2412 layer_factory.hpp:77] Creating layer BatchNorm22
I0818 10:49:16.317291  2412 net.cpp:128] Creating Layer BatchNorm22
I0818 10:49:16.317296  2412 net.cpp:558] BatchNorm22 <- Convolution22
I0818 10:49:16.317306  2412 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0818 10:49:16.317561  2412 net.cpp:172] Setting up BatchNorm22
I0818 10:49:16.317574  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.317579  2412 net.cpp:194] Memory required for data: 827851264
I0818 10:49:16.317589  2412 layer_factory.hpp:77] Creating layer Scale22
I0818 10:49:16.317598  2412 net.cpp:128] Creating Layer Scale22
I0818 10:49:16.317603  2412 net.cpp:558] Scale22 <- Convolution22
I0818 10:49:16.317608  2412 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0818 10:49:16.317651  2412 layer_factory.hpp:77] Creating layer Scale22
I0818 10:49:16.317796  2412 net.cpp:172] Setting up Scale22
I0818 10:49:16.317808  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.317812  2412 net.cpp:194] Memory required for data: 832045568
I0818 10:49:16.317821  2412 layer_factory.hpp:77] Creating layer Eltwise10
I0818 10:49:16.317826  2412 net.cpp:128] Creating Layer Eltwise10
I0818 10:49:16.317831  2412 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0818 10:49:16.317836  2412 net.cpp:558] Eltwise10 <- Convolution22
I0818 10:49:16.317847  2412 net.cpp:522] Eltwise10 -> Eltwise10
I0818 10:49:16.317868  2412 net.cpp:172] Setting up Eltwise10
I0818 10:49:16.317878  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.317881  2412 net.cpp:194] Memory required for data: 836239872
I0818 10:49:16.317885  2412 layer_factory.hpp:77] Creating layer ReLU21
I0818 10:49:16.317893  2412 net.cpp:128] Creating Layer ReLU21
I0818 10:49:16.317896  2412 net.cpp:558] ReLU21 <- Eltwise10
I0818 10:49:16.317903  2412 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0818 10:49:16.319303  2412 net.cpp:172] Setting up ReLU21
I0818 10:49:16.319322  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.319326  2412 net.cpp:194] Memory required for data: 840434176
I0818 10:49:16.319331  2412 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0818 10:49:16.319339  2412 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0818 10:49:16.319344  2412 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0818 10:49:16.319350  2412 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0818 10:49:16.319361  2412 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0818 10:49:16.319427  2412 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0818 10:49:16.319435  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.319442  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.319445  2412 net.cpp:194] Memory required for data: 848822784
I0818 10:49:16.319449  2412 layer_factory.hpp:77] Creating layer Convolution23
I0818 10:49:16.319463  2412 net.cpp:128] Creating Layer Convolution23
I0818 10:49:16.319468  2412 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0818 10:49:16.319476  2412 net.cpp:522] Convolution23 -> Convolution23
I0818 10:49:16.326063  2412 net.cpp:172] Setting up Convolution23
I0818 10:49:16.326089  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.326093  2412 net.cpp:194] Memory required for data: 853017088
I0818 10:49:16.326103  2412 layer_factory.hpp:77] Creating layer BatchNorm23
I0818 10:49:16.326114  2412 net.cpp:128] Creating Layer BatchNorm23
I0818 10:49:16.326119  2412 net.cpp:558] BatchNorm23 <- Convolution23
I0818 10:49:16.326128  2412 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0818 10:49:16.326387  2412 net.cpp:172] Setting up BatchNorm23
I0818 10:49:16.326400  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.326403  2412 net.cpp:194] Memory required for data: 857211392
I0818 10:49:16.326413  2412 layer_factory.hpp:77] Creating layer Scale23
I0818 10:49:16.326426  2412 net.cpp:128] Creating Layer Scale23
I0818 10:49:16.326431  2412 net.cpp:558] Scale23 <- Convolution23
I0818 10:49:16.326436  2412 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0818 10:49:16.326475  2412 layer_factory.hpp:77] Creating layer Scale23
I0818 10:49:16.326622  2412 net.cpp:172] Setting up Scale23
I0818 10:49:16.326628  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.326632  2412 net.cpp:194] Memory required for data: 861405696
I0818 10:49:16.326640  2412 layer_factory.hpp:77] Creating layer ReLU22
I0818 10:49:16.326648  2412 net.cpp:128] Creating Layer ReLU22
I0818 10:49:16.326663  2412 net.cpp:558] ReLU22 <- Convolution23
I0818 10:49:16.326670  2412 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0818 10:49:16.328145  2412 net.cpp:172] Setting up ReLU22
I0818 10:49:16.328161  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.328166  2412 net.cpp:194] Memory required for data: 865600000
I0818 10:49:16.328171  2412 layer_factory.hpp:77] Creating layer Convolution24
I0818 10:49:16.328183  2412 net.cpp:128] Creating Layer Convolution24
I0818 10:49:16.328188  2412 net.cpp:558] Convolution24 <- Convolution23
I0818 10:49:16.328197  2412 net.cpp:522] Convolution24 -> Convolution24
I0818 10:49:16.329638  2412 net.cpp:172] Setting up Convolution24
I0818 10:49:16.329664  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.329668  2412 net.cpp:194] Memory required for data: 869794304
I0818 10:49:16.329679  2412 layer_factory.hpp:77] Creating layer BatchNorm24
I0818 10:49:16.329686  2412 net.cpp:128] Creating Layer BatchNorm24
I0818 10:49:16.329691  2412 net.cpp:558] BatchNorm24 <- Convolution24
I0818 10:49:16.329700  2412 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0818 10:49:16.329955  2412 net.cpp:172] Setting up BatchNorm24
I0818 10:49:16.329965  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.329970  2412 net.cpp:194] Memory required for data: 873988608
I0818 10:49:16.329979  2412 layer_factory.hpp:77] Creating layer Scale24
I0818 10:49:16.329987  2412 net.cpp:128] Creating Layer Scale24
I0818 10:49:16.329991  2412 net.cpp:558] Scale24 <- Convolution24
I0818 10:49:16.329996  2412 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0818 10:49:16.330039  2412 layer_factory.hpp:77] Creating layer Scale24
I0818 10:49:16.330185  2412 net.cpp:172] Setting up Scale24
I0818 10:49:16.330193  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.330198  2412 net.cpp:194] Memory required for data: 878182912
I0818 10:49:16.330204  2412 layer_factory.hpp:77] Creating layer Eltwise11
I0818 10:49:16.330214  2412 net.cpp:128] Creating Layer Eltwise11
I0818 10:49:16.330235  2412 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0818 10:49:16.330241  2412 net.cpp:558] Eltwise11 <- Convolution24
I0818 10:49:16.330247  2412 net.cpp:522] Eltwise11 -> Eltwise11
I0818 10:49:16.330272  2412 net.cpp:172] Setting up Eltwise11
I0818 10:49:16.330279  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.330283  2412 net.cpp:194] Memory required for data: 882377216
I0818 10:49:16.330287  2412 layer_factory.hpp:77] Creating layer ReLU23
I0818 10:49:16.330294  2412 net.cpp:128] Creating Layer ReLU23
I0818 10:49:16.330298  2412 net.cpp:558] ReLU23 <- Eltwise11
I0818 10:49:16.330307  2412 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0818 10:49:16.330811  2412 net.cpp:172] Setting up ReLU23
I0818 10:49:16.330828  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.330833  2412 net.cpp:194] Memory required for data: 886571520
I0818 10:49:16.330838  2412 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0818 10:49:16.330847  2412 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0818 10:49:16.330852  2412 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0818 10:49:16.330860  2412 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0818 10:49:16.330869  2412 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0818 10:49:16.330924  2412 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0818 10:49:16.330930  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.330936  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.330940  2412 net.cpp:194] Memory required for data: 894960128
I0818 10:49:16.330943  2412 layer_factory.hpp:77] Creating layer Convolution25
I0818 10:49:16.330957  2412 net.cpp:128] Creating Layer Convolution25
I0818 10:49:16.330962  2412 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0818 10:49:16.330971  2412 net.cpp:522] Convolution25 -> Convolution25
I0818 10:49:16.336769  2412 net.cpp:172] Setting up Convolution25
I0818 10:49:16.336796  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.336800  2412 net.cpp:194] Memory required for data: 899154432
I0818 10:49:16.336815  2412 layer_factory.hpp:77] Creating layer BatchNorm25
I0818 10:49:16.336827  2412 net.cpp:128] Creating Layer BatchNorm25
I0818 10:49:16.336833  2412 net.cpp:558] BatchNorm25 <- Convolution25
I0818 10:49:16.336843  2412 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0818 10:49:16.337096  2412 net.cpp:172] Setting up BatchNorm25
I0818 10:49:16.337107  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.337111  2412 net.cpp:194] Memory required for data: 903348736
I0818 10:49:16.337121  2412 layer_factory.hpp:77] Creating layer Scale25
I0818 10:49:16.337133  2412 net.cpp:128] Creating Layer Scale25
I0818 10:49:16.337138  2412 net.cpp:558] Scale25 <- Convolution25
I0818 10:49:16.337143  2412 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0818 10:49:16.337185  2412 layer_factory.hpp:77] Creating layer Scale25
I0818 10:49:16.337332  2412 net.cpp:172] Setting up Scale25
I0818 10:49:16.337342  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.337347  2412 net.cpp:194] Memory required for data: 907543040
I0818 10:49:16.337354  2412 layer_factory.hpp:77] Creating layer ReLU24
I0818 10:49:16.337368  2412 net.cpp:128] Creating Layer ReLU24
I0818 10:49:16.337371  2412 net.cpp:558] ReLU24 <- Convolution25
I0818 10:49:16.337378  2412 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0818 10:49:16.338843  2412 net.cpp:172] Setting up ReLU24
I0818 10:49:16.338861  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.338865  2412 net.cpp:194] Memory required for data: 911737344
I0818 10:49:16.338871  2412 layer_factory.hpp:77] Creating layer Convolution26
I0818 10:49:16.338889  2412 net.cpp:128] Creating Layer Convolution26
I0818 10:49:16.338899  2412 net.cpp:558] Convolution26 <- Convolution25
I0818 10:49:16.338907  2412 net.cpp:522] Convolution26 -> Convolution26
I0818 10:49:16.345553  2412 net.cpp:172] Setting up Convolution26
I0818 10:49:16.345594  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.345600  2412 net.cpp:194] Memory required for data: 915931648
I0818 10:49:16.345614  2412 layer_factory.hpp:77] Creating layer BatchNorm26
I0818 10:49:16.345625  2412 net.cpp:128] Creating Layer BatchNorm26
I0818 10:49:16.345635  2412 net.cpp:558] BatchNorm26 <- Convolution26
I0818 10:49:16.345645  2412 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0818 10:49:16.345906  2412 net.cpp:172] Setting up BatchNorm26
I0818 10:49:16.345917  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.345921  2412 net.cpp:194] Memory required for data: 920125952
I0818 10:49:16.345930  2412 layer_factory.hpp:77] Creating layer Scale26
I0818 10:49:16.345937  2412 net.cpp:128] Creating Layer Scale26
I0818 10:49:16.345942  2412 net.cpp:558] Scale26 <- Convolution26
I0818 10:49:16.345949  2412 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0818 10:49:16.345988  2412 layer_factory.hpp:77] Creating layer Scale26
I0818 10:49:16.346132  2412 net.cpp:172] Setting up Scale26
I0818 10:49:16.346143  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.346146  2412 net.cpp:194] Memory required for data: 924320256
I0818 10:49:16.346154  2412 layer_factory.hpp:77] Creating layer Eltwise12
I0818 10:49:16.346161  2412 net.cpp:128] Creating Layer Eltwise12
I0818 10:49:16.346165  2412 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0818 10:49:16.346170  2412 net.cpp:558] Eltwise12 <- Convolution26
I0818 10:49:16.346176  2412 net.cpp:522] Eltwise12 -> Eltwise12
I0818 10:49:16.346200  2412 net.cpp:172] Setting up Eltwise12
I0818 10:49:16.346207  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.346211  2412 net.cpp:194] Memory required for data: 928514560
I0818 10:49:16.346215  2412 layer_factory.hpp:77] Creating layer ReLU25
I0818 10:49:16.346221  2412 net.cpp:128] Creating Layer ReLU25
I0818 10:49:16.346226  2412 net.cpp:558] ReLU25 <- Eltwise12
I0818 10:49:16.346235  2412 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0818 10:49:16.347606  2412 net.cpp:172] Setting up ReLU25
I0818 10:49:16.347626  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.347630  2412 net.cpp:194] Memory required for data: 932708864
I0818 10:49:16.347635  2412 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0818 10:49:16.347657  2412 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0818 10:49:16.347663  2412 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0818 10:49:16.347671  2412 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0818 10:49:16.347685  2412 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0818 10:49:16.347735  2412 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0818 10:49:16.347743  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.347749  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.347754  2412 net.cpp:194] Memory required for data: 941097472
I0818 10:49:16.347759  2412 layer_factory.hpp:77] Creating layer Convolution27
I0818 10:49:16.347774  2412 net.cpp:128] Creating Layer Convolution27
I0818 10:49:16.347777  2412 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0818 10:49:16.347785  2412 net.cpp:522] Convolution27 -> Convolution27
I0818 10:49:16.354271  2412 net.cpp:172] Setting up Convolution27
I0818 10:49:16.354297  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.354301  2412 net.cpp:194] Memory required for data: 945291776
I0818 10:49:16.354311  2412 layer_factory.hpp:77] Creating layer BatchNorm27
I0818 10:49:16.354322  2412 net.cpp:128] Creating Layer BatchNorm27
I0818 10:49:16.354327  2412 net.cpp:558] BatchNorm27 <- Convolution27
I0818 10:49:16.354336  2412 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0818 10:49:16.354601  2412 net.cpp:172] Setting up BatchNorm27
I0818 10:49:16.354614  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.354617  2412 net.cpp:194] Memory required for data: 949486080
I0818 10:49:16.354642  2412 layer_factory.hpp:77] Creating layer Scale27
I0818 10:49:16.354668  2412 net.cpp:128] Creating Layer Scale27
I0818 10:49:16.354674  2412 net.cpp:558] Scale27 <- Convolution27
I0818 10:49:16.354681  2412 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0818 10:49:16.354725  2412 layer_factory.hpp:77] Creating layer Scale27
I0818 10:49:16.354874  2412 net.cpp:172] Setting up Scale27
I0818 10:49:16.354882  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.354887  2412 net.cpp:194] Memory required for data: 953680384
I0818 10:49:16.354894  2412 layer_factory.hpp:77] Creating layer ReLU26
I0818 10:49:16.354902  2412 net.cpp:128] Creating Layer ReLU26
I0818 10:49:16.354905  2412 net.cpp:558] ReLU26 <- Convolution27
I0818 10:49:16.354915  2412 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0818 10:49:16.356323  2412 net.cpp:172] Setting up ReLU26
I0818 10:49:16.356348  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.356353  2412 net.cpp:194] Memory required for data: 957874688
I0818 10:49:16.356357  2412 layer_factory.hpp:77] Creating layer Convolution28
I0818 10:49:16.356372  2412 net.cpp:128] Creating Layer Convolution28
I0818 10:49:16.356377  2412 net.cpp:558] Convolution28 <- Convolution27
I0818 10:49:16.356392  2412 net.cpp:522] Convolution28 -> Convolution28
I0818 10:49:16.363059  2412 net.cpp:172] Setting up Convolution28
I0818 10:49:16.363085  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.363088  2412 net.cpp:194] Memory required for data: 962068992
I0818 10:49:16.363099  2412 layer_factory.hpp:77] Creating layer BatchNorm28
I0818 10:49:16.363113  2412 net.cpp:128] Creating Layer BatchNorm28
I0818 10:49:16.363118  2412 net.cpp:558] BatchNorm28 <- Convolution28
I0818 10:49:16.363126  2412 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0818 10:49:16.363386  2412 net.cpp:172] Setting up BatchNorm28
I0818 10:49:16.363399  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.363402  2412 net.cpp:194] Memory required for data: 966263296
I0818 10:49:16.363412  2412 layer_factory.hpp:77] Creating layer Scale28
I0818 10:49:16.363421  2412 net.cpp:128] Creating Layer Scale28
I0818 10:49:16.363425  2412 net.cpp:558] Scale28 <- Convolution28
I0818 10:49:16.363431  2412 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0818 10:49:16.363473  2412 layer_factory.hpp:77] Creating layer Scale28
I0818 10:49:16.363622  2412 net.cpp:172] Setting up Scale28
I0818 10:49:16.363631  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.363634  2412 net.cpp:194] Memory required for data: 970457600
I0818 10:49:16.363642  2412 layer_factory.hpp:77] Creating layer Eltwise13
I0818 10:49:16.363651  2412 net.cpp:128] Creating Layer Eltwise13
I0818 10:49:16.363656  2412 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0818 10:49:16.363662  2412 net.cpp:558] Eltwise13 <- Convolution28
I0818 10:49:16.363667  2412 net.cpp:522] Eltwise13 -> Eltwise13
I0818 10:49:16.363692  2412 net.cpp:172] Setting up Eltwise13
I0818 10:49:16.363698  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.363703  2412 net.cpp:194] Memory required for data: 974651904
I0818 10:49:16.363706  2412 layer_factory.hpp:77] Creating layer ReLU27
I0818 10:49:16.363713  2412 net.cpp:128] Creating Layer ReLU27
I0818 10:49:16.363718  2412 net.cpp:558] ReLU27 <- Eltwise13
I0818 10:49:16.363724  2412 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0818 10:49:16.365123  2412 net.cpp:172] Setting up ReLU27
I0818 10:49:16.365141  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.365146  2412 net.cpp:194] Memory required for data: 978846208
I0818 10:49:16.365150  2412 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0818 10:49:16.365159  2412 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0818 10:49:16.365164  2412 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0818 10:49:16.365172  2412 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0818 10:49:16.365180  2412 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0818 10:49:16.365252  2412 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0818 10:49:16.365260  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.365267  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.365270  2412 net.cpp:194] Memory required for data: 987234816
I0818 10:49:16.365274  2412 layer_factory.hpp:77] Creating layer Convolution29
I0818 10:49:16.365286  2412 net.cpp:128] Creating Layer Convolution29
I0818 10:49:16.365291  2412 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0818 10:49:16.365300  2412 net.cpp:522] Convolution29 -> Convolution29
I0818 10:49:16.371857  2412 net.cpp:172] Setting up Convolution29
I0818 10:49:16.371883  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.371888  2412 net.cpp:194] Memory required for data: 991429120
I0818 10:49:16.371898  2412 layer_factory.hpp:77] Creating layer BatchNorm29
I0818 10:49:16.371909  2412 net.cpp:128] Creating Layer BatchNorm29
I0818 10:49:16.371917  2412 net.cpp:558] BatchNorm29 <- Convolution29
I0818 10:49:16.371924  2412 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0818 10:49:16.372189  2412 net.cpp:172] Setting up BatchNorm29
I0818 10:49:16.372200  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.372205  2412 net.cpp:194] Memory required for data: 995623424
I0818 10:49:16.372215  2412 layer_factory.hpp:77] Creating layer Scale29
I0818 10:49:16.372226  2412 net.cpp:128] Creating Layer Scale29
I0818 10:49:16.372231  2412 net.cpp:558] Scale29 <- Convolution29
I0818 10:49:16.372236  2412 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0818 10:49:16.372279  2412 layer_factory.hpp:77] Creating layer Scale29
I0818 10:49:16.372432  2412 net.cpp:172] Setting up Scale29
I0818 10:49:16.372440  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.372444  2412 net.cpp:194] Memory required for data: 999817728
I0818 10:49:16.372452  2412 layer_factory.hpp:77] Creating layer ReLU28
I0818 10:49:16.372460  2412 net.cpp:128] Creating Layer ReLU28
I0818 10:49:16.372465  2412 net.cpp:558] ReLU28 <- Convolution29
I0818 10:49:16.372470  2412 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0818 10:49:16.373885  2412 net.cpp:172] Setting up ReLU28
I0818 10:49:16.373903  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.373908  2412 net.cpp:194] Memory required for data: 1004012032
I0818 10:49:16.373913  2412 layer_factory.hpp:77] Creating layer Convolution30
I0818 10:49:16.373929  2412 net.cpp:128] Creating Layer Convolution30
I0818 10:49:16.373934  2412 net.cpp:558] Convolution30 <- Convolution29
I0818 10:49:16.373941  2412 net.cpp:522] Convolution30 -> Convolution30
I0818 10:49:16.380615  2412 net.cpp:172] Setting up Convolution30
I0818 10:49:16.380643  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.380647  2412 net.cpp:194] Memory required for data: 1008206336
I0818 10:49:16.380658  2412 layer_factory.hpp:77] Creating layer BatchNorm30
I0818 10:49:16.380669  2412 net.cpp:128] Creating Layer BatchNorm30
I0818 10:49:16.380674  2412 net.cpp:558] BatchNorm30 <- Convolution30
I0818 10:49:16.380684  2412 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0818 10:49:16.380950  2412 net.cpp:172] Setting up BatchNorm30
I0818 10:49:16.380962  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.380966  2412 net.cpp:194] Memory required for data: 1012400640
I0818 10:49:16.380976  2412 layer_factory.hpp:77] Creating layer Scale30
I0818 10:49:16.380983  2412 net.cpp:128] Creating Layer Scale30
I0818 10:49:16.380987  2412 net.cpp:558] Scale30 <- Convolution30
I0818 10:49:16.380996  2412 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0818 10:49:16.381037  2412 layer_factory.hpp:77] Creating layer Scale30
I0818 10:49:16.381187  2412 net.cpp:172] Setting up Scale30
I0818 10:49:16.381196  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.381199  2412 net.cpp:194] Memory required for data: 1016594944
I0818 10:49:16.381207  2412 layer_factory.hpp:77] Creating layer Eltwise14
I0818 10:49:16.381230  2412 net.cpp:128] Creating Layer Eltwise14
I0818 10:49:16.381235  2412 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0818 10:49:16.381240  2412 net.cpp:558] Eltwise14 <- Convolution30
I0818 10:49:16.381249  2412 net.cpp:522] Eltwise14 -> Eltwise14
I0818 10:49:16.381273  2412 net.cpp:172] Setting up Eltwise14
I0818 10:49:16.381279  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.381283  2412 net.cpp:194] Memory required for data: 1020789248
I0818 10:49:16.381289  2412 layer_factory.hpp:77] Creating layer ReLU29
I0818 10:49:16.381299  2412 net.cpp:128] Creating Layer ReLU29
I0818 10:49:16.381304  2412 net.cpp:558] ReLU29 <- Eltwise14
I0818 10:49:16.381309  2412 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0818 10:49:16.382694  2412 net.cpp:172] Setting up ReLU29
I0818 10:49:16.382709  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.382712  2412 net.cpp:194] Memory required for data: 1024983552
I0818 10:49:16.382717  2412 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0818 10:49:16.382726  2412 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0818 10:49:16.382731  2412 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0818 10:49:16.382738  2412 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0818 10:49:16.382746  2412 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0818 10:49:16.382799  2412 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0818 10:49:16.382807  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.382812  2412 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:49:16.382817  2412 net.cpp:194] Memory required for data: 1033372160
I0818 10:49:16.382820  2412 layer_factory.hpp:77] Creating layer Convolution31
I0818 10:49:16.382833  2412 net.cpp:128] Creating Layer Convolution31
I0818 10:49:16.382838  2412 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0818 10:49:16.382848  2412 net.cpp:522] Convolution31 -> Convolution31
I0818 10:49:16.389426  2412 net.cpp:172] Setting up Convolution31
I0818 10:49:16.389452  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.389457  2412 net.cpp:194] Memory required for data: 1035469312
I0818 10:49:16.389467  2412 layer_factory.hpp:77] Creating layer BatchNorm31
I0818 10:49:16.389482  2412 net.cpp:128] Creating Layer BatchNorm31
I0818 10:49:16.389487  2412 net.cpp:558] BatchNorm31 <- Convolution31
I0818 10:49:16.389497  2412 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0818 10:49:16.389772  2412 net.cpp:172] Setting up BatchNorm31
I0818 10:49:16.389783  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.389787  2412 net.cpp:194] Memory required for data: 1037566464
I0818 10:49:16.389797  2412 layer_factory.hpp:77] Creating layer Scale31
I0818 10:49:16.389808  2412 net.cpp:128] Creating Layer Scale31
I0818 10:49:16.389812  2412 net.cpp:558] Scale31 <- Convolution31
I0818 10:49:16.389818  2412 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0818 10:49:16.389863  2412 layer_factory.hpp:77] Creating layer Scale31
I0818 10:49:16.390019  2412 net.cpp:172] Setting up Scale31
I0818 10:49:16.390033  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.390036  2412 net.cpp:194] Memory required for data: 1039663616
I0818 10:49:16.390044  2412 layer_factory.hpp:77] Creating layer Convolution32
I0818 10:49:16.390058  2412 net.cpp:128] Creating Layer Convolution32
I0818 10:49:16.390064  2412 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_1
I0818 10:49:16.390072  2412 net.cpp:522] Convolution32 -> Convolution32
I0818 10:49:16.396188  2412 net.cpp:172] Setting up Convolution32
I0818 10:49:16.396216  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.396220  2412 net.cpp:194] Memory required for data: 1041760768
I0818 10:49:16.396232  2412 layer_factory.hpp:77] Creating layer BatchNorm32
I0818 10:49:16.396245  2412 net.cpp:128] Creating Layer BatchNorm32
I0818 10:49:16.396250  2412 net.cpp:558] BatchNorm32 <- Convolution32
I0818 10:49:16.396262  2412 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0818 10:49:16.396565  2412 net.cpp:172] Setting up BatchNorm32
I0818 10:49:16.396577  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.396582  2412 net.cpp:194] Memory required for data: 1043857920
I0818 10:49:16.396592  2412 layer_factory.hpp:77] Creating layer Scale32
I0818 10:49:16.396600  2412 net.cpp:128] Creating Layer Scale32
I0818 10:49:16.396605  2412 net.cpp:558] Scale32 <- Convolution32
I0818 10:49:16.396610  2412 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0818 10:49:16.396656  2412 layer_factory.hpp:77] Creating layer Scale32
I0818 10:49:16.396816  2412 net.cpp:172] Setting up Scale32
I0818 10:49:16.396827  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.396831  2412 net.cpp:194] Memory required for data: 1045955072
I0818 10:49:16.396839  2412 layer_factory.hpp:77] Creating layer ReLU30
I0818 10:49:16.396849  2412 net.cpp:128] Creating Layer ReLU30
I0818 10:49:16.396854  2412 net.cpp:558] ReLU30 <- Convolution32
I0818 10:49:16.396859  2412 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0818 10:49:16.397997  2412 net.cpp:172] Setting up ReLU30
I0818 10:49:16.398022  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.398027  2412 net.cpp:194] Memory required for data: 1048052224
I0818 10:49:16.398035  2412 layer_factory.hpp:77] Creating layer Convolution33
I0818 10:49:16.398048  2412 net.cpp:128] Creating Layer Convolution33
I0818 10:49:16.398054  2412 net.cpp:558] Convolution33 <- Convolution32
I0818 10:49:16.398068  2412 net.cpp:522] Convolution33 -> Convolution33
I0818 10:49:16.405005  2412 net.cpp:172] Setting up Convolution33
I0818 10:49:16.405032  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.405037  2412 net.cpp:194] Memory required for data: 1050149376
I0818 10:49:16.405047  2412 layer_factory.hpp:77] Creating layer BatchNorm33
I0818 10:49:16.405059  2412 net.cpp:128] Creating Layer BatchNorm33
I0818 10:49:16.405064  2412 net.cpp:558] BatchNorm33 <- Convolution33
I0818 10:49:16.405071  2412 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0818 10:49:16.405364  2412 net.cpp:172] Setting up BatchNorm33
I0818 10:49:16.405376  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.405380  2412 net.cpp:194] Memory required for data: 1052246528
I0818 10:49:16.405390  2412 layer_factory.hpp:77] Creating layer Scale33
I0818 10:49:16.405400  2412 net.cpp:128] Creating Layer Scale33
I0818 10:49:16.405405  2412 net.cpp:558] Scale33 <- Convolution33
I0818 10:49:16.405409  2412 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0818 10:49:16.405454  2412 layer_factory.hpp:77] Creating layer Scale33
I0818 10:49:16.405616  2412 net.cpp:172] Setting up Scale33
I0818 10:49:16.405627  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.405630  2412 net.cpp:194] Memory required for data: 1054343680
I0818 10:49:16.405639  2412 layer_factory.hpp:77] Creating layer Eltwise15
I0818 10:49:16.405649  2412 net.cpp:128] Creating Layer Eltwise15
I0818 10:49:16.405653  2412 net.cpp:558] Eltwise15 <- Convolution31
I0818 10:49:16.405658  2412 net.cpp:558] Eltwise15 <- Convolution33
I0818 10:49:16.405666  2412 net.cpp:522] Eltwise15 -> Eltwise15
I0818 10:49:16.405690  2412 net.cpp:172] Setting up Eltwise15
I0818 10:49:16.405699  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.405701  2412 net.cpp:194] Memory required for data: 1056440832
I0818 10:49:16.405706  2412 layer_factory.hpp:77] Creating layer ReLU31
I0818 10:49:16.405714  2412 net.cpp:128] Creating Layer ReLU31
I0818 10:49:16.405717  2412 net.cpp:558] ReLU31 <- Eltwise15
I0818 10:49:16.405730  2412 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0818 10:49:16.406828  2412 net.cpp:172] Setting up ReLU31
I0818 10:49:16.406842  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.406847  2412 net.cpp:194] Memory required for data: 1058537984
I0818 10:49:16.406853  2412 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0818 10:49:16.406862  2412 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0818 10:49:16.406885  2412 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0818 10:49:16.406896  2412 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0818 10:49:16.406906  2412 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0818 10:49:16.406970  2412 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0818 10:49:16.406980  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.406986  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.406989  2412 net.cpp:194] Memory required for data: 1062732288
I0818 10:49:16.406994  2412 layer_factory.hpp:77] Creating layer Convolution34
I0818 10:49:16.407006  2412 net.cpp:128] Creating Layer Convolution34
I0818 10:49:16.407011  2412 net.cpp:558] Convolution34 <- Eltwise15_ReLU31_0_split_0
I0818 10:49:16.407021  2412 net.cpp:522] Convolution34 -> Convolution34
I0818 10:49:16.413580  2412 net.cpp:172] Setting up Convolution34
I0818 10:49:16.413607  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.413611  2412 net.cpp:194] Memory required for data: 1064829440
I0818 10:49:16.413622  2412 layer_factory.hpp:77] Creating layer BatchNorm34
I0818 10:49:16.413635  2412 net.cpp:128] Creating Layer BatchNorm34
I0818 10:49:16.413640  2412 net.cpp:558] BatchNorm34 <- Convolution34
I0818 10:49:16.413645  2412 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0818 10:49:16.413928  2412 net.cpp:172] Setting up BatchNorm34
I0818 10:49:16.413941  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.413945  2412 net.cpp:194] Memory required for data: 1066926592
I0818 10:49:16.413954  2412 layer_factory.hpp:77] Creating layer Scale34
I0818 10:49:16.413961  2412 net.cpp:128] Creating Layer Scale34
I0818 10:49:16.413965  2412 net.cpp:558] Scale34 <- Convolution34
I0818 10:49:16.413971  2412 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0818 10:49:16.414018  2412 layer_factory.hpp:77] Creating layer Scale34
I0818 10:49:16.414175  2412 net.cpp:172] Setting up Scale34
I0818 10:49:16.414182  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.414186  2412 net.cpp:194] Memory required for data: 1069023744
I0818 10:49:16.414194  2412 layer_factory.hpp:77] Creating layer ReLU32
I0818 10:49:16.414202  2412 net.cpp:128] Creating Layer ReLU32
I0818 10:49:16.414207  2412 net.cpp:558] ReLU32 <- Convolution34
I0818 10:49:16.414212  2412 net.cpp:509] ReLU32 -> Convolution34 (in-place)
I0818 10:49:16.415621  2412 net.cpp:172] Setting up ReLU32
I0818 10:49:16.415639  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.415644  2412 net.cpp:194] Memory required for data: 1071120896
I0818 10:49:16.415649  2412 layer_factory.hpp:77] Creating layer Convolution35
I0818 10:49:16.415665  2412 net.cpp:128] Creating Layer Convolution35
I0818 10:49:16.415671  2412 net.cpp:558] Convolution35 <- Convolution34
I0818 10:49:16.415680  2412 net.cpp:522] Convolution35 -> Convolution35
I0818 10:49:16.422557  2412 net.cpp:172] Setting up Convolution35
I0818 10:49:16.422585  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.422588  2412 net.cpp:194] Memory required for data: 1073218048
I0818 10:49:16.422600  2412 layer_factory.hpp:77] Creating layer BatchNorm35
I0818 10:49:16.422610  2412 net.cpp:128] Creating Layer BatchNorm35
I0818 10:49:16.422616  2412 net.cpp:558] BatchNorm35 <- Convolution35
I0818 10:49:16.422624  2412 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0818 10:49:16.422937  2412 net.cpp:172] Setting up BatchNorm35
I0818 10:49:16.422951  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.422955  2412 net.cpp:194] Memory required for data: 1075315200
I0818 10:49:16.422966  2412 layer_factory.hpp:77] Creating layer Scale35
I0818 10:49:16.422976  2412 net.cpp:128] Creating Layer Scale35
I0818 10:49:16.422981  2412 net.cpp:558] Scale35 <- Convolution35
I0818 10:49:16.422986  2412 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0818 10:49:16.423032  2412 layer_factory.hpp:77] Creating layer Scale35
I0818 10:49:16.423194  2412 net.cpp:172] Setting up Scale35
I0818 10:49:16.423202  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.423223  2412 net.cpp:194] Memory required for data: 1077412352
I0818 10:49:16.423231  2412 layer_factory.hpp:77] Creating layer Eltwise16
I0818 10:49:16.423240  2412 net.cpp:128] Creating Layer Eltwise16
I0818 10:49:16.423246  2412 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0818 10:49:16.423251  2412 net.cpp:558] Eltwise16 <- Convolution35
I0818 10:49:16.423260  2412 net.cpp:522] Eltwise16 -> Eltwise16
I0818 10:49:16.423285  2412 net.cpp:172] Setting up Eltwise16
I0818 10:49:16.423292  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.423296  2412 net.cpp:194] Memory required for data: 1079509504
I0818 10:49:16.423300  2412 layer_factory.hpp:77] Creating layer ReLU33
I0818 10:49:16.423306  2412 net.cpp:128] Creating Layer ReLU33
I0818 10:49:16.423311  2412 net.cpp:558] ReLU33 <- Eltwise16
I0818 10:49:16.423318  2412 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0818 10:49:16.424394  2412 net.cpp:172] Setting up ReLU33
I0818 10:49:16.424417  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.424422  2412 net.cpp:194] Memory required for data: 1081606656
I0818 10:49:16.424427  2412 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0818 10:49:16.424433  2412 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0818 10:49:16.424443  2412 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0818 10:49:16.424449  2412 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0818 10:49:16.424458  2412 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0818 10:49:16.424520  2412 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0818 10:49:16.424526  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.424532  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.424536  2412 net.cpp:194] Memory required for data: 1085800960
I0818 10:49:16.424540  2412 layer_factory.hpp:77] Creating layer Convolution36
I0818 10:49:16.424552  2412 net.cpp:128] Creating Layer Convolution36
I0818 10:49:16.424557  2412 net.cpp:558] Convolution36 <- Eltwise16_ReLU33_0_split_0
I0818 10:49:16.424566  2412 net.cpp:522] Convolution36 -> Convolution36
I0818 10:49:16.431093  2412 net.cpp:172] Setting up Convolution36
I0818 10:49:16.431119  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.431124  2412 net.cpp:194] Memory required for data: 1087898112
I0818 10:49:16.431134  2412 layer_factory.hpp:77] Creating layer BatchNorm36
I0818 10:49:16.431145  2412 net.cpp:128] Creating Layer BatchNorm36
I0818 10:49:16.431150  2412 net.cpp:558] BatchNorm36 <- Convolution36
I0818 10:49:16.431157  2412 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0818 10:49:16.431443  2412 net.cpp:172] Setting up BatchNorm36
I0818 10:49:16.431457  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.431460  2412 net.cpp:194] Memory required for data: 1089995264
I0818 10:49:16.431470  2412 layer_factory.hpp:77] Creating layer Scale36
I0818 10:49:16.431479  2412 net.cpp:128] Creating Layer Scale36
I0818 10:49:16.431484  2412 net.cpp:558] Scale36 <- Convolution36
I0818 10:49:16.431490  2412 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0818 10:49:16.431535  2412 layer_factory.hpp:77] Creating layer Scale36
I0818 10:49:16.431700  2412 net.cpp:172] Setting up Scale36
I0818 10:49:16.431708  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.431712  2412 net.cpp:194] Memory required for data: 1092092416
I0818 10:49:16.431720  2412 layer_factory.hpp:77] Creating layer ReLU34
I0818 10:49:16.431730  2412 net.cpp:128] Creating Layer ReLU34
I0818 10:49:16.431735  2412 net.cpp:558] ReLU34 <- Convolution36
I0818 10:49:16.431740  2412 net.cpp:509] ReLU34 -> Convolution36 (in-place)
I0818 10:49:16.433151  2412 net.cpp:172] Setting up ReLU34
I0818 10:49:16.433167  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.433171  2412 net.cpp:194] Memory required for data: 1094189568
I0818 10:49:16.433176  2412 layer_factory.hpp:77] Creating layer Convolution37
I0818 10:49:16.433192  2412 net.cpp:128] Creating Layer Convolution37
I0818 10:49:16.433214  2412 net.cpp:558] Convolution37 <- Convolution36
I0818 10:49:16.433223  2412 net.cpp:522] Convolution37 -> Convolution37
I0818 10:49:16.440120  2412 net.cpp:172] Setting up Convolution37
I0818 10:49:16.440146  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.440151  2412 net.cpp:194] Memory required for data: 1096286720
I0818 10:49:16.440162  2412 layer_factory.hpp:77] Creating layer BatchNorm37
I0818 10:49:16.440173  2412 net.cpp:128] Creating Layer BatchNorm37
I0818 10:49:16.440178  2412 net.cpp:558] BatchNorm37 <- Convolution37
I0818 10:49:16.440192  2412 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0818 10:49:16.440491  2412 net.cpp:172] Setting up BatchNorm37
I0818 10:49:16.440503  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.440508  2412 net.cpp:194] Memory required for data: 1098383872
I0818 10:49:16.440541  2412 layer_factory.hpp:77] Creating layer Scale37
I0818 10:49:16.440551  2412 net.cpp:128] Creating Layer Scale37
I0818 10:49:16.440554  2412 net.cpp:558] Scale37 <- Convolution37
I0818 10:49:16.440560  2412 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0818 10:49:16.440605  2412 layer_factory.hpp:77] Creating layer Scale37
I0818 10:49:16.440768  2412 net.cpp:172] Setting up Scale37
I0818 10:49:16.440778  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.440783  2412 net.cpp:194] Memory required for data: 1100481024
I0818 10:49:16.440790  2412 layer_factory.hpp:77] Creating layer Eltwise17
I0818 10:49:16.440798  2412 net.cpp:128] Creating Layer Eltwise17
I0818 10:49:16.440802  2412 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0818 10:49:16.440807  2412 net.cpp:558] Eltwise17 <- Convolution37
I0818 10:49:16.440816  2412 net.cpp:522] Eltwise17 -> Eltwise17
I0818 10:49:16.440840  2412 net.cpp:172] Setting up Eltwise17
I0818 10:49:16.440847  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.440851  2412 net.cpp:194] Memory required for data: 1102578176
I0818 10:49:16.440855  2412 layer_factory.hpp:77] Creating layer ReLU35
I0818 10:49:16.440865  2412 net.cpp:128] Creating Layer ReLU35
I0818 10:49:16.440870  2412 net.cpp:558] ReLU35 <- Eltwise17
I0818 10:49:16.440874  2412 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0818 10:49:16.441915  2412 net.cpp:172] Setting up ReLU35
I0818 10:49:16.441929  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.441933  2412 net.cpp:194] Memory required for data: 1104675328
I0818 10:49:16.441938  2412 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0818 10:49:16.441947  2412 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0818 10:49:16.441952  2412 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0818 10:49:16.441958  2412 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0818 10:49:16.441967  2412 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0818 10:49:16.442023  2412 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0818 10:49:16.442029  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.442035  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.442039  2412 net.cpp:194] Memory required for data: 1108869632
I0818 10:49:16.442044  2412 layer_factory.hpp:77] Creating layer Convolution38
I0818 10:49:16.442057  2412 net.cpp:128] Creating Layer Convolution38
I0818 10:49:16.442062  2412 net.cpp:558] Convolution38 <- Eltwise17_ReLU35_0_split_0
I0818 10:49:16.442070  2412 net.cpp:522] Convolution38 -> Convolution38
I0818 10:49:16.448676  2412 net.cpp:172] Setting up Convolution38
I0818 10:49:16.448701  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.448706  2412 net.cpp:194] Memory required for data: 1110966784
I0818 10:49:16.448716  2412 layer_factory.hpp:77] Creating layer BatchNorm38
I0818 10:49:16.448726  2412 net.cpp:128] Creating Layer BatchNorm38
I0818 10:49:16.448732  2412 net.cpp:558] BatchNorm38 <- Convolution38
I0818 10:49:16.448740  2412 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0818 10:49:16.449033  2412 net.cpp:172] Setting up BatchNorm38
I0818 10:49:16.449059  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.449064  2412 net.cpp:194] Memory required for data: 1113063936
I0818 10:49:16.449074  2412 layer_factory.hpp:77] Creating layer Scale38
I0818 10:49:16.449082  2412 net.cpp:128] Creating Layer Scale38
I0818 10:49:16.449087  2412 net.cpp:558] Scale38 <- Convolution38
I0818 10:49:16.449093  2412 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0818 10:49:16.449141  2412 layer_factory.hpp:77] Creating layer Scale38
I0818 10:49:16.449304  2412 net.cpp:172] Setting up Scale38
I0818 10:49:16.449311  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.449316  2412 net.cpp:194] Memory required for data: 1115161088
I0818 10:49:16.449323  2412 layer_factory.hpp:77] Creating layer ReLU36
I0818 10:49:16.449332  2412 net.cpp:128] Creating Layer ReLU36
I0818 10:49:16.449337  2412 net.cpp:558] ReLU36 <- Convolution38
I0818 10:49:16.449344  2412 net.cpp:509] ReLU36 -> Convolution38 (in-place)
I0818 10:49:16.450738  2412 net.cpp:172] Setting up ReLU36
I0818 10:49:16.450764  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.450768  2412 net.cpp:194] Memory required for data: 1117258240
I0818 10:49:16.450773  2412 layer_factory.hpp:77] Creating layer Convolution39
I0818 10:49:16.450790  2412 net.cpp:128] Creating Layer Convolution39
I0818 10:49:16.450796  2412 net.cpp:558] Convolution39 <- Convolution38
I0818 10:49:16.450806  2412 net.cpp:522] Convolution39 -> Convolution39
I0818 10:49:16.457691  2412 net.cpp:172] Setting up Convolution39
I0818 10:49:16.457718  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.457722  2412 net.cpp:194] Memory required for data: 1119355392
I0818 10:49:16.457733  2412 layer_factory.hpp:77] Creating layer BatchNorm39
I0818 10:49:16.457744  2412 net.cpp:128] Creating Layer BatchNorm39
I0818 10:49:16.457749  2412 net.cpp:558] BatchNorm39 <- Convolution39
I0818 10:49:16.457759  2412 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0818 10:49:16.458063  2412 net.cpp:172] Setting up BatchNorm39
I0818 10:49:16.458076  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.458081  2412 net.cpp:194] Memory required for data: 1121452544
I0818 10:49:16.458091  2412 layer_factory.hpp:77] Creating layer Scale39
I0818 10:49:16.458097  2412 net.cpp:128] Creating Layer Scale39
I0818 10:49:16.458102  2412 net.cpp:558] Scale39 <- Convolution39
I0818 10:49:16.458108  2412 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0818 10:49:16.458155  2412 layer_factory.hpp:77] Creating layer Scale39
I0818 10:49:16.458320  2412 net.cpp:172] Setting up Scale39
I0818 10:49:16.458331  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.458335  2412 net.cpp:194] Memory required for data: 1123549696
I0818 10:49:16.458343  2412 layer_factory.hpp:77] Creating layer Eltwise18
I0818 10:49:16.458350  2412 net.cpp:128] Creating Layer Eltwise18
I0818 10:49:16.458359  2412 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0818 10:49:16.458364  2412 net.cpp:558] Eltwise18 <- Convolution39
I0818 10:49:16.458369  2412 net.cpp:522] Eltwise18 -> Eltwise18
I0818 10:49:16.458392  2412 net.cpp:172] Setting up Eltwise18
I0818 10:49:16.458400  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.458403  2412 net.cpp:194] Memory required for data: 1125646848
I0818 10:49:16.458408  2412 layer_factory.hpp:77] Creating layer ReLU37
I0818 10:49:16.458417  2412 net.cpp:128] Creating Layer ReLU37
I0818 10:49:16.458421  2412 net.cpp:558] ReLU37 <- Eltwise18
I0818 10:49:16.458427  2412 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0818 10:49:16.459497  2412 net.cpp:172] Setting up ReLU37
I0818 10:49:16.459513  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.459518  2412 net.cpp:194] Memory required for data: 1127744000
I0818 10:49:16.459523  2412 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0818 10:49:16.459532  2412 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0818 10:49:16.459538  2412 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0818 10:49:16.459564  2412 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0818 10:49:16.459574  2412 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0818 10:49:16.459632  2412 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0818 10:49:16.459641  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.459647  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.459651  2412 net.cpp:194] Memory required for data: 1131938304
I0818 10:49:16.459656  2412 layer_factory.hpp:77] Creating layer Convolution40
I0818 10:49:16.459668  2412 net.cpp:128] Creating Layer Convolution40
I0818 10:49:16.459673  2412 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_0
I0818 10:49:16.459681  2412 net.cpp:522] Convolution40 -> Convolution40
I0818 10:49:16.463779  2412 net.cpp:172] Setting up Convolution40
I0818 10:49:16.463804  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.463809  2412 net.cpp:194] Memory required for data: 1134035456
I0818 10:49:16.463819  2412 layer_factory.hpp:77] Creating layer BatchNorm40
I0818 10:49:16.463830  2412 net.cpp:128] Creating Layer BatchNorm40
I0818 10:49:16.463835  2412 net.cpp:558] BatchNorm40 <- Convolution40
I0818 10:49:16.463845  2412 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0818 10:49:16.464135  2412 net.cpp:172] Setting up BatchNorm40
I0818 10:49:16.464146  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.464150  2412 net.cpp:194] Memory required for data: 1136132608
I0818 10:49:16.464160  2412 layer_factory.hpp:77] Creating layer Scale40
I0818 10:49:16.464166  2412 net.cpp:128] Creating Layer Scale40
I0818 10:49:16.464171  2412 net.cpp:558] Scale40 <- Convolution40
I0818 10:49:16.464184  2412 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0818 10:49:16.464227  2412 layer_factory.hpp:77] Creating layer Scale40
I0818 10:49:16.464388  2412 net.cpp:172] Setting up Scale40
I0818 10:49:16.464399  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.464403  2412 net.cpp:194] Memory required for data: 1138229760
I0818 10:49:16.464411  2412 layer_factory.hpp:77] Creating layer ReLU38
I0818 10:49:16.464417  2412 net.cpp:128] Creating Layer ReLU38
I0818 10:49:16.464421  2412 net.cpp:558] ReLU38 <- Convolution40
I0818 10:49:16.464429  2412 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0818 10:49:16.464673  2412 net.cpp:172] Setting up ReLU38
I0818 10:49:16.464689  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.464692  2412 net.cpp:194] Memory required for data: 1140326912
I0818 10:49:16.464696  2412 layer_factory.hpp:77] Creating layer Convolution41
I0818 10:49:16.464709  2412 net.cpp:128] Creating Layer Convolution41
I0818 10:49:16.464715  2412 net.cpp:558] Convolution41 <- Convolution40
I0818 10:49:16.464723  2412 net.cpp:522] Convolution41 -> Convolution41
I0818 10:49:16.467239  2412 net.cpp:172] Setting up Convolution41
I0818 10:49:16.467267  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.467270  2412 net.cpp:194] Memory required for data: 1142424064
I0818 10:49:16.467281  2412 layer_factory.hpp:77] Creating layer BatchNorm41
I0818 10:49:16.467290  2412 net.cpp:128] Creating Layer BatchNorm41
I0818 10:49:16.467295  2412 net.cpp:558] BatchNorm41 <- Convolution41
I0818 10:49:16.467304  2412 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0818 10:49:16.467600  2412 net.cpp:172] Setting up BatchNorm41
I0818 10:49:16.467607  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.467612  2412 net.cpp:194] Memory required for data: 1144521216
I0818 10:49:16.467622  2412 layer_factory.hpp:77] Creating layer Scale41
I0818 10:49:16.467628  2412 net.cpp:128] Creating Layer Scale41
I0818 10:49:16.467633  2412 net.cpp:558] Scale41 <- Convolution41
I0818 10:49:16.467638  2412 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0818 10:49:16.467682  2412 layer_factory.hpp:77] Creating layer Scale41
I0818 10:49:16.467849  2412 net.cpp:172] Setting up Scale41
I0818 10:49:16.467857  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.467875  2412 net.cpp:194] Memory required for data: 1146618368
I0818 10:49:16.467885  2412 layer_factory.hpp:77] Creating layer Eltwise19
I0818 10:49:16.467893  2412 net.cpp:128] Creating Layer Eltwise19
I0818 10:49:16.467898  2412 net.cpp:558] Eltwise19 <- Eltwise18_ReLU37_0_split_1
I0818 10:49:16.467903  2412 net.cpp:558] Eltwise19 <- Convolution41
I0818 10:49:16.467909  2412 net.cpp:522] Eltwise19 -> Eltwise19
I0818 10:49:16.467934  2412 net.cpp:172] Setting up Eltwise19
I0818 10:49:16.467941  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.467945  2412 net.cpp:194] Memory required for data: 1148715520
I0818 10:49:16.467949  2412 layer_factory.hpp:77] Creating layer ReLU39
I0818 10:49:16.467958  2412 net.cpp:128] Creating Layer ReLU39
I0818 10:49:16.467962  2412 net.cpp:558] ReLU39 <- Eltwise19
I0818 10:49:16.467968  2412 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0818 10:49:16.468220  2412 net.cpp:172] Setting up ReLU39
I0818 10:49:16.468230  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.468233  2412 net.cpp:194] Memory required for data: 1150812672
I0818 10:49:16.468238  2412 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0818 10:49:16.468247  2412 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0818 10:49:16.468252  2412 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0818 10:49:16.468261  2412 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0818 10:49:16.468271  2412 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0818 10:49:16.468325  2412 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0818 10:49:16.468335  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.468341  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.468345  2412 net.cpp:194] Memory required for data: 1155006976
I0818 10:49:16.468350  2412 layer_factory.hpp:77] Creating layer Convolution42
I0818 10:49:16.468363  2412 net.cpp:128] Creating Layer Convolution42
I0818 10:49:16.468367  2412 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0818 10:49:16.468374  2412 net.cpp:522] Convolution42 -> Convolution42
I0818 10:49:16.473819  2412 net.cpp:172] Setting up Convolution42
I0818 10:49:16.473847  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.473851  2412 net.cpp:194] Memory required for data: 1157104128
I0818 10:49:16.473866  2412 layer_factory.hpp:77] Creating layer BatchNorm42
I0818 10:49:16.473881  2412 net.cpp:128] Creating Layer BatchNorm42
I0818 10:49:16.473887  2412 net.cpp:558] BatchNorm42 <- Convolution42
I0818 10:49:16.473897  2412 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0818 10:49:16.474189  2412 net.cpp:172] Setting up BatchNorm42
I0818 10:49:16.474202  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.474206  2412 net.cpp:194] Memory required for data: 1159201280
I0818 10:49:16.474216  2412 layer_factory.hpp:77] Creating layer Scale42
I0818 10:49:16.474223  2412 net.cpp:128] Creating Layer Scale42
I0818 10:49:16.474227  2412 net.cpp:558] Scale42 <- Convolution42
I0818 10:49:16.474236  2412 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0818 10:49:16.474282  2412 layer_factory.hpp:77] Creating layer Scale42
I0818 10:49:16.474452  2412 net.cpp:172] Setting up Scale42
I0818 10:49:16.474462  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.474465  2412 net.cpp:194] Memory required for data: 1161298432
I0818 10:49:16.474473  2412 layer_factory.hpp:77] Creating layer ReLU40
I0818 10:49:16.474479  2412 net.cpp:128] Creating Layer ReLU40
I0818 10:49:16.474484  2412 net.cpp:558] ReLU40 <- Convolution42
I0818 10:49:16.474495  2412 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0818 10:49:16.475838  2412 net.cpp:172] Setting up ReLU40
I0818 10:49:16.475864  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.475868  2412 net.cpp:194] Memory required for data: 1163395584
I0818 10:49:16.475873  2412 layer_factory.hpp:77] Creating layer Convolution43
I0818 10:49:16.475894  2412 net.cpp:128] Creating Layer Convolution43
I0818 10:49:16.475921  2412 net.cpp:558] Convolution43 <- Convolution42
I0818 10:49:16.475932  2412 net.cpp:522] Convolution43 -> Convolution43
I0818 10:49:16.482894  2412 net.cpp:172] Setting up Convolution43
I0818 10:49:16.482916  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.482921  2412 net.cpp:194] Memory required for data: 1165492736
I0818 10:49:16.482934  2412 layer_factory.hpp:77] Creating layer BatchNorm43
I0818 10:49:16.482944  2412 net.cpp:128] Creating Layer BatchNorm43
I0818 10:49:16.482951  2412 net.cpp:558] BatchNorm43 <- Convolution43
I0818 10:49:16.482960  2412 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0818 10:49:16.483263  2412 net.cpp:172] Setting up BatchNorm43
I0818 10:49:16.483273  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.483278  2412 net.cpp:194] Memory required for data: 1167589888
I0818 10:49:16.483289  2412 layer_factory.hpp:77] Creating layer Scale43
I0818 10:49:16.483297  2412 net.cpp:128] Creating Layer Scale43
I0818 10:49:16.483302  2412 net.cpp:558] Scale43 <- Convolution43
I0818 10:49:16.483309  2412 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0818 10:49:16.483351  2412 layer_factory.hpp:77] Creating layer Scale43
I0818 10:49:16.483516  2412 net.cpp:172] Setting up Scale43
I0818 10:49:16.483523  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.483527  2412 net.cpp:194] Memory required for data: 1169687040
I0818 10:49:16.483536  2412 layer_factory.hpp:77] Creating layer Eltwise20
I0818 10:49:16.483546  2412 net.cpp:128] Creating Layer Eltwise20
I0818 10:49:16.483552  2412 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0818 10:49:16.483557  2412 net.cpp:558] Eltwise20 <- Convolution43
I0818 10:49:16.483564  2412 net.cpp:522] Eltwise20 -> Eltwise20
I0818 10:49:16.483597  2412 net.cpp:172] Setting up Eltwise20
I0818 10:49:16.483608  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.483611  2412 net.cpp:194] Memory required for data: 1171784192
I0818 10:49:16.483615  2412 layer_factory.hpp:77] Creating layer ReLU41
I0818 10:49:16.483621  2412 net.cpp:128] Creating Layer ReLU41
I0818 10:49:16.483626  2412 net.cpp:558] ReLU41 <- Eltwise20
I0818 10:49:16.483631  2412 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0818 10:49:16.484688  2412 net.cpp:172] Setting up ReLU41
I0818 10:49:16.484709  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.484712  2412 net.cpp:194] Memory required for data: 1173881344
I0818 10:49:16.484717  2412 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0818 10:49:16.484730  2412 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0818 10:49:16.484735  2412 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0818 10:49:16.484745  2412 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0818 10:49:16.484757  2412 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0818 10:49:16.484814  2412 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0818 10:49:16.484822  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.484827  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.484832  2412 net.cpp:194] Memory required for data: 1178075648
I0818 10:49:16.484835  2412 layer_factory.hpp:77] Creating layer Convolution44
I0818 10:49:16.484851  2412 net.cpp:128] Creating Layer Convolution44
I0818 10:49:16.484858  2412 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0818 10:49:16.484864  2412 net.cpp:522] Convolution44 -> Convolution44
I0818 10:49:16.491473  2412 net.cpp:172] Setting up Convolution44
I0818 10:49:16.491499  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.491503  2412 net.cpp:194] Memory required for data: 1180172800
I0818 10:49:16.491514  2412 layer_factory.hpp:77] Creating layer BatchNorm44
I0818 10:49:16.491526  2412 net.cpp:128] Creating Layer BatchNorm44
I0818 10:49:16.491531  2412 net.cpp:558] BatchNorm44 <- Convolution44
I0818 10:49:16.491540  2412 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0818 10:49:16.491835  2412 net.cpp:172] Setting up BatchNorm44
I0818 10:49:16.491863  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.491868  2412 net.cpp:194] Memory required for data: 1182269952
I0818 10:49:16.491878  2412 layer_factory.hpp:77] Creating layer Scale44
I0818 10:49:16.491884  2412 net.cpp:128] Creating Layer Scale44
I0818 10:49:16.491889  2412 net.cpp:558] Scale44 <- Convolution44
I0818 10:49:16.491894  2412 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0818 10:49:16.491943  2412 layer_factory.hpp:77] Creating layer Scale44
I0818 10:49:16.492108  2412 net.cpp:172] Setting up Scale44
I0818 10:49:16.492120  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.492125  2412 net.cpp:194] Memory required for data: 1184367104
I0818 10:49:16.492132  2412 layer_factory.hpp:77] Creating layer ReLU42
I0818 10:49:16.492139  2412 net.cpp:128] Creating Layer ReLU42
I0818 10:49:16.492143  2412 net.cpp:558] ReLU42 <- Convolution44
I0818 10:49:16.492152  2412 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0818 10:49:16.493497  2412 net.cpp:172] Setting up ReLU42
I0818 10:49:16.493515  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.493520  2412 net.cpp:194] Memory required for data: 1186464256
I0818 10:49:16.493525  2412 layer_factory.hpp:77] Creating layer Convolution45
I0818 10:49:16.493537  2412 net.cpp:128] Creating Layer Convolution45
I0818 10:49:16.493542  2412 net.cpp:558] Convolution45 <- Convolution44
I0818 10:49:16.493552  2412 net.cpp:522] Convolution45 -> Convolution45
I0818 10:49:16.500486  2412 net.cpp:172] Setting up Convolution45
I0818 10:49:16.500512  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.500517  2412 net.cpp:194] Memory required for data: 1188561408
I0818 10:49:16.500528  2412 layer_factory.hpp:77] Creating layer BatchNorm45
I0818 10:49:16.500538  2412 net.cpp:128] Creating Layer BatchNorm45
I0818 10:49:16.500545  2412 net.cpp:558] BatchNorm45 <- Convolution45
I0818 10:49:16.500550  2412 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0818 10:49:16.500859  2412 net.cpp:172] Setting up BatchNorm45
I0818 10:49:16.500872  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.500876  2412 net.cpp:194] Memory required for data: 1190658560
I0818 10:49:16.500885  2412 layer_factory.hpp:77] Creating layer Scale45
I0818 10:49:16.500895  2412 net.cpp:128] Creating Layer Scale45
I0818 10:49:16.500900  2412 net.cpp:558] Scale45 <- Convolution45
I0818 10:49:16.500905  2412 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0818 10:49:16.500952  2412 layer_factory.hpp:77] Creating layer Scale45
I0818 10:49:16.501121  2412 net.cpp:172] Setting up Scale45
I0818 10:49:16.501132  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.501137  2412 net.cpp:194] Memory required for data: 1192755712
I0818 10:49:16.501144  2412 layer_factory.hpp:77] Creating layer Eltwise21
I0818 10:49:16.501153  2412 net.cpp:128] Creating Layer Eltwise21
I0818 10:49:16.501158  2412 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0818 10:49:16.501163  2412 net.cpp:558] Eltwise21 <- Convolution45
I0818 10:49:16.501169  2412 net.cpp:522] Eltwise21 -> Eltwise21
I0818 10:49:16.501195  2412 net.cpp:172] Setting up Eltwise21
I0818 10:49:16.501202  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.501205  2412 net.cpp:194] Memory required for data: 1194852864
I0818 10:49:16.501210  2412 layer_factory.hpp:77] Creating layer ReLU43
I0818 10:49:16.501216  2412 net.cpp:128] Creating Layer ReLU43
I0818 10:49:16.501221  2412 net.cpp:558] ReLU43 <- Eltwise21
I0818 10:49:16.501229  2412 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0818 10:49:16.502267  2412 net.cpp:172] Setting up ReLU43
I0818 10:49:16.502293  2412 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:49:16.502297  2412 net.cpp:194] Memory required for data: 1196950016
I0818 10:49:16.502302  2412 layer_factory.hpp:77] Creating layer Pooling1
I0818 10:49:16.502316  2412 net.cpp:128] Creating Layer Pooling1
I0818 10:49:16.502321  2412 net.cpp:558] Pooling1 <- Eltwise21
I0818 10:49:16.502331  2412 net.cpp:522] Pooling1 -> Pooling1
I0818 10:49:16.504554  2412 net.cpp:172] Setting up Pooling1
I0818 10:49:16.504576  2412 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0818 10:49:16.504581  2412 net.cpp:194] Memory required for data: 1196982784
I0818 10:49:16.504586  2412 layer_factory.hpp:77] Creating layer InnerProduct1
I0818 10:49:16.504598  2412 net.cpp:128] Creating Layer InnerProduct1
I0818 10:49:16.504604  2412 net.cpp:558] InnerProduct1 <- Pooling1
I0818 10:49:16.504613  2412 net.cpp:522] InnerProduct1 -> InnerProduct1
I0818 10:49:16.504812  2412 net.cpp:172] Setting up InnerProduct1
I0818 10:49:16.504823  2412 net.cpp:186] Top shape: 128 10 (1280)
I0818 10:49:16.504827  2412 net.cpp:194] Memory required for data: 1196987904
I0818 10:49:16.504837  2412 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:49:16.504844  2412 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0818 10:49:16.504849  2412 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0818 10:49:16.504854  2412 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0818 10:49:16.504864  2412 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0818 10:49:16.504876  2412 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:49:16.506803  2412 net.cpp:172] Setting up SoftmaxWithLoss1
I0818 10:49:16.506820  2412 net.cpp:186] Top shape: (1)
I0818 10:49:16.506824  2412 net.cpp:189]     with loss weight 1
I0818 10:49:16.506856  2412 net.cpp:194] Memory required for data: 1196987908
I0818 10:49:16.506865  2412 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0818 10:49:16.506872  2412 net.cpp:301] InnerProduct1 needs backward computation.
I0818 10:49:16.506877  2412 net.cpp:301] Pooling1 needs backward computation.
I0818 10:49:16.506881  2412 net.cpp:301] ReLU43 needs backward computation.
I0818 10:49:16.506886  2412 net.cpp:301] Eltwise21 needs backward computation.
I0818 10:49:16.506896  2412 net.cpp:301] Scale45 needs backward computation.
I0818 10:49:16.506901  2412 net.cpp:301] BatchNorm45 needs backward computation.
I0818 10:49:16.506904  2412 net.cpp:301] Convolution45 needs backward computation.
I0818 10:49:16.506908  2412 net.cpp:301] ReLU42 needs backward computation.
I0818 10:49:16.506912  2412 net.cpp:301] Scale44 needs backward computation.
I0818 10:49:16.506916  2412 net.cpp:301] BatchNorm44 needs backward computation.
I0818 10:49:16.506920  2412 net.cpp:301] Convolution44 needs backward computation.
I0818 10:49:16.506924  2412 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0818 10:49:16.506929  2412 net.cpp:301] ReLU41 needs backward computation.
I0818 10:49:16.506933  2412 net.cpp:301] Eltwise20 needs backward computation.
I0818 10:49:16.506938  2412 net.cpp:301] Scale43 needs backward computation.
I0818 10:49:16.506942  2412 net.cpp:301] BatchNorm43 needs backward computation.
I0818 10:49:16.506947  2412 net.cpp:301] Convolution43 needs backward computation.
I0818 10:49:16.506950  2412 net.cpp:301] ReLU40 needs backward computation.
I0818 10:49:16.506954  2412 net.cpp:301] Scale42 needs backward computation.
I0818 10:49:16.506959  2412 net.cpp:301] BatchNorm42 needs backward computation.
I0818 10:49:16.506963  2412 net.cpp:301] Convolution42 needs backward computation.
I0818 10:49:16.506968  2412 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0818 10:49:16.506973  2412 net.cpp:301] ReLU39 needs backward computation.
I0818 10:49:16.506978  2412 net.cpp:301] Eltwise19 needs backward computation.
I0818 10:49:16.506983  2412 net.cpp:301] Scale41 needs backward computation.
I0818 10:49:16.506988  2412 net.cpp:301] BatchNorm41 needs backward computation.
I0818 10:49:16.506991  2412 net.cpp:301] Convolution41 needs backward computation.
I0818 10:49:16.506996  2412 net.cpp:301] ReLU38 needs backward computation.
I0818 10:49:16.507000  2412 net.cpp:301] Scale40 needs backward computation.
I0818 10:49:16.507004  2412 net.cpp:301] BatchNorm40 needs backward computation.
I0818 10:49:16.507009  2412 net.cpp:301] Convolution40 needs backward computation.
I0818 10:49:16.507014  2412 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0818 10:49:16.507035  2412 net.cpp:301] ReLU37 needs backward computation.
I0818 10:49:16.507040  2412 net.cpp:301] Eltwise18 needs backward computation.
I0818 10:49:16.507045  2412 net.cpp:301] Scale39 needs backward computation.
I0818 10:49:16.507050  2412 net.cpp:301] BatchNorm39 needs backward computation.
I0818 10:49:16.507053  2412 net.cpp:301] Convolution39 needs backward computation.
I0818 10:49:16.507057  2412 net.cpp:301] ReLU36 needs backward computation.
I0818 10:49:16.507062  2412 net.cpp:301] Scale38 needs backward computation.
I0818 10:49:16.507066  2412 net.cpp:301] BatchNorm38 needs backward computation.
I0818 10:49:16.507071  2412 net.cpp:301] Convolution38 needs backward computation.
I0818 10:49:16.507076  2412 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0818 10:49:16.507081  2412 net.cpp:301] ReLU35 needs backward computation.
I0818 10:49:16.507087  2412 net.cpp:301] Eltwise17 needs backward computation.
I0818 10:49:16.507093  2412 net.cpp:301] Scale37 needs backward computation.
I0818 10:49:16.507097  2412 net.cpp:301] BatchNorm37 needs backward computation.
I0818 10:49:16.507102  2412 net.cpp:301] Convolution37 needs backward computation.
I0818 10:49:16.507107  2412 net.cpp:301] ReLU34 needs backward computation.
I0818 10:49:16.507112  2412 net.cpp:301] Scale36 needs backward computation.
I0818 10:49:16.507115  2412 net.cpp:301] BatchNorm36 needs backward computation.
I0818 10:49:16.507119  2412 net.cpp:301] Convolution36 needs backward computation.
I0818 10:49:16.507124  2412 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0818 10:49:16.507129  2412 net.cpp:301] ReLU33 needs backward computation.
I0818 10:49:16.507133  2412 net.cpp:301] Eltwise16 needs backward computation.
I0818 10:49:16.507139  2412 net.cpp:301] Scale35 needs backward computation.
I0818 10:49:16.507143  2412 net.cpp:301] BatchNorm35 needs backward computation.
I0818 10:49:16.507148  2412 net.cpp:301] Convolution35 needs backward computation.
I0818 10:49:16.507153  2412 net.cpp:301] ReLU32 needs backward computation.
I0818 10:49:16.507158  2412 net.cpp:301] Scale34 needs backward computation.
I0818 10:49:16.507161  2412 net.cpp:301] BatchNorm34 needs backward computation.
I0818 10:49:16.507165  2412 net.cpp:301] Convolution34 needs backward computation.
I0818 10:49:16.507171  2412 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0818 10:49:16.507175  2412 net.cpp:301] ReLU31 needs backward computation.
I0818 10:49:16.507180  2412 net.cpp:301] Eltwise15 needs backward computation.
I0818 10:49:16.507185  2412 net.cpp:301] Scale33 needs backward computation.
I0818 10:49:16.507190  2412 net.cpp:301] BatchNorm33 needs backward computation.
I0818 10:49:16.507194  2412 net.cpp:301] Convolution33 needs backward computation.
I0818 10:49:16.507200  2412 net.cpp:301] ReLU30 needs backward computation.
I0818 10:49:16.507205  2412 net.cpp:301] Scale32 needs backward computation.
I0818 10:49:16.507208  2412 net.cpp:301] BatchNorm32 needs backward computation.
I0818 10:49:16.507212  2412 net.cpp:301] Convolution32 needs backward computation.
I0818 10:49:16.507217  2412 net.cpp:301] Scale31 needs backward computation.
I0818 10:49:16.507222  2412 net.cpp:301] BatchNorm31 needs backward computation.
I0818 10:49:16.507226  2412 net.cpp:301] Convolution31 needs backward computation.
I0818 10:49:16.507231  2412 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0818 10:49:16.507236  2412 net.cpp:301] ReLU29 needs backward computation.
I0818 10:49:16.507241  2412 net.cpp:301] Eltwise14 needs backward computation.
I0818 10:49:16.507246  2412 net.cpp:301] Scale30 needs backward computation.
I0818 10:49:16.507251  2412 net.cpp:301] BatchNorm30 needs backward computation.
I0818 10:49:16.507256  2412 net.cpp:301] Convolution30 needs backward computation.
I0818 10:49:16.507261  2412 net.cpp:301] ReLU28 needs backward computation.
I0818 10:49:16.507264  2412 net.cpp:301] Scale29 needs backward computation.
I0818 10:49:16.507269  2412 net.cpp:301] BatchNorm29 needs backward computation.
I0818 10:49:16.507280  2412 net.cpp:301] Convolution29 needs backward computation.
I0818 10:49:16.507285  2412 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0818 10:49:16.507290  2412 net.cpp:301] ReLU27 needs backward computation.
I0818 10:49:16.507295  2412 net.cpp:301] Eltwise13 needs backward computation.
I0818 10:49:16.507300  2412 net.cpp:301] Scale28 needs backward computation.
I0818 10:49:16.507304  2412 net.cpp:301] BatchNorm28 needs backward computation.
I0818 10:49:16.507309  2412 net.cpp:301] Convolution28 needs backward computation.
I0818 10:49:16.507316  2412 net.cpp:301] ReLU26 needs backward computation.
I0818 10:49:16.507321  2412 net.cpp:301] Scale27 needs backward computation.
I0818 10:49:16.507325  2412 net.cpp:301] BatchNorm27 needs backward computation.
I0818 10:49:16.507329  2412 net.cpp:301] Convolution27 needs backward computation.
I0818 10:49:16.507335  2412 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0818 10:49:16.507339  2412 net.cpp:301] ReLU25 needs backward computation.
I0818 10:49:16.507344  2412 net.cpp:301] Eltwise12 needs backward computation.
I0818 10:49:16.507349  2412 net.cpp:301] Scale26 needs backward computation.
I0818 10:49:16.507354  2412 net.cpp:301] BatchNorm26 needs backward computation.
I0818 10:49:16.507359  2412 net.cpp:301] Convolution26 needs backward computation.
I0818 10:49:16.507369  2412 net.cpp:301] ReLU24 needs backward computation.
I0818 10:49:16.507374  2412 net.cpp:301] Scale25 needs backward computation.
I0818 10:49:16.507378  2412 net.cpp:301] BatchNorm25 needs backward computation.
I0818 10:49:16.507382  2412 net.cpp:301] Convolution25 needs backward computation.
I0818 10:49:16.507387  2412 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0818 10:49:16.507392  2412 net.cpp:301] ReLU23 needs backward computation.
I0818 10:49:16.507397  2412 net.cpp:301] Eltwise11 needs backward computation.
I0818 10:49:16.507402  2412 net.cpp:301] Scale24 needs backward computation.
I0818 10:49:16.507407  2412 net.cpp:301] BatchNorm24 needs backward computation.
I0818 10:49:16.507411  2412 net.cpp:301] Convolution24 needs backward computation.
I0818 10:49:16.507417  2412 net.cpp:301] ReLU22 needs backward computation.
I0818 10:49:16.507421  2412 net.cpp:301] Scale23 needs backward computation.
I0818 10:49:16.507426  2412 net.cpp:301] BatchNorm23 needs backward computation.
I0818 10:49:16.507431  2412 net.cpp:301] Convolution23 needs backward computation.
I0818 10:49:16.507436  2412 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0818 10:49:16.507441  2412 net.cpp:301] ReLU21 needs backward computation.
I0818 10:49:16.507444  2412 net.cpp:301] Eltwise10 needs backward computation.
I0818 10:49:16.507449  2412 net.cpp:301] Scale22 needs backward computation.
I0818 10:49:16.507454  2412 net.cpp:301] BatchNorm22 needs backward computation.
I0818 10:49:16.507458  2412 net.cpp:301] Convolution22 needs backward computation.
I0818 10:49:16.507463  2412 net.cpp:301] ReLU20 needs backward computation.
I0818 10:49:16.507468  2412 net.cpp:301] Scale21 needs backward computation.
I0818 10:49:16.507472  2412 net.cpp:301] BatchNorm21 needs backward computation.
I0818 10:49:16.507477  2412 net.cpp:301] Convolution21 needs backward computation.
I0818 10:49:16.507481  2412 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0818 10:49:16.507488  2412 net.cpp:301] ReLU19 needs backward computation.
I0818 10:49:16.507491  2412 net.cpp:301] Eltwise9 needs backward computation.
I0818 10:49:16.507499  2412 net.cpp:301] Scale20 needs backward computation.
I0818 10:49:16.507504  2412 net.cpp:301] BatchNorm20 needs backward computation.
I0818 10:49:16.507508  2412 net.cpp:301] Convolution20 needs backward computation.
I0818 10:49:16.507513  2412 net.cpp:301] ReLU18 needs backward computation.
I0818 10:49:16.507519  2412 net.cpp:301] Scale19 needs backward computation.
I0818 10:49:16.507522  2412 net.cpp:301] BatchNorm19 needs backward computation.
I0818 10:49:16.507527  2412 net.cpp:301] Convolution19 needs backward computation.
I0818 10:49:16.507539  2412 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0818 10:49:16.507544  2412 net.cpp:301] ReLU17 needs backward computation.
I0818 10:49:16.507550  2412 net.cpp:301] Eltwise8 needs backward computation.
I0818 10:49:16.507555  2412 net.cpp:301] Scale18 needs backward computation.
I0818 10:49:16.507560  2412 net.cpp:301] BatchNorm18 needs backward computation.
I0818 10:49:16.507563  2412 net.cpp:301] Convolution18 needs backward computation.
I0818 10:49:16.507568  2412 net.cpp:301] ReLU16 needs backward computation.
I0818 10:49:16.507572  2412 net.cpp:301] Scale17 needs backward computation.
I0818 10:49:16.507577  2412 net.cpp:301] BatchNorm17 needs backward computation.
I0818 10:49:16.507581  2412 net.cpp:301] Convolution17 needs backward computation.
I0818 10:49:16.507587  2412 net.cpp:301] Scale16 needs backward computation.
I0818 10:49:16.507591  2412 net.cpp:301] BatchNorm16 needs backward computation.
I0818 10:49:16.507596  2412 net.cpp:301] Convolution16 needs backward computation.
I0818 10:49:16.507601  2412 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0818 10:49:16.507606  2412 net.cpp:301] ReLU15 needs backward computation.
I0818 10:49:16.507611  2412 net.cpp:301] Eltwise7 needs backward computation.
I0818 10:49:16.507616  2412 net.cpp:301] Scale15 needs backward computation.
I0818 10:49:16.507622  2412 net.cpp:301] BatchNorm15 needs backward computation.
I0818 10:49:16.507625  2412 net.cpp:301] Convolution15 needs backward computation.
I0818 10:49:16.507630  2412 net.cpp:301] ReLU14 needs backward computation.
I0818 10:49:16.507634  2412 net.cpp:301] Scale14 needs backward computation.
I0818 10:49:16.507639  2412 net.cpp:301] BatchNorm14 needs backward computation.
I0818 10:49:16.507643  2412 net.cpp:301] Convolution14 needs backward computation.
I0818 10:49:16.507648  2412 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0818 10:49:16.507653  2412 net.cpp:301] ReLU13 needs backward computation.
I0818 10:49:16.507658  2412 net.cpp:301] Eltwise6 needs backward computation.
I0818 10:49:16.507663  2412 net.cpp:301] Scale13 needs backward computation.
I0818 10:49:16.507668  2412 net.cpp:301] BatchNorm13 needs backward computation.
I0818 10:49:16.507673  2412 net.cpp:301] Convolution13 needs backward computation.
I0818 10:49:16.507676  2412 net.cpp:301] ReLU12 needs backward computation.
I0818 10:49:16.507681  2412 net.cpp:301] Scale12 needs backward computation.
I0818 10:49:16.507685  2412 net.cpp:301] BatchNorm12 needs backward computation.
I0818 10:49:16.507690  2412 net.cpp:301] Convolution12 needs backward computation.
I0818 10:49:16.507695  2412 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0818 10:49:16.507699  2412 net.cpp:301] ReLU11 needs backward computation.
I0818 10:49:16.507704  2412 net.cpp:301] Eltwise5 needs backward computation.
I0818 10:49:16.507709  2412 net.cpp:301] Scale11 needs backward computation.
I0818 10:49:16.507714  2412 net.cpp:301] BatchNorm11 needs backward computation.
I0818 10:49:16.507719  2412 net.cpp:301] Convolution11 needs backward computation.
I0818 10:49:16.507725  2412 net.cpp:301] ReLU10 needs backward computation.
I0818 10:49:16.507730  2412 net.cpp:301] Scale10 needs backward computation.
I0818 10:49:16.507735  2412 net.cpp:301] BatchNorm10 needs backward computation.
I0818 10:49:16.507738  2412 net.cpp:301] Convolution10 needs backward computation.
I0818 10:49:16.507745  2412 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0818 10:49:16.507748  2412 net.cpp:301] ReLU9 needs backward computation.
I0818 10:49:16.507753  2412 net.cpp:301] Eltwise4 needs backward computation.
I0818 10:49:16.507760  2412 net.cpp:301] Scale9 needs backward computation.
I0818 10:49:16.507764  2412 net.cpp:301] BatchNorm9 needs backward computation.
I0818 10:49:16.507769  2412 net.cpp:301] Convolution9 needs backward computation.
I0818 10:49:16.507774  2412 net.cpp:301] ReLU8 needs backward computation.
I0818 10:49:16.507778  2412 net.cpp:301] Scale8 needs backward computation.
I0818 10:49:16.507791  2412 net.cpp:301] BatchNorm8 needs backward computation.
I0818 10:49:16.507796  2412 net.cpp:301] Convolution8 needs backward computation.
I0818 10:49:16.507800  2412 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0818 10:49:16.507804  2412 net.cpp:301] ReLU7 needs backward computation.
I0818 10:49:16.507809  2412 net.cpp:301] Eltwise3 needs backward computation.
I0818 10:49:16.507815  2412 net.cpp:301] Scale7 needs backward computation.
I0818 10:49:16.507819  2412 net.cpp:301] BatchNorm7 needs backward computation.
I0818 10:49:16.507823  2412 net.cpp:301] Convolution7 needs backward computation.
I0818 10:49:16.507828  2412 net.cpp:301] ReLU6 needs backward computation.
I0818 10:49:16.507833  2412 net.cpp:301] Scale6 needs backward computation.
I0818 10:49:16.507838  2412 net.cpp:301] BatchNorm6 needs backward computation.
I0818 10:49:16.507841  2412 net.cpp:301] Convolution6 needs backward computation.
I0818 10:49:16.507846  2412 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0818 10:49:16.507858  2412 net.cpp:301] ReLU5 needs backward computation.
I0818 10:49:16.507861  2412 net.cpp:301] Eltwise2 needs backward computation.
I0818 10:49:16.507866  2412 net.cpp:301] Scale5 needs backward computation.
I0818 10:49:16.507872  2412 net.cpp:301] BatchNorm5 needs backward computation.
I0818 10:49:16.507876  2412 net.cpp:301] Convolution5 needs backward computation.
I0818 10:49:16.507881  2412 net.cpp:301] ReLU4 needs backward computation.
I0818 10:49:16.507885  2412 net.cpp:301] Scale4 needs backward computation.
I0818 10:49:16.507889  2412 net.cpp:301] BatchNorm4 needs backward computation.
I0818 10:49:16.507894  2412 net.cpp:301] Convolution4 needs backward computation.
I0818 10:49:16.507899  2412 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0818 10:49:16.507905  2412 net.cpp:301] ReLU3 needs backward computation.
I0818 10:49:16.507910  2412 net.cpp:301] Eltwise1 needs backward computation.
I0818 10:49:16.507917  2412 net.cpp:301] Scale3 needs backward computation.
I0818 10:49:16.507922  2412 net.cpp:301] BatchNorm3 needs backward computation.
I0818 10:49:16.507926  2412 net.cpp:301] Convolution3 needs backward computation.
I0818 10:49:16.507931  2412 net.cpp:301] ReLU2 needs backward computation.
I0818 10:49:16.507936  2412 net.cpp:301] Scale2 needs backward computation.
I0818 10:49:16.507941  2412 net.cpp:301] BatchNorm2 needs backward computation.
I0818 10:49:16.507946  2412 net.cpp:301] Convolution2 needs backward computation.
I0818 10:49:16.507951  2412 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0818 10:49:16.507956  2412 net.cpp:301] ReLU1 needs backward computation.
I0818 10:49:16.507961  2412 net.cpp:301] Scale1 needs backward computation.
I0818 10:49:16.507964  2412 net.cpp:301] BatchNorm1 needs backward computation.
I0818 10:49:16.507968  2412 net.cpp:301] Convolution1 needs backward computation.
I0818 10:49:16.507974  2412 net.cpp:303] Data1 does not need backward computation.
I0818 10:49:16.507978  2412 net.cpp:348] This network produces output SoftmaxWithLoss1
I0818 10:49:16.508090  2412 net.cpp:363] Network initialization done.
I0818 10:49:16.510377  2412 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_44.prototxt
I0818 10:49:16.510404  2412 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0818 10:49:16.510413  2412 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_44.prototxt
I0818 10:49:16.510577  2412 net.cpp:390] layer_param.include_size():1
I0818 10:49:16.510584  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510589  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510593  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510598  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510601  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510607  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510609  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510629  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510633  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510637  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510641  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510645  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510665  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510671  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510675  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510679  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510682  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510686  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510690  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510695  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510699  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510704  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510706  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510710  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510715  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510718  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510722  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510726  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510730  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510735  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510738  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510742  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510746  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510751  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510753  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510757  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510761  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510766  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510769  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510773  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510777  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510782  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510784  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510788  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510792  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510797  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510800  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510804  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510808  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510812  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510815  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510819  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510823  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510828  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510831  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510836  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510839  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510843  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510848  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510851  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510855  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510859  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510864  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510867  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510870  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510882  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510887  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510891  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510895  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510898  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510902  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510906  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510910  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510915  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510917  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510921  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510926  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510929  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510933  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510937  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510941  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510946  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510948  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510953  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510957  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510960  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510964  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510968  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510972  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510975  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510979  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510983  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510987  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510991  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.510994  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.510998  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511003  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511006  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511010  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511014  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511018  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511021  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511025  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511029  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511034  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511037  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511040  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511044  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511049  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511052  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511056  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511060  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511063  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511067  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511071  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511075  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511080  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511082  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511086  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511090  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511095  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511098  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511101  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511106  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511116  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511119  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511123  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511127  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511132  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511135  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511139  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511142  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511147  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511150  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511154  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511157  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511162  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511165  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511169  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511173  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511176  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511180  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511184  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511188  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511193  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511195  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511199  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511204  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511206  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511210  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511214  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511219  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511222  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511226  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511229  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511234  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511237  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511240  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511245  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511248  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511251  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511256  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511260  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511263  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511267  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511271  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511274  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511278  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511282  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511286  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511291  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511294  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511297  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511301  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511306  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511309  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511312  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511317  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511320  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511324  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511327  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511332  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511342  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511345  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511349  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511353  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511356  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511360  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511364  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511368  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511373  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511376  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511379  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511384  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511387  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511391  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511394  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511399  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511402  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511406  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511409  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511413  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511417  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511421  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511425  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511428  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511432  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511436  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511440  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511443  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511447  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511451  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511454  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511458  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511462  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511466  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511469  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511473  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511477  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511482  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511484  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511488  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511492  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511497  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511500  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511503  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511507  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511512  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511515  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511519  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511523  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511526  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511530  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511534  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511538  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511541  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511545  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511549  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511553  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511556  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511560  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511570  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511574  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511579  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511581  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511585  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511590  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511593  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511596  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511600  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511605  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511608  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511611  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511615  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511620  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511623  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511626  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511631  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511634  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511638  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511642  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511646  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511649  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511653  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511657  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511662  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511664  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511668  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511672  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511677  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511680  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511684  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511687  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511692  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511695  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511699  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511703  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511706  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511710  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511714  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511718  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511721  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511725  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511729  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511734  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511737  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511741  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511745  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511749  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511752  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511756  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511760  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511765  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511768  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511771  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511775  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511780  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511783  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511786  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511796  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511801  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511804  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511808  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511812  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511816  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511821  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511823  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511827  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511831  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511835  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511838  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511843  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511847  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511850  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511854  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511858  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511862  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511865  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511869  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511873  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511878  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511880  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511884  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511888  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511891  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511895  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511899  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511904  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511907  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511911  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511914  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511919  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511922  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511926  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511929  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511934  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511937  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511941  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511945  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511948  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511952  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511956  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511960  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511965  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511968  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511971  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511976  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511979  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511983  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511987  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511991  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.511994  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.511998  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512002  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512006  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512009  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512013  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512017  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512027  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512032  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512034  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512038  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512042  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512046  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512049  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512053  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512058  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512061  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512064  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512068  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512073  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512076  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512079  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512084  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512087  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512091  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512095  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512099  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512102  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512106  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512110  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512115  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512117  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512121  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512125  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512130  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512132  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512136  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512140  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512145  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512147  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512151  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512156  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512159  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512163  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512167  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512171  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512174  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512178  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512182  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512185  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512190  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512193  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512198  2412 net.cpp:390] layer_param.include_size():0
I0818 10:49:16.512202  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.512205  2412 net.cpp:390] layer_param.include_size():1
I0818 10:49:16.512209  2412 net.cpp:391] layer_param.exclude_size():0
I0818 10:49:16.513447  2412 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215684
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
I0818 10:49:16.514200  2412 layer_factory.hpp:77] Creating layer Data1
I0818 10:49:16.514284  2412 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0818 10:49:16.514314  2412 net.cpp:128] Creating Layer Data1
I0818 10:49:16.514322  2412 net.cpp:522] Data1 -> Data1
I0818 10:49:16.514331  2412 net.cpp:522] Data1 -> Data2
I0818 10:49:16.514494  2412 data_layer.cpp:45] output data size: 10,3,32,32
I0818 10:49:16.523568  2412 net.cpp:172] Setting up Data1
I0818 10:49:16.523594  2412 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0818 10:49:16.523600  2412 net.cpp:186] Top shape: 10 (10)
I0818 10:49:16.523604  2412 net.cpp:194] Memory required for data: 122920
I0818 10:49:16.523610  2412 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0818 10:49:16.523624  2412 net.cpp:128] Creating Layer Data2_Data1_1_split
I0818 10:49:16.523630  2412 net.cpp:558] Data2_Data1_1_split <- Data2
I0818 10:49:16.523638  2412 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0818 10:49:16.523651  2412 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0818 10:49:16.523809  2412 net.cpp:172] Setting up Data2_Data1_1_split
I0818 10:49:16.523818  2412 net.cpp:186] Top shape: 10 (10)
I0818 10:49:16.523823  2412 net.cpp:186] Top shape: 10 (10)
I0818 10:49:16.523825  2412 net.cpp:194] Memory required for data: 123000
I0818 10:49:16.523830  2412 layer_factory.hpp:77] Creating layer Convolution1
I0818 10:49:16.523849  2412 net.cpp:128] Creating Layer Convolution1
I0818 10:49:16.523854  2412 net.cpp:558] Convolution1 <- Data1
I0818 10:49:16.523860  2412 net.cpp:522] Convolution1 -> Convolution1
I0818 10:49:16.532552  2412 net.cpp:172] Setting up Convolution1
I0818 10:49:16.532582  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.532588  2412 net.cpp:194] Memory required for data: 778360
I0818 10:49:16.532608  2412 layer_factory.hpp:77] Creating layer BatchNorm1
I0818 10:49:16.532618  2412 net.cpp:128] Creating Layer BatchNorm1
I0818 10:49:16.532624  2412 net.cpp:558] BatchNorm1 <- Convolution1
I0818 10:49:16.532641  2412 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0818 10:49:16.533154  2412 net.cpp:172] Setting up BatchNorm1
I0818 10:49:16.533167  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.533174  2412 net.cpp:194] Memory required for data: 1433720
I0818 10:49:16.533195  2412 layer_factory.hpp:77] Creating layer Scale1
I0818 10:49:16.533211  2412 net.cpp:128] Creating Layer Scale1
I0818 10:49:16.533218  2412 net.cpp:558] Scale1 <- Convolution1
I0818 10:49:16.533227  2412 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0818 10:49:16.533323  2412 layer_factory.hpp:77] Creating layer Scale1
I0818 10:49:16.533661  2412 net.cpp:172] Setting up Scale1
I0818 10:49:16.533674  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.533680  2412 net.cpp:194] Memory required for data: 2089080
I0818 10:49:16.533694  2412 layer_factory.hpp:77] Creating layer ReLU1
I0818 10:49:16.533705  2412 net.cpp:128] Creating Layer ReLU1
I0818 10:49:16.533716  2412 net.cpp:558] ReLU1 <- Convolution1
I0818 10:49:16.533733  2412 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0818 10:49:16.534495  2412 net.cpp:172] Setting up ReLU1
I0818 10:49:16.534518  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.534524  2412 net.cpp:194] Memory required for data: 2744440
I0818 10:49:16.534555  2412 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0818 10:49:16.534567  2412 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0818 10:49:16.534574  2412 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0818 10:49:16.534590  2412 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0818 10:49:16.534602  2412 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0818 10:49:16.534718  2412 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0818 10:49:16.534735  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.534744  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.534750  2412 net.cpp:194] Memory required for data: 4055160
I0818 10:49:16.534756  2412 layer_factory.hpp:77] Creating layer Convolution2
I0818 10:49:16.534775  2412 net.cpp:128] Creating Layer Convolution2
I0818 10:49:16.534781  2412 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0818 10:49:16.534795  2412 net.cpp:522] Convolution2 -> Convolution2
I0818 10:49:16.541220  2412 net.cpp:172] Setting up Convolution2
I0818 10:49:16.541256  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.541268  2412 net.cpp:194] Memory required for data: 4710520
I0818 10:49:16.541285  2412 layer_factory.hpp:77] Creating layer BatchNorm2
I0818 10:49:16.541306  2412 net.cpp:128] Creating Layer BatchNorm2
I0818 10:49:16.541313  2412 net.cpp:558] BatchNorm2 <- Convolution2
I0818 10:49:16.541323  2412 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0818 10:49:16.541782  2412 net.cpp:172] Setting up BatchNorm2
I0818 10:49:16.541797  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.541807  2412 net.cpp:194] Memory required for data: 5365880
I0818 10:49:16.541822  2412 layer_factory.hpp:77] Creating layer Scale2
I0818 10:49:16.541836  2412 net.cpp:128] Creating Layer Scale2
I0818 10:49:16.541842  2412 net.cpp:558] Scale2 <- Convolution2
I0818 10:49:16.541858  2412 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0818 10:49:16.541942  2412 layer_factory.hpp:77] Creating layer Scale2
I0818 10:49:16.542196  2412 net.cpp:172] Setting up Scale2
I0818 10:49:16.542212  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.542223  2412 net.cpp:194] Memory required for data: 6021240
I0818 10:49:16.542235  2412 layer_factory.hpp:77] Creating layer ReLU2
I0818 10:49:16.542248  2412 net.cpp:128] Creating Layer ReLU2
I0818 10:49:16.542254  2412 net.cpp:558] ReLU2 <- Convolution2
I0818 10:49:16.542265  2412 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0818 10:49:16.543146  2412 net.cpp:172] Setting up ReLU2
I0818 10:49:16.543166  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.543170  2412 net.cpp:194] Memory required for data: 6676600
I0818 10:49:16.543175  2412 layer_factory.hpp:77] Creating layer Convolution3
I0818 10:49:16.543192  2412 net.cpp:128] Creating Layer Convolution3
I0818 10:49:16.543203  2412 net.cpp:558] Convolution3 <- Convolution2
I0818 10:49:16.543213  2412 net.cpp:522] Convolution3 -> Convolution3
I0818 10:49:16.549837  2412 net.cpp:172] Setting up Convolution3
I0818 10:49:16.549865  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.549870  2412 net.cpp:194] Memory required for data: 7331960
I0818 10:49:16.549880  2412 layer_factory.hpp:77] Creating layer BatchNorm3
I0818 10:49:16.549890  2412 net.cpp:128] Creating Layer BatchNorm3
I0818 10:49:16.549896  2412 net.cpp:558] BatchNorm3 <- Convolution3
I0818 10:49:16.549904  2412 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0818 10:49:16.550209  2412 net.cpp:172] Setting up BatchNorm3
I0818 10:49:16.550220  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.550225  2412 net.cpp:194] Memory required for data: 7987320
I0818 10:49:16.550236  2412 layer_factory.hpp:77] Creating layer Scale3
I0818 10:49:16.550243  2412 net.cpp:128] Creating Layer Scale3
I0818 10:49:16.550247  2412 net.cpp:558] Scale3 <- Convolution3
I0818 10:49:16.550253  2412 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0818 10:49:16.550308  2412 layer_factory.hpp:77] Creating layer Scale3
I0818 10:49:16.550501  2412 net.cpp:172] Setting up Scale3
I0818 10:49:16.550509  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.550514  2412 net.cpp:194] Memory required for data: 8642680
I0818 10:49:16.550523  2412 layer_factory.hpp:77] Creating layer Eltwise1
I0818 10:49:16.550534  2412 net.cpp:128] Creating Layer Eltwise1
I0818 10:49:16.550539  2412 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0818 10:49:16.550544  2412 net.cpp:558] Eltwise1 <- Convolution3
I0818 10:49:16.550549  2412 net.cpp:522] Eltwise1 -> Eltwise1
I0818 10:49:16.550582  2412 net.cpp:172] Setting up Eltwise1
I0818 10:49:16.550590  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.550593  2412 net.cpp:194] Memory required for data: 9298040
I0818 10:49:16.550598  2412 layer_factory.hpp:77] Creating layer ReLU3
I0818 10:49:16.550604  2412 net.cpp:128] Creating Layer ReLU3
I0818 10:49:16.550608  2412 net.cpp:558] ReLU3 <- Eltwise1
I0818 10:49:16.550614  2412 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0818 10:49:16.551883  2412 net.cpp:172] Setting up ReLU3
I0818 10:49:16.551903  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.551908  2412 net.cpp:194] Memory required for data: 9953400
I0818 10:49:16.551911  2412 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0818 10:49:16.551923  2412 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0818 10:49:16.551928  2412 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0818 10:49:16.551935  2412 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0818 10:49:16.551944  2412 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0818 10:49:16.552002  2412 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0818 10:49:16.552011  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.552016  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.552019  2412 net.cpp:194] Memory required for data: 11264120
I0818 10:49:16.552023  2412 layer_factory.hpp:77] Creating layer Convolution4
I0818 10:49:16.552039  2412 net.cpp:128] Creating Layer Convolution4
I0818 10:49:16.552044  2412 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0818 10:49:16.552052  2412 net.cpp:522] Convolution4 -> Convolution4
I0818 10:49:16.558600  2412 net.cpp:172] Setting up Convolution4
I0818 10:49:16.558626  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.558631  2412 net.cpp:194] Memory required for data: 11919480
I0818 10:49:16.558645  2412 layer_factory.hpp:77] Creating layer BatchNorm4
I0818 10:49:16.558663  2412 net.cpp:128] Creating Layer BatchNorm4
I0818 10:49:16.558670  2412 net.cpp:558] BatchNorm4 <- Convolution4
I0818 10:49:16.558677  2412 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0818 10:49:16.558984  2412 net.cpp:172] Setting up BatchNorm4
I0818 10:49:16.558995  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.558998  2412 net.cpp:194] Memory required for data: 12574840
I0818 10:49:16.559008  2412 layer_factory.hpp:77] Creating layer Scale4
I0818 10:49:16.559015  2412 net.cpp:128] Creating Layer Scale4
I0818 10:49:16.559020  2412 net.cpp:558] Scale4 <- Convolution4
I0818 10:49:16.559028  2412 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0818 10:49:16.559082  2412 layer_factory.hpp:77] Creating layer Scale4
I0818 10:49:16.559254  2412 net.cpp:172] Setting up Scale4
I0818 10:49:16.559265  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.559269  2412 net.cpp:194] Memory required for data: 13230200
I0818 10:49:16.559278  2412 layer_factory.hpp:77] Creating layer ReLU4
I0818 10:49:16.559283  2412 net.cpp:128] Creating Layer ReLU4
I0818 10:49:16.559288  2412 net.cpp:558] ReLU4 <- Convolution4
I0818 10:49:16.559299  2412 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0818 10:49:16.560626  2412 net.cpp:172] Setting up ReLU4
I0818 10:49:16.560644  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.560650  2412 net.cpp:194] Memory required for data: 13885560
I0818 10:49:16.560654  2412 layer_factory.hpp:77] Creating layer Convolution5
I0818 10:49:16.560689  2412 net.cpp:128] Creating Layer Convolution5
I0818 10:49:16.560695  2412 net.cpp:558] Convolution5 <- Convolution4
I0818 10:49:16.560704  2412 net.cpp:522] Convolution5 -> Convolution5
I0818 10:49:16.567337  2412 net.cpp:172] Setting up Convolution5
I0818 10:49:16.567365  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.567370  2412 net.cpp:194] Memory required for data: 14540920
I0818 10:49:16.567384  2412 layer_factory.hpp:77] Creating layer BatchNorm5
I0818 10:49:16.567392  2412 net.cpp:128] Creating Layer BatchNorm5
I0818 10:49:16.567397  2412 net.cpp:558] BatchNorm5 <- Convolution5
I0818 10:49:16.567406  2412 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0818 10:49:16.567708  2412 net.cpp:172] Setting up BatchNorm5
I0818 10:49:16.567720  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.567724  2412 net.cpp:194] Memory required for data: 15196280
I0818 10:49:16.567739  2412 layer_factory.hpp:77] Creating layer Scale5
I0818 10:49:16.567750  2412 net.cpp:128] Creating Layer Scale5
I0818 10:49:16.567754  2412 net.cpp:558] Scale5 <- Convolution5
I0818 10:49:16.567760  2412 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0818 10:49:16.567813  2412 layer_factory.hpp:77] Creating layer Scale5
I0818 10:49:16.567979  2412 net.cpp:172] Setting up Scale5
I0818 10:49:16.567989  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.567993  2412 net.cpp:194] Memory required for data: 15851640
I0818 10:49:16.568001  2412 layer_factory.hpp:77] Creating layer Eltwise2
I0818 10:49:16.568011  2412 net.cpp:128] Creating Layer Eltwise2
I0818 10:49:16.568017  2412 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0818 10:49:16.568022  2412 net.cpp:558] Eltwise2 <- Convolution5
I0818 10:49:16.568030  2412 net.cpp:522] Eltwise2 -> Eltwise2
I0818 10:49:16.568059  2412 net.cpp:172] Setting up Eltwise2
I0818 10:49:16.568068  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.568073  2412 net.cpp:194] Memory required for data: 16507000
I0818 10:49:16.568076  2412 layer_factory.hpp:77] Creating layer ReLU5
I0818 10:49:16.568085  2412 net.cpp:128] Creating Layer ReLU5
I0818 10:49:16.568090  2412 net.cpp:558] ReLU5 <- Eltwise2
I0818 10:49:16.568095  2412 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0818 10:49:16.569396  2412 net.cpp:172] Setting up ReLU5
I0818 10:49:16.569422  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.569427  2412 net.cpp:194] Memory required for data: 17162360
I0818 10:49:16.569432  2412 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0818 10:49:16.569447  2412 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0818 10:49:16.569453  2412 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0818 10:49:16.569459  2412 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0818 10:49:16.569470  2412 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0818 10:49:16.569537  2412 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0818 10:49:16.569545  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.569551  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.569555  2412 net.cpp:194] Memory required for data: 18473080
I0818 10:49:16.569559  2412 layer_factory.hpp:77] Creating layer Convolution6
I0818 10:49:16.569573  2412 net.cpp:128] Creating Layer Convolution6
I0818 10:49:16.569578  2412 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0818 10:49:16.569587  2412 net.cpp:522] Convolution6 -> Convolution6
I0818 10:49:16.576159  2412 net.cpp:172] Setting up Convolution6
I0818 10:49:16.576184  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.576189  2412 net.cpp:194] Memory required for data: 19128440
I0818 10:49:16.576198  2412 layer_factory.hpp:77] Creating layer BatchNorm6
I0818 10:49:16.576210  2412 net.cpp:128] Creating Layer BatchNorm6
I0818 10:49:16.576215  2412 net.cpp:558] BatchNorm6 <- Convolution6
I0818 10:49:16.576223  2412 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0818 10:49:16.576525  2412 net.cpp:172] Setting up BatchNorm6
I0818 10:49:16.576552  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.576557  2412 net.cpp:194] Memory required for data: 19783800
I0818 10:49:16.576567  2412 layer_factory.hpp:77] Creating layer Scale6
I0818 10:49:16.576576  2412 net.cpp:128] Creating Layer Scale6
I0818 10:49:16.576581  2412 net.cpp:558] Scale6 <- Convolution6
I0818 10:49:16.576587  2412 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0818 10:49:16.576642  2412 layer_factory.hpp:77] Creating layer Scale6
I0818 10:49:16.576817  2412 net.cpp:172] Setting up Scale6
I0818 10:49:16.576830  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.576834  2412 net.cpp:194] Memory required for data: 20439160
I0818 10:49:16.576843  2412 layer_factory.hpp:77] Creating layer ReLU6
I0818 10:49:16.576850  2412 net.cpp:128] Creating Layer ReLU6
I0818 10:49:16.576855  2412 net.cpp:558] ReLU6 <- Convolution6
I0818 10:49:16.576860  2412 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0818 10:49:16.578176  2412 net.cpp:172] Setting up ReLU6
I0818 10:49:16.578193  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.578197  2412 net.cpp:194] Memory required for data: 21094520
I0818 10:49:16.578202  2412 layer_factory.hpp:77] Creating layer Convolution7
I0818 10:49:16.578217  2412 net.cpp:128] Creating Layer Convolution7
I0818 10:49:16.578222  2412 net.cpp:558] Convolution7 <- Convolution6
I0818 10:49:16.578229  2412 net.cpp:522] Convolution7 -> Convolution7
I0818 10:49:16.584877  2412 net.cpp:172] Setting up Convolution7
I0818 10:49:16.584903  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.584906  2412 net.cpp:194] Memory required for data: 21749880
I0818 10:49:16.584916  2412 layer_factory.hpp:77] Creating layer BatchNorm7
I0818 10:49:16.584931  2412 net.cpp:128] Creating Layer BatchNorm7
I0818 10:49:16.584938  2412 net.cpp:558] BatchNorm7 <- Convolution7
I0818 10:49:16.584944  2412 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0818 10:49:16.585253  2412 net.cpp:172] Setting up BatchNorm7
I0818 10:49:16.585264  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.585268  2412 net.cpp:194] Memory required for data: 22405240
I0818 10:49:16.585278  2412 layer_factory.hpp:77] Creating layer Scale7
I0818 10:49:16.585285  2412 net.cpp:128] Creating Layer Scale7
I0818 10:49:16.585289  2412 net.cpp:558] Scale7 <- Convolution7
I0818 10:49:16.585295  2412 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0818 10:49:16.585350  2412 layer_factory.hpp:77] Creating layer Scale7
I0818 10:49:16.585521  2412 net.cpp:172] Setting up Scale7
I0818 10:49:16.585533  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.585537  2412 net.cpp:194] Memory required for data: 23060600
I0818 10:49:16.585546  2412 layer_factory.hpp:77] Creating layer Eltwise3
I0818 10:49:16.585554  2412 net.cpp:128] Creating Layer Eltwise3
I0818 10:49:16.585559  2412 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0818 10:49:16.585564  2412 net.cpp:558] Eltwise3 <- Convolution7
I0818 10:49:16.585572  2412 net.cpp:522] Eltwise3 -> Eltwise3
I0818 10:49:16.585602  2412 net.cpp:172] Setting up Eltwise3
I0818 10:49:16.585608  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.585613  2412 net.cpp:194] Memory required for data: 23715960
I0818 10:49:16.585616  2412 layer_factory.hpp:77] Creating layer ReLU7
I0818 10:49:16.585626  2412 net.cpp:128] Creating Layer ReLU7
I0818 10:49:16.585630  2412 net.cpp:558] ReLU7 <- Eltwise3
I0818 10:49:16.585636  2412 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0818 10:49:16.586935  2412 net.cpp:172] Setting up ReLU7
I0818 10:49:16.586952  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.586958  2412 net.cpp:194] Memory required for data: 24371320
I0818 10:49:16.586963  2412 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0818 10:49:16.586982  2412 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0818 10:49:16.586987  2412 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0818 10:49:16.586994  2412 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0818 10:49:16.587023  2412 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0818 10:49:16.587088  2412 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0818 10:49:16.587100  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.587106  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.587110  2412 net.cpp:194] Memory required for data: 25682040
I0818 10:49:16.587115  2412 layer_factory.hpp:77] Creating layer Convolution8
I0818 10:49:16.587127  2412 net.cpp:128] Creating Layer Convolution8
I0818 10:49:16.587131  2412 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0818 10:49:16.587141  2412 net.cpp:522] Convolution8 -> Convolution8
I0818 10:49:16.593711  2412 net.cpp:172] Setting up Convolution8
I0818 10:49:16.593737  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.593742  2412 net.cpp:194] Memory required for data: 26337400
I0818 10:49:16.593752  2412 layer_factory.hpp:77] Creating layer BatchNorm8
I0818 10:49:16.593765  2412 net.cpp:128] Creating Layer BatchNorm8
I0818 10:49:16.593770  2412 net.cpp:558] BatchNorm8 <- Convolution8
I0818 10:49:16.593776  2412 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0818 10:49:16.594090  2412 net.cpp:172] Setting up BatchNorm8
I0818 10:49:16.594102  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.594107  2412 net.cpp:194] Memory required for data: 26992760
I0818 10:49:16.594116  2412 layer_factory.hpp:77] Creating layer Scale8
I0818 10:49:16.594125  2412 net.cpp:128] Creating Layer Scale8
I0818 10:49:16.594128  2412 net.cpp:558] Scale8 <- Convolution8
I0818 10:49:16.594133  2412 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0818 10:49:16.594192  2412 layer_factory.hpp:77] Creating layer Scale8
I0818 10:49:16.594362  2412 net.cpp:172] Setting up Scale8
I0818 10:49:16.594374  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.594378  2412 net.cpp:194] Memory required for data: 27648120
I0818 10:49:16.594385  2412 layer_factory.hpp:77] Creating layer ReLU8
I0818 10:49:16.594393  2412 net.cpp:128] Creating Layer ReLU8
I0818 10:49:16.594396  2412 net.cpp:558] ReLU8 <- Convolution8
I0818 10:49:16.594404  2412 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0818 10:49:16.595768  2412 net.cpp:172] Setting up ReLU8
I0818 10:49:16.595794  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.595799  2412 net.cpp:194] Memory required for data: 28303480
I0818 10:49:16.595804  2412 layer_factory.hpp:77] Creating layer Convolution9
I0818 10:49:16.595823  2412 net.cpp:128] Creating Layer Convolution9
I0818 10:49:16.595834  2412 net.cpp:558] Convolution9 <- Convolution8
I0818 10:49:16.595844  2412 net.cpp:522] Convolution9 -> Convolution9
I0818 10:49:16.602488  2412 net.cpp:172] Setting up Convolution9
I0818 10:49:16.602516  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.602521  2412 net.cpp:194] Memory required for data: 28958840
I0818 10:49:16.602535  2412 layer_factory.hpp:77] Creating layer BatchNorm9
I0818 10:49:16.602543  2412 net.cpp:128] Creating Layer BatchNorm9
I0818 10:49:16.602548  2412 net.cpp:558] BatchNorm9 <- Convolution9
I0818 10:49:16.602557  2412 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0818 10:49:16.602893  2412 net.cpp:172] Setting up BatchNorm9
I0818 10:49:16.602905  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.602910  2412 net.cpp:194] Memory required for data: 29614200
I0818 10:49:16.602919  2412 layer_factory.hpp:77] Creating layer Scale9
I0818 10:49:16.602926  2412 net.cpp:128] Creating Layer Scale9
I0818 10:49:16.602931  2412 net.cpp:558] Scale9 <- Convolution9
I0818 10:49:16.602938  2412 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0818 10:49:16.602991  2412 layer_factory.hpp:77] Creating layer Scale9
I0818 10:49:16.603165  2412 net.cpp:172] Setting up Scale9
I0818 10:49:16.603178  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.603183  2412 net.cpp:194] Memory required for data: 30269560
I0818 10:49:16.603191  2412 layer_factory.hpp:77] Creating layer Eltwise4
I0818 10:49:16.603219  2412 net.cpp:128] Creating Layer Eltwise4
I0818 10:49:16.603224  2412 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0818 10:49:16.603229  2412 net.cpp:558] Eltwise4 <- Convolution9
I0818 10:49:16.603235  2412 net.cpp:522] Eltwise4 -> Eltwise4
I0818 10:49:16.603271  2412 net.cpp:172] Setting up Eltwise4
I0818 10:49:16.603281  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.603284  2412 net.cpp:194] Memory required for data: 30924920
I0818 10:49:16.603288  2412 layer_factory.hpp:77] Creating layer ReLU9
I0818 10:49:16.603297  2412 net.cpp:128] Creating Layer ReLU9
I0818 10:49:16.603302  2412 net.cpp:558] ReLU9 <- Eltwise4
I0818 10:49:16.603307  2412 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0818 10:49:16.604538  2412 net.cpp:172] Setting up ReLU9
I0818 10:49:16.604555  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.604560  2412 net.cpp:194] Memory required for data: 31580280
I0818 10:49:16.604564  2412 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0818 10:49:16.604573  2412 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0818 10:49:16.604578  2412 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0818 10:49:16.604585  2412 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0818 10:49:16.604593  2412 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0818 10:49:16.604657  2412 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0818 10:49:16.604665  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.604671  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.604676  2412 net.cpp:194] Memory required for data: 32891000
I0818 10:49:16.604679  2412 layer_factory.hpp:77] Creating layer Convolution10
I0818 10:49:16.604696  2412 net.cpp:128] Creating Layer Convolution10
I0818 10:49:16.604701  2412 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0818 10:49:16.604709  2412 net.cpp:522] Convolution10 -> Convolution10
I0818 10:49:16.611311  2412 net.cpp:172] Setting up Convolution10
I0818 10:49:16.611337  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.611342  2412 net.cpp:194] Memory required for data: 33546360
I0818 10:49:16.611363  2412 layer_factory.hpp:77] Creating layer BatchNorm10
I0818 10:49:16.611372  2412 net.cpp:128] Creating Layer BatchNorm10
I0818 10:49:16.611377  2412 net.cpp:558] BatchNorm10 <- Convolution10
I0818 10:49:16.611389  2412 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0818 10:49:16.611696  2412 net.cpp:172] Setting up BatchNorm10
I0818 10:49:16.611708  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.611712  2412 net.cpp:194] Memory required for data: 34201720
I0818 10:49:16.611722  2412 layer_factory.hpp:77] Creating layer Scale10
I0818 10:49:16.611732  2412 net.cpp:128] Creating Layer Scale10
I0818 10:49:16.611737  2412 net.cpp:558] Scale10 <- Convolution10
I0818 10:49:16.611742  2412 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0818 10:49:16.611796  2412 layer_factory.hpp:77] Creating layer Scale10
I0818 10:49:16.611970  2412 net.cpp:172] Setting up Scale10
I0818 10:49:16.611981  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.611985  2412 net.cpp:194] Memory required for data: 34857080
I0818 10:49:16.611994  2412 layer_factory.hpp:77] Creating layer ReLU10
I0818 10:49:16.611999  2412 net.cpp:128] Creating Layer ReLU10
I0818 10:49:16.612004  2412 net.cpp:558] ReLU10 <- Convolution10
I0818 10:49:16.612012  2412 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0818 10:49:16.613344  2412 net.cpp:172] Setting up ReLU10
I0818 10:49:16.613361  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.613365  2412 net.cpp:194] Memory required for data: 35512440
I0818 10:49:16.613370  2412 layer_factory.hpp:77] Creating layer Convolution11
I0818 10:49:16.613384  2412 net.cpp:128] Creating Layer Convolution11
I0818 10:49:16.613389  2412 net.cpp:558] Convolution11 <- Convolution10
I0818 10:49:16.613397  2412 net.cpp:522] Convolution11 -> Convolution11
I0818 10:49:16.620086  2412 net.cpp:172] Setting up Convolution11
I0818 10:49:16.620127  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.620131  2412 net.cpp:194] Memory required for data: 36167800
I0818 10:49:16.620142  2412 layer_factory.hpp:77] Creating layer BatchNorm11
I0818 10:49:16.620153  2412 net.cpp:128] Creating Layer BatchNorm11
I0818 10:49:16.620158  2412 net.cpp:558] BatchNorm11 <- Convolution11
I0818 10:49:16.620165  2412 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0818 10:49:16.620483  2412 net.cpp:172] Setting up BatchNorm11
I0818 10:49:16.620496  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.620501  2412 net.cpp:194] Memory required for data: 36823160
I0818 10:49:16.620509  2412 layer_factory.hpp:77] Creating layer Scale11
I0818 10:49:16.620517  2412 net.cpp:128] Creating Layer Scale11
I0818 10:49:16.620522  2412 net.cpp:558] Scale11 <- Convolution11
I0818 10:49:16.620529  2412 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0818 10:49:16.620584  2412 layer_factory.hpp:77] Creating layer Scale11
I0818 10:49:16.620756  2412 net.cpp:172] Setting up Scale11
I0818 10:49:16.620767  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.620771  2412 net.cpp:194] Memory required for data: 37478520
I0818 10:49:16.620779  2412 layer_factory.hpp:77] Creating layer Eltwise5
I0818 10:49:16.620786  2412 net.cpp:128] Creating Layer Eltwise5
I0818 10:49:16.620790  2412 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0818 10:49:16.620795  2412 net.cpp:558] Eltwise5 <- Convolution11
I0818 10:49:16.620803  2412 net.cpp:522] Eltwise5 -> Eltwise5
I0818 10:49:16.620833  2412 net.cpp:172] Setting up Eltwise5
I0818 10:49:16.620843  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.620847  2412 net.cpp:194] Memory required for data: 38133880
I0818 10:49:16.620851  2412 layer_factory.hpp:77] Creating layer ReLU11
I0818 10:49:16.620857  2412 net.cpp:128] Creating Layer ReLU11
I0818 10:49:16.620862  2412 net.cpp:558] ReLU11 <- Eltwise5
I0818 10:49:16.620867  2412 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0818 10:49:16.622135  2412 net.cpp:172] Setting up ReLU11
I0818 10:49:16.622161  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.622165  2412 net.cpp:194] Memory required for data: 38789240
I0818 10:49:16.622170  2412 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0818 10:49:16.622184  2412 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0818 10:49:16.622189  2412 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0818 10:49:16.622200  2412 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0818 10:49:16.622210  2412 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0818 10:49:16.622272  2412 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0818 10:49:16.622282  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.622287  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.622292  2412 net.cpp:194] Memory required for data: 40099960
I0818 10:49:16.622297  2412 layer_factory.hpp:77] Creating layer Convolution12
I0818 10:49:16.622308  2412 net.cpp:128] Creating Layer Convolution12
I0818 10:49:16.622313  2412 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0818 10:49:16.622320  2412 net.cpp:522] Convolution12 -> Convolution12
I0818 10:49:16.628857  2412 net.cpp:172] Setting up Convolution12
I0818 10:49:16.628885  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.628888  2412 net.cpp:194] Memory required for data: 40755320
I0818 10:49:16.628899  2412 layer_factory.hpp:77] Creating layer BatchNorm12
I0818 10:49:16.628909  2412 net.cpp:128] Creating Layer BatchNorm12
I0818 10:49:16.628916  2412 net.cpp:558] BatchNorm12 <- Convolution12
I0818 10:49:16.628921  2412 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0818 10:49:16.629238  2412 net.cpp:172] Setting up BatchNorm12
I0818 10:49:16.629251  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.629256  2412 net.cpp:194] Memory required for data: 41410680
I0818 10:49:16.629264  2412 layer_factory.hpp:77] Creating layer Scale12
I0818 10:49:16.629287  2412 net.cpp:128] Creating Layer Scale12
I0818 10:49:16.629292  2412 net.cpp:558] Scale12 <- Convolution12
I0818 10:49:16.629300  2412 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0818 10:49:16.629359  2412 layer_factory.hpp:77] Creating layer Scale12
I0818 10:49:16.629530  2412 net.cpp:172] Setting up Scale12
I0818 10:49:16.629542  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.629546  2412 net.cpp:194] Memory required for data: 42066040
I0818 10:49:16.629554  2412 layer_factory.hpp:77] Creating layer ReLU12
I0818 10:49:16.629561  2412 net.cpp:128] Creating Layer ReLU12
I0818 10:49:16.629565  2412 net.cpp:558] ReLU12 <- Convolution12
I0818 10:49:16.629573  2412 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0818 10:49:16.630873  2412 net.cpp:172] Setting up ReLU12
I0818 10:49:16.630892  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.630897  2412 net.cpp:194] Memory required for data: 42721400
I0818 10:49:16.630900  2412 layer_factory.hpp:77] Creating layer Convolution13
I0818 10:49:16.630913  2412 net.cpp:128] Creating Layer Convolution13
I0818 10:49:16.630918  2412 net.cpp:558] Convolution13 <- Convolution12
I0818 10:49:16.630928  2412 net.cpp:522] Convolution13 -> Convolution13
I0818 10:49:16.637609  2412 net.cpp:172] Setting up Convolution13
I0818 10:49:16.637637  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.637641  2412 net.cpp:194] Memory required for data: 43376760
I0818 10:49:16.637651  2412 layer_factory.hpp:77] Creating layer BatchNorm13
I0818 10:49:16.637660  2412 net.cpp:128] Creating Layer BatchNorm13
I0818 10:49:16.637665  2412 net.cpp:558] BatchNorm13 <- Convolution13
I0818 10:49:16.637679  2412 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0818 10:49:16.637992  2412 net.cpp:172] Setting up BatchNorm13
I0818 10:49:16.638005  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.638010  2412 net.cpp:194] Memory required for data: 44032120
I0818 10:49:16.638020  2412 layer_factory.hpp:77] Creating layer Scale13
I0818 10:49:16.638026  2412 net.cpp:128] Creating Layer Scale13
I0818 10:49:16.638031  2412 net.cpp:558] Scale13 <- Convolution13
I0818 10:49:16.638041  2412 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0818 10:49:16.638093  2412 layer_factory.hpp:77] Creating layer Scale13
I0818 10:49:16.638265  2412 net.cpp:172] Setting up Scale13
I0818 10:49:16.638279  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.638284  2412 net.cpp:194] Memory required for data: 44687480
I0818 10:49:16.638293  2412 layer_factory.hpp:77] Creating layer Eltwise6
I0818 10:49:16.638305  2412 net.cpp:128] Creating Layer Eltwise6
I0818 10:49:16.638311  2412 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0818 10:49:16.638316  2412 net.cpp:558] Eltwise6 <- Convolution13
I0818 10:49:16.638322  2412 net.cpp:522] Eltwise6 -> Eltwise6
I0818 10:49:16.638352  2412 net.cpp:172] Setting up Eltwise6
I0818 10:49:16.638360  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.638365  2412 net.cpp:194] Memory required for data: 45342840
I0818 10:49:16.638368  2412 layer_factory.hpp:77] Creating layer ReLU13
I0818 10:49:16.638375  2412 net.cpp:128] Creating Layer ReLU13
I0818 10:49:16.638381  2412 net.cpp:558] ReLU13 <- Eltwise6
I0818 10:49:16.638386  2412 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0818 10:49:16.639669  2412 net.cpp:172] Setting up ReLU13
I0818 10:49:16.639688  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.639693  2412 net.cpp:194] Memory required for data: 45998200
I0818 10:49:16.639698  2412 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0818 10:49:16.639704  2412 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0818 10:49:16.639709  2412 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0818 10:49:16.639719  2412 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0818 10:49:16.639726  2412 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0818 10:49:16.639787  2412 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0818 10:49:16.639811  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.639816  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.639820  2412 net.cpp:194] Memory required for data: 47308920
I0818 10:49:16.639824  2412 layer_factory.hpp:77] Creating layer Convolution14
I0818 10:49:16.639837  2412 net.cpp:128] Creating Layer Convolution14
I0818 10:49:16.639842  2412 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0818 10:49:16.639852  2412 net.cpp:522] Convolution14 -> Convolution14
I0818 10:49:16.645366  2412 net.cpp:172] Setting up Convolution14
I0818 10:49:16.645392  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.645397  2412 net.cpp:194] Memory required for data: 47964280
I0818 10:49:16.645409  2412 layer_factory.hpp:77] Creating layer BatchNorm14
I0818 10:49:16.645419  2412 net.cpp:128] Creating Layer BatchNorm14
I0818 10:49:16.645427  2412 net.cpp:558] BatchNorm14 <- Convolution14
I0818 10:49:16.645437  2412 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0818 10:49:16.645752  2412 net.cpp:172] Setting up BatchNorm14
I0818 10:49:16.645763  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.645767  2412 net.cpp:194] Memory required for data: 48619640
I0818 10:49:16.645777  2412 layer_factory.hpp:77] Creating layer Scale14
I0818 10:49:16.645789  2412 net.cpp:128] Creating Layer Scale14
I0818 10:49:16.645794  2412 net.cpp:558] Scale14 <- Convolution14
I0818 10:49:16.645799  2412 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0818 10:49:16.645855  2412 layer_factory.hpp:77] Creating layer Scale14
I0818 10:49:16.646026  2412 net.cpp:172] Setting up Scale14
I0818 10:49:16.646036  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.646040  2412 net.cpp:194] Memory required for data: 49275000
I0818 10:49:16.646049  2412 layer_factory.hpp:77] Creating layer ReLU14
I0818 10:49:16.646066  2412 net.cpp:128] Creating Layer ReLU14
I0818 10:49:16.646071  2412 net.cpp:558] ReLU14 <- Convolution14
I0818 10:49:16.646076  2412 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0818 10:49:16.646330  2412 net.cpp:172] Setting up ReLU14
I0818 10:49:16.646343  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.646347  2412 net.cpp:194] Memory required for data: 49930360
I0818 10:49:16.646353  2412 layer_factory.hpp:77] Creating layer Convolution15
I0818 10:49:16.646365  2412 net.cpp:128] Creating Layer Convolution15
I0818 10:49:16.646373  2412 net.cpp:558] Convolution15 <- Convolution14
I0818 10:49:16.646384  2412 net.cpp:522] Convolution15 -> Convolution15
I0818 10:49:16.648167  2412 net.cpp:172] Setting up Convolution15
I0818 10:49:16.648192  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.648196  2412 net.cpp:194] Memory required for data: 50585720
I0818 10:49:16.648210  2412 layer_factory.hpp:77] Creating layer BatchNorm15
I0818 10:49:16.648219  2412 net.cpp:128] Creating Layer BatchNorm15
I0818 10:49:16.648226  2412 net.cpp:558] BatchNorm15 <- Convolution15
I0818 10:49:16.648233  2412 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0818 10:49:16.648548  2412 net.cpp:172] Setting up BatchNorm15
I0818 10:49:16.648561  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.648564  2412 net.cpp:194] Memory required for data: 51241080
I0818 10:49:16.648574  2412 layer_factory.hpp:77] Creating layer Scale15
I0818 10:49:16.648581  2412 net.cpp:128] Creating Layer Scale15
I0818 10:49:16.648586  2412 net.cpp:558] Scale15 <- Convolution15
I0818 10:49:16.648591  2412 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0818 10:49:16.648649  2412 layer_factory.hpp:77] Creating layer Scale15
I0818 10:49:16.648828  2412 net.cpp:172] Setting up Scale15
I0818 10:49:16.648839  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.648844  2412 net.cpp:194] Memory required for data: 51896440
I0818 10:49:16.648852  2412 layer_factory.hpp:77] Creating layer Eltwise7
I0818 10:49:16.648859  2412 net.cpp:128] Creating Layer Eltwise7
I0818 10:49:16.648864  2412 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0818 10:49:16.648886  2412 net.cpp:558] Eltwise7 <- Convolution15
I0818 10:49:16.648895  2412 net.cpp:522] Eltwise7 -> Eltwise7
I0818 10:49:16.648928  2412 net.cpp:172] Setting up Eltwise7
I0818 10:49:16.648936  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.648939  2412 net.cpp:194] Memory required for data: 52551800
I0818 10:49:16.648944  2412 layer_factory.hpp:77] Creating layer ReLU15
I0818 10:49:16.648954  2412 net.cpp:128] Creating Layer ReLU15
I0818 10:49:16.648958  2412 net.cpp:558] ReLU15 <- Eltwise7
I0818 10:49:16.648964  2412 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0818 10:49:16.649461  2412 net.cpp:172] Setting up ReLU15
I0818 10:49:16.649487  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.649492  2412 net.cpp:194] Memory required for data: 53207160
I0818 10:49:16.649497  2412 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0818 10:49:16.649505  2412 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0818 10:49:16.649510  2412 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0818 10:49:16.649518  2412 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0818 10:49:16.649529  2412 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0818 10:49:16.649592  2412 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0818 10:49:16.649600  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.649605  2412 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:49:16.649610  2412 net.cpp:194] Memory required for data: 54517880
I0818 10:49:16.649613  2412 layer_factory.hpp:77] Creating layer Convolution16
I0818 10:49:16.649628  2412 net.cpp:128] Creating Layer Convolution16
I0818 10:49:16.649632  2412 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0818 10:49:16.649641  2412 net.cpp:522] Convolution16 -> Convolution16
I0818 10:49:16.655205  2412 net.cpp:172] Setting up Convolution16
I0818 10:49:16.655232  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.655236  2412 net.cpp:194] Memory required for data: 54845560
I0818 10:49:16.655249  2412 layer_factory.hpp:77] Creating layer BatchNorm16
I0818 10:49:16.655261  2412 net.cpp:128] Creating Layer BatchNorm16
I0818 10:49:16.655267  2412 net.cpp:558] BatchNorm16 <- Convolution16
I0818 10:49:16.655280  2412 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0818 10:49:16.655588  2412 net.cpp:172] Setting up BatchNorm16
I0818 10:49:16.655601  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.655606  2412 net.cpp:194] Memory required for data: 55173240
I0818 10:49:16.655617  2412 layer_factory.hpp:77] Creating layer Scale16
I0818 10:49:16.655627  2412 net.cpp:128] Creating Layer Scale16
I0818 10:49:16.655632  2412 net.cpp:558] Scale16 <- Convolution16
I0818 10:49:16.655637  2412 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0818 10:49:16.655696  2412 layer_factory.hpp:77] Creating layer Scale16
I0818 10:49:16.655872  2412 net.cpp:172] Setting up Scale16
I0818 10:49:16.655880  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.655884  2412 net.cpp:194] Memory required for data: 55500920
I0818 10:49:16.655892  2412 layer_factory.hpp:77] Creating layer Convolution17
I0818 10:49:16.655911  2412 net.cpp:128] Creating Layer Convolution17
I0818 10:49:16.655916  2412 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_1
I0818 10:49:16.655923  2412 net.cpp:522] Convolution17 -> Convolution17
I0818 10:49:16.661798  2412 net.cpp:172] Setting up Convolution17
I0818 10:49:16.661824  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.661828  2412 net.cpp:194] Memory required for data: 55828600
I0818 10:49:16.661839  2412 layer_factory.hpp:77] Creating layer BatchNorm17
I0818 10:49:16.661850  2412 net.cpp:128] Creating Layer BatchNorm17
I0818 10:49:16.661856  2412 net.cpp:558] BatchNorm17 <- Convolution17
I0818 10:49:16.661870  2412 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0818 10:49:16.662173  2412 net.cpp:172] Setting up BatchNorm17
I0818 10:49:16.662184  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.662190  2412 net.cpp:194] Memory required for data: 56156280
I0818 10:49:16.662220  2412 layer_factory.hpp:77] Creating layer Scale17
I0818 10:49:16.662230  2412 net.cpp:128] Creating Layer Scale17
I0818 10:49:16.662235  2412 net.cpp:558] Scale17 <- Convolution17
I0818 10:49:16.662240  2412 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0818 10:49:16.662297  2412 layer_factory.hpp:77] Creating layer Scale17
I0818 10:49:16.662472  2412 net.cpp:172] Setting up Scale17
I0818 10:49:16.662479  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.662483  2412 net.cpp:194] Memory required for data: 56483960
I0818 10:49:16.662492  2412 layer_factory.hpp:77] Creating layer ReLU16
I0818 10:49:16.662500  2412 net.cpp:128] Creating Layer ReLU16
I0818 10:49:16.662504  2412 net.cpp:558] ReLU16 <- Convolution17
I0818 10:49:16.662510  2412 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0818 10:49:16.663823  2412 net.cpp:172] Setting up ReLU16
I0818 10:49:16.663843  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.663847  2412 net.cpp:194] Memory required for data: 56811640
I0818 10:49:16.663852  2412 layer_factory.hpp:77] Creating layer Convolution18
I0818 10:49:16.663869  2412 net.cpp:128] Creating Layer Convolution18
I0818 10:49:16.663875  2412 net.cpp:558] Convolution18 <- Convolution17
I0818 10:49:16.663883  2412 net.cpp:522] Convolution18 -> Convolution18
I0818 10:49:16.670610  2412 net.cpp:172] Setting up Convolution18
I0818 10:49:16.670636  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.670640  2412 net.cpp:194] Memory required for data: 57139320
I0818 10:49:16.670658  2412 layer_factory.hpp:77] Creating layer BatchNorm18
I0818 10:49:16.670675  2412 net.cpp:128] Creating Layer BatchNorm18
I0818 10:49:16.670681  2412 net.cpp:558] BatchNorm18 <- Convolution18
I0818 10:49:16.670688  2412 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0818 10:49:16.671005  2412 net.cpp:172] Setting up BatchNorm18
I0818 10:49:16.671020  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.671023  2412 net.cpp:194] Memory required for data: 57467000
I0818 10:49:16.671033  2412 layer_factory.hpp:77] Creating layer Scale18
I0818 10:49:16.671039  2412 net.cpp:128] Creating Layer Scale18
I0818 10:49:16.671044  2412 net.cpp:558] Scale18 <- Convolution18
I0818 10:49:16.671051  2412 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0818 10:49:16.671106  2412 layer_factory.hpp:77] Creating layer Scale18
I0818 10:49:16.671281  2412 net.cpp:172] Setting up Scale18
I0818 10:49:16.671293  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.671298  2412 net.cpp:194] Memory required for data: 57794680
I0818 10:49:16.671305  2412 layer_factory.hpp:77] Creating layer Eltwise8
I0818 10:49:16.671314  2412 net.cpp:128] Creating Layer Eltwise8
I0818 10:49:16.671319  2412 net.cpp:558] Eltwise8 <- Convolution16
I0818 10:49:16.671324  2412 net.cpp:558] Eltwise8 <- Convolution18
I0818 10:49:16.671331  2412 net.cpp:522] Eltwise8 -> Eltwise8
I0818 10:49:16.671357  2412 net.cpp:172] Setting up Eltwise8
I0818 10:49:16.671365  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.671368  2412 net.cpp:194] Memory required for data: 58122360
I0818 10:49:16.671373  2412 layer_factory.hpp:77] Creating layer ReLU17
I0818 10:49:16.671382  2412 net.cpp:128] Creating Layer ReLU17
I0818 10:49:16.671386  2412 net.cpp:558] ReLU17 <- Eltwise8
I0818 10:49:16.671392  2412 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0818 10:49:16.672614  2412 net.cpp:172] Setting up ReLU17
I0818 10:49:16.672631  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.672634  2412 net.cpp:194] Memory required for data: 58450040
I0818 10:49:16.672639  2412 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0818 10:49:16.672648  2412 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0818 10:49:16.672653  2412 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0818 10:49:16.672660  2412 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0818 10:49:16.672668  2412 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0818 10:49:16.672749  2412 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0818 10:49:16.672756  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.672762  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.672766  2412 net.cpp:194] Memory required for data: 59105400
I0818 10:49:16.672770  2412 layer_factory.hpp:77] Creating layer Convolution19
I0818 10:49:16.672785  2412 net.cpp:128] Creating Layer Convolution19
I0818 10:49:16.672791  2412 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0818 10:49:16.672797  2412 net.cpp:522] Convolution19 -> Convolution19
I0818 10:49:16.679884  2412 net.cpp:172] Setting up Convolution19
I0818 10:49:16.679911  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.679916  2412 net.cpp:194] Memory required for data: 59433080
I0818 10:49:16.679927  2412 layer_factory.hpp:77] Creating layer BatchNorm19
I0818 10:49:16.679939  2412 net.cpp:128] Creating Layer BatchNorm19
I0818 10:49:16.679944  2412 net.cpp:558] BatchNorm19 <- Convolution19
I0818 10:49:16.679953  2412 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0818 10:49:16.680274  2412 net.cpp:172] Setting up BatchNorm19
I0818 10:49:16.680287  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.680291  2412 net.cpp:194] Memory required for data: 59760760
I0818 10:49:16.680315  2412 layer_factory.hpp:77] Creating layer Scale19
I0818 10:49:16.680327  2412 net.cpp:128] Creating Layer Scale19
I0818 10:49:16.680332  2412 net.cpp:558] Scale19 <- Convolution19
I0818 10:49:16.680338  2412 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0818 10:49:16.680397  2412 layer_factory.hpp:77] Creating layer Scale19
I0818 10:49:16.680568  2412 net.cpp:172] Setting up Scale19
I0818 10:49:16.680580  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.680586  2412 net.cpp:194] Memory required for data: 60088440
I0818 10:49:16.680594  2412 layer_factory.hpp:77] Creating layer ReLU18
I0818 10:49:16.680603  2412 net.cpp:128] Creating Layer ReLU18
I0818 10:49:16.680608  2412 net.cpp:558] ReLU18 <- Convolution19
I0818 10:49:16.680613  2412 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0818 10:49:16.681390  2412 net.cpp:172] Setting up ReLU18
I0818 10:49:16.681411  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.681416  2412 net.cpp:194] Memory required for data: 60416120
I0818 10:49:16.681421  2412 layer_factory.hpp:77] Creating layer Convolution20
I0818 10:49:16.681434  2412 net.cpp:128] Creating Layer Convolution20
I0818 10:49:16.681442  2412 net.cpp:558] Convolution20 <- Convolution19
I0818 10:49:16.681452  2412 net.cpp:522] Convolution20 -> Convolution20
I0818 10:49:16.688148  2412 net.cpp:172] Setting up Convolution20
I0818 10:49:16.688174  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.688179  2412 net.cpp:194] Memory required for data: 60743800
I0818 10:49:16.688189  2412 layer_factory.hpp:77] Creating layer BatchNorm20
I0818 10:49:16.688199  2412 net.cpp:128] Creating Layer BatchNorm20
I0818 10:49:16.688205  2412 net.cpp:558] BatchNorm20 <- Convolution20
I0818 10:49:16.688211  2412 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0818 10:49:16.688529  2412 net.cpp:172] Setting up BatchNorm20
I0818 10:49:16.688540  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.688544  2412 net.cpp:194] Memory required for data: 61071480
I0818 10:49:16.688555  2412 layer_factory.hpp:77] Creating layer Scale20
I0818 10:49:16.688561  2412 net.cpp:128] Creating Layer Scale20
I0818 10:49:16.688565  2412 net.cpp:558] Scale20 <- Convolution20
I0818 10:49:16.688573  2412 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0818 10:49:16.688627  2412 layer_factory.hpp:77] Creating layer Scale20
I0818 10:49:16.688802  2412 net.cpp:172] Setting up Scale20
I0818 10:49:16.688814  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.688819  2412 net.cpp:194] Memory required for data: 61399160
I0818 10:49:16.688827  2412 layer_factory.hpp:77] Creating layer Eltwise9
I0818 10:49:16.688834  2412 net.cpp:128] Creating Layer Eltwise9
I0818 10:49:16.688854  2412 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0818 10:49:16.688860  2412 net.cpp:558] Eltwise9 <- Convolution20
I0818 10:49:16.688869  2412 net.cpp:522] Eltwise9 -> Eltwise9
I0818 10:49:16.688896  2412 net.cpp:172] Setting up Eltwise9
I0818 10:49:16.688904  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.688907  2412 net.cpp:194] Memory required for data: 61726840
I0818 10:49:16.688912  2412 layer_factory.hpp:77] Creating layer ReLU19
I0818 10:49:16.688922  2412 net.cpp:128] Creating Layer ReLU19
I0818 10:49:16.688927  2412 net.cpp:558] ReLU19 <- Eltwise9
I0818 10:49:16.688933  2412 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0818 10:49:16.690188  2412 net.cpp:172] Setting up ReLU19
I0818 10:49:16.690207  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.690212  2412 net.cpp:194] Memory required for data: 62054520
I0818 10:49:16.690217  2412 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0818 10:49:16.690225  2412 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0818 10:49:16.690230  2412 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0818 10:49:16.690237  2412 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0818 10:49:16.690245  2412 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0818 10:49:16.690309  2412 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0818 10:49:16.690315  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.690321  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.690325  2412 net.cpp:194] Memory required for data: 62709880
I0818 10:49:16.690330  2412 layer_factory.hpp:77] Creating layer Convolution21
I0818 10:49:16.690342  2412 net.cpp:128] Creating Layer Convolution21
I0818 10:49:16.690347  2412 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0818 10:49:16.690356  2412 net.cpp:522] Convolution21 -> Convolution21
I0818 10:49:16.693734  2412 net.cpp:172] Setting up Convolution21
I0818 10:49:16.693760  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.693764  2412 net.cpp:194] Memory required for data: 63037560
I0818 10:49:16.693775  2412 layer_factory.hpp:77] Creating layer BatchNorm21
I0818 10:49:16.693789  2412 net.cpp:128] Creating Layer BatchNorm21
I0818 10:49:16.693794  2412 net.cpp:558] BatchNorm21 <- Convolution21
I0818 10:49:16.693807  2412 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0818 10:49:16.694116  2412 net.cpp:172] Setting up BatchNorm21
I0818 10:49:16.694129  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.694133  2412 net.cpp:194] Memory required for data: 63365240
I0818 10:49:16.694144  2412 layer_factory.hpp:77] Creating layer Scale21
I0818 10:49:16.694150  2412 net.cpp:128] Creating Layer Scale21
I0818 10:49:16.694154  2412 net.cpp:558] Scale21 <- Convolution21
I0818 10:49:16.694162  2412 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0818 10:49:16.694218  2412 layer_factory.hpp:77] Creating layer Scale21
I0818 10:49:16.694392  2412 net.cpp:172] Setting up Scale21
I0818 10:49:16.694404  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.694409  2412 net.cpp:194] Memory required for data: 63692920
I0818 10:49:16.694417  2412 layer_factory.hpp:77] Creating layer ReLU20
I0818 10:49:16.694423  2412 net.cpp:128] Creating Layer ReLU20
I0818 10:49:16.694428  2412 net.cpp:558] ReLU20 <- Convolution21
I0818 10:49:16.694437  2412 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0818 10:49:16.694706  2412 net.cpp:172] Setting up ReLU20
I0818 10:49:16.694720  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.694723  2412 net.cpp:194] Memory required for data: 64020600
I0818 10:49:16.694728  2412 layer_factory.hpp:77] Creating layer Convolution22
I0818 10:49:16.694741  2412 net.cpp:128] Creating Layer Convolution22
I0818 10:49:16.694746  2412 net.cpp:558] Convolution22 <- Convolution21
I0818 10:49:16.694756  2412 net.cpp:522] Convolution22 -> Convolution22
I0818 10:49:16.701009  2412 net.cpp:172] Setting up Convolution22
I0818 10:49:16.701035  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.701054  2412 net.cpp:194] Memory required for data: 64348280
I0818 10:49:16.701064  2412 layer_factory.hpp:77] Creating layer BatchNorm22
I0818 10:49:16.701072  2412 net.cpp:128] Creating Layer BatchNorm22
I0818 10:49:16.701079  2412 net.cpp:558] BatchNorm22 <- Convolution22
I0818 10:49:16.701089  2412 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0818 10:49:16.701409  2412 net.cpp:172] Setting up BatchNorm22
I0818 10:49:16.701421  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.701426  2412 net.cpp:194] Memory required for data: 64675960
I0818 10:49:16.701436  2412 layer_factory.hpp:77] Creating layer Scale22
I0818 10:49:16.701442  2412 net.cpp:128] Creating Layer Scale22
I0818 10:49:16.701447  2412 net.cpp:558] Scale22 <- Convolution22
I0818 10:49:16.701457  2412 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0818 10:49:16.701511  2412 layer_factory.hpp:77] Creating layer Scale22
I0818 10:49:16.701686  2412 net.cpp:172] Setting up Scale22
I0818 10:49:16.701699  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.701704  2412 net.cpp:194] Memory required for data: 65003640
I0818 10:49:16.701711  2412 layer_factory.hpp:77] Creating layer Eltwise10
I0818 10:49:16.701719  2412 net.cpp:128] Creating Layer Eltwise10
I0818 10:49:16.701723  2412 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0818 10:49:16.701730  2412 net.cpp:558] Eltwise10 <- Convolution22
I0818 10:49:16.701738  2412 net.cpp:522] Eltwise10 -> Eltwise10
I0818 10:49:16.701764  2412 net.cpp:172] Setting up Eltwise10
I0818 10:49:16.701771  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.701776  2412 net.cpp:194] Memory required for data: 65331320
I0818 10:49:16.701779  2412 layer_factory.hpp:77] Creating layer ReLU21
I0818 10:49:16.701786  2412 net.cpp:128] Creating Layer ReLU21
I0818 10:49:16.701789  2412 net.cpp:558] ReLU21 <- Eltwise10
I0818 10:49:16.701798  2412 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0818 10:49:16.703069  2412 net.cpp:172] Setting up ReLU21
I0818 10:49:16.703095  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.703099  2412 net.cpp:194] Memory required for data: 65659000
I0818 10:49:16.703104  2412 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0818 10:49:16.703114  2412 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0818 10:49:16.703119  2412 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0818 10:49:16.703127  2412 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0818 10:49:16.703136  2412 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0818 10:49:16.703202  2412 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0818 10:49:16.703209  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.703214  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.703218  2412 net.cpp:194] Memory required for data: 66314360
I0818 10:49:16.703222  2412 layer_factory.hpp:77] Creating layer Convolution23
I0818 10:49:16.703235  2412 net.cpp:128] Creating Layer Convolution23
I0818 10:49:16.703240  2412 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0818 10:49:16.703249  2412 net.cpp:522] Convolution23 -> Convolution23
I0818 10:49:16.709818  2412 net.cpp:172] Setting up Convolution23
I0818 10:49:16.709846  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.709849  2412 net.cpp:194] Memory required for data: 66642040
I0818 10:49:16.709859  2412 layer_factory.hpp:77] Creating layer BatchNorm23
I0818 10:49:16.709870  2412 net.cpp:128] Creating Layer BatchNorm23
I0818 10:49:16.709877  2412 net.cpp:558] BatchNorm23 <- Convolution23
I0818 10:49:16.709883  2412 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0818 10:49:16.710198  2412 net.cpp:172] Setting up BatchNorm23
I0818 10:49:16.710211  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.710214  2412 net.cpp:194] Memory required for data: 66969720
I0818 10:49:16.710224  2412 layer_factory.hpp:77] Creating layer Scale23
I0818 10:49:16.710237  2412 net.cpp:128] Creating Layer Scale23
I0818 10:49:16.710259  2412 net.cpp:558] Scale23 <- Convolution23
I0818 10:49:16.710265  2412 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0818 10:49:16.710326  2412 layer_factory.hpp:77] Creating layer Scale23
I0818 10:49:16.710505  2412 net.cpp:172] Setting up Scale23
I0818 10:49:16.710513  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.710517  2412 net.cpp:194] Memory required for data: 67297400
I0818 10:49:16.710526  2412 layer_factory.hpp:77] Creating layer ReLU22
I0818 10:49:16.710532  2412 net.cpp:128] Creating Layer ReLU22
I0818 10:49:16.710537  2412 net.cpp:558] ReLU22 <- Convolution23
I0818 10:49:16.710546  2412 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0818 10:49:16.711830  2412 net.cpp:172] Setting up ReLU22
I0818 10:49:16.711850  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.711855  2412 net.cpp:194] Memory required for data: 67625080
I0818 10:49:16.711859  2412 layer_factory.hpp:77] Creating layer Convolution24
I0818 10:49:16.711877  2412 net.cpp:128] Creating Layer Convolution24
I0818 10:49:16.711884  2412 net.cpp:558] Convolution24 <- Convolution23
I0818 10:49:16.711892  2412 net.cpp:522] Convolution24 -> Convolution24
I0818 10:49:16.718520  2412 net.cpp:172] Setting up Convolution24
I0818 10:49:16.718546  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.718551  2412 net.cpp:194] Memory required for data: 67952760
I0818 10:49:16.718561  2412 layer_factory.hpp:77] Creating layer BatchNorm24
I0818 10:49:16.718572  2412 net.cpp:128] Creating Layer BatchNorm24
I0818 10:49:16.718577  2412 net.cpp:558] BatchNorm24 <- Convolution24
I0818 10:49:16.718586  2412 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0818 10:49:16.718925  2412 net.cpp:172] Setting up BatchNorm24
I0818 10:49:16.718940  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.718943  2412 net.cpp:194] Memory required for data: 68280440
I0818 10:49:16.718953  2412 layer_factory.hpp:77] Creating layer Scale24
I0818 10:49:16.718963  2412 net.cpp:128] Creating Layer Scale24
I0818 10:49:16.718968  2412 net.cpp:558] Scale24 <- Convolution24
I0818 10:49:16.718973  2412 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0818 10:49:16.719033  2412 layer_factory.hpp:77] Creating layer Scale24
I0818 10:49:16.719214  2412 net.cpp:172] Setting up Scale24
I0818 10:49:16.719226  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.719231  2412 net.cpp:194] Memory required for data: 68608120
I0818 10:49:16.719239  2412 layer_factory.hpp:77] Creating layer Eltwise11
I0818 10:49:16.719249  2412 net.cpp:128] Creating Layer Eltwise11
I0818 10:49:16.719254  2412 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0818 10:49:16.719259  2412 net.cpp:558] Eltwise11 <- Convolution24
I0818 10:49:16.719264  2412 net.cpp:522] Eltwise11 -> Eltwise11
I0818 10:49:16.719293  2412 net.cpp:172] Setting up Eltwise11
I0818 10:49:16.719300  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.719305  2412 net.cpp:194] Memory required for data: 68935800
I0818 10:49:16.719310  2412 layer_factory.hpp:77] Creating layer ReLU23
I0818 10:49:16.719316  2412 net.cpp:128] Creating Layer ReLU23
I0818 10:49:16.719319  2412 net.cpp:558] ReLU23 <- Eltwise11
I0818 10:49:16.719327  2412 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0818 10:49:16.720538  2412 net.cpp:172] Setting up ReLU23
I0818 10:49:16.720556  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.720561  2412 net.cpp:194] Memory required for data: 69263480
I0818 10:49:16.720564  2412 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0818 10:49:16.720571  2412 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0818 10:49:16.720577  2412 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0818 10:49:16.720585  2412 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0818 10:49:16.720593  2412 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0818 10:49:16.720659  2412 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0818 10:49:16.720666  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.720688  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.720693  2412 net.cpp:194] Memory required for data: 69918840
I0818 10:49:16.720697  2412 layer_factory.hpp:77] Creating layer Convolution25
I0818 10:49:16.720710  2412 net.cpp:128] Creating Layer Convolution25
I0818 10:49:16.720715  2412 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0818 10:49:16.720724  2412 net.cpp:522] Convolution25 -> Convolution25
I0818 10:49:16.727303  2412 net.cpp:172] Setting up Convolution25
I0818 10:49:16.727329  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.727334  2412 net.cpp:194] Memory required for data: 70246520
I0818 10:49:16.727344  2412 layer_factory.hpp:77] Creating layer BatchNorm25
I0818 10:49:16.727357  2412 net.cpp:128] Creating Layer BatchNorm25
I0818 10:49:16.727362  2412 net.cpp:558] BatchNorm25 <- Convolution25
I0818 10:49:16.727370  2412 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0818 10:49:16.727689  2412 net.cpp:172] Setting up BatchNorm25
I0818 10:49:16.727701  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.727705  2412 net.cpp:194] Memory required for data: 70574200
I0818 10:49:16.727715  2412 layer_factory.hpp:77] Creating layer Scale25
I0818 10:49:16.727725  2412 net.cpp:128] Creating Layer Scale25
I0818 10:49:16.727730  2412 net.cpp:558] Scale25 <- Convolution25
I0818 10:49:16.727735  2412 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0818 10:49:16.727792  2412 layer_factory.hpp:77] Creating layer Scale25
I0818 10:49:16.727975  2412 net.cpp:172] Setting up Scale25
I0818 10:49:16.727986  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.727991  2412 net.cpp:194] Memory required for data: 70901880
I0818 10:49:16.727999  2412 layer_factory.hpp:77] Creating layer ReLU24
I0818 10:49:16.728011  2412 net.cpp:128] Creating Layer ReLU24
I0818 10:49:16.728016  2412 net.cpp:558] ReLU24 <- Convolution25
I0818 10:49:16.728024  2412 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0818 10:49:16.729353  2412 net.cpp:172] Setting up ReLU24
I0818 10:49:16.729372  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.729375  2412 net.cpp:194] Memory required for data: 71229560
I0818 10:49:16.729380  2412 layer_factory.hpp:77] Creating layer Convolution26
I0818 10:49:16.729394  2412 net.cpp:128] Creating Layer Convolution26
I0818 10:49:16.729400  2412 net.cpp:558] Convolution26 <- Convolution25
I0818 10:49:16.729408  2412 net.cpp:522] Convolution26 -> Convolution26
I0818 10:49:16.736109  2412 net.cpp:172] Setting up Convolution26
I0818 10:49:16.736136  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.736140  2412 net.cpp:194] Memory required for data: 71557240
I0818 10:49:16.736151  2412 layer_factory.hpp:77] Creating layer BatchNorm26
I0818 10:49:16.736162  2412 net.cpp:128] Creating Layer BatchNorm26
I0818 10:49:16.736168  2412 net.cpp:558] BatchNorm26 <- Convolution26
I0818 10:49:16.736176  2412 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0818 10:49:16.736498  2412 net.cpp:172] Setting up BatchNorm26
I0818 10:49:16.736512  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.736516  2412 net.cpp:194] Memory required for data: 71884920
I0818 10:49:16.736526  2412 layer_factory.hpp:77] Creating layer Scale26
I0818 10:49:16.736533  2412 net.cpp:128] Creating Layer Scale26
I0818 10:49:16.736537  2412 net.cpp:558] Scale26 <- Convolution26
I0818 10:49:16.736546  2412 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0818 10:49:16.736601  2412 layer_factory.hpp:77] Creating layer Scale26
I0818 10:49:16.736783  2412 net.cpp:172] Setting up Scale26
I0818 10:49:16.736794  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.736799  2412 net.cpp:194] Memory required for data: 72212600
I0818 10:49:16.736806  2412 layer_factory.hpp:77] Creating layer Eltwise12
I0818 10:49:16.736814  2412 net.cpp:128] Creating Layer Eltwise12
I0818 10:49:16.736819  2412 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0818 10:49:16.736824  2412 net.cpp:558] Eltwise12 <- Convolution26
I0818 10:49:16.736848  2412 net.cpp:522] Eltwise12 -> Eltwise12
I0818 10:49:16.736876  2412 net.cpp:172] Setting up Eltwise12
I0818 10:49:16.736886  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.736891  2412 net.cpp:194] Memory required for data: 72540280
I0818 10:49:16.736896  2412 layer_factory.hpp:77] Creating layer ReLU25
I0818 10:49:16.736912  2412 net.cpp:128] Creating Layer ReLU25
I0818 10:49:16.736917  2412 net.cpp:558] ReLU25 <- Eltwise12
I0818 10:49:16.736923  2412 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0818 10:49:16.738123  2412 net.cpp:172] Setting up ReLU25
I0818 10:49:16.738147  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.738152  2412 net.cpp:194] Memory required for data: 72867960
I0818 10:49:16.738157  2412 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0818 10:49:16.738168  2412 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0818 10:49:16.738173  2412 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0818 10:49:16.738180  2412 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0818 10:49:16.738189  2412 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0818 10:49:16.738256  2412 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0818 10:49:16.738265  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.738270  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.738273  2412 net.cpp:194] Memory required for data: 73523320
I0818 10:49:16.738278  2412 layer_factory.hpp:77] Creating layer Convolution27
I0818 10:49:16.738291  2412 net.cpp:128] Creating Layer Convolution27
I0818 10:49:16.738296  2412 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0818 10:49:16.738303  2412 net.cpp:522] Convolution27 -> Convolution27
I0818 10:49:16.744879  2412 net.cpp:172] Setting up Convolution27
I0818 10:49:16.744905  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.744910  2412 net.cpp:194] Memory required for data: 73851000
I0818 10:49:16.744920  2412 layer_factory.hpp:77] Creating layer BatchNorm27
I0818 10:49:16.744931  2412 net.cpp:128] Creating Layer BatchNorm27
I0818 10:49:16.744937  2412 net.cpp:558] BatchNorm27 <- Convolution27
I0818 10:49:16.744946  2412 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0818 10:49:16.745268  2412 net.cpp:172] Setting up BatchNorm27
I0818 10:49:16.745281  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.745285  2412 net.cpp:194] Memory required for data: 74178680
I0818 10:49:16.745296  2412 layer_factory.hpp:77] Creating layer Scale27
I0818 10:49:16.745302  2412 net.cpp:128] Creating Layer Scale27
I0818 10:49:16.745307  2412 net.cpp:558] Scale27 <- Convolution27
I0818 10:49:16.745316  2412 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0818 10:49:16.745373  2412 layer_factory.hpp:77] Creating layer Scale27
I0818 10:49:16.745555  2412 net.cpp:172] Setting up Scale27
I0818 10:49:16.745566  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.745571  2412 net.cpp:194] Memory required for data: 74506360
I0818 10:49:16.745579  2412 layer_factory.hpp:77] Creating layer ReLU26
I0818 10:49:16.745585  2412 net.cpp:128] Creating Layer ReLU26
I0818 10:49:16.745590  2412 net.cpp:558] ReLU26 <- Convolution27
I0818 10:49:16.745599  2412 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0818 10:49:16.746923  2412 net.cpp:172] Setting up ReLU26
I0818 10:49:16.746944  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.746948  2412 net.cpp:194] Memory required for data: 74834040
I0818 10:49:16.746953  2412 layer_factory.hpp:77] Creating layer Convolution28
I0818 10:49:16.746966  2412 net.cpp:128] Creating Layer Convolution28
I0818 10:49:16.746973  2412 net.cpp:558] Convolution28 <- Convolution27
I0818 10:49:16.746982  2412 net.cpp:522] Convolution28 -> Convolution28
I0818 10:49:16.753602  2412 net.cpp:172] Setting up Convolution28
I0818 10:49:16.753629  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.753633  2412 net.cpp:194] Memory required for data: 75161720
I0818 10:49:16.753659  2412 layer_factory.hpp:77] Creating layer BatchNorm28
I0818 10:49:16.753669  2412 net.cpp:128] Creating Layer BatchNorm28
I0818 10:49:16.753679  2412 net.cpp:558] BatchNorm28 <- Convolution28
I0818 10:49:16.753687  2412 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0818 10:49:16.754017  2412 net.cpp:172] Setting up BatchNorm28
I0818 10:49:16.754032  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.754037  2412 net.cpp:194] Memory required for data: 75489400
I0818 10:49:16.754047  2412 layer_factory.hpp:77] Creating layer Scale28
I0818 10:49:16.754055  2412 net.cpp:128] Creating Layer Scale28
I0818 10:49:16.754060  2412 net.cpp:558] Scale28 <- Convolution28
I0818 10:49:16.754067  2412 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0818 10:49:16.754124  2412 layer_factory.hpp:77] Creating layer Scale28
I0818 10:49:16.754305  2412 net.cpp:172] Setting up Scale28
I0818 10:49:16.754313  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.754317  2412 net.cpp:194] Memory required for data: 75817080
I0818 10:49:16.754325  2412 layer_factory.hpp:77] Creating layer Eltwise13
I0818 10:49:16.754333  2412 net.cpp:128] Creating Layer Eltwise13
I0818 10:49:16.754338  2412 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0818 10:49:16.754343  2412 net.cpp:558] Eltwise13 <- Convolution28
I0818 10:49:16.754350  2412 net.cpp:522] Eltwise13 -> Eltwise13
I0818 10:49:16.754376  2412 net.cpp:172] Setting up Eltwise13
I0818 10:49:16.754384  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.754387  2412 net.cpp:194] Memory required for data: 76144760
I0818 10:49:16.754391  2412 layer_factory.hpp:77] Creating layer ReLU27
I0818 10:49:16.754397  2412 net.cpp:128] Creating Layer ReLU27
I0818 10:49:16.754405  2412 net.cpp:558] ReLU27 <- Eltwise13
I0818 10:49:16.754410  2412 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0818 10:49:16.755620  2412 net.cpp:172] Setting up ReLU27
I0818 10:49:16.755636  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.755641  2412 net.cpp:194] Memory required for data: 76472440
I0818 10:49:16.755646  2412 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0818 10:49:16.755656  2412 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0818 10:49:16.755662  2412 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0818 10:49:16.755669  2412 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0818 10:49:16.755678  2412 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0818 10:49:16.755744  2412 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0818 10:49:16.755755  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.755762  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.755766  2412 net.cpp:194] Memory required for data: 77127800
I0818 10:49:16.755770  2412 layer_factory.hpp:77] Creating layer Convolution29
I0818 10:49:16.755782  2412 net.cpp:128] Creating Layer Convolution29
I0818 10:49:16.755786  2412 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0818 10:49:16.755798  2412 net.cpp:522] Convolution29 -> Convolution29
I0818 10:49:16.762639  2412 net.cpp:172] Setting up Convolution29
I0818 10:49:16.762673  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.762679  2412 net.cpp:194] Memory required for data: 77455480
I0818 10:49:16.762689  2412 layer_factory.hpp:77] Creating layer BatchNorm29
I0818 10:49:16.762701  2412 net.cpp:128] Creating Layer BatchNorm29
I0818 10:49:16.762706  2412 net.cpp:558] BatchNorm29 <- Convolution29
I0818 10:49:16.762715  2412 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0818 10:49:16.763058  2412 net.cpp:172] Setting up BatchNorm29
I0818 10:49:16.763072  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.763075  2412 net.cpp:194] Memory required for data: 77783160
I0818 10:49:16.763085  2412 layer_factory.hpp:77] Creating layer Scale29
I0818 10:49:16.763093  2412 net.cpp:128] Creating Layer Scale29
I0818 10:49:16.763098  2412 net.cpp:558] Scale29 <- Convolution29
I0818 10:49:16.763119  2412 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0818 10:49:16.763180  2412 layer_factory.hpp:77] Creating layer Scale29
I0818 10:49:16.763362  2412 net.cpp:172] Setting up Scale29
I0818 10:49:16.763375  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.763379  2412 net.cpp:194] Memory required for data: 78110840
I0818 10:49:16.763388  2412 layer_factory.hpp:77] Creating layer ReLU28
I0818 10:49:16.763394  2412 net.cpp:128] Creating Layer ReLU28
I0818 10:49:16.763401  2412 net.cpp:558] ReLU28 <- Convolution29
I0818 10:49:16.763407  2412 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0818 10:49:16.764446  2412 net.cpp:172] Setting up ReLU28
I0818 10:49:16.764468  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.764472  2412 net.cpp:194] Memory required for data: 78438520
I0818 10:49:16.764477  2412 layer_factory.hpp:77] Creating layer Convolution30
I0818 10:49:16.764497  2412 net.cpp:128] Creating Layer Convolution30
I0818 10:49:16.764503  2412 net.cpp:558] Convolution30 <- Convolution29
I0818 10:49:16.764513  2412 net.cpp:522] Convolution30 -> Convolution30
I0818 10:49:16.771220  2412 net.cpp:172] Setting up Convolution30
I0818 10:49:16.771246  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.771251  2412 net.cpp:194] Memory required for data: 78766200
I0818 10:49:16.771261  2412 layer_factory.hpp:77] Creating layer BatchNorm30
I0818 10:49:16.771270  2412 net.cpp:128] Creating Layer BatchNorm30
I0818 10:49:16.771275  2412 net.cpp:558] BatchNorm30 <- Convolution30
I0818 10:49:16.771284  2412 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0818 10:49:16.771610  2412 net.cpp:172] Setting up BatchNorm30
I0818 10:49:16.771621  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.771626  2412 net.cpp:194] Memory required for data: 79093880
I0818 10:49:16.771636  2412 layer_factory.hpp:77] Creating layer Scale30
I0818 10:49:16.771643  2412 net.cpp:128] Creating Layer Scale30
I0818 10:49:16.771647  2412 net.cpp:558] Scale30 <- Convolution30
I0818 10:49:16.771656  2412 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0818 10:49:16.771711  2412 layer_factory.hpp:77] Creating layer Scale30
I0818 10:49:16.771893  2412 net.cpp:172] Setting up Scale30
I0818 10:49:16.771905  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.771911  2412 net.cpp:194] Memory required for data: 79421560
I0818 10:49:16.771919  2412 layer_factory.hpp:77] Creating layer Eltwise14
I0818 10:49:16.771926  2412 net.cpp:128] Creating Layer Eltwise14
I0818 10:49:16.771931  2412 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0818 10:49:16.771936  2412 net.cpp:558] Eltwise14 <- Convolution30
I0818 10:49:16.771945  2412 net.cpp:522] Eltwise14 -> Eltwise14
I0818 10:49:16.771972  2412 net.cpp:172] Setting up Eltwise14
I0818 10:49:16.771980  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.771983  2412 net.cpp:194] Memory required for data: 79749240
I0818 10:49:16.771987  2412 layer_factory.hpp:77] Creating layer ReLU29
I0818 10:49:16.771994  2412 net.cpp:128] Creating Layer ReLU29
I0818 10:49:16.771998  2412 net.cpp:558] ReLU29 <- Eltwise14
I0818 10:49:16.772007  2412 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0818 10:49:16.773231  2412 net.cpp:172] Setting up ReLU29
I0818 10:49:16.773245  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.773249  2412 net.cpp:194] Memory required for data: 80076920
I0818 10:49:16.773254  2412 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0818 10:49:16.773264  2412 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0818 10:49:16.773269  2412 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0818 10:49:16.773275  2412 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0818 10:49:16.773284  2412 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0818 10:49:16.773347  2412 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0818 10:49:16.773355  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.773361  2412 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:49:16.773381  2412 net.cpp:194] Memory required for data: 80732280
I0818 10:49:16.773386  2412 layer_factory.hpp:77] Creating layer Convolution31
I0818 10:49:16.773397  2412 net.cpp:128] Creating Layer Convolution31
I0818 10:49:16.773402  2412 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0818 10:49:16.773411  2412 net.cpp:522] Convolution31 -> Convolution31
I0818 10:49:16.779968  2412 net.cpp:172] Setting up Convolution31
I0818 10:49:16.779994  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.779999  2412 net.cpp:194] Memory required for data: 80896120
I0818 10:49:16.780009  2412 layer_factory.hpp:77] Creating layer BatchNorm31
I0818 10:49:16.780020  2412 net.cpp:128] Creating Layer BatchNorm31
I0818 10:49:16.780025  2412 net.cpp:558] BatchNorm31 <- Convolution31
I0818 10:49:16.780035  2412 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0818 10:49:16.780378  2412 net.cpp:172] Setting up BatchNorm31
I0818 10:49:16.780391  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.780395  2412 net.cpp:194] Memory required for data: 81059960
I0818 10:49:16.780405  2412 layer_factory.hpp:77] Creating layer Scale31
I0818 10:49:16.780414  2412 net.cpp:128] Creating Layer Scale31
I0818 10:49:16.780419  2412 net.cpp:558] Scale31 <- Convolution31
I0818 10:49:16.780426  2412 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0818 10:49:16.780484  2412 layer_factory.hpp:77] Creating layer Scale31
I0818 10:49:16.780675  2412 net.cpp:172] Setting up Scale31
I0818 10:49:16.780688  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.780692  2412 net.cpp:194] Memory required for data: 81223800
I0818 10:49:16.780700  2412 layer_factory.hpp:77] Creating layer Convolution32
I0818 10:49:16.780714  2412 net.cpp:128] Creating Layer Convolution32
I0818 10:49:16.780719  2412 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_1
I0818 10:49:16.780730  2412 net.cpp:522] Convolution32 -> Convolution32
I0818 10:49:16.786546  2412 net.cpp:172] Setting up Convolution32
I0818 10:49:16.786572  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.786577  2412 net.cpp:194] Memory required for data: 81387640
I0818 10:49:16.786587  2412 layer_factory.hpp:77] Creating layer BatchNorm32
I0818 10:49:16.786598  2412 net.cpp:128] Creating Layer BatchNorm32
I0818 10:49:16.786604  2412 net.cpp:558] BatchNorm32 <- Convolution32
I0818 10:49:16.786610  2412 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0818 10:49:16.786976  2412 net.cpp:172] Setting up BatchNorm32
I0818 10:49:16.786990  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.786995  2412 net.cpp:194] Memory required for data: 81551480
I0818 10:49:16.787005  2412 layer_factory.hpp:77] Creating layer Scale32
I0818 10:49:16.787012  2412 net.cpp:128] Creating Layer Scale32
I0818 10:49:16.787017  2412 net.cpp:558] Scale32 <- Convolution32
I0818 10:49:16.787024  2412 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0818 10:49:16.787086  2412 layer_factory.hpp:77] Creating layer Scale32
I0818 10:49:16.787279  2412 net.cpp:172] Setting up Scale32
I0818 10:49:16.787292  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.787297  2412 net.cpp:194] Memory required for data: 81715320
I0818 10:49:16.787304  2412 layer_factory.hpp:77] Creating layer ReLU30
I0818 10:49:16.787312  2412 net.cpp:128] Creating Layer ReLU30
I0818 10:49:16.787317  2412 net.cpp:558] ReLU30 <- Convolution32
I0818 10:49:16.787324  2412 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0818 10:49:16.788580  2412 net.cpp:172] Setting up ReLU30
I0818 10:49:16.788599  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.788602  2412 net.cpp:194] Memory required for data: 81879160
I0818 10:49:16.788607  2412 layer_factory.hpp:77] Creating layer Convolution33
I0818 10:49:16.788619  2412 net.cpp:128] Creating Layer Convolution33
I0818 10:49:16.788625  2412 net.cpp:558] Convolution33 <- Convolution32
I0818 10:49:16.788635  2412 net.cpp:522] Convolution33 -> Convolution33
I0818 10:49:16.795361  2412 net.cpp:172] Setting up Convolution33
I0818 10:49:16.795401  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.795406  2412 net.cpp:194] Memory required for data: 82043000
I0818 10:49:16.795418  2412 layer_factory.hpp:77] Creating layer BatchNorm33
I0818 10:49:16.795428  2412 net.cpp:128] Creating Layer BatchNorm33
I0818 10:49:16.795433  2412 net.cpp:558] BatchNorm33 <- Convolution33
I0818 10:49:16.795440  2412 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0818 10:49:16.795784  2412 net.cpp:172] Setting up BatchNorm33
I0818 10:49:16.795797  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.795801  2412 net.cpp:194] Memory required for data: 82206840
I0818 10:49:16.795811  2412 layer_factory.hpp:77] Creating layer Scale33
I0818 10:49:16.795820  2412 net.cpp:128] Creating Layer Scale33
I0818 10:49:16.795825  2412 net.cpp:558] Scale33 <- Convolution33
I0818 10:49:16.795831  2412 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0818 10:49:16.795892  2412 layer_factory.hpp:77] Creating layer Scale33
I0818 10:49:16.796083  2412 net.cpp:172] Setting up Scale33
I0818 10:49:16.796094  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.796099  2412 net.cpp:194] Memory required for data: 82370680
I0818 10:49:16.796108  2412 layer_factory.hpp:77] Creating layer Eltwise15
I0818 10:49:16.796116  2412 net.cpp:128] Creating Layer Eltwise15
I0818 10:49:16.796121  2412 net.cpp:558] Eltwise15 <- Convolution31
I0818 10:49:16.796125  2412 net.cpp:558] Eltwise15 <- Convolution33
I0818 10:49:16.796134  2412 net.cpp:522] Eltwise15 -> Eltwise15
I0818 10:49:16.796167  2412 net.cpp:172] Setting up Eltwise15
I0818 10:49:16.796175  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.796178  2412 net.cpp:194] Memory required for data: 82534520
I0818 10:49:16.796182  2412 layer_factory.hpp:77] Creating layer ReLU31
I0818 10:49:16.796192  2412 net.cpp:128] Creating Layer ReLU31
I0818 10:49:16.796196  2412 net.cpp:558] ReLU31 <- Eltwise15
I0818 10:49:16.796202  2412 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0818 10:49:16.797405  2412 net.cpp:172] Setting up ReLU31
I0818 10:49:16.797426  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.797431  2412 net.cpp:194] Memory required for data: 82698360
I0818 10:49:16.797436  2412 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0818 10:49:16.797448  2412 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0818 10:49:16.797454  2412 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0818 10:49:16.797461  2412 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0818 10:49:16.797474  2412 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0818 10:49:16.797544  2412 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0818 10:49:16.797551  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.797556  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.797560  2412 net.cpp:194] Memory required for data: 83026040
I0818 10:49:16.797564  2412 layer_factory.hpp:77] Creating layer Convolution34
I0818 10:49:16.797578  2412 net.cpp:128] Creating Layer Convolution34
I0818 10:49:16.797583  2412 net.cpp:558] Convolution34 <- Eltwise15_ReLU31_0_split_0
I0818 10:49:16.797590  2412 net.cpp:522] Convolution34 -> Convolution34
I0818 10:49:16.804426  2412 net.cpp:172] Setting up Convolution34
I0818 10:49:16.804452  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.804458  2412 net.cpp:194] Memory required for data: 83189880
I0818 10:49:16.804469  2412 layer_factory.hpp:77] Creating layer BatchNorm34
I0818 10:49:16.804478  2412 net.cpp:128] Creating Layer BatchNorm34
I0818 10:49:16.804483  2412 net.cpp:558] BatchNorm34 <- Convolution34
I0818 10:49:16.804492  2412 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0818 10:49:16.804850  2412 net.cpp:172] Setting up BatchNorm34
I0818 10:49:16.804863  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.804868  2412 net.cpp:194] Memory required for data: 83353720
I0818 10:49:16.804877  2412 layer_factory.hpp:77] Creating layer Scale34
I0818 10:49:16.804885  2412 net.cpp:128] Creating Layer Scale34
I0818 10:49:16.804906  2412 net.cpp:558] Scale34 <- Convolution34
I0818 10:49:16.804914  2412 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0818 10:49:16.804975  2412 layer_factory.hpp:77] Creating layer Scale34
I0818 10:49:16.805168  2412 net.cpp:172] Setting up Scale34
I0818 10:49:16.805181  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.805184  2412 net.cpp:194] Memory required for data: 83517560
I0818 10:49:16.805193  2412 layer_factory.hpp:77] Creating layer ReLU32
I0818 10:49:16.805199  2412 net.cpp:128] Creating Layer ReLU32
I0818 10:49:16.805204  2412 net.cpp:558] ReLU32 <- Convolution34
I0818 10:49:16.805212  2412 net.cpp:509] ReLU32 -> Convolution34 (in-place)
I0818 10:49:16.806188  2412 net.cpp:172] Setting up ReLU32
I0818 10:49:16.806205  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.806210  2412 net.cpp:194] Memory required for data: 83681400
I0818 10:49:16.806216  2412 layer_factory.hpp:77] Creating layer Convolution35
I0818 10:49:16.806228  2412 net.cpp:128] Creating Layer Convolution35
I0818 10:49:16.806234  2412 net.cpp:558] Convolution35 <- Convolution34
I0818 10:49:16.806244  2412 net.cpp:522] Convolution35 -> Convolution35
I0818 10:49:16.812933  2412 net.cpp:172] Setting up Convolution35
I0818 10:49:16.812959  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.812963  2412 net.cpp:194] Memory required for data: 83845240
I0818 10:49:16.812973  2412 layer_factory.hpp:77] Creating layer BatchNorm35
I0818 10:49:16.812983  2412 net.cpp:128] Creating Layer BatchNorm35
I0818 10:49:16.812988  2412 net.cpp:558] BatchNorm35 <- Convolution35
I0818 10:49:16.812996  2412 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0818 10:49:16.813340  2412 net.cpp:172] Setting up BatchNorm35
I0818 10:49:16.813352  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.813356  2412 net.cpp:194] Memory required for data: 84009080
I0818 10:49:16.813366  2412 layer_factory.hpp:77] Creating layer Scale35
I0818 10:49:16.813376  2412 net.cpp:128] Creating Layer Scale35
I0818 10:49:16.813381  2412 net.cpp:558] Scale35 <- Convolution35
I0818 10:49:16.813387  2412 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0818 10:49:16.813447  2412 layer_factory.hpp:77] Creating layer Scale35
I0818 10:49:16.813640  2412 net.cpp:172] Setting up Scale35
I0818 10:49:16.813652  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.813657  2412 net.cpp:194] Memory required for data: 84172920
I0818 10:49:16.813664  2412 layer_factory.hpp:77] Creating layer Eltwise16
I0818 10:49:16.813673  2412 net.cpp:128] Creating Layer Eltwise16
I0818 10:49:16.813678  2412 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0818 10:49:16.813683  2412 net.cpp:558] Eltwise16 <- Convolution35
I0818 10:49:16.813691  2412 net.cpp:522] Eltwise16 -> Eltwise16
I0818 10:49:16.813724  2412 net.cpp:172] Setting up Eltwise16
I0818 10:49:16.813731  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.813735  2412 net.cpp:194] Memory required for data: 84336760
I0818 10:49:16.813740  2412 layer_factory.hpp:77] Creating layer ReLU33
I0818 10:49:16.813748  2412 net.cpp:128] Creating Layer ReLU33
I0818 10:49:16.813753  2412 net.cpp:558] ReLU33 <- Eltwise16
I0818 10:49:16.813758  2412 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0818 10:49:16.815222  2412 net.cpp:172] Setting up ReLU33
I0818 10:49:16.815249  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.815254  2412 net.cpp:194] Memory required for data: 84500600
I0818 10:49:16.815259  2412 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0818 10:49:16.815268  2412 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0818 10:49:16.815274  2412 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0818 10:49:16.815281  2412 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0818 10:49:16.815297  2412 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0818 10:49:16.815372  2412 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0818 10:49:16.815379  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.815400  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.815405  2412 net.cpp:194] Memory required for data: 84828280
I0818 10:49:16.815409  2412 layer_factory.hpp:77] Creating layer Convolution36
I0818 10:49:16.815424  2412 net.cpp:128] Creating Layer Convolution36
I0818 10:49:16.815429  2412 net.cpp:558] Convolution36 <- Eltwise16_ReLU33_0_split_0
I0818 10:49:16.815438  2412 net.cpp:522] Convolution36 -> Convolution36
I0818 10:49:16.821946  2412 net.cpp:172] Setting up Convolution36
I0818 10:49:16.821976  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.821981  2412 net.cpp:194] Memory required for data: 84992120
I0818 10:49:16.821992  2412 layer_factory.hpp:77] Creating layer BatchNorm36
I0818 10:49:16.822001  2412 net.cpp:128] Creating Layer BatchNorm36
I0818 10:49:16.822006  2412 net.cpp:558] BatchNorm36 <- Convolution36
I0818 10:49:16.822021  2412 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0818 10:49:16.822384  2412 net.cpp:172] Setting up BatchNorm36
I0818 10:49:16.822396  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.822401  2412 net.cpp:194] Memory required for data: 85155960
I0818 10:49:16.822410  2412 layer_factory.hpp:77] Creating layer Scale36
I0818 10:49:16.822418  2412 net.cpp:128] Creating Layer Scale36
I0818 10:49:16.822423  2412 net.cpp:558] Scale36 <- Convolution36
I0818 10:49:16.822432  2412 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0818 10:49:16.822489  2412 layer_factory.hpp:77] Creating layer Scale36
I0818 10:49:16.822696  2412 net.cpp:172] Setting up Scale36
I0818 10:49:16.822705  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.822710  2412 net.cpp:194] Memory required for data: 85319800
I0818 10:49:16.822717  2412 layer_factory.hpp:77] Creating layer ReLU34
I0818 10:49:16.822726  2412 net.cpp:128] Creating Layer ReLU34
I0818 10:49:16.822731  2412 net.cpp:558] ReLU34 <- Convolution36
I0818 10:49:16.822737  2412 net.cpp:509] ReLU34 -> Convolution36 (in-place)
I0818 10:49:16.823719  2412 net.cpp:172] Setting up ReLU34
I0818 10:49:16.823738  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.823742  2412 net.cpp:194] Memory required for data: 85483640
I0818 10:49:16.823747  2412 layer_factory.hpp:77] Creating layer Convolution37
I0818 10:49:16.823765  2412 net.cpp:128] Creating Layer Convolution37
I0818 10:49:16.823771  2412 net.cpp:558] Convolution37 <- Convolution36
I0818 10:49:16.823778  2412 net.cpp:522] Convolution37 -> Convolution37
I0818 10:49:16.827417  2412 net.cpp:172] Setting up Convolution37
I0818 10:49:16.827443  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.827448  2412 net.cpp:194] Memory required for data: 85647480
I0818 10:49:16.827458  2412 layer_factory.hpp:77] Creating layer BatchNorm37
I0818 10:49:16.827468  2412 net.cpp:128] Creating Layer BatchNorm37
I0818 10:49:16.827473  2412 net.cpp:558] BatchNorm37 <- Convolution37
I0818 10:49:16.827482  2412 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0818 10:49:16.827833  2412 net.cpp:172] Setting up BatchNorm37
I0818 10:49:16.827847  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.827852  2412 net.cpp:194] Memory required for data: 85811320
I0818 10:49:16.827880  2412 layer_factory.hpp:77] Creating layer Scale37
I0818 10:49:16.827888  2412 net.cpp:128] Creating Layer Scale37
I0818 10:49:16.827893  2412 net.cpp:558] Scale37 <- Convolution37
I0818 10:49:16.827899  2412 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0818 10:49:16.827960  2412 layer_factory.hpp:77] Creating layer Scale37
I0818 10:49:16.828155  2412 net.cpp:172] Setting up Scale37
I0818 10:49:16.828167  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.828171  2412 net.cpp:194] Memory required for data: 85975160
I0818 10:49:16.828179  2412 layer_factory.hpp:77] Creating layer Eltwise17
I0818 10:49:16.828188  2412 net.cpp:128] Creating Layer Eltwise17
I0818 10:49:16.828193  2412 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0818 10:49:16.828198  2412 net.cpp:558] Eltwise17 <- Convolution37
I0818 10:49:16.828220  2412 net.cpp:522] Eltwise17 -> Eltwise17
I0818 10:49:16.828258  2412 net.cpp:172] Setting up Eltwise17
I0818 10:49:16.828264  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.828269  2412 net.cpp:194] Memory required for data: 86139000
I0818 10:49:16.828272  2412 layer_factory.hpp:77] Creating layer ReLU35
I0818 10:49:16.828279  2412 net.cpp:128] Creating Layer ReLU35
I0818 10:49:16.828284  2412 net.cpp:558] ReLU35 <- Eltwise17
I0818 10:49:16.828292  2412 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0818 10:49:16.828819  2412 net.cpp:172] Setting up ReLU35
I0818 10:49:16.828840  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.828845  2412 net.cpp:194] Memory required for data: 86302840
I0818 10:49:16.828850  2412 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0818 10:49:16.828858  2412 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0818 10:49:16.828863  2412 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0818 10:49:16.828873  2412 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0818 10:49:16.828883  2412 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0818 10:49:16.828953  2412 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0818 10:49:16.828961  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.828968  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.828971  2412 net.cpp:194] Memory required for data: 86630520
I0818 10:49:16.828975  2412 layer_factory.hpp:77] Creating layer Convolution38
I0818 10:49:16.828989  2412 net.cpp:128] Creating Layer Convolution38
I0818 10:49:16.828994  2412 net.cpp:558] Convolution38 <- Eltwise17_ReLU35_0_split_0
I0818 10:49:16.829005  2412 net.cpp:522] Convolution38 -> Convolution38
I0818 10:49:16.831389  2412 net.cpp:172] Setting up Convolution38
I0818 10:49:16.831418  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.831421  2412 net.cpp:194] Memory required for data: 86794360
I0818 10:49:16.831432  2412 layer_factory.hpp:77] Creating layer BatchNorm38
I0818 10:49:16.831441  2412 net.cpp:128] Creating Layer BatchNorm38
I0818 10:49:16.831446  2412 net.cpp:558] BatchNorm38 <- Convolution38
I0818 10:49:16.831456  2412 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0818 10:49:16.831816  2412 net.cpp:172] Setting up BatchNorm38
I0818 10:49:16.831827  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.831832  2412 net.cpp:194] Memory required for data: 86958200
I0818 10:49:16.831842  2412 layer_factory.hpp:77] Creating layer Scale38
I0818 10:49:16.831852  2412 net.cpp:128] Creating Layer Scale38
I0818 10:49:16.831857  2412 net.cpp:558] Scale38 <- Convolution38
I0818 10:49:16.831862  2412 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0818 10:49:16.831923  2412 layer_factory.hpp:77] Creating layer Scale38
I0818 10:49:16.832118  2412 net.cpp:172] Setting up Scale38
I0818 10:49:16.832129  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.832134  2412 net.cpp:194] Memory required for data: 87122040
I0818 10:49:16.832141  2412 layer_factory.hpp:77] Creating layer ReLU36
I0818 10:49:16.832149  2412 net.cpp:128] Creating Layer ReLU36
I0818 10:49:16.832154  2412 net.cpp:558] ReLU36 <- Convolution38
I0818 10:49:16.832160  2412 net.cpp:509] ReLU36 -> Convolution38 (in-place)
I0818 10:49:16.832821  2412 net.cpp:172] Setting up ReLU36
I0818 10:49:16.832840  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.832845  2412 net.cpp:194] Memory required for data: 87285880
I0818 10:49:16.832850  2412 layer_factory.hpp:77] Creating layer Convolution39
I0818 10:49:16.832870  2412 net.cpp:128] Creating Layer Convolution39
I0818 10:49:16.832875  2412 net.cpp:558] Convolution39 <- Convolution38
I0818 10:49:16.832882  2412 net.cpp:522] Convolution39 -> Convolution39
I0818 10:49:16.839615  2412 net.cpp:172] Setting up Convolution39
I0818 10:49:16.839642  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.839646  2412 net.cpp:194] Memory required for data: 87449720
I0818 10:49:16.839659  2412 layer_factory.hpp:77] Creating layer BatchNorm39
I0818 10:49:16.839695  2412 net.cpp:128] Creating Layer BatchNorm39
I0818 10:49:16.839701  2412 net.cpp:558] BatchNorm39 <- Convolution39
I0818 10:49:16.839712  2412 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0818 10:49:16.840075  2412 net.cpp:172] Setting up BatchNorm39
I0818 10:49:16.840086  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.840091  2412 net.cpp:194] Memory required for data: 87613560
I0818 10:49:16.840102  2412 layer_factory.hpp:77] Creating layer Scale39
I0818 10:49:16.840109  2412 net.cpp:128] Creating Layer Scale39
I0818 10:49:16.840114  2412 net.cpp:558] Scale39 <- Convolution39
I0818 10:49:16.840121  2412 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0818 10:49:16.840186  2412 layer_factory.hpp:77] Creating layer Scale39
I0818 10:49:16.840384  2412 net.cpp:172] Setting up Scale39
I0818 10:49:16.840392  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.840396  2412 net.cpp:194] Memory required for data: 87777400
I0818 10:49:16.840404  2412 layer_factory.hpp:77] Creating layer Eltwise18
I0818 10:49:16.840415  2412 net.cpp:128] Creating Layer Eltwise18
I0818 10:49:16.840420  2412 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0818 10:49:16.840426  2412 net.cpp:558] Eltwise18 <- Convolution39
I0818 10:49:16.840432  2412 net.cpp:522] Eltwise18 -> Eltwise18
I0818 10:49:16.840471  2412 net.cpp:172] Setting up Eltwise18
I0818 10:49:16.840477  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.840481  2412 net.cpp:194] Memory required for data: 87941240
I0818 10:49:16.840486  2412 layer_factory.hpp:77] Creating layer ReLU37
I0818 10:49:16.840492  2412 net.cpp:128] Creating Layer ReLU37
I0818 10:49:16.840497  2412 net.cpp:558] ReLU37 <- Eltwise18
I0818 10:49:16.840502  2412 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0818 10:49:16.841639  2412 net.cpp:172] Setting up ReLU37
I0818 10:49:16.841657  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.841661  2412 net.cpp:194] Memory required for data: 88105080
I0818 10:49:16.841666  2412 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0818 10:49:16.841675  2412 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0818 10:49:16.841679  2412 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0818 10:49:16.841688  2412 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0818 10:49:16.841701  2412 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0818 10:49:16.841770  2412 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0818 10:49:16.841778  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.841784  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.841787  2412 net.cpp:194] Memory required for data: 88432760
I0818 10:49:16.841791  2412 layer_factory.hpp:77] Creating layer Convolution40
I0818 10:49:16.841809  2412 net.cpp:128] Creating Layer Convolution40
I0818 10:49:16.841814  2412 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_0
I0818 10:49:16.841823  2412 net.cpp:522] Convolution40 -> Convolution40
I0818 10:49:16.848716  2412 net.cpp:172] Setting up Convolution40
I0818 10:49:16.848742  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.848747  2412 net.cpp:194] Memory required for data: 88596600
I0818 10:49:16.848758  2412 layer_factory.hpp:77] Creating layer BatchNorm40
I0818 10:49:16.848769  2412 net.cpp:128] Creating Layer BatchNorm40
I0818 10:49:16.848775  2412 net.cpp:558] BatchNorm40 <- Convolution40
I0818 10:49:16.848781  2412 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0818 10:49:16.849150  2412 net.cpp:172] Setting up BatchNorm40
I0818 10:49:16.849162  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.849166  2412 net.cpp:194] Memory required for data: 88760440
I0818 10:49:16.849177  2412 layer_factory.hpp:77] Creating layer Scale40
I0818 10:49:16.849184  2412 net.cpp:128] Creating Layer Scale40
I0818 10:49:16.849189  2412 net.cpp:558] Scale40 <- Convolution40
I0818 10:49:16.849195  2412 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0818 10:49:16.849278  2412 layer_factory.hpp:77] Creating layer Scale40
I0818 10:49:16.849483  2412 net.cpp:172] Setting up Scale40
I0818 10:49:16.849490  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.849494  2412 net.cpp:194] Memory required for data: 88924280
I0818 10:49:16.849503  2412 layer_factory.hpp:77] Creating layer ReLU38
I0818 10:49:16.849509  2412 net.cpp:128] Creating Layer ReLU38
I0818 10:49:16.849514  2412 net.cpp:558] ReLU38 <- Convolution40
I0818 10:49:16.849522  2412 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0818 10:49:16.850507  2412 net.cpp:172] Setting up ReLU38
I0818 10:49:16.850531  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.850536  2412 net.cpp:194] Memory required for data: 89088120
I0818 10:49:16.850541  2412 layer_factory.hpp:77] Creating layer Convolution41
I0818 10:49:16.850555  2412 net.cpp:128] Creating Layer Convolution41
I0818 10:49:16.850561  2412 net.cpp:558] Convolution41 <- Convolution40
I0818 10:49:16.850571  2412 net.cpp:522] Convolution41 -> Convolution41
I0818 10:49:16.857280  2412 net.cpp:172] Setting up Convolution41
I0818 10:49:16.857307  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.857311  2412 net.cpp:194] Memory required for data: 89251960
I0818 10:49:16.857322  2412 layer_factory.hpp:77] Creating layer BatchNorm41
I0818 10:49:16.857333  2412 net.cpp:128] Creating Layer BatchNorm41
I0818 10:49:16.857338  2412 net.cpp:558] BatchNorm41 <- Convolution41
I0818 10:49:16.857345  2412 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0818 10:49:16.857707  2412 net.cpp:172] Setting up BatchNorm41
I0818 10:49:16.857718  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.857723  2412 net.cpp:194] Memory required for data: 89415800
I0818 10:49:16.857733  2412 layer_factory.hpp:77] Creating layer Scale41
I0818 10:49:16.857740  2412 net.cpp:128] Creating Layer Scale41
I0818 10:49:16.857744  2412 net.cpp:558] Scale41 <- Convolution41
I0818 10:49:16.857750  2412 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0818 10:49:16.857815  2412 layer_factory.hpp:77] Creating layer Scale41
I0818 10:49:16.858012  2412 net.cpp:172] Setting up Scale41
I0818 10:49:16.858026  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.858031  2412 net.cpp:194] Memory required for data: 89579640
I0818 10:49:16.858039  2412 layer_factory.hpp:77] Creating layer Eltwise19
I0818 10:49:16.858047  2412 net.cpp:128] Creating Layer Eltwise19
I0818 10:49:16.858052  2412 net.cpp:558] Eltwise19 <- Eltwise18_ReLU37_0_split_1
I0818 10:49:16.858057  2412 net.cpp:558] Eltwise19 <- Convolution41
I0818 10:49:16.858063  2412 net.cpp:522] Eltwise19 -> Eltwise19
I0818 10:49:16.858098  2412 net.cpp:172] Setting up Eltwise19
I0818 10:49:16.858105  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.858109  2412 net.cpp:194] Memory required for data: 89743480
I0818 10:49:16.858114  2412 layer_factory.hpp:77] Creating layer ReLU39
I0818 10:49:16.858124  2412 net.cpp:128] Creating Layer ReLU39
I0818 10:49:16.858127  2412 net.cpp:558] ReLU39 <- Eltwise19
I0818 10:49:16.858132  2412 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0818 10:49:16.859268  2412 net.cpp:172] Setting up ReLU39
I0818 10:49:16.859285  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.859290  2412 net.cpp:194] Memory required for data: 89907320
I0818 10:49:16.859294  2412 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0818 10:49:16.859308  2412 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0818 10:49:16.859313  2412 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0818 10:49:16.859320  2412 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0818 10:49:16.859328  2412 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0818 10:49:16.859400  2412 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0818 10:49:16.859407  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.859412  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.859416  2412 net.cpp:194] Memory required for data: 90235000
I0818 10:49:16.859436  2412 layer_factory.hpp:77] Creating layer Convolution42
I0818 10:49:16.859448  2412 net.cpp:128] Creating Layer Convolution42
I0818 10:49:16.859453  2412 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0818 10:49:16.859463  2412 net.cpp:522] Convolution42 -> Convolution42
I0818 10:49:16.866253  2412 net.cpp:172] Setting up Convolution42
I0818 10:49:16.866279  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.866284  2412 net.cpp:194] Memory required for data: 90398840
I0818 10:49:16.866294  2412 layer_factory.hpp:77] Creating layer BatchNorm42
I0818 10:49:16.866307  2412 net.cpp:128] Creating Layer BatchNorm42
I0818 10:49:16.866312  2412 net.cpp:558] BatchNorm42 <- Convolution42
I0818 10:49:16.866322  2412 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0818 10:49:16.866701  2412 net.cpp:172] Setting up BatchNorm42
I0818 10:49:16.866716  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.866721  2412 net.cpp:194] Memory required for data: 90562680
I0818 10:49:16.866732  2412 layer_factory.hpp:77] Creating layer Scale42
I0818 10:49:16.866740  2412 net.cpp:128] Creating Layer Scale42
I0818 10:49:16.866744  2412 net.cpp:558] Scale42 <- Convolution42
I0818 10:49:16.866752  2412 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0818 10:49:16.866812  2412 layer_factory.hpp:77] Creating layer Scale42
I0818 10:49:16.867027  2412 net.cpp:172] Setting up Scale42
I0818 10:49:16.867039  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.867043  2412 net.cpp:194] Memory required for data: 90726520
I0818 10:49:16.867051  2412 layer_factory.hpp:77] Creating layer ReLU40
I0818 10:49:16.867058  2412 net.cpp:128] Creating Layer ReLU40
I0818 10:49:16.867063  2412 net.cpp:558] ReLU40 <- Convolution42
I0818 10:49:16.867070  2412 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0818 10:49:16.868000  2412 net.cpp:172] Setting up ReLU40
I0818 10:49:16.868018  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.868022  2412 net.cpp:194] Memory required for data: 90890360
I0818 10:49:16.868027  2412 layer_factory.hpp:77] Creating layer Convolution43
I0818 10:49:16.868041  2412 net.cpp:128] Creating Layer Convolution43
I0818 10:49:16.868046  2412 net.cpp:558] Convolution43 <- Convolution42
I0818 10:49:16.868057  2412 net.cpp:522] Convolution43 -> Convolution43
I0818 10:49:16.874732  2412 net.cpp:172] Setting up Convolution43
I0818 10:49:16.874755  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.874759  2412 net.cpp:194] Memory required for data: 91054200
I0818 10:49:16.874774  2412 layer_factory.hpp:77] Creating layer BatchNorm43
I0818 10:49:16.874786  2412 net.cpp:128] Creating Layer BatchNorm43
I0818 10:49:16.874796  2412 net.cpp:558] BatchNorm43 <- Convolution43
I0818 10:49:16.874805  2412 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0818 10:49:16.875167  2412 net.cpp:172] Setting up BatchNorm43
I0818 10:49:16.875180  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.875185  2412 net.cpp:194] Memory required for data: 91218040
I0818 10:49:16.875195  2412 layer_factory.hpp:77] Creating layer Scale43
I0818 10:49:16.875203  2412 net.cpp:128] Creating Layer Scale43
I0818 10:49:16.875208  2412 net.cpp:558] Scale43 <- Convolution43
I0818 10:49:16.875217  2412 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0818 10:49:16.875283  2412 layer_factory.hpp:77] Creating layer Scale43
I0818 10:49:16.875488  2412 net.cpp:172] Setting up Scale43
I0818 10:49:16.875501  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.875505  2412 net.cpp:194] Memory required for data: 91381880
I0818 10:49:16.875514  2412 layer_factory.hpp:77] Creating layer Eltwise20
I0818 10:49:16.875524  2412 net.cpp:128] Creating Layer Eltwise20
I0818 10:49:16.875528  2412 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0818 10:49:16.875533  2412 net.cpp:558] Eltwise20 <- Convolution43
I0818 10:49:16.875542  2412 net.cpp:522] Eltwise20 -> Eltwise20
I0818 10:49:16.875578  2412 net.cpp:172] Setting up Eltwise20
I0818 10:49:16.875589  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.875612  2412 net.cpp:194] Memory required for data: 91545720
I0818 10:49:16.875617  2412 layer_factory.hpp:77] Creating layer ReLU41
I0818 10:49:16.875622  2412 net.cpp:128] Creating Layer ReLU41
I0818 10:49:16.875627  2412 net.cpp:558] ReLU41 <- Eltwise20
I0818 10:49:16.875635  2412 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0818 10:49:16.876180  2412 net.cpp:172] Setting up ReLU41
I0818 10:49:16.876204  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.876209  2412 net.cpp:194] Memory required for data: 91709560
I0818 10:49:16.876214  2412 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0818 10:49:16.876222  2412 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0818 10:49:16.876227  2412 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0818 10:49:16.876237  2412 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0818 10:49:16.876246  2412 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0818 10:49:16.876318  2412 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0818 10:49:16.876330  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.876336  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.876340  2412 net.cpp:194] Memory required for data: 92037240
I0818 10:49:16.876344  2412 layer_factory.hpp:77] Creating layer Convolution44
I0818 10:49:16.876358  2412 net.cpp:128] Creating Layer Convolution44
I0818 10:49:16.876363  2412 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0818 10:49:16.876372  2412 net.cpp:522] Convolution44 -> Convolution44
I0818 10:49:16.882948  2412 net.cpp:172] Setting up Convolution44
I0818 10:49:16.882971  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.882975  2412 net.cpp:194] Memory required for data: 92201080
I0818 10:49:16.882987  2412 layer_factory.hpp:77] Creating layer BatchNorm44
I0818 10:49:16.882997  2412 net.cpp:128] Creating Layer BatchNorm44
I0818 10:49:16.883003  2412 net.cpp:558] BatchNorm44 <- Convolution44
I0818 10:49:16.883010  2412 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0818 10:49:16.883379  2412 net.cpp:172] Setting up BatchNorm44
I0818 10:49:16.883393  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.883397  2412 net.cpp:194] Memory required for data: 92364920
I0818 10:49:16.883407  2412 layer_factory.hpp:77] Creating layer Scale44
I0818 10:49:16.883415  2412 net.cpp:128] Creating Layer Scale44
I0818 10:49:16.883419  2412 net.cpp:558] Scale44 <- Convolution44
I0818 10:49:16.883425  2412 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0818 10:49:16.883491  2412 layer_factory.hpp:77] Creating layer Scale44
I0818 10:49:16.883692  2412 net.cpp:172] Setting up Scale44
I0818 10:49:16.883707  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.883710  2412 net.cpp:194] Memory required for data: 92528760
I0818 10:49:16.883718  2412 layer_factory.hpp:77] Creating layer ReLU42
I0818 10:49:16.883724  2412 net.cpp:128] Creating Layer ReLU42
I0818 10:49:16.883729  2412 net.cpp:558] ReLU42 <- Convolution44
I0818 10:49:16.883734  2412 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0818 10:49:16.884670  2412 net.cpp:172] Setting up ReLU42
I0818 10:49:16.884690  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.884696  2412 net.cpp:194] Memory required for data: 92692600
I0818 10:49:16.884701  2412 layer_factory.hpp:77] Creating layer Convolution45
I0818 10:49:16.884716  2412 net.cpp:128] Creating Layer Convolution45
I0818 10:49:16.884722  2412 net.cpp:558] Convolution45 <- Convolution44
I0818 10:49:16.884732  2412 net.cpp:522] Convolution45 -> Convolution45
I0818 10:49:16.891412  2412 net.cpp:172] Setting up Convolution45
I0818 10:49:16.891441  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.891446  2412 net.cpp:194] Memory required for data: 92856440
I0818 10:49:16.891463  2412 layer_factory.hpp:77] Creating layer BatchNorm45
I0818 10:49:16.891472  2412 net.cpp:128] Creating Layer BatchNorm45
I0818 10:49:16.891477  2412 net.cpp:558] BatchNorm45 <- Convolution45
I0818 10:49:16.891508  2412 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0818 10:49:16.891868  2412 net.cpp:172] Setting up BatchNorm45
I0818 10:49:16.891880  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.891883  2412 net.cpp:194] Memory required for data: 93020280
I0818 10:49:16.891893  2412 layer_factory.hpp:77] Creating layer Scale45
I0818 10:49:16.891908  2412 net.cpp:128] Creating Layer Scale45
I0818 10:49:16.891913  2412 net.cpp:558] Scale45 <- Convolution45
I0818 10:49:16.891921  2412 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0818 10:49:16.891980  2412 layer_factory.hpp:77] Creating layer Scale45
I0818 10:49:16.892189  2412 net.cpp:172] Setting up Scale45
I0818 10:49:16.892199  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.892204  2412 net.cpp:194] Memory required for data: 93184120
I0818 10:49:16.892211  2412 layer_factory.hpp:77] Creating layer Eltwise21
I0818 10:49:16.892221  2412 net.cpp:128] Creating Layer Eltwise21
I0818 10:49:16.892231  2412 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0818 10:49:16.892237  2412 net.cpp:558] Eltwise21 <- Convolution45
I0818 10:49:16.892243  2412 net.cpp:522] Eltwise21 -> Eltwise21
I0818 10:49:16.892279  2412 net.cpp:172] Setting up Eltwise21
I0818 10:49:16.892287  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.892290  2412 net.cpp:194] Memory required for data: 93347960
I0818 10:49:16.892295  2412 layer_factory.hpp:77] Creating layer ReLU43
I0818 10:49:16.892302  2412 net.cpp:128] Creating Layer ReLU43
I0818 10:49:16.892307  2412 net.cpp:558] ReLU43 <- Eltwise21
I0818 10:49:16.892314  2412 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0818 10:49:16.893411  2412 net.cpp:172] Setting up ReLU43
I0818 10:49:16.893429  2412 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:49:16.893434  2412 net.cpp:194] Memory required for data: 93511800
I0818 10:49:16.893438  2412 layer_factory.hpp:77] Creating layer Pooling1
I0818 10:49:16.893448  2412 net.cpp:128] Creating Layer Pooling1
I0818 10:49:16.893453  2412 net.cpp:558] Pooling1 <- Eltwise21
I0818 10:49:16.893465  2412 net.cpp:522] Pooling1 -> Pooling1
I0818 10:49:16.895617  2412 net.cpp:172] Setting up Pooling1
I0818 10:49:16.895638  2412 net.cpp:186] Top shape: 10 64 1 1 (640)
I0818 10:49:16.895642  2412 net.cpp:194] Memory required for data: 93514360
I0818 10:49:16.895648  2412 layer_factory.hpp:77] Creating layer InnerProduct1
I0818 10:49:16.895659  2412 net.cpp:128] Creating Layer InnerProduct1
I0818 10:49:16.895665  2412 net.cpp:558] InnerProduct1 <- Pooling1
I0818 10:49:16.895673  2412 net.cpp:522] InnerProduct1 -> InnerProduct1
I0818 10:49:16.895889  2412 net.cpp:172] Setting up InnerProduct1
I0818 10:49:16.895901  2412 net.cpp:186] Top shape: 10 10 (100)
I0818 10:49:16.895905  2412 net.cpp:194] Memory required for data: 93514760
I0818 10:49:16.895915  2412 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0818 10:49:16.895923  2412 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0818 10:49:16.895928  2412 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0818 10:49:16.895936  2412 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0818 10:49:16.895943  2412 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0818 10:49:16.896003  2412 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0818 10:49:16.896010  2412 net.cpp:186] Top shape: 10 10 (100)
I0818 10:49:16.896015  2412 net.cpp:186] Top shape: 10 10 (100)
I0818 10:49:16.896019  2412 net.cpp:194] Memory required for data: 93515560
I0818 10:49:16.896023  2412 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:49:16.896035  2412 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0818 10:49:16.896040  2412 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0818 10:49:16.896045  2412 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0818 10:49:16.896052  2412 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0818 10:49:16.896080  2412 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:49:16.897977  2412 net.cpp:172] Setting up SoftmaxWithLoss1
I0818 10:49:16.898002  2412 net.cpp:186] Top shape: (1)
I0818 10:49:16.898006  2412 net.cpp:189]     with loss weight 1
I0818 10:49:16.898025  2412 net.cpp:194] Memory required for data: 93515564
I0818 10:49:16.898030  2412 layer_factory.hpp:77] Creating layer Accuracy1
I0818 10:49:16.898046  2412 net.cpp:128] Creating Layer Accuracy1
I0818 10:49:16.898052  2412 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0818 10:49:16.898058  2412 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0818 10:49:16.898067  2412 net.cpp:522] Accuracy1 -> Accuracy1
I0818 10:49:16.898078  2412 net.cpp:172] Setting up Accuracy1
I0818 10:49:16.898084  2412 net.cpp:186] Top shape: (1)
I0818 10:49:16.898088  2412 net.cpp:194] Memory required for data: 93515568
I0818 10:49:16.898093  2412 net.cpp:303] Accuracy1 does not need backward computation.
I0818 10:49:16.898098  2412 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0818 10:49:16.898103  2412 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0818 10:49:16.898108  2412 net.cpp:301] InnerProduct1 needs backward computation.
I0818 10:49:16.898113  2412 net.cpp:301] Pooling1 needs backward computation.
I0818 10:49:16.898116  2412 net.cpp:301] ReLU43 needs backward computation.
I0818 10:49:16.898120  2412 net.cpp:301] Eltwise21 needs backward computation.
I0818 10:49:16.898125  2412 net.cpp:301] Scale45 needs backward computation.
I0818 10:49:16.898129  2412 net.cpp:301] BatchNorm45 needs backward computation.
I0818 10:49:16.898133  2412 net.cpp:301] Convolution45 needs backward computation.
I0818 10:49:16.898138  2412 net.cpp:301] ReLU42 needs backward computation.
I0818 10:49:16.898142  2412 net.cpp:301] Scale44 needs backward computation.
I0818 10:49:16.898146  2412 net.cpp:301] BatchNorm44 needs backward computation.
I0818 10:49:16.898150  2412 net.cpp:301] Convolution44 needs backward computation.
I0818 10:49:16.898154  2412 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0818 10:49:16.898159  2412 net.cpp:301] ReLU41 needs backward computation.
I0818 10:49:16.898164  2412 net.cpp:301] Eltwise20 needs backward computation.
I0818 10:49:16.898169  2412 net.cpp:301] Scale43 needs backward computation.
I0818 10:49:16.898172  2412 net.cpp:301] BatchNorm43 needs backward computation.
I0818 10:49:16.898176  2412 net.cpp:301] Convolution43 needs backward computation.
I0818 10:49:16.898181  2412 net.cpp:301] ReLU40 needs backward computation.
I0818 10:49:16.898185  2412 net.cpp:301] Scale42 needs backward computation.
I0818 10:49:16.898190  2412 net.cpp:301] BatchNorm42 needs backward computation.
I0818 10:49:16.898195  2412 net.cpp:301] Convolution42 needs backward computation.
I0818 10:49:16.898200  2412 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0818 10:49:16.898205  2412 net.cpp:301] ReLU39 needs backward computation.
I0818 10:49:16.898208  2412 net.cpp:301] Eltwise19 needs backward computation.
I0818 10:49:16.898213  2412 net.cpp:301] Scale41 needs backward computation.
I0818 10:49:16.898218  2412 net.cpp:301] BatchNorm41 needs backward computation.
I0818 10:49:16.898224  2412 net.cpp:301] Convolution41 needs backward computation.
I0818 10:49:16.898228  2412 net.cpp:301] ReLU38 needs backward computation.
I0818 10:49:16.898233  2412 net.cpp:301] Scale40 needs backward computation.
I0818 10:49:16.898237  2412 net.cpp:301] BatchNorm40 needs backward computation.
I0818 10:49:16.898242  2412 net.cpp:301] Convolution40 needs backward computation.
I0818 10:49:16.898247  2412 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0818 10:49:16.898252  2412 net.cpp:301] ReLU37 needs backward computation.
I0818 10:49:16.898257  2412 net.cpp:301] Eltwise18 needs backward computation.
I0818 10:49:16.898262  2412 net.cpp:301] Scale39 needs backward computation.
I0818 10:49:16.898265  2412 net.cpp:301] BatchNorm39 needs backward computation.
I0818 10:49:16.898270  2412 net.cpp:301] Convolution39 needs backward computation.
I0818 10:49:16.898289  2412 net.cpp:301] ReLU36 needs backward computation.
I0818 10:49:16.898294  2412 net.cpp:301] Scale38 needs backward computation.
I0818 10:49:16.898299  2412 net.cpp:301] BatchNorm38 needs backward computation.
I0818 10:49:16.898303  2412 net.cpp:301] Convolution38 needs backward computation.
I0818 10:49:16.898309  2412 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0818 10:49:16.898313  2412 net.cpp:301] ReLU35 needs backward computation.
I0818 10:49:16.898319  2412 net.cpp:301] Eltwise17 needs backward computation.
I0818 10:49:16.898324  2412 net.cpp:301] Scale37 needs backward computation.
I0818 10:49:16.898329  2412 net.cpp:301] BatchNorm37 needs backward computation.
I0818 10:49:16.898332  2412 net.cpp:301] Convolution37 needs backward computation.
I0818 10:49:16.898342  2412 net.cpp:301] ReLU34 needs backward computation.
I0818 10:49:16.898346  2412 net.cpp:301] Scale36 needs backward computation.
I0818 10:49:16.898351  2412 net.cpp:301] BatchNorm36 needs backward computation.
I0818 10:49:16.898355  2412 net.cpp:301] Convolution36 needs backward computation.
I0818 10:49:16.898360  2412 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0818 10:49:16.898365  2412 net.cpp:301] ReLU33 needs backward computation.
I0818 10:49:16.898371  2412 net.cpp:301] Eltwise16 needs backward computation.
I0818 10:49:16.898376  2412 net.cpp:301] Scale35 needs backward computation.
I0818 10:49:16.898381  2412 net.cpp:301] BatchNorm35 needs backward computation.
I0818 10:49:16.898386  2412 net.cpp:301] Convolution35 needs backward computation.
I0818 10:49:16.898391  2412 net.cpp:301] ReLU32 needs backward computation.
I0818 10:49:16.898394  2412 net.cpp:301] Scale34 needs backward computation.
I0818 10:49:16.898399  2412 net.cpp:301] BatchNorm34 needs backward computation.
I0818 10:49:16.898403  2412 net.cpp:301] Convolution34 needs backward computation.
I0818 10:49:16.898408  2412 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0818 10:49:16.898413  2412 net.cpp:301] ReLU31 needs backward computation.
I0818 10:49:16.898417  2412 net.cpp:301] Eltwise15 needs backward computation.
I0818 10:49:16.898423  2412 net.cpp:301] Scale33 needs backward computation.
I0818 10:49:16.898427  2412 net.cpp:301] BatchNorm33 needs backward computation.
I0818 10:49:16.898432  2412 net.cpp:301] Convolution33 needs backward computation.
I0818 10:49:16.898437  2412 net.cpp:301] ReLU30 needs backward computation.
I0818 10:49:16.898442  2412 net.cpp:301] Scale32 needs backward computation.
I0818 10:49:16.898445  2412 net.cpp:301] BatchNorm32 needs backward computation.
I0818 10:49:16.898449  2412 net.cpp:301] Convolution32 needs backward computation.
I0818 10:49:16.898454  2412 net.cpp:301] Scale31 needs backward computation.
I0818 10:49:16.898459  2412 net.cpp:301] BatchNorm31 needs backward computation.
I0818 10:49:16.898463  2412 net.cpp:301] Convolution31 needs backward computation.
I0818 10:49:16.898468  2412 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0818 10:49:16.898473  2412 net.cpp:301] ReLU29 needs backward computation.
I0818 10:49:16.898478  2412 net.cpp:301] Eltwise14 needs backward computation.
I0818 10:49:16.898484  2412 net.cpp:301] Scale30 needs backward computation.
I0818 10:49:16.898489  2412 net.cpp:301] BatchNorm30 needs backward computation.
I0818 10:49:16.898494  2412 net.cpp:301] Convolution30 needs backward computation.
I0818 10:49:16.898497  2412 net.cpp:301] ReLU28 needs backward computation.
I0818 10:49:16.898502  2412 net.cpp:301] Scale29 needs backward computation.
I0818 10:49:16.898506  2412 net.cpp:301] BatchNorm29 needs backward computation.
I0818 10:49:16.898511  2412 net.cpp:301] Convolution29 needs backward computation.
I0818 10:49:16.898516  2412 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0818 10:49:16.898524  2412 net.cpp:301] ReLU27 needs backward computation.
I0818 10:49:16.898528  2412 net.cpp:301] Eltwise13 needs backward computation.
I0818 10:49:16.898540  2412 net.cpp:301] Scale28 needs backward computation.
I0818 10:49:16.898545  2412 net.cpp:301] BatchNorm28 needs backward computation.
I0818 10:49:16.898550  2412 net.cpp:301] Convolution28 needs backward computation.
I0818 10:49:16.898555  2412 net.cpp:301] ReLU26 needs backward computation.
I0818 10:49:16.898558  2412 net.cpp:301] Scale27 needs backward computation.
I0818 10:49:16.898563  2412 net.cpp:301] BatchNorm27 needs backward computation.
I0818 10:49:16.898567  2412 net.cpp:301] Convolution27 needs backward computation.
I0818 10:49:16.898573  2412 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0818 10:49:16.898578  2412 net.cpp:301] ReLU25 needs backward computation.
I0818 10:49:16.898583  2412 net.cpp:301] Eltwise12 needs backward computation.
I0818 10:49:16.898588  2412 net.cpp:301] Scale26 needs backward computation.
I0818 10:49:16.898592  2412 net.cpp:301] BatchNorm26 needs backward computation.
I0818 10:49:16.898597  2412 net.cpp:301] Convolution26 needs backward computation.
I0818 10:49:16.898602  2412 net.cpp:301] ReLU24 needs backward computation.
I0818 10:49:16.898607  2412 net.cpp:301] Scale25 needs backward computation.
I0818 10:49:16.898612  2412 net.cpp:301] BatchNorm25 needs backward computation.
I0818 10:49:16.898615  2412 net.cpp:301] Convolution25 needs backward computation.
I0818 10:49:16.898620  2412 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0818 10:49:16.898625  2412 net.cpp:301] ReLU23 needs backward computation.
I0818 10:49:16.898629  2412 net.cpp:301] Eltwise11 needs backward computation.
I0818 10:49:16.898634  2412 net.cpp:301] Scale24 needs backward computation.
I0818 10:49:16.898639  2412 net.cpp:301] BatchNorm24 needs backward computation.
I0818 10:49:16.898643  2412 net.cpp:301] Convolution24 needs backward computation.
I0818 10:49:16.898680  2412 net.cpp:301] ReLU22 needs backward computation.
I0818 10:49:16.898685  2412 net.cpp:301] Scale23 needs backward computation.
I0818 10:49:16.898690  2412 net.cpp:301] BatchNorm23 needs backward computation.
I0818 10:49:16.898694  2412 net.cpp:301] Convolution23 needs backward computation.
I0818 10:49:16.898699  2412 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0818 10:49:16.898705  2412 net.cpp:301] ReLU21 needs backward computation.
I0818 10:49:16.898710  2412 net.cpp:301] Eltwise10 needs backward computation.
I0818 10:49:16.898715  2412 net.cpp:301] Scale22 needs backward computation.
I0818 10:49:16.898720  2412 net.cpp:301] BatchNorm22 needs backward computation.
I0818 10:49:16.898723  2412 net.cpp:301] Convolution22 needs backward computation.
I0818 10:49:16.898728  2412 net.cpp:301] ReLU20 needs backward computation.
I0818 10:49:16.898733  2412 net.cpp:301] Scale21 needs backward computation.
I0818 10:49:16.898737  2412 net.cpp:301] BatchNorm21 needs backward computation.
I0818 10:49:16.898742  2412 net.cpp:301] Convolution21 needs backward computation.
I0818 10:49:16.898746  2412 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0818 10:49:16.898752  2412 net.cpp:301] ReLU19 needs backward computation.
I0818 10:49:16.898757  2412 net.cpp:301] Eltwise9 needs backward computation.
I0818 10:49:16.898762  2412 net.cpp:301] Scale20 needs backward computation.
I0818 10:49:16.898767  2412 net.cpp:301] BatchNorm20 needs backward computation.
I0818 10:49:16.898772  2412 net.cpp:301] Convolution20 needs backward computation.
I0818 10:49:16.898777  2412 net.cpp:301] ReLU18 needs backward computation.
I0818 10:49:16.898782  2412 net.cpp:301] Scale19 needs backward computation.
I0818 10:49:16.898785  2412 net.cpp:301] BatchNorm19 needs backward computation.
I0818 10:49:16.898790  2412 net.cpp:301] Convolution19 needs backward computation.
I0818 10:49:16.898798  2412 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0818 10:49:16.898803  2412 net.cpp:301] ReLU17 needs backward computation.
I0818 10:49:16.898808  2412 net.cpp:301] Eltwise8 needs backward computation.
I0818 10:49:16.898814  2412 net.cpp:301] Scale18 needs backward computation.
I0818 10:49:16.898826  2412 net.cpp:301] BatchNorm18 needs backward computation.
I0818 10:49:16.898831  2412 net.cpp:301] Convolution18 needs backward computation.
I0818 10:49:16.898836  2412 net.cpp:301] ReLU16 needs backward computation.
I0818 10:49:16.898841  2412 net.cpp:301] Scale17 needs backward computation.
I0818 10:49:16.898846  2412 net.cpp:301] BatchNorm17 needs backward computation.
I0818 10:49:16.898850  2412 net.cpp:301] Convolution17 needs backward computation.
I0818 10:49:16.898855  2412 net.cpp:301] Scale16 needs backward computation.
I0818 10:49:16.898861  2412 net.cpp:301] BatchNorm16 needs backward computation.
I0818 10:49:16.898865  2412 net.cpp:301] Convolution16 needs backward computation.
I0818 10:49:16.898870  2412 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0818 10:49:16.898875  2412 net.cpp:301] ReLU15 needs backward computation.
I0818 10:49:16.898880  2412 net.cpp:301] Eltwise7 needs backward computation.
I0818 10:49:16.898885  2412 net.cpp:301] Scale15 needs backward computation.
I0818 10:49:16.898890  2412 net.cpp:301] BatchNorm15 needs backward computation.
I0818 10:49:16.898895  2412 net.cpp:301] Convolution15 needs backward computation.
I0818 10:49:16.898900  2412 net.cpp:301] ReLU14 needs backward computation.
I0818 10:49:16.898905  2412 net.cpp:301] Scale14 needs backward computation.
I0818 10:49:16.898908  2412 net.cpp:301] BatchNorm14 needs backward computation.
I0818 10:49:16.898913  2412 net.cpp:301] Convolution14 needs backward computation.
I0818 10:49:16.898918  2412 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0818 10:49:16.898923  2412 net.cpp:301] ReLU13 needs backward computation.
I0818 10:49:16.898928  2412 net.cpp:301] Eltwise6 needs backward computation.
I0818 10:49:16.898934  2412 net.cpp:301] Scale13 needs backward computation.
I0818 10:49:16.898938  2412 net.cpp:301] BatchNorm13 needs backward computation.
I0818 10:49:16.898943  2412 net.cpp:301] Convolution13 needs backward computation.
I0818 10:49:16.898947  2412 net.cpp:301] ReLU12 needs backward computation.
I0818 10:49:16.898952  2412 net.cpp:301] Scale12 needs backward computation.
I0818 10:49:16.898957  2412 net.cpp:301] BatchNorm12 needs backward computation.
I0818 10:49:16.898962  2412 net.cpp:301] Convolution12 needs backward computation.
I0818 10:49:16.898965  2412 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0818 10:49:16.898972  2412 net.cpp:301] ReLU11 needs backward computation.
I0818 10:49:16.898978  2412 net.cpp:301] Eltwise5 needs backward computation.
I0818 10:49:16.898983  2412 net.cpp:301] Scale11 needs backward computation.
I0818 10:49:16.898988  2412 net.cpp:301] BatchNorm11 needs backward computation.
I0818 10:49:16.898993  2412 net.cpp:301] Convolution11 needs backward computation.
I0818 10:49:16.898998  2412 net.cpp:301] ReLU10 needs backward computation.
I0818 10:49:16.899003  2412 net.cpp:301] Scale10 needs backward computation.
I0818 10:49:16.899008  2412 net.cpp:301] BatchNorm10 needs backward computation.
I0818 10:49:16.899011  2412 net.cpp:301] Convolution10 needs backward computation.
I0818 10:49:16.899018  2412 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0818 10:49:16.899022  2412 net.cpp:301] ReLU9 needs backward computation.
I0818 10:49:16.899026  2412 net.cpp:301] Eltwise4 needs backward computation.
I0818 10:49:16.899034  2412 net.cpp:301] Scale9 needs backward computation.
I0818 10:49:16.899037  2412 net.cpp:301] BatchNorm9 needs backward computation.
I0818 10:49:16.899042  2412 net.cpp:301] Convolution9 needs backward computation.
I0818 10:49:16.899047  2412 net.cpp:301] ReLU8 needs backward computation.
I0818 10:49:16.899052  2412 net.cpp:301] Scale8 needs backward computation.
I0818 10:49:16.899056  2412 net.cpp:301] BatchNorm8 needs backward computation.
I0818 10:49:16.899061  2412 net.cpp:301] Convolution8 needs backward computation.
I0818 10:49:16.899066  2412 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0818 10:49:16.899071  2412 net.cpp:301] ReLU7 needs backward computation.
I0818 10:49:16.899083  2412 net.cpp:301] Eltwise3 needs backward computation.
I0818 10:49:16.899089  2412 net.cpp:301] Scale7 needs backward computation.
I0818 10:49:16.899094  2412 net.cpp:301] BatchNorm7 needs backward computation.
I0818 10:49:16.899099  2412 net.cpp:301] Convolution7 needs backward computation.
I0818 10:49:16.899103  2412 net.cpp:301] ReLU6 needs backward computation.
I0818 10:49:16.899108  2412 net.cpp:301] Scale6 needs backward computation.
I0818 10:49:16.899112  2412 net.cpp:301] BatchNorm6 needs backward computation.
I0818 10:49:16.899117  2412 net.cpp:301] Convolution6 needs backward computation.
I0818 10:49:16.899122  2412 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0818 10:49:16.899127  2412 net.cpp:301] ReLU5 needs backward computation.
I0818 10:49:16.899132  2412 net.cpp:301] Eltwise2 needs backward computation.
I0818 10:49:16.899137  2412 net.cpp:301] Scale5 needs backward computation.
I0818 10:49:16.899142  2412 net.cpp:301] BatchNorm5 needs backward computation.
I0818 10:49:16.899147  2412 net.cpp:301] Convolution5 needs backward computation.
I0818 10:49:16.899152  2412 net.cpp:301] ReLU4 needs backward computation.
I0818 10:49:16.899155  2412 net.cpp:301] Scale4 needs backward computation.
I0818 10:49:16.899160  2412 net.cpp:301] BatchNorm4 needs backward computation.
I0818 10:49:16.899164  2412 net.cpp:301] Convolution4 needs backward computation.
I0818 10:49:16.899170  2412 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0818 10:49:16.899175  2412 net.cpp:301] ReLU3 needs backward computation.
I0818 10:49:16.899179  2412 net.cpp:301] Eltwise1 needs backward computation.
I0818 10:49:16.899185  2412 net.cpp:301] Scale3 needs backward computation.
I0818 10:49:16.899190  2412 net.cpp:301] BatchNorm3 needs backward computation.
I0818 10:49:16.899194  2412 net.cpp:301] Convolution3 needs backward computation.
I0818 10:49:16.899199  2412 net.cpp:301] ReLU2 needs backward computation.
I0818 10:49:16.899204  2412 net.cpp:301] Scale2 needs backward computation.
I0818 10:49:16.899209  2412 net.cpp:301] BatchNorm2 needs backward computation.
I0818 10:49:16.899214  2412 net.cpp:301] Convolution2 needs backward computation.
I0818 10:49:16.899220  2412 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0818 10:49:16.899224  2412 net.cpp:301] ReLU1 needs backward computation.
I0818 10:49:16.899230  2412 net.cpp:301] Scale1 needs backward computation.
I0818 10:49:16.899240  2412 net.cpp:301] BatchNorm1 needs backward computation.
I0818 10:49:16.899245  2412 net.cpp:301] Convolution1 needs backward computation.
I0818 10:49:16.899250  2412 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0818 10:49:16.899255  2412 net.cpp:303] Data1 does not need backward computation.
I0818 10:49:16.899260  2412 net.cpp:348] This network produces output Accuracy1
I0818 10:49:16.899263  2412 net.cpp:348] This network produces output SoftmaxWithLoss1
I0818 10:49:16.899374  2412 net.cpp:363] Network initialization done.
I0818 10:49:16.900334  2412 solver.cpp:110] Solver scaffolding done.
I0818 10:49:16.916229  2412 caffe.cpp:313] Starting Optimization
I0818 10:49:16.916261  2412 solver.cpp:425] Solving resnet_cifar10
I0818 10:49:16.916266  2412 solver.cpp:427] Learning Rate Policy: multistep
I0818 10:49:16.924029  2412 solver.cpp:514] Iteration 0, Testing net (#0)
I0818 10:49:51.152621  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:49:51.232201  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0
I0818 10:49:51.232254  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.30259 (* 1 = 2.30259 loss)
I0818 10:49:51.726817  2412 solver.cpp:357] Iteration 0 (0 iter/s, 34.8115s/100 iters), loss = 3.44698
I0818 10:49:51.726882  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 3.44698 (* 1 = 3.44698 loss)
I0818 10:49:51.726918  2412 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0818 10:50:33.243542  2412 solver.cpp:357] Iteration 100 (2.40864 iter/s, 41.5173s/100 iters), loss = 1.8725
I0818 10:50:33.243777  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.8725 (* 1 = 1.8725 loss)
I0818 10:50:33.243789  2412 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0818 10:51:22.581697  2412 solver.cpp:357] Iteration 200 (2.02689 iter/s, 49.3366s/100 iters), loss = 1.59752
I0818 10:51:22.581851  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.59752 (* 1 = 1.59752 loss)
I0818 10:51:22.581866  2412 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0818 10:52:11.945287  2412 solver.cpp:357] Iteration 300 (2.02579 iter/s, 49.3634s/100 iters), loss = 1.6047
I0818 10:52:11.945432  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.6047 (* 1 = 1.6047 loss)
I0818 10:52:11.945442  2412 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0818 10:52:54.521690  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:53:01.318296  2412 solver.cpp:357] Iteration 400 (2.02538 iter/s, 49.3734s/100 iters), loss = 1.46429
I0818 10:53:01.318374  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.46429 (* 1 = 1.46429 loss)
I0818 10:53:01.318385  2412 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0818 10:53:42.313498  2412 solver.cpp:514] Iteration 500, Testing net (#0)
I0818 10:54:16.122149  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:54:16.211653  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1367
I0818 10:54:16.211704  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.4943 (* 1 = 2.4943 loss)
I0818 10:54:16.642357  2412 solver.cpp:357] Iteration 500 (1.32761 iter/s, 75.3236s/100 iters), loss = 1.3672
I0818 10:54:16.642426  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.3672 (* 1 = 1.3672 loss)
I0818 10:54:16.642436  2412 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0818 10:55:06.207314  2412 solver.cpp:357] Iteration 600 (2.0175 iter/s, 49.5662s/100 iters), loss = 1.29525
I0818 10:55:06.207427  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.29525 (* 1 = 1.29525 loss)
I0818 10:55:06.207437  2412 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0818 10:55:55.578670  2412 solver.cpp:357] Iteration 700 (2.02541 iter/s, 49.3727s/100 iters), loss = 0.993186
I0818 10:55:55.578811  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.993186 (* 1 = 0.993186 loss)
I0818 10:55:55.578822  2412 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0818 10:56:33.903326  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:56:44.483675  2412 solver.cpp:357] Iteration 800 (2.04472 iter/s, 48.9064s/100 iters), loss = 1.01585
I0818 10:56:44.483736  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.01585 (* 1 = 1.01585 loss)
I0818 10:56:44.483747  2412 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0818 10:57:24.448851  2412 solver.cpp:357] Iteration 900 (2.5021 iter/s, 39.9664s/100 iters), loss = 1.07295
I0818 10:57:24.448998  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.07295 (* 1 = 1.07295 loss)
I0818 10:57:24.449014  2412 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0818 10:58:13.486186  2412 solver.cpp:514] Iteration 1000, Testing net (#0)
I0818 10:58:52.808864  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:58:52.999321  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.111399
I0818 10:58:52.999476  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.65408 (* 1 = 2.65408 loss)
I0818 10:58:53.550794  2412 solver.cpp:357] Iteration 1000 (1.1223 iter/s, 89.1029s/100 iters), loss = 0.934372
I0818 10:58:53.550859  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.934372 (* 1 = 0.934372 loss)
I0818 10:58:53.550870  2412 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0818 11:00:04.297420  2412 solver.cpp:357] Iteration 1100 (1.41341 iter/s, 70.7511s/100 iters), loss = 0.793253
I0818 11:00:04.297658  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.793253 (* 1 = 0.793253 loss)
I0818 11:00:04.297670  2412 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0818 11:00:54.107563  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:01:15.551360  2412 solver.cpp:357] Iteration 1200 (1.40336 iter/s, 71.2575s/100 iters), loss = 0.916132
I0818 11:01:15.551427  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.916132 (* 1 = 0.916132 loss)
I0818 11:01:15.551437  2412 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0818 11:02:28.950971  2412 solver.cpp:357] Iteration 1300 (1.36234 iter/s, 73.4032s/100 iters), loss = 0.67913
I0818 11:02:28.951090  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.67913 (* 1 = 0.67913 loss)
I0818 11:02:28.951118  2412 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0818 11:03:44.632902  2412 solver.cpp:357] Iteration 1400 (1.32131 iter/s, 75.6824s/100 iters), loss = 0.74913
I0818 11:03:44.633083  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.74913 (* 1 = 0.74913 loss)
I0818 11:03:44.633111  2412 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0818 11:04:59.410473  2412 solver.cpp:514] Iteration 1500, Testing net (#0)
I0818 11:05:55.395066  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:05:55.616027  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.102
I0818 11:05:55.616086  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.61769 (* 1 = 2.61769 loss)
I0818 11:05:56.250020  2412 solver.cpp:357] Iteration 1500 (0.759738 iter/s, 131.624s/100 iters), loss = 0.782648
I0818 11:05:56.250087  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.782648 (* 1 = 0.782648 loss)
I0818 11:05:56.250097  2412 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0818 11:06:35.584165  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:07:04.742759  2412 solver.cpp:357] Iteration 1600 (1.45993 iter/s, 68.4962s/100 iters), loss = 0.716785
I0818 11:07:04.742836  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.716785 (* 1 = 0.716785 loss)
I0818 11:07:04.742848  2412 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0818 11:08:20.031610  2412 solver.cpp:357] Iteration 1700 (1.32823 iter/s, 75.2883s/100 iters), loss = 0.880523
I0818 11:08:20.031788  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.880523 (* 1 = 0.880523 loss)
I0818 11:08:20.031801  2412 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0818 11:09:30.759124  2412 solver.cpp:357] Iteration 1800 (1.4139 iter/s, 70.7265s/100 iters), loss = 0.782173
I0818 11:09:30.759279  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.782173 (* 1 = 0.782173 loss)
I0818 11:09:30.759291  2412 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0818 11:10:46.020046  2412 solver.cpp:357] Iteration 1900 (1.32872 iter/s, 75.2601s/100 iters), loss = 0.661874
I0818 11:10:46.020215  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.661874 (* 1 = 0.661874 loss)
I0818 11:10:46.020238  2412 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0818 11:11:23.159158  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:11:55.982026  2412 solver.cpp:514] Iteration 2000, Testing net (#0)
I0818 11:12:49.566305  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:12:49.869014  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.117199
I0818 11:12:49.869078  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.43242 (* 1 = 2.43242 loss)
I0818 11:12:50.445016  2412 solver.cpp:357] Iteration 2000 (0.803675 iter/s, 124.428s/100 iters), loss = 0.638835
I0818 11:12:50.445085  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.638835 (* 1 = 0.638835 loss)
I0818 11:12:50.445094  2412 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0818 11:14:05.596858  2412 solver.cpp:357] Iteration 2100 (1.33062 iter/s, 75.1529s/100 iters), loss = 0.682307
I0818 11:14:05.597136  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.682307 (* 1 = 0.682307 loss)
I0818 11:14:05.597167  2412 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0818 11:15:20.697624  2412 solver.cpp:357] Iteration 2200 (1.33154 iter/s, 75.1012s/100 iters), loss = 0.673644
I0818 11:15:20.697808  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.673644 (* 1 = 0.673644 loss)
I0818 11:15:20.697820  2412 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0818 11:16:36.328101  2412 solver.cpp:357] Iteration 2300 (1.3222 iter/s, 75.6314s/100 iters), loss = 0.584803
I0818 11:16:36.328296  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.584803 (* 1 = 0.584803 loss)
I0818 11:16:36.328322  2412 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0818 11:17:02.896667  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:17:44.966353  2412 solver.cpp:357] Iteration 2400 (1.45697 iter/s, 68.6358s/100 iters), loss = 0.569328
I0818 11:17:44.966512  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.569328 (* 1 = 0.569328 loss)
I0818 11:17:44.966524  2412 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0818 11:18:56.506425  2412 solver.cpp:514] Iteration 2500, Testing net (#0)
I0818 11:19:50.929181  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:19:51.130017  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1419
I0818 11:19:51.130069  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.37336 (* 1 = 2.37336 loss)
I0818 11:19:51.655553  2412 solver.cpp:357] Iteration 2500 (0.789323 iter/s, 126.691s/100 iters), loss = 0.634928
I0818 11:19:51.655638  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.634928 (* 1 = 0.634928 loss)
I0818 11:19:51.655649  2412 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0818 11:21:07.084991  2412 solver.cpp:357] Iteration 2600 (1.32574 iter/s, 75.4296s/100 iters), loss = 0.758132
I0818 11:21:07.085161  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.758132 (* 1 = 0.758132 loss)
I0818 11:21:07.085186  2412 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0818 11:22:19.056069  2412 solver.cpp:357] Iteration 2700 (1.38944 iter/s, 71.9712s/100 iters), loss = 0.782223
I0818 11:22:19.056258  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.782223 (* 1 = 0.782223 loss)
I0818 11:22:19.056288  2412 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0818 11:22:38.736726  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:23:30.647372  2412 solver.cpp:357] Iteration 2800 (1.39681 iter/s, 71.5915s/100 iters), loss = 0.532908
I0818 11:23:30.647511  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.532908 (* 1 = 0.532908 loss)
I0818 11:23:30.647521  2412 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0818 11:24:45.874567  2412 solver.cpp:357] Iteration 2900 (1.32926 iter/s, 75.2296s/100 iters), loss = 0.684358
I0818 11:24:45.874753  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.684358 (* 1 = 0.684358 loss)
I0818 11:24:45.874765  2412 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0818 11:26:00.792066  2412 solver.cpp:514] Iteration 3000, Testing net (#0)
I0818 11:26:56.860405  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:26:57.080963  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1213
I0818 11:26:57.081025  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.34082 (* 1 = 2.34082 loss)
I0818 11:26:57.737107  2412 solver.cpp:357] Iteration 3000 (0.758363 iter/s, 131.863s/100 iters), loss = 0.621104
I0818 11:26:57.737179  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.621104 (* 1 = 0.621104 loss)
I0818 11:26:57.737190  2412 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0818 11:28:04.881145  2412 solver.cpp:357] Iteration 3100 (1.48933 iter/s, 67.1443s/100 iters), loss = 0.588817
I0818 11:28:04.881392  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.588817 (* 1 = 0.588817 loss)
I0818 11:28:04.881431  2412 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0818 11:28:19.252410  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:29:16.393824  2412 solver.cpp:357] Iteration 3200 (1.39839 iter/s, 71.5109s/100 iters), loss = 0.52736
I0818 11:29:16.394224  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52736 (* 1 = 0.52736 loss)
I0818 11:29:16.394289  2412 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0818 11:30:31.831310  2412 solver.cpp:357] Iteration 3300 (1.32559 iter/s, 75.4381s/100 iters), loss = 0.527321
I0818 11:30:31.831485  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.527321 (* 1 = 0.527321 loss)
I0818 11:30:31.831498  2412 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0818 11:31:47.371944  2412 solver.cpp:357] Iteration 3400 (1.32382 iter/s, 75.5392s/100 iters), loss = 0.447822
I0818 11:31:47.372879  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.447822 (* 1 = 0.447822 loss)
I0818 11:31:47.372916  2412 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0818 11:32:58.624583  2412 solver.cpp:514] Iteration 3500, Testing net (#0)
I0818 11:33:50.967327  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:33:51.199215  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1543
I0818 11:33:51.199278  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.41594 (* 1 = 2.41594 loss)
I0818 11:33:51.746407  2412 solver.cpp:357] Iteration 3500 (0.804007 iter/s, 124.377s/100 iters), loss = 0.644748
I0818 11:33:51.746476  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.644748 (* 1 = 0.644748 loss)
I0818 11:33:51.746487  2412 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0818 11:34:00.359830  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:35:07.618078  2412 solver.cpp:357] Iteration 3600 (1.318 iter/s, 75.8724s/100 iters), loss = 0.579162
I0818 11:35:07.618269  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.579162 (* 1 = 0.579162 loss)
I0818 11:35:07.618312  2412 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0818 11:36:23.223496  2412 solver.cpp:357] Iteration 3700 (1.32261 iter/s, 75.6081s/100 iters), loss = 0.495555
I0818 11:36:23.223626  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495555 (* 1 = 0.495555 loss)
I0818 11:36:23.223636  2412 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0818 11:37:38.746095  2412 solver.cpp:357] Iteration 3800 (1.32406 iter/s, 75.5254s/100 iters), loss = 0.453213
I0818 11:37:38.746289  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453213 (* 1 = 0.453213 loss)
I0818 11:37:38.746316  2412 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0818 11:38:42.684914  2412 solver.cpp:357] Iteration 3900 (1.56399 iter/s, 63.9391s/100 iters), loss = 0.606239
I0818 11:38:42.685113  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.606239 (* 1 = 0.606239 loss)
I0818 11:38:42.685140  2412 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0818 11:38:44.234400  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:39:57.352073  2412 solver.cpp:514] Iteration 4000, Testing net (#0)
I0818 11:40:53.135823  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:40:53.400692  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2097
I0818 11:40:53.400748  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.33298 (* 1 = 2.33298 loss)
I0818 11:40:54.019381  2412 solver.cpp:357] Iteration 4000 (0.761397 iter/s, 131.337s/100 iters), loss = 0.611514
I0818 11:40:54.019455  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.611514 (* 1 = 0.611514 loss)
I0818 11:40:54.019466  2412 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0818 11:42:09.448626  2412 solver.cpp:357] Iteration 4100 (1.32573 iter/s, 75.43s/100 iters), loss = 0.515128
I0818 11:42:09.448777  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.515128 (* 1 = 0.515128 loss)
I0818 11:42:09.448789  2412 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0818 11:43:22.252665  2412 solver.cpp:357] Iteration 4200 (1.37358 iter/s, 72.8026s/100 iters), loss = 0.533694
I0818 11:43:22.252912  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.533694 (* 1 = 0.533694 loss)
I0818 11:43:22.252925  2412 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0818 11:44:27.194736  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:44:32.941787  2412 solver.cpp:357] Iteration 4300 (1.41463 iter/s, 70.6897s/100 iters), loss = 0.464318
I0818 11:44:32.941860  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464318 (* 1 = 0.464318 loss)
I0818 11:44:32.941871  2412 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0818 11:45:48.297358  2412 solver.cpp:357] Iteration 4400 (1.32703 iter/s, 75.3564s/100 iters), loss = 0.530182
I0818 11:45:48.297538  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530182 (* 1 = 0.530182 loss)
I0818 11:45:48.297564  2412 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0818 11:47:03.138294  2412 solver.cpp:514] Iteration 4500, Testing net (#0)
I0818 11:47:56.706812  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:47:56.934795  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3018
I0818 11:47:56.934844  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.96469 (* 1 = 1.96469 loss)
I0818 11:47:57.480742  2412 solver.cpp:357] Iteration 4500 (0.774076 iter/s, 129.186s/100 iters), loss = 0.588363
I0818 11:47:57.480868  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.588363 (* 1 = 0.588363 loss)
I0818 11:47:57.480897  2412 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0818 11:49:03.729104  2412 solver.cpp:357] Iteration 4600 (1.50946 iter/s, 66.2488s/100 iters), loss = 0.428618
I0818 11:49:03.729224  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428618 (* 1 = 0.428618 loss)
I0818 11:49:03.729238  2412 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0818 11:50:06.197270  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:50:18.783965  2412 solver.cpp:357] Iteration 4700 (1.33237 iter/s, 75.0544s/100 iters), loss = 0.437305
I0818 11:50:18.784039  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437305 (* 1 = 0.437305 loss)
I0818 11:50:18.784050  2412 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0818 11:51:34.182426  2412 solver.cpp:357] Iteration 4800 (1.32631 iter/s, 75.3972s/100 iters), loss = 0.454925
I0818 11:51:34.182582  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.454925 (* 1 = 0.454925 loss)
I0818 11:51:34.182596  2412 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0818 11:52:49.939142  2412 solver.cpp:357] Iteration 4900 (1.32004 iter/s, 75.7554s/100 iters), loss = 0.520659
I0818 11:52:49.939271  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.520659 (* 1 = 0.520659 loss)
I0818 11:52:49.939285  2412 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0818 11:54:01.679024  2412 solver.cpp:514] Iteration 5000, Testing net (#0)
I0818 11:54:52.802215  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:54:52.940399  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.366
I0818 11:54:52.940450  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.77557 (* 1 = 1.77557 loss)
I0818 11:54:53.563642  2412 solver.cpp:357] Iteration 5000 (0.808896 iter/s, 123.625s/100 iters), loss = 0.44966
I0818 11:54:53.563721  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44966 (* 1 = 0.44966 loss)
I0818 11:54:53.563735  2412 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0818 11:55:49.552175  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:56:09.167258  2412 solver.cpp:357] Iteration 5100 (1.32271 iter/s, 75.6023s/100 iters), loss = 0.501112
I0818 11:56:09.167330  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.501112 (* 1 = 0.501112 loss)
I0818 11:56:09.167341  2412 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0818 11:57:23.095485  2412 solver.cpp:357] Iteration 5200 (1.35265 iter/s, 73.929s/100 iters), loss = 0.612364
I0818 11:57:23.095676  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.612364 (* 1 = 0.612364 loss)
I0818 11:57:23.095690  2412 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0818 11:58:34.871908  2412 solver.cpp:357] Iteration 5300 (1.3932 iter/s, 71.7771s/100 iters), loss = 0.613548
I0818 11:58:34.872051  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.613548 (* 1 = 0.613548 loss)
I0818 11:58:34.872062  2412 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0818 11:59:43.366314  2412 solver.cpp:357] Iteration 5400 (1.45996 iter/s, 68.4949s/100 iters), loss = 0.344497
I0818 11:59:43.366483  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344497 (* 1 = 0.344497 loss)
I0818 11:59:43.366497  2412 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0818 12:00:31.882516  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:00:58.221283  2412 solver.cpp:514] Iteration 5500, Testing net (#0)
I0818 12:01:54.268076  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:01:54.445403  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.340199
I0818 12:01:54.445449  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.95424 (* 1 = 1.95424 loss)
I0818 12:01:55.107442  2412 solver.cpp:357] Iteration 5500 (0.759046 iter/s, 131.744s/100 iters), loss = 0.44671
I0818 12:01:55.107509  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44671 (* 1 = 0.44671 loss)
I0818 12:01:55.107520  2412 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0818 12:03:10.611829  2412 solver.cpp:357] Iteration 5600 (1.32437 iter/s, 75.5073s/100 iters), loss = 0.528377
I0818 12:03:10.611966  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.528377 (* 1 = 0.528377 loss)
I0818 12:03:10.611977  2412 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0818 12:04:24.780128  2412 solver.cpp:357] Iteration 5700 (1.34827 iter/s, 74.1691s/100 iters), loss = 0.42469
I0818 12:04:24.780313  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42469 (* 1 = 0.42469 loss)
I0818 12:04:24.780326  2412 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0818 12:05:34.469404  2412 solver.cpp:357] Iteration 5800 (1.43493 iter/s, 69.6899s/100 iters), loss = 0.33041
I0818 12:05:34.469514  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.33041 (* 1 = 0.33041 loss)
I0818 12:05:34.469527  2412 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0818 12:06:16.106940  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:06:49.914386  2412 solver.cpp:357] Iteration 5900 (1.32549 iter/s, 75.4437s/100 iters), loss = 0.551276
I0818 12:06:49.914732  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.551276 (* 1 = 0.551276 loss)
I0818 12:06:49.914796  2412 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0818 12:07:59.663610  2412 solver.cpp:514] Iteration 6000, Testing net (#0)
I0818 12:08:55.447808  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:08:55.738485  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5684
I0818 12:08:55.738538  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32575 (* 1 = 1.32575 loss)
I0818 12:08:56.234128  2412 solver.cpp:357] Iteration 6000 (0.791637 iter/s, 126.321s/100 iters), loss = 0.534292
I0818 12:08:56.234201  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.534292 (* 1 = 0.534292 loss)
I0818 12:08:56.234215  2412 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0818 12:10:06.333251  2412 solver.cpp:357] Iteration 6100 (1.42665 iter/s, 70.0943s/100 iters), loss = 0.385287
I0818 12:10:06.333457  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385287 (* 1 = 0.385287 loss)
I0818 12:10:06.333485  2412 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0818 12:11:19.845373  2412 solver.cpp:357] Iteration 6200 (1.36039 iter/s, 73.5081s/100 iters), loss = 0.380813
I0818 12:11:19.845558  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380813 (* 1 = 0.380813 loss)
I0818 12:11:19.845573  2412 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0818 12:11:53.853710  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:12:35.124670  2412 solver.cpp:357] Iteration 6300 (1.32849 iter/s, 75.2736s/100 iters), loss = 0.355262
I0818 12:12:35.124840  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355262 (* 1 = 0.355262 loss)
I0818 12:12:35.124853  2412 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0818 12:13:50.533664  2412 solver.cpp:357] Iteration 6400 (1.32619 iter/s, 75.4038s/100 iters), loss = 0.504559
I0818 12:13:50.533838  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.504559 (* 1 = 0.504559 loss)
I0818 12:13:50.533849  2412 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0818 12:15:03.605033  2412 solver.cpp:514] Iteration 6500, Testing net (#0)
I0818 12:15:53.454707  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:15:53.746914  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6051
I0818 12:15:53.746974  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.14422 (* 1 = 1.14422 loss)
I0818 12:15:54.249166  2412 solver.cpp:357] Iteration 6500 (0.808324 iter/s, 123.713s/100 iters), loss = 0.393275
I0818 12:15:54.249243  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393275 (* 1 = 0.393275 loss)
I0818 12:15:54.249254  2412 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0818 12:17:07.394306  2412 solver.cpp:357] Iteration 6600 (1.36722 iter/s, 73.141s/100 iters), loss = 0.410451
I0818 12:17:07.394449  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410451 (* 1 = 0.410451 loss)
I0818 12:17:07.394461  2412 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0818 12:17:32.943728  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:18:20.453053  2412 solver.cpp:357] Iteration 6700 (1.36876 iter/s, 73.0589s/100 iters), loss = 0.520215
I0818 12:18:20.453414  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.520215 (* 1 = 0.520215 loss)
I0818 12:18:20.453444  2412 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0818 12:19:36.160248  2412 solver.cpp:357] Iteration 6800 (1.32091 iter/s, 75.7057s/100 iters), loss = 0.444925
I0818 12:19:36.160429  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444925 (* 1 = 0.444925 loss)
I0818 12:19:36.160444  2412 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0818 12:20:45.673214  2412 solver.cpp:357] Iteration 6900 (1.43865 iter/s, 69.5094s/100 iters), loss = 0.461624
I0818 12:20:45.673353  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461624 (* 1 = 0.461624 loss)
I0818 12:20:45.673365  2412 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0818 12:21:58.965826  2412 solver.cpp:514] Iteration 7000, Testing net (#0)
I0818 12:22:54.903843  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:22:55.111768  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.613201
I0818 12:22:55.111831  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.21444 (* 1 = 1.21444 loss)
I0818 12:22:55.764394  2412 solver.cpp:357] Iteration 7000 (0.768693 iter/s, 130.091s/100 iters), loss = 0.450038
I0818 12:22:55.764472  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450038 (* 1 = 0.450038 loss)
I0818 12:22:55.764483  2412 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0818 12:23:16.185004  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:24:11.131122  2412 solver.cpp:357] Iteration 7100 (1.3269 iter/s, 75.3637s/100 iters), loss = 0.453053
I0818 12:24:11.131265  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453053 (* 1 = 0.453053 loss)
I0818 12:24:11.131278  2412 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0818 12:25:26.592005  2412 solver.cpp:357] Iteration 7200 (1.32524 iter/s, 75.458s/100 iters), loss = 0.613233
I0818 12:25:26.592150  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.613233 (* 1 = 0.613233 loss)
I0818 12:25:26.592161  2412 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0818 12:26:34.283819  2412 solver.cpp:357] Iteration 7300 (1.47735 iter/s, 67.6889s/100 iters), loss = 0.480877
I0818 12:26:34.284014  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480877 (* 1 = 0.480877 loss)
I0818 12:26:34.284027  2412 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0818 12:27:45.786744  2412 solver.cpp:357] Iteration 7400 (1.3986 iter/s, 71.5002s/100 iters), loss = 0.395838
I0818 12:27:45.786923  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395838 (* 1 = 0.395838 loss)
I0818 12:27:45.786937  2412 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0818 12:27:58.624013  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:29:00.376435  2412 solver.cpp:514] Iteration 7500, Testing net (#0)
I0818 12:29:56.562698  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:29:56.797782  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4696
I0818 12:29:56.797839  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.86197 (* 1 = 1.86197 loss)
I0818 12:29:57.449784  2412 solver.cpp:357] Iteration 7500 (0.759522 iter/s, 131.662s/100 iters), loss = 0.472246
I0818 12:29:57.449865  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472246 (* 1 = 0.472246 loss)
I0818 12:29:57.449877  2412 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0818 12:31:08.554010  2412 solver.cpp:357] Iteration 7600 (1.40644 iter/s, 71.1016s/100 iters), loss = 0.434996
I0818 12:31:08.554177  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.434996 (* 1 = 0.434996 loss)
I0818 12:31:08.554191  2412 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0818 12:32:21.356997  2412 solver.cpp:357] Iteration 7700 (1.37358 iter/s, 72.8026s/100 iters), loss = 0.591966
I0818 12:32:21.357153  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.591966 (* 1 = 0.591966 loss)
I0818 12:32:21.357165  2412 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0818 12:33:36.777839  2412 solver.cpp:357] Iteration 7800 (1.32593 iter/s, 75.4185s/100 iters), loss = 0.461055
I0818 12:33:36.778008  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461055 (* 1 = 0.461055 loss)
I0818 12:33:36.778021  2412 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0818 12:33:42.907888  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:34:51.881429  2412 solver.cpp:357] Iteration 7900 (1.33154 iter/s, 75.1013s/100 iters), loss = 0.45015
I0818 12:34:51.881574  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45015 (* 1 = 0.45015 loss)
I0818 12:34:51.881588  2412 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0818 12:36:06.272552  2412 solver.cpp:514] Iteration 8000, Testing net (#0)
I0818 12:36:50.537317  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:36:50.761693  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5968
I0818 12:36:50.761750  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.33437 (* 1 = 1.33437 loss)
I0818 12:36:51.309355  2412 solver.cpp:357] Iteration 8000 (0.837333 iter/s, 119.427s/100 iters), loss = 0.312752
I0818 12:36:51.309424  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312752 (* 1 = 0.312752 loss)
I0818 12:36:51.309435  2412 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0818 12:38:05.820945  2412 solver.cpp:357] Iteration 8100 (1.34208 iter/s, 74.5114s/100 iters), loss = 0.377387
I0818 12:38:05.821116  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377387 (* 1 = 0.377387 loss)
I0818 12:38:05.821130  2412 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0818 12:39:20.854290  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:39:21.425577  2412 solver.cpp:357] Iteration 8200 (1.32267 iter/s, 75.6045s/100 iters), loss = 0.420147
I0818 12:39:21.425650  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.420147 (* 1 = 0.420147 loss)
I0818 12:39:21.425662  2412 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0818 12:40:36.666054  2412 solver.cpp:357] Iteration 8300 (1.32907 iter/s, 75.2404s/100 iters), loss = 0.544541
I0818 12:40:36.666285  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544541 (* 1 = 0.544541 loss)
I0818 12:40:36.666311  2412 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0818 12:41:47.722080  2412 solver.cpp:357] Iteration 8400 (1.40731 iter/s, 71.0577s/100 iters), loss = 0.40069
I0818 12:41:47.722235  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40069 (* 1 = 0.40069 loss)
I0818 12:41:47.722249  2412 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0818 12:43:00.054878  2412 solver.cpp:514] Iteration 8500, Testing net (#0)
I0818 12:43:56.252684  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:43:56.443480  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6003
I0818 12:43:56.443526  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.33057 (* 1 = 1.33057 loss)
I0818 12:43:57.051112  2412 solver.cpp:357] Iteration 8500 (0.773231 iter/s, 129.327s/100 iters), loss = 0.380014
I0818 12:43:57.051184  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380014 (* 1 = 0.380014 loss)
I0818 12:43:57.051198  2412 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0818 12:45:04.364274  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:45:12.602368  2412 solver.cpp:357] Iteration 8600 (1.32377 iter/s, 75.5417s/100 iters), loss = 0.495369
I0818 12:45:12.602435  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495369 (* 1 = 0.495369 loss)
I0818 12:45:12.602445  2412 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0818 12:46:25.588631  2412 solver.cpp:357] Iteration 8700 (1.3702 iter/s, 72.982s/100 iters), loss = 0.364416
I0818 12:46:25.588814  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364416 (* 1 = 0.364416 loss)
I0818 12:46:25.588826  2412 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0818 12:47:31.983798  2412 solver.cpp:357] Iteration 8800 (1.5063 iter/s, 66.3877s/100 iters), loss = 0.4113
I0818 12:47:31.983968  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4113 (* 1 = 0.4113 loss)
I0818 12:47:31.983978  2412 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0818 12:48:47.027875  2412 solver.cpp:357] Iteration 8900 (1.33264 iter/s, 75.0389s/100 iters), loss = 0.404763
I0818 12:48:47.028046  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404763 (* 1 = 0.404763 loss)
I0818 12:48:47.028059  2412 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0818 12:49:47.632391  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:50:01.908167  2412 solver.cpp:514] Iteration 9000, Testing net (#0)
I0818 12:50:57.806398  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:50:58.053256  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6656
I0818 12:50:58.053313  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.14729 (* 1 = 1.14729 loss)
I0818 12:50:58.724680  2412 solver.cpp:357] Iteration 9000 (0.759354 iter/s, 131.691s/100 iters), loss = 0.276378
I0818 12:50:58.724756  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.276378 (* 1 = 0.276378 loss)
I0818 12:50:58.724766  2412 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0818 12:52:10.962546  2412 solver.cpp:357] Iteration 9100 (1.38435 iter/s, 72.2363s/100 iters), loss = 0.565146
I0818 12:52:10.962719  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.565146 (* 1 = 0.565146 loss)
I0818 12:52:10.962733  2412 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0818 12:53:22.790802  2412 solver.cpp:357] Iteration 9200 (1.39232 iter/s, 71.8228s/100 iters), loss = 0.386568
I0818 12:53:22.790940  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386568 (* 1 = 0.386568 loss)
I0818 12:53:22.790951  2412 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0818 12:54:37.826442  2412 solver.cpp:357] Iteration 9300 (1.33272 iter/s, 75.0346s/100 iters), loss = 0.303171
I0818 12:54:37.826587  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.303171 (* 1 = 0.303171 loss)
I0818 12:54:37.826599  2412 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0818 12:55:30.850572  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:55:52.089129  2412 solver.cpp:357] Iteration 9400 (1.34662 iter/s, 74.26s/100 iters), loss = 0.542689
I0818 12:55:52.089198  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.542689 (* 1 = 0.542689 loss)
I0818 12:55:52.089210  2412 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0818 12:57:03.232743  2412 solver.cpp:514] Iteration 9500, Testing net (#0)
I0818 12:57:51.756194  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:57:51.976089  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6782
I0818 12:57:51.976155  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.932975 (* 1 = 0.932975 loss)
I0818 12:57:52.488919  2412 solver.cpp:357] Iteration 9500 (0.830584 iter/s, 120.397s/100 iters), loss = 0.464263
I0818 12:57:52.489001  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464263 (* 1 = 0.464263 loss)
I0818 12:57:52.489012  2412 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0818 12:59:07.856199  2412 solver.cpp:357] Iteration 9600 (1.32691 iter/s, 75.363s/100 iters), loss = 0.48226
I0818 12:59:07.856364  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48226 (* 1 = 0.48226 loss)
I0818 12:59:07.856377  2412 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0818 13:00:23.327613  2412 solver.cpp:357] Iteration 9700 (1.32508 iter/s, 75.4673s/100 iters), loss = 0.383839
I0818 13:00:23.327755  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383839 (* 1 = 0.383839 loss)
I0818 13:00:23.327766  2412 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0818 13:01:09.578938  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:01:38.855305  2412 solver.cpp:357] Iteration 9800 (1.32405 iter/s, 75.5259s/100 iters), loss = 0.419494
I0818 13:01:38.855383  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419494 (* 1 = 0.419494 loss)
I0818 13:01:38.855396  2412 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0818 13:02:50.705529  2412 solver.cpp:357] Iteration 9900 (1.39186 iter/s, 71.8464s/100 iters), loss = 0.328222
I0818 13:02:50.705684  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328222 (* 1 = 0.328222 loss)
I0818 13:02:50.705698  2412 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0818 13:04:02.005070  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_10000.caffemodel
I0818 13:04:02.044407  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_10000.solverstate
I0818 13:04:02.051575  2412 solver.cpp:514] Iteration 10000, Testing net (#0)
I0818 13:04:58.127025  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:04:58.254536  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.619901
I0818 13:04:58.254609  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30467 (* 1 = 1.30467 loss)
I0818 13:04:58.894448  2412 solver.cpp:357] Iteration 10000 (0.780118 iter/s, 128.186s/100 iters), loss = 0.419682
I0818 13:04:58.894522  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419682 (* 1 = 0.419682 loss)
I0818 13:04:58.894536  2412 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0818 13:06:10.216084  2412 solver.cpp:357] Iteration 10100 (1.40217 iter/s, 71.318s/100 iters), loss = 0.382164
I0818 13:06:10.216262  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382164 (* 1 = 0.382164 loss)
I0818 13:06:10.216275  2412 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0818 13:06:48.483392  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:07:24.585495  2412 solver.cpp:357] Iteration 10200 (1.3447 iter/s, 74.366s/100 iters), loss = 0.496513
I0818 13:07:24.585636  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496513 (* 1 = 0.496513 loss)
I0818 13:07:24.585649  2412 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0818 13:08:33.111588  2412 solver.cpp:357] Iteration 10300 (1.45934 iter/s, 68.5239s/100 iters), loss = 0.313299
I0818 13:08:33.111908  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313299 (* 1 = 0.313299 loss)
I0818 13:08:33.111920  2412 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0818 13:09:48.415900  2412 solver.cpp:357] Iteration 10400 (1.32797 iter/s, 75.3031s/100 iters), loss = 0.426818
I0818 13:09:48.416028  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426818 (* 1 = 0.426818 loss)
I0818 13:09:48.416038  2412 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0818 13:11:03.105068  2412 solver.cpp:514] Iteration 10500, Testing net (#0)
I0818 13:11:59.149372  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:11:59.325778  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6641
I0818 13:11:59.325846  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.933766 (* 1 = 0.933766 loss)
I0818 13:11:59.965848  2412 solver.cpp:357] Iteration 10500 (0.760158 iter/s, 131.552s/100 iters), loss = 0.474367
I0818 13:11:59.965925  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474367 (* 1 = 0.474367 loss)
I0818 13:11:59.965939  2412 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0818 13:12:31.860574  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:13:13.447473  2412 solver.cpp:357] Iteration 10600 (1.36094 iter/s, 73.4784s/100 iters), loss = 0.448957
I0818 13:13:13.447806  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448957 (* 1 = 0.448957 loss)
I0818 13:13:13.447870  2412 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0818 13:14:23.612768  2412 solver.cpp:357] Iteration 10700 (1.42523 iter/s, 70.1641s/100 iters), loss = 0.351556
I0818 13:14:23.612895  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351556 (* 1 = 0.351556 loss)
I0818 13:14:23.612905  2412 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0818 13:15:36.362713  2412 solver.cpp:357] Iteration 10800 (1.37457 iter/s, 72.7503s/100 iters), loss = 0.365659
I0818 13:15:36.362826  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365659 (* 1 = 0.365659 loss)
I0818 13:15:36.362838  2412 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0818 13:16:49.720451  2412 solver.cpp:357] Iteration 10900 (1.3632 iter/s, 73.3567s/100 iters), loss = 0.455301
I0818 13:16:49.720574  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.455301 (* 1 = 0.455301 loss)
I0818 13:16:49.720585  2412 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0818 13:17:14.656222  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:18:04.681672  2412 solver.cpp:514] Iteration 11000, Testing net (#0)
I0818 13:18:54.070334  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:18:54.303689  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6888
I0818 13:18:54.303741  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.884343 (* 1 = 0.884343 loss)
I0818 13:18:54.735627  2412 solver.cpp:357] Iteration 11000 (0.799873 iter/s, 125.02s/100 iters), loss = 0.333373
I0818 13:18:54.735697  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.333373 (* 1 = 0.333373 loss)
I0818 13:18:54.735708  2412 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0818 13:20:09.130604  2412 solver.cpp:357] Iteration 11100 (1.34416 iter/s, 74.396s/100 iters), loss = 0.334127
I0818 13:20:09.130738  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334127 (* 1 = 0.334127 loss)
I0818 13:20:09.130748  2412 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0818 13:21:24.637279  2412 solver.cpp:357] Iteration 11200 (1.32434 iter/s, 75.5095s/100 iters), loss = 0.304272
I0818 13:21:24.637430  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.304272 (* 1 = 0.304272 loss)
I0818 13:21:24.637442  2412 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0818 13:22:40.214267  2412 solver.cpp:357] Iteration 11300 (1.32315 iter/s, 75.5773s/100 iters), loss = 0.429455
I0818 13:22:40.214555  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429455 (* 1 = 0.429455 loss)
I0818 13:22:40.214567  2412 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0818 13:22:58.461643  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:23:53.416744  2412 solver.cpp:357] Iteration 11400 (1.36608 iter/s, 73.2023s/100 iters), loss = 0.410449
I0818 13:23:53.416913  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410449 (* 1 = 0.410449 loss)
I0818 13:23:53.416924  2412 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0818 13:25:02.343905  2412 solver.cpp:514] Iteration 11500, Testing net (#0)
I0818 13:25:54.392590  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:25:54.681092  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6904
I0818 13:25:54.681155  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.983863 (* 1 = 0.983863 loss)
I0818 13:25:55.301937  2412 solver.cpp:357] Iteration 11500 (0.820431 iter/s, 121.887s/100 iters), loss = 0.514403
I0818 13:25:55.302007  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.514403 (* 1 = 0.514403 loss)
I0818 13:25:55.302017  2412 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0818 13:27:10.767624  2412 solver.cpp:357] Iteration 11600 (1.32508 iter/s, 75.467s/100 iters), loss = 0.427092
I0818 13:27:10.767814  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.427092 (* 1 = 0.427092 loss)
I0818 13:27:10.767827  2412 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0818 13:28:25.969307  2412 solver.cpp:357] Iteration 11700 (1.32974 iter/s, 75.2028s/100 iters), loss = 0.398858
I0818 13:28:25.969581  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398858 (* 1 = 0.398858 loss)
I0818 13:28:25.969612  2412 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0818 13:28:36.687768  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:29:34.966524  2412 solver.cpp:357] Iteration 11800 (1.44936 iter/s, 68.9958s/100 iters), loss = 0.621398
I0818 13:29:34.966673  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.621398 (* 1 = 0.621398 loss)
I0818 13:29:34.966691  2412 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0818 13:30:50.008003  2412 solver.cpp:357] Iteration 11900 (1.33258 iter/s, 75.0423s/100 iters), loss = 0.462861
I0818 13:30:50.008175  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462861 (* 1 = 0.462861 loss)
I0818 13:30:50.008188  2412 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0818 13:32:04.818542  2412 solver.cpp:514] Iteration 12000, Testing net (#0)
I0818 13:33:00.828725  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:33:01.065009  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.604001
I0818 13:33:01.065078  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.36246 (* 1 = 1.36246 loss)
I0818 13:33:01.561036  2412 solver.cpp:357] Iteration 12000 (0.760146 iter/s, 131.554s/100 iters), loss = 0.3924
I0818 13:33:01.561113  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3924 (* 1 = 0.3924 loss)
I0818 13:33:01.561126  2412 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0818 13:34:16.113625  2412 solver.cpp:357] Iteration 12100 (1.34133 iter/s, 74.5531s/100 iters), loss = 0.48899
I0818 13:34:16.113970  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48899 (* 1 = 0.48899 loss)
I0818 13:34:16.114001  2412 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0818 13:34:19.520499  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:35:21.567559  2412 solver.cpp:357] Iteration 12200 (1.52784 iter/s, 65.4519s/100 iters), loss = 0.367482
I0818 13:35:21.567876  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367482 (* 1 = 0.367482 loss)
I0818 13:35:21.567940  2412 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0818 13:36:36.481894  2412 solver.cpp:357] Iteration 12300 (1.33489 iter/s, 74.9126s/100 iters), loss = 0.305936
I0818 13:36:36.482065  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305936 (* 1 = 0.305936 loss)
I0818 13:36:36.482076  2412 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0818 13:37:52.029506  2412 solver.cpp:357] Iteration 12400 (1.32363 iter/s, 75.5499s/100 iters), loss = 0.634163
I0818 13:37:52.029822  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.634163 (* 1 = 0.634163 loss)
I0818 13:37:52.029884  2412 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0818 13:39:03.820137  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:39:06.825209  2412 solver.cpp:514] Iteration 12500, Testing net (#0)
I0818 13:39:57.257058  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:39:57.359375  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5603
I0818 13:39:57.359489  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.76927 (* 1 = 1.76927 loss)
I0818 13:39:57.943672  2412 solver.cpp:357] Iteration 12500 (0.794193 iter/s, 125.914s/100 iters), loss = 0.442184
I0818 13:39:57.943747  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442184 (* 1 = 0.442184 loss)
I0818 13:39:57.943760  2412 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0818 13:41:11.389479  2412 solver.cpp:357] Iteration 12600 (1.36155 iter/s, 73.4459s/100 iters), loss = 0.393434
I0818 13:41:11.389706  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393434 (* 1 = 0.393434 loss)
I0818 13:41:11.389736  2412 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0818 13:42:26.985807  2412 solver.cpp:357] Iteration 12700 (1.32285 iter/s, 75.5944s/100 iters), loss = 0.519594
I0818 13:42:26.986120  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.519594 (* 1 = 0.519594 loss)
I0818 13:42:26.986183  2412 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0818 13:43:42.547782  2412 solver.cpp:357] Iteration 12800 (1.32342 iter/s, 75.5621s/100 iters), loss = 0.403493
I0818 13:43:42.547910  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403493 (* 1 = 0.403493 loss)
I0818 13:43:42.547924  2412 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0818 13:44:45.451898  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:44:53.838521  2412 solver.cpp:357] Iteration 12900 (1.40275 iter/s, 71.2886s/100 iters), loss = 0.544067
I0818 13:44:53.838595  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544067 (* 1 = 0.544067 loss)
I0818 13:44:53.838608  2412 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0818 13:46:01.004187  2412 solver.cpp:514] Iteration 13000, Testing net (#0)
I0818 13:46:57.328498  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:46:57.504184  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.573201
I0818 13:46:57.504238  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.72501 (* 1 = 1.72501 loss)
I0818 13:46:58.135943  2412 solver.cpp:357] Iteration 13000 (0.804512 iter/s, 124.299s/100 iters), loss = 0.405849
I0818 13:46:58.136023  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.405849 (* 1 = 0.405849 loss)
I0818 13:46:58.136034  2412 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0818 13:48:13.360662  2412 solver.cpp:357] Iteration 13100 (1.32939 iter/s, 75.2226s/100 iters), loss = 0.347443
I0818 13:48:13.360771  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347443 (* 1 = 0.347443 loss)
I0818 13:48:13.360782  2412 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0818 13:49:28.370909  2412 solver.cpp:357] Iteration 13200 (1.33313 iter/s, 75.0112s/100 iters), loss = 0.314053
I0818 13:49:28.371212  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.314053 (* 1 = 0.314053 loss)
I0818 13:49:28.371275  2412 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0818 13:50:23.258687  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:50:38.230034  2412 solver.cpp:357] Iteration 13300 (1.43146 iter/s, 69.8589s/100 iters), loss = 0.410698
I0818 13:50:38.230109  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410698 (* 1 = 0.410698 loss)
I0818 13:50:38.230121  2412 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0818 13:51:52.411438  2412 solver.cpp:357] Iteration 13400 (1.34805 iter/s, 74.1811s/100 iters), loss = 0.491717
I0818 13:51:52.411643  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491717 (* 1 = 0.491717 loss)
I0818 13:51:52.411654  2412 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0818 13:53:07.367382  2412 solver.cpp:514] Iteration 13500, Testing net (#0)
I0818 13:54:02.663309  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:54:02.868387  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6778
I0818 13:54:02.868438  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.982699 (* 1 = 0.982699 loss)
I0818 13:54:03.438094  2412 solver.cpp:357] Iteration 13500 (0.763207 iter/s, 131.026s/100 iters), loss = 0.396438
I0818 13:54:03.438159  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396438 (* 1 = 0.396438 loss)
I0818 13:54:03.438170  2412 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0818 13:55:14.581135  2412 solver.cpp:357] Iteration 13600 (1.40562 iter/s, 71.1431s/100 iters), loss = 0.272104
I0818 13:55:14.581475  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.272104 (* 1 = 0.272104 loss)
I0818 13:55:14.581539  2412 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0818 13:55:58.591450  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:56:22.798142  2412 solver.cpp:357] Iteration 13700 (1.466 iter/s, 68.213s/100 iters), loss = 0.399869
I0818 13:56:22.798219  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399869 (* 1 = 0.399869 loss)
I0818 13:56:22.798231  2412 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0818 13:57:38.510704  2412 solver.cpp:357] Iteration 13800 (1.32085 iter/s, 75.7088s/100 iters), loss = 0.304055
I0818 13:57:38.510844  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.304055 (* 1 = 0.304055 loss)
I0818 13:57:38.510859  2412 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0818 13:58:53.812366  2412 solver.cpp:357] Iteration 13900 (1.32805 iter/s, 75.2981s/100 iters), loss = 0.256943
I0818 13:58:53.812484  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.256943 (* 1 = 0.256943 loss)
I0818 13:58:53.812496  2412 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0818 14:00:08.898216  2412 solver.cpp:514] Iteration 14000, Testing net (#0)
I0818 14:01:00.602252  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:01:00.696797  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5534
I0818 14:01:00.696846  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.26392 (* 1 = 2.26392 loss)
I0818 14:01:01.280305  2412 solver.cpp:357] Iteration 14000 (0.784515 iter/s, 127.467s/100 iters), loss = 0.278424
I0818 14:01:01.280375  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.278424 (* 1 = 0.278424 loss)
I0818 14:01:01.280385  2412 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0818 14:01:42.530421  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:02:14.165421  2412 solver.cpp:357] Iteration 14100 (1.372 iter/s, 72.8861s/100 iters), loss = 0.337362
I0818 14:02:14.165544  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.337362 (* 1 = 0.337362 loss)
I0818 14:02:14.165555  2412 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0818 14:03:29.690367  2412 solver.cpp:357] Iteration 14200 (1.32408 iter/s, 75.5239s/100 iters), loss = 0.480603
I0818 14:03:29.694753  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480603 (* 1 = 0.480603 loss)
I0818 14:03:29.694769  2412 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0818 14:04:40.781020  2412 solver.cpp:357] Iteration 14300 (1.40673 iter/s, 71.0869s/100 iters), loss = 0.423304
I0818 14:04:40.781144  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423304 (* 1 = 0.423304 loss)
I0818 14:04:40.781157  2412 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0818 14:05:55.521771  2412 solver.cpp:357] Iteration 14400 (1.33797 iter/s, 74.7399s/100 iters), loss = 0.3441
I0818 14:05:55.521929  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3441 (* 1 = 0.3441 loss)
I0818 14:05:55.521941  2412 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0818 14:06:27.508776  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:07:03.594761  2412 solver.cpp:514] Iteration 14500, Testing net (#0)
I0818 14:07:59.437023  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:07:59.679616  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5705
I0818 14:07:59.679684  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.74048 (* 1 = 1.74048 loss)
I0818 14:08:00.335510  2412 solver.cpp:357] Iteration 14500 (0.80118 iter/s, 124.816s/100 iters), loss = 0.367292
I0818 14:08:00.335577  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367292 (* 1 = 0.367292 loss)
I0818 14:08:00.335588  2412 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0818 14:09:15.776038  2412 solver.cpp:357] Iteration 14600 (1.32552 iter/s, 75.4419s/100 iters), loss = 0.448301
I0818 14:09:15.776178  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448301 (* 1 = 0.448301 loss)
I0818 14:09:15.776190  2412 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0818 14:10:31.049437  2412 solver.cpp:357] Iteration 14700 (1.3285 iter/s, 75.2727s/100 iters), loss = 0.413264
I0818 14:10:31.049574  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413264 (* 1 = 0.413264 loss)
I0818 14:10:31.049587  2412 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0818 14:11:42.159741  2412 solver.cpp:357] Iteration 14800 (1.40632 iter/s, 71.1074s/100 iters), loss = 0.371711
I0818 14:11:42.159860  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371711 (* 1 = 0.371711 loss)
I0818 14:11:42.159873  2412 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0818 14:12:09.237690  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:12:54.833685  2412 solver.cpp:357] Iteration 14900 (1.37604 iter/s, 72.6721s/100 iters), loss = 0.271826
I0818 14:12:54.833829  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.271826 (* 1 = 0.271826 loss)
I0818 14:12:54.833843  2412 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0818 14:14:06.862865  2412 solver.cpp:514] Iteration 15000, Testing net (#0)
I0818 14:15:00.698952  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:15:00.966377  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5315
I0818 14:15:00.966429  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.80583 (* 1 = 1.80583 loss)
I0818 14:15:01.617447  2412 solver.cpp:357] Iteration 15000 (0.788754 iter/s, 126.782s/100 iters), loss = 0.447
I0818 14:15:01.617517  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.447 (* 1 = 0.447 loss)
I0818 14:15:01.617527  2412 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0818 14:16:16.824255  2412 solver.cpp:357] Iteration 15100 (1.32968 iter/s, 75.2063s/100 iters), loss = 0.468407
I0818 14:16:16.824405  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468407 (* 1 = 0.468407 loss)
I0818 14:16:16.824419  2412 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0818 14:17:25.276587  2412 solver.cpp:357] Iteration 15200 (1.46093 iter/s, 68.4495s/100 iters), loss = 0.46569
I0818 14:17:25.276913  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.46569 (* 1 = 0.46569 loss)
I0818 14:17:25.276979  2412 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0818 14:17:47.870921  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:18:40.368314  2412 solver.cpp:357] Iteration 15300 (1.33175 iter/s, 75.0891s/100 iters), loss = 0.397702
I0818 14:18:40.368620  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397702 (* 1 = 0.397702 loss)
I0818 14:18:40.368683  2412 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0818 14:19:55.804095  2412 solver.cpp:357] Iteration 15400 (1.32564 iter/s, 75.4352s/100 iters), loss = 0.467226
I0818 14:19:55.804440  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467226 (* 1 = 0.467226 loss)
I0818 14:19:55.804489  2412 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0818 14:21:08.840741  2412 solver.cpp:514] Iteration 15500, Testing net (#0)
I0818 14:22:01.263293  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:22:01.484035  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6942
I0818 14:22:01.484176  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.03276 (* 1 = 1.03276 loss)
I0818 14:22:01.982365  2412 solver.cpp:357] Iteration 15500 (0.792538 iter/s, 126.177s/100 iters), loss = 0.475184
I0818 14:22:01.982559  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475184 (* 1 = 0.475184 loss)
I0818 14:22:01.982606  2412 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0818 14:23:12.344780  2412 solver.cpp:357] Iteration 15600 (1.42123 iter/s, 70.3618s/100 iters), loss = 0.401229
I0818 14:23:12.344985  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401229 (* 1 = 0.401229 loss)
I0818 14:23:12.345016  2412 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0818 14:23:26.513312  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:24:23.833652  2412 solver.cpp:357] Iteration 15700 (1.39883 iter/s, 71.4883s/100 iters), loss = 0.409412
I0818 14:24:23.833791  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409412 (* 1 = 0.409412 loss)
I0818 14:24:23.833803  2412 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0818 14:25:39.012043  2412 solver.cpp:357] Iteration 15800 (1.33021 iter/s, 75.1759s/100 iters), loss = 0.475402
I0818 14:25:39.012282  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475402 (* 1 = 0.475402 loss)
I0818 14:25:39.012293  2412 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0818 14:26:54.640983  2412 solver.cpp:357] Iteration 15900 (1.32239 iter/s, 75.6205s/100 iters), loss = 0.330093
I0818 14:26:54.641149  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330093 (* 1 = 0.330093 loss)
I0818 14:26:54.641160  2412 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0818 14:28:02.272939  2412 solver.cpp:514] Iteration 16000, Testing net (#0)
I0818 14:28:58.246718  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:28:58.505486  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6444
I0818 14:28:58.505548  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08535 (* 1 = 1.08535 loss)
I0818 14:28:59.109804  2412 solver.cpp:357] Iteration 16000 (0.803479 iter/s, 124.459s/100 iters), loss = 0.483037
I0818 14:28:59.109881  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483037 (* 1 = 0.483037 loss)
I0818 14:28:59.109891  2412 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0818 14:29:07.534489  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:30:14.589910  2412 solver.cpp:357] Iteration 16100 (1.32496 iter/s, 75.4741s/100 iters), loss = 0.471564
I0818 14:30:14.590101  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471564 (* 1 = 0.471564 loss)
I0818 14:30:14.590127  2412 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0818 14:31:29.866298  2412 solver.cpp:357] Iteration 16200 (1.32853 iter/s, 75.271s/100 iters), loss = 0.375882
I0818 14:31:29.866557  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375882 (* 1 = 0.375882 loss)
I0818 14:31:29.866617  2412 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0818 14:32:41.107117  2412 solver.cpp:357] Iteration 16300 (1.40383 iter/s, 71.2339s/100 iters), loss = 0.352738
I0818 14:32:41.110816  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352738 (* 1 = 0.352738 loss)
I0818 14:32:41.110855  2412 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0818 14:33:43.083317  2412 solver.cpp:357] Iteration 16400 (1.61368 iter/s, 61.9702s/100 iters), loss = 0.474292
I0818 14:33:43.086808  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474292 (* 1 = 0.474292 loss)
I0818 14:33:43.086845  2412 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0818 14:33:44.847450  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:34:55.646347  2412 solver.cpp:514] Iteration 16500, Testing net (#0)
I0818 14:35:55.112263  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:35:55.419770  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6921
I0818 14:35:55.419960  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.936083 (* 1 = 0.936083 loss)
I0818 14:35:55.960101  2412 solver.cpp:357] Iteration 16500 (0.75262 iter/s, 132.869s/100 iters), loss = 0.404599
I0818 14:35:55.960291  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404599 (* 1 = 0.404599 loss)
I0818 14:35:55.960337  2412 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0818 14:37:08.343422  2412 solver.cpp:357] Iteration 16600 (1.3816 iter/s, 72.3798s/100 iters), loss = 0.393059
I0818 14:37:08.349050  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393059 (* 1 = 0.393059 loss)
I0818 14:37:08.349083  2412 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0818 14:38:14.828774  2412 solver.cpp:357] Iteration 16700 (1.50431 iter/s, 66.4758s/100 iters), loss = 0.412419
I0818 14:38:14.829082  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412419 (* 1 = 0.412419 loss)
I0818 14:38:14.829113  2412 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0818 14:39:21.444463  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:39:27.270890  2412 solver.cpp:357] Iteration 16800 (1.38051 iter/s, 72.4369s/100 iters), loss = 0.426847
I0818 14:39:27.271061  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426847 (* 1 = 0.426847 loss)
I0818 14:39:27.271091  2412 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0818 14:40:39.837625  2412 solver.cpp:357] Iteration 16900 (1.37814 iter/s, 72.5617s/100 iters), loss = 0.380094
I0818 14:40:39.837930  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380094 (* 1 = 0.380094 loss)
I0818 14:40:39.837977  2412 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0818 14:41:53.651631  2412 solver.cpp:514] Iteration 17000, Testing net (#0)
I0818 14:42:48.909217  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:42:49.162134  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6614
I0818 14:42:49.162309  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07317 (* 1 = 1.07317 loss)
I0818 14:42:49.686745  2412 solver.cpp:357] Iteration 17000 (0.770146 iter/s, 129.846s/100 iters), loss = 0.378183
I0818 14:42:49.686939  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378183 (* 1 = 0.378183 loss)
I0818 14:42:49.686969  2412 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0818 14:43:50.926542  2412 solver.cpp:357] Iteration 17100 (1.63303 iter/s, 61.2358s/100 iters), loss = 0.429331
I0818 14:43:50.930914  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429331 (* 1 = 0.429331 loss)
I0818 14:43:50.930979  2412 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0818 14:44:53.071622  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:45:04.569675  2412 solver.cpp:357] Iteration 17200 (1.35798 iter/s, 73.6385s/100 iters), loss = 0.278078
I0818 14:45:04.569959  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.278078 (* 1 = 0.278078 loss)
I0818 14:45:04.570080  2412 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0818 14:46:18.877212  2412 solver.cpp:357] Iteration 17300 (1.3458 iter/s, 74.3052s/100 iters), loss = 0.412152
I0818 14:46:18.877408  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412152 (* 1 = 0.412152 loss)
I0818 14:46:18.877435  2412 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0818 14:47:31.844254  2412 solver.cpp:357] Iteration 17400 (1.37052 iter/s, 72.9648s/100 iters), loss = 0.432733
I0818 14:47:31.846894  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432733 (* 1 = 0.432733 loss)
I0818 14:47:31.846958  2412 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0818 14:48:40.357476  2412 solver.cpp:514] Iteration 17500, Testing net (#0)
I0818 14:49:35.696907  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:49:35.950788  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7255
I0818 14:49:35.951056  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.845755 (* 1 = 0.845755 loss)
I0818 14:49:36.603713  2412 solver.cpp:357] Iteration 17500 (0.801586 iter/s, 124.753s/100 iters), loss = 0.339896
I0818 14:49:36.604142  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339896 (* 1 = 0.339896 loss)
I0818 14:49:36.604321  2412 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0818 14:50:30.203071  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:50:49.773946  2412 solver.cpp:357] Iteration 17600 (1.36671 iter/s, 73.1682s/100 iters), loss = 0.346195
I0818 14:50:49.774129  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346195 (* 1 = 0.346195 loss)
I0818 14:50:49.774175  2412 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0818 14:52:02.446763  2412 solver.cpp:357] Iteration 17700 (1.37604 iter/s, 72.6722s/100 iters), loss = 0.488628
I0818 14:52:02.447008  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488628 (* 1 = 0.488628 loss)
I0818 14:52:02.447037  2412 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0818 14:53:06.534724  2412 solver.cpp:357] Iteration 17800 (1.56043 iter/s, 64.0848s/100 iters), loss = 0.363735
I0818 14:53:06.534989  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363735 (* 1 = 0.363735 loss)
I0818 14:53:06.535015  2412 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0818 14:54:14.977457  2412 solver.cpp:357] Iteration 17900 (1.46109 iter/s, 68.4422s/100 iters), loss = 0.354896
I0818 14:54:14.977993  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.354896 (* 1 = 0.354896 loss)
I0818 14:54:14.978164  2412 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0818 14:54:59.907697  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:55:26.042788  2412 solver.cpp:514] Iteration 18000, Testing net (#0)
I0818 14:56:22.927614  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:56:23.118536  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.716
I0818 14:56:23.118724  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.887856 (* 1 = 0.887856 loss)
I0818 14:56:23.669220  2412 solver.cpp:357] Iteration 18000 (0.77705 iter/s, 128.692s/100 iters), loss = 0.342763
I0818 14:56:23.669400  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342763 (* 1 = 0.342763 loss)
I0818 14:56:23.669445  2412 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0818 14:57:37.372341  2412 solver.cpp:357] Iteration 18100 (1.35683 iter/s, 73.7013s/100 iters), loss = 0.300321
I0818 14:57:37.378795  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300321 (* 1 = 0.300321 loss)
I0818 14:57:37.378832  2412 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0818 14:58:50.890254  2412 solver.cpp:357] Iteration 18200 (1.36036 iter/s, 73.5097s/100 iters), loss = 0.374849
I0818 14:58:50.894942  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374849 (* 1 = 0.374849 loss)
I0818 14:58:50.895048  2412 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0818 15:00:00.095782  2412 solver.cpp:357] Iteration 18300 (1.4451 iter/s, 69.1993s/100 iters), loss = 0.288321
I0818 15:00:00.096035  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.288321 (* 1 = 0.288321 loss)
I0818 15:00:00.096079  2412 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0818 15:00:40.067764  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:01:13.257032  2412 solver.cpp:357] Iteration 18400 (1.36667 iter/s, 73.1704s/100 iters), loss = 0.464269
I0818 15:01:13.262847  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464269 (* 1 = 0.464269 loss)
I0818 15:01:13.262888  2412 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0818 15:02:19.045568  2412 solver.cpp:514] Iteration 18500, Testing net (#0)
I0818 15:03:13.978155  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:03:14.278039  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5532
I0818 15:03:14.278214  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.53637 (* 1 = 1.53637 loss)
I0818 15:03:14.898967  2412 solver.cpp:357] Iteration 18500 (0.822015 iter/s, 121.652s/100 iters), loss = 0.347244
I0818 15:03:14.899181  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347244 (* 1 = 0.347244 loss)
I0818 15:03:14.899229  2412 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0818 15:04:25.790465  2412 solver.cpp:357] Iteration 18600 (1.41047 iter/s, 70.8985s/100 iters), loss = 0.445402
I0818 15:04:25.794944  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445402 (* 1 = 0.445402 loss)
I0818 15:04:25.795047  2412 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0818 15:05:36.279961  2412 solver.cpp:357] Iteration 18700 (1.41862 iter/s, 70.4913s/100 iters), loss = 0.245535
I0818 15:05:36.290802  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.245535 (* 1 = 0.245535 loss)
I0818 15:05:36.290843  2412 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0818 15:06:08.677767  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:06:49.874785  2412 solver.cpp:357] Iteration 18800 (1.35889 iter/s, 73.5897s/100 iters), loss = 0.281382
I0818 15:06:49.874948  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281382 (* 1 = 0.281382 loss)
I0818 15:06:49.874961  2412 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0818 15:08:05.086046  2412 solver.cpp:357] Iteration 18900 (1.32954 iter/s, 75.2142s/100 iters), loss = 0.417298
I0818 15:08:05.086169  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417298 (* 1 = 0.417298 loss)
I0818 15:08:05.086179  2412 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0818 15:09:19.544905  2412 solver.cpp:514] Iteration 19000, Testing net (#0)
I0818 15:10:08.595083  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:10:08.810643  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.397599
I0818 15:10:08.811026  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.321 (* 1 = 3.321 loss)
I0818 15:10:09.383092  2412 solver.cpp:357] Iteration 19000 (0.80446 iter/s, 124.307s/100 iters), loss = 0.327399
I0818 15:10:09.383546  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327399 (* 1 = 0.327399 loss)
I0818 15:10:09.383724  2412 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0818 15:11:21.714262  2412 solver.cpp:357] Iteration 19100 (1.3825 iter/s, 72.3326s/100 iters), loss = 0.356253
I0818 15:11:21.714498  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356253 (* 1 = 0.356253 loss)
I0818 15:11:21.714587  2412 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0818 15:11:43.190130  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:12:28.453629  2412 solver.cpp:357] Iteration 19200 (1.49835 iter/s, 66.7399s/100 iters), loss = 0.427022
I0818 15:12:28.458812  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.427022 (* 1 = 0.427022 loss)
I0818 15:12:28.458848  2412 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0818 15:13:42.085706  2412 solver.cpp:357] Iteration 19300 (1.35817 iter/s, 73.6284s/100 iters), loss = 0.383259
I0818 15:13:42.090857  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383259 (* 1 = 0.383259 loss)
I0818 15:13:42.090914  2412 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0818 15:14:54.831159  2412 solver.cpp:357] Iteration 19400 (1.3747 iter/s, 72.743s/100 iters), loss = 0.465497
I0818 15:14:54.834780  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.465497 (* 1 = 0.465497 loss)
I0818 15:14:54.834812  2412 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0818 15:16:03.639691  2412 solver.cpp:514] Iteration 19500, Testing net (#0)
I0818 15:16:59.401249  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:16:59.548912  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5541
I0818 15:16:59.548962  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.78266 (* 1 = 1.78266 loss)
I0818 15:17:00.211320  2412 solver.cpp:357] Iteration 19500 (0.797557 iter/s, 125.383s/100 iters), loss = 0.330706
I0818 15:17:00.211396  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330706 (* 1 = 0.330706 loss)
I0818 15:17:00.211407  2412 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0818 15:17:20.767238  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:18:15.668088  2412 solver.cpp:357] Iteration 19600 (1.32523 iter/s, 75.4586s/100 iters), loss = 0.409302
I0818 15:18:15.668212  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409302 (* 1 = 0.409302 loss)
I0818 15:18:15.668226  2412 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0818 15:19:31.197403  2412 solver.cpp:357] Iteration 19700 (1.324 iter/s, 75.5289s/100 iters), loss = 0.533318
I0818 15:19:31.197557  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.533318 (* 1 = 0.533318 loss)
I0818 15:19:31.197569  2412 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0818 15:20:40.199761  2412 solver.cpp:357] Iteration 19800 (1.44925 iter/s, 69.0014s/100 iters), loss = 0.327109
I0818 15:20:40.199903  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327109 (* 1 = 0.327109 loss)
I0818 15:20:40.199916  2412 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0818 15:21:49.827359  2412 solver.cpp:357] Iteration 19900 (1.43623 iter/s, 69.6266s/100 iters), loss = 0.332776
I0818 15:21:49.827471  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.332776 (* 1 = 0.332776 loss)
I0818 15:21:49.827481  2412 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0818 15:22:02.822405  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:23:04.561305  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_20000.caffemodel
I0818 15:23:04.587884  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_20000.solverstate
I0818 15:23:04.594727  2412 solver.cpp:514] Iteration 20000, Testing net (#0)
I0818 15:24:00.591223  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:24:00.901290  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.754499
I0818 15:24:00.901350  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.7797 (* 1 = 0.7797 loss)
I0818 15:24:01.468101  2412 solver.cpp:357] Iteration 20000 (0.75961 iter/s, 131.647s/100 iters), loss = 0.332153
I0818 15:24:01.468168  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.332153 (* 1 = 0.332153 loss)
I0818 15:24:01.468178  2412 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0818 15:25:16.488037  2412 solver.cpp:357] Iteration 20100 (1.33296 iter/s, 75.0209s/100 iters), loss = 0.351033
I0818 15:25:16.490795  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351033 (* 1 = 0.351033 loss)
I0818 15:25:16.490833  2412 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0818 15:26:22.096971  2412 solver.cpp:357] Iteration 20200 (1.52422 iter/s, 65.6073s/100 iters), loss = 0.369322
I0818 15:26:22.102768  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369322 (* 1 = 0.369322 loss)
I0818 15:26:22.102811  2412 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0818 15:27:35.153050  2412 solver.cpp:357] Iteration 20300 (1.36892 iter/s, 73.0505s/100 iters), loss = 0.296194
I0818 15:27:35.153316  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296194 (* 1 = 0.296194 loss)
I0818 15:27:35.153349  2412 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0818 15:27:41.792138  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:28:48.879076  2412 solver.cpp:357] Iteration 20400 (1.35636 iter/s, 73.7268s/100 iters), loss = 0.363898
I0818 15:28:48.879325  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363898 (* 1 = 0.363898 loss)
I0818 15:28:48.879354  2412 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0818 15:30:03.348807  2412 solver.cpp:514] Iteration 20500, Testing net (#0)
I0818 15:30:52.733978  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:30:52.896374  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7085
I0818 15:30:52.896477  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05232 (* 1 = 1.05232 loss)
I0818 15:30:53.385855  2412 solver.cpp:357] Iteration 20500 (0.803139 iter/s, 124.511s/100 iters), loss = 0.325229
I0818 15:30:53.385920  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325229 (* 1 = 0.325229 loss)
I0818 15:30:53.385931  2412 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0818 15:32:03.012821  2412 solver.cpp:357] Iteration 20600 (1.43617 iter/s, 69.6296s/100 iters), loss = 0.298966
I0818 15:32:03.012946  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.298966 (* 1 = 0.298966 loss)
I0818 15:32:03.012959  2412 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0818 15:33:17.734545  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:33:18.249835  2412 solver.cpp:357] Iteration 20700 (1.32912 iter/s, 75.2377s/100 iters), loss = 0.408262
I0818 15:33:18.249914  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408262 (* 1 = 0.408262 loss)
I0818 15:33:18.249925  2412 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0818 15:34:33.612743  2412 solver.cpp:357] Iteration 20800 (1.32698 iter/s, 75.3593s/100 iters), loss = 0.436314
I0818 15:34:33.612896  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.436314 (* 1 = 0.436314 loss)
I0818 15:34:33.612910  2412 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0818 15:35:49.052886  2412 solver.cpp:357] Iteration 20900 (1.3257 iter/s, 75.4318s/100 iters), loss = 0.31919
I0818 15:35:49.053045  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31919 (* 1 = 0.31919 loss)
I0818 15:35:49.053056  2412 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0818 15:36:56.630280  2412 solver.cpp:514] Iteration 21000, Testing net (#0)
I0818 15:37:52.652930  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:37:52.840911  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6708
I0818 15:37:52.840968  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.06875 (* 1 = 1.06875 loss)
I0818 15:37:53.392177  2412 solver.cpp:357] Iteration 21000 (0.804311 iter/s, 124.33s/100 iters), loss = 0.324398
I0818 15:37:53.392257  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.324398 (* 1 = 0.324398 loss)
I0818 15:37:53.392271  2412 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0818 15:39:00.680872  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:39:08.949203  2412 solver.cpp:357] Iteration 21100 (1.32362 iter/s, 75.5506s/100 iters), loss = 0.426903
I0818 15:39:08.949280  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426903 (* 1 = 0.426903 loss)
I0818 15:39:08.949292  2412 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0818 15:40:22.858999  2412 solver.cpp:357] Iteration 21200 (1.35311 iter/s, 73.9039s/100 iters), loss = 0.415757
I0818 15:40:22.859150  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415757 (* 1 = 0.415757 loss)
I0818 15:40:22.859161  2412 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0818 15:41:31.636574  2412 solver.cpp:357] Iteration 21300 (1.45403 iter/s, 68.7743s/100 iters), loss = 0.491825
I0818 15:41:31.636884  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491825 (* 1 = 0.491825 loss)
I0818 15:41:31.636950  2412 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0818 15:42:43.415546  2412 solver.cpp:357] Iteration 21400 (1.39322 iter/s, 71.7761s/100 iters), loss = 0.386696
I0818 15:42:43.415904  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386696 (* 1 = 0.386696 loss)
I0818 15:42:43.415951  2412 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0818 15:43:42.000838  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:43:56.375258  2412 solver.cpp:514] Iteration 21500, Testing net (#0)
I0818 15:44:51.707657  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:44:51.985625  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.602901
I0818 15:44:51.986034  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.15045 (* 1 = 1.15045 loss)
I0818 15:44:52.590752  2412 solver.cpp:357] Iteration 21500 (0.774159 iter/s, 129.173s/100 iters), loss = 0.256771
I0818 15:44:52.591163  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.256771 (* 1 = 0.256771 loss)
I0818 15:44:52.591339  2412 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0818 15:46:07.027523  2412 solver.cpp:357] Iteration 21600 (1.3435 iter/s, 74.4326s/100 iters), loss = 0.452446
I0818 15:46:07.027760  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.452446 (* 1 = 0.452446 loss)
I0818 15:46:07.027789  2412 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0818 15:47:11.468695  2412 solver.cpp:357] Iteration 21700 (1.55185 iter/s, 64.4392s/100 iters), loss = 0.391942
I0818 15:47:11.468968  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.391942 (* 1 = 0.391942 loss)
I0818 15:47:11.469015  2412 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0818 15:48:23.652413  2412 solver.cpp:357] Iteration 21800 (1.38543 iter/s, 72.1799s/100 iters), loss = 0.29995
I0818 15:48:23.652668  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29995 (* 1 = 0.29995 loss)
I0818 15:48:23.652698  2412 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0818 15:49:16.993577  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:49:37.097208  2412 solver.cpp:357] Iteration 21900 (1.3616 iter/s, 73.4432s/100 iters), loss = 0.488419
I0818 15:49:37.097604  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488419 (* 1 = 0.488419 loss)
I0818 15:49:37.097774  2412 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0818 15:50:43.377533  2412 solver.cpp:514] Iteration 22000, Testing net (#0)
I0818 15:51:40.348717  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:51:40.614166  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5193
I0818 15:51:40.618801  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.47984 (* 1 = 1.47984 loss)
I0818 15:51:41.116221  2412 solver.cpp:357] Iteration 22000 (0.806321 iter/s, 124.02s/100 iters), loss = 0.364726
I0818 15:51:41.116426  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364726 (* 1 = 0.364726 loss)
I0818 15:51:41.116473  2412 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0818 15:52:48.085525  2412 solver.cpp:357] Iteration 22100 (1.4933 iter/s, 66.9659s/100 iters), loss = 0.397589
I0818 15:52:48.090937  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397589 (* 1 = 0.397589 loss)
I0818 15:52:48.091043  2412 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0818 15:54:02.493456  2412 solver.cpp:357] Iteration 22200 (1.34408 iter/s, 74.4004s/100 iters), loss = 0.335865
I0818 15:54:02.500700  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335865 (* 1 = 0.335865 loss)
I0818 15:54:02.500762  2412 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0818 15:54:46.688501  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:55:15.526775  2412 solver.cpp:357] Iteration 22300 (1.36945 iter/s, 73.0219s/100 iters), loss = 0.35587
I0818 15:55:15.527102  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355871 (* 1 = 0.355871 loss)
I0818 15:55:15.527191  2412 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0818 15:56:28.195482  2412 solver.cpp:357] Iteration 22400 (1.37609 iter/s, 72.6696s/100 iters), loss = 0.340157
I0818 15:56:28.195703  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340157 (* 1 = 0.340157 loss)
I0818 15:56:28.195731  2412 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0818 15:57:37.346463  2412 solver.cpp:514] Iteration 22500, Testing net (#0)
I0818 15:58:32.663800  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:58:32.894361  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5493
I0818 15:58:32.894747  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.4216 (* 1 = 1.4216 loss)
I0818 15:58:33.462787  2412 solver.cpp:357] Iteration 22500 (0.798327 iter/s, 125.262s/100 iters), loss = 0.401861
I0818 15:58:33.463168  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401861 (* 1 = 0.401861 loss)
I0818 15:58:33.463340  2412 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0818 15:59:43.704298  2412 solver.cpp:357] Iteration 22600 (1.42364 iter/s, 70.2425s/100 iters), loss = 0.379601
I0818 15:59:43.710875  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379601 (* 1 = 0.379601 loss)
I0818 15:59:43.710935  2412 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0818 16:00:17.333472  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:00:53.199489  2412 solver.cpp:357] Iteration 22700 (1.43906 iter/s, 69.49s/100 iters), loss = 0.380836
I0818 16:00:53.199651  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380836 (* 1 = 0.380836 loss)
I0818 16:00:53.199673  2412 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0818 16:02:05.895229  2412 solver.cpp:357] Iteration 22800 (1.37557 iter/s, 72.697s/100 iters), loss = 0.416795
I0818 16:02:05.895510  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416795 (* 1 = 0.416795 loss)
I0818 16:02:05.895602  2412 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0818 16:03:12.310744  2412 solver.cpp:357] Iteration 22900 (1.5057 iter/s, 66.4142s/100 iters), loss = 0.390807
I0818 16:03:12.311111  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390807 (* 1 = 0.390807 loss)
I0818 16:03:12.311197  2412 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0818 16:04:24.684109  2412 solver.cpp:514] Iteration 23000, Testing net (#0)
I0818 16:05:22.615629  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:05:22.825182  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6024
I0818 16:05:22.830842  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.47636 (* 1 = 1.47636 loss)
I0818 16:05:23.495353  2412 solver.cpp:357] Iteration 23000 (0.76227 iter/s, 131.187s/100 iters), loss = 0.457258
I0818 16:05:23.495558  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457258 (* 1 = 0.457258 loss)
I0818 16:05:23.495605  2412 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0818 16:05:54.938354  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:06:36.335798  2412 solver.cpp:357] Iteration 23100 (1.37287 iter/s, 72.8399s/100 iters), loss = 0.399552
I0818 16:06:36.342833  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399552 (* 1 = 0.399552 loss)
I0818 16:06:36.342878  2412 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0818 16:07:49.187183  2412 solver.cpp:357] Iteration 23200 (1.37278 iter/s, 72.8449s/100 iters), loss = 0.386418
I0818 16:07:49.187433  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386418 (* 1 = 0.386418 loss)
I0818 16:07:49.187479  2412 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0818 16:08:51.953377  2412 solver.cpp:357] Iteration 23300 (1.59329 iter/s, 62.7631s/100 iters), loss = 0.31629
I0818 16:08:51.958794  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31629 (* 1 = 0.31629 loss)
I0818 16:08:51.958829  2412 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0818 16:10:03.587641  2412 solver.cpp:357] Iteration 23400 (1.39609 iter/s, 71.6288s/100 iters), loss = 0.339441
I0818 16:10:03.590919  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339441 (* 1 = 0.339441 loss)
I0818 16:10:03.591025  2412 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0818 16:10:28.276113  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:11:17.356879  2412 solver.cpp:514] Iteration 23500, Testing net (#0)
I0818 16:12:12.731734  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:12:12.845201  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.705
I0818 16:12:12.845378  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.897171 (* 1 = 0.897171 loss)
I0818 16:12:13.506249  2412 solver.cpp:357] Iteration 23500 (0.769725 iter/s, 129.917s/100 iters), loss = 0.289567
I0818 16:12:13.506438  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.289567 (* 1 = 0.289567 loss)
I0818 16:12:13.506484  2412 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0818 16:13:23.212215  2412 solver.cpp:357] Iteration 23600 (1.43466 iter/s, 69.7029s/100 iters), loss = 0.309093
I0818 16:13:23.212420  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.309093 (* 1 = 0.309093 loss)
I0818 16:13:23.212445  2412 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0818 16:14:31.983197  2412 solver.cpp:357] Iteration 23700 (1.45408 iter/s, 68.772s/100 iters), loss = 0.264477
I0818 16:14:31.983479  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.264477 (* 1 = 0.264477 loss)
I0818 16:14:31.983507  2412 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0818 16:15:46.036571  2412 solver.cpp:357] Iteration 23800 (1.35043 iter/s, 74.0505s/100 iters), loss = 0.429086
I0818 16:15:46.036921  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429087 (* 1 = 0.429087 loss)
I0818 16:15:46.037019  2412 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0818 16:16:02.391095  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:16:58.863703  2412 solver.cpp:357] Iteration 23900 (1.3731 iter/s, 72.8281s/100 iters), loss = 0.377038
I0818 16:16:58.866847  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377038 (* 1 = 0.377038 loss)
I0818 16:16:58.866904  2412 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0818 16:18:10.394731  2412 solver.cpp:514] Iteration 24000, Testing net (#0)
I0818 16:18:52.881542  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:18:53.014367  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6719
I0818 16:18:53.018834  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.15807 (* 1 = 1.15807 loss)
I0818 16:18:53.394767  2412 solver.cpp:357] Iteration 24000 (0.873165 iter/s, 114.526s/100 iters), loss = 0.406691
I0818 16:18:53.395145  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.406691 (* 1 = 0.406691 loss)
I0818 16:18:53.395314  2412 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0818 16:20:05.300442  2412 solver.cpp:357] Iteration 24100 (1.39069 iter/s, 71.9066s/100 iters), loss = 0.346707
I0818 16:20:05.300858  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346707 (* 1 = 0.346707 loss)
I0818 16:20:05.300951  2412 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0818 16:21:18.895246  2412 solver.cpp:357] Iteration 24200 (1.3588 iter/s, 73.5942s/100 iters), loss = 0.424044
I0818 16:21:18.898821  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424044 (* 1 = 0.424044 loss)
I0818 16:21:18.898922  2412 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0818 16:21:29.833061  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:22:31.877647  2412 solver.cpp:357] Iteration 24300 (1.37027 iter/s, 72.9784s/100 iters), loss = 0.651969
I0818 16:22:31.877823  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.651969 (* 1 = 0.651969 loss)
I0818 16:22:31.877856  2412 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0818 16:23:44.892319  2412 solver.cpp:357] Iteration 24400 (1.3696 iter/s, 73.014s/100 iters), loss = 0.323926
I0818 16:23:44.894867  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.323926 (* 1 = 0.323926 loss)
I0818 16:23:44.894925  2412 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0818 16:24:53.087723  2412 solver.cpp:514] Iteration 24500, Testing net (#0)
I0818 16:25:50.097331  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:25:50.360734  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6594
I0818 16:25:50.360893  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30524 (* 1 = 1.30524 loss)
I0818 16:25:50.926741  2412 solver.cpp:357] Iteration 24500 (0.793456 iter/s, 126.031s/100 iters), loss = 0.347519
I0818 16:25:50.926945  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347519 (* 1 = 0.347519 loss)
I0818 16:25:50.926992  2412 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0818 16:27:03.749123  2412 solver.cpp:357] Iteration 24600 (1.37318 iter/s, 72.8237s/100 iters), loss = 0.328307
I0818 16:27:03.749372  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328307 (* 1 = 0.328307 loss)
I0818 16:27:03.749397  2412 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0818 16:27:07.835779  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:28:10.238140  2412 solver.cpp:357] Iteration 24700 (1.50402 iter/s, 66.4883s/100 iters), loss = 0.377462
I0818 16:28:10.238538  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377462 (* 1 = 0.377462 loss)
I0818 16:28:10.238633  2412 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0818 16:29:19.141547  2412 solver.cpp:357] Iteration 24800 (1.45135 iter/s, 68.9013s/100 iters), loss = 0.294875
I0818 16:29:19.141744  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.294875 (* 1 = 0.294875 loss)
I0818 16:29:19.141769  2412 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0818 16:30:27.054746  2412 solver.cpp:357] Iteration 24900 (1.47249 iter/s, 67.9122s/100 iters), loss = 0.536363
I0818 16:30:27.054996  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.536363 (* 1 = 0.536363 loss)
I0818 16:30:27.055025  2412 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0818 16:31:36.313452  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:31:39.212162  2412 solver.cpp:514] Iteration 25000, Testing net (#0)
I0818 16:32:34.392454  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:32:34.634775  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.595301
I0818 16:32:34.635141  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.31587 (* 1 = 1.31587 loss)
I0818 16:32:35.243145  2412 solver.cpp:357] Iteration 25000 (0.780109 iter/s, 128.187s/100 iters), loss = 0.383427
I0818 16:32:35.243552  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383427 (* 1 = 0.383427 loss)
I0818 16:32:35.243724  2412 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0818 16:33:49.677248  2412 solver.cpp:357] Iteration 25100 (1.34348 iter/s, 74.4335s/100 iters), loss = 0.400915
I0818 16:33:49.683004  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400915 (* 1 = 0.400915 loss)
I0818 16:33:49.683107  2412 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0818 16:34:58.704064  2412 solver.cpp:357] Iteration 25200 (1.44886 iter/s, 69.0197s/100 iters), loss = 0.563376
I0818 16:34:58.704289  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.563376 (* 1 = 0.563376 loss)
I0818 16:34:58.704334  2412 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0818 16:36:11.533774  2412 solver.cpp:357] Iteration 25300 (1.37304 iter/s, 72.8311s/100 iters), loss = 0.377004
I0818 16:36:11.533993  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377004 (* 1 = 0.377004 loss)
I0818 16:36:11.534078  2412 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0818 16:37:14.337344  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:37:24.115267  2412 solver.cpp:357] Iteration 25400 (1.37777 iter/s, 72.5809s/100 iters), loss = 0.556219
I0818 16:37:24.115675  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.556219 (* 1 = 0.556219 loss)
I0818 16:37:24.115723  2412 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0818 16:38:29.574184  2412 solver.cpp:514] Iteration 25500, Testing net (#0)
I0818 16:39:24.888628  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:39:25.084003  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5188
I0818 16:39:25.084164  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.20813 (* 1 = 2.20813 loss)
I0818 16:39:25.652170  2412 solver.cpp:357] Iteration 25500 (0.822792 iter/s, 121.537s/100 iters), loss = 0.302329
I0818 16:39:25.652586  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302329 (* 1 = 0.302329 loss)
I0818 16:39:25.652757  2412 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0818 16:40:31.876265  2412 solver.cpp:357] Iteration 25600 (1.51009 iter/s, 66.2213s/100 iters), loss = 0.257228
I0818 16:40:31.876482  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.257228 (* 1 = 0.257228 loss)
I0818 16:40:31.876513  2412 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0818 16:41:45.102639  2412 solver.cpp:357] Iteration 25700 (1.36564 iter/s, 73.2258s/100 iters), loss = 0.463898
I0818 16:41:45.106837  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463898 (* 1 = 0.463898 loss)
I0818 16:41:45.106879  2412 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0818 16:42:41.086577  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:42:58.382009  2412 solver.cpp:357] Iteration 25800 (1.36465 iter/s, 73.2786s/100 iters), loss = 0.492415
I0818 16:42:58.382081  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.492415 (* 1 = 0.492415 loss)
I0818 16:42:58.382091  2412 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0818 16:44:13.792744  2412 solver.cpp:357] Iteration 25900 (1.326 iter/s, 75.4149s/100 iters), loss = 0.497467
I0818 16:44:13.792898  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497467 (* 1 = 0.497467 loss)
I0818 16:44:13.792909  2412 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0818 16:45:24.174347  2412 solver.cpp:514] Iteration 26000, Testing net (#0)
I0818 16:46:17.710515  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:46:17.974594  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7375
I0818 16:46:17.974737  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.82506 (* 1 = 0.82506 loss)
I0818 16:46:18.465761  2412 solver.cpp:357] Iteration 26000 (0.802039 iter/s, 124.682s/100 iters), loss = 0.347069
I0818 16:46:18.465919  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347069 (* 1 = 0.347069 loss)
I0818 16:46:18.465946  2412 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0818 16:47:27.821581  2412 solver.cpp:357] Iteration 26100 (1.44183 iter/s, 69.3561s/100 iters), loss = 0.284222
I0818 16:47:27.821849  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.284222 (* 1 = 0.284222 loss)
I0818 16:47:27.821895  2412 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0818 16:48:16.689085  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:48:41.390944  2412 solver.cpp:357] Iteration 26200 (1.35926 iter/s, 73.5696s/100 iters), loss = 0.402429
I0818 16:48:41.391141  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402429 (* 1 = 0.402429 loss)
I0818 16:48:41.391187  2412 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0818 16:49:56.358696  2412 solver.cpp:357] Iteration 26300 (1.33387 iter/s, 74.9699s/100 iters), loss = 0.274218
I0818 16:49:56.358866  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.274218 (* 1 = 0.274218 loss)
I0818 16:49:56.358880  2412 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0818 16:51:05.123477  2412 solver.cpp:357] Iteration 26400 (1.45425 iter/s, 68.7641s/100 iters), loss = 0.357829
I0818 16:51:05.123603  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357829 (* 1 = 0.357829 loss)
I0818 16:51:05.123615  2412 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0818 16:52:20.317263  2412 solver.cpp:514] Iteration 26500, Testing net (#0)
I0818 16:53:16.251297  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:53:16.533146  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5406
I0818 16:53:16.533210  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.01776 (* 1 = 2.01776 loss)
I0818 16:53:17.178483  2412 solver.cpp:357] Iteration 26500 (0.757235 iter/s, 132.059s/100 iters), loss = 0.342273
I0818 16:53:17.178548  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342273 (* 1 = 0.342273 loss)
I0818 16:53:17.178560  2412 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0818 16:54:01.258918  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:54:32.857689  2412 solver.cpp:357] Iteration 26600 (1.32134 iter/s, 75.6805s/100 iters), loss = 0.30284
I0818 16:54:32.857890  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.30284 (* 1 = 0.30284 loss)
I0818 16:54:32.857919  2412 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0818 16:55:45.181882  2412 solver.cpp:357] Iteration 26700 (1.38269 iter/s, 72.323s/100 iters), loss = 0.4508
I0818 16:55:45.186931  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4508 (* 1 = 0.4508 loss)
I0818 16:55:45.187037  2412 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0818 16:56:52.734263  2412 solver.cpp:357] Iteration 26800 (1.48045 iter/s, 67.547s/100 iters), loss = 0.378571
I0818 16:56:52.734534  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378571 (* 1 = 0.378571 loss)
I0818 16:56:52.734581  2412 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0818 16:57:59.573138  2412 solver.cpp:357] Iteration 26900 (1.49617 iter/s, 66.8373s/100 iters), loss = 0.298927
I0818 16:57:59.573310  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.298928 (* 1 = 0.298928 loss)
I0818 16:57:59.573321  2412 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0818 16:58:36.800832  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:59:14.698591  2412 solver.cpp:514] Iteration 27000, Testing net (#0)
I0818 17:00:10.612680  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:00:10.728370  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.423899
I0818 17:00:10.728430  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.41243 (* 1 = 2.41243 loss)
I0818 17:00:11.364161  2412 solver.cpp:357] Iteration 27000 (0.75876 iter/s, 131.794s/100 iters), loss = 0.395037
I0818 17:00:11.364239  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395037 (* 1 = 0.395037 loss)
I0818 17:00:11.364253  2412 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0818 17:01:18.894093  2412 solver.cpp:357] Iteration 27100 (1.48086 iter/s, 67.5281s/100 iters), loss = 0.36818
I0818 17:01:18.894316  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36818 (* 1 = 0.36818 loss)
I0818 17:01:18.894361  2412 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0818 17:02:33.138043  2412 solver.cpp:357] Iteration 27200 (1.34687 iter/s, 74.2462s/100 iters), loss = 0.410234
I0818 17:02:33.138371  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410234 (* 1 = 0.410234 loss)
I0818 17:02:33.138422  2412 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0818 17:03:46.749778  2412 solver.cpp:357] Iteration 27300 (1.35851 iter/s, 73.61s/100 iters), loss = 0.270111
I0818 17:03:46.750089  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.270111 (* 1 = 0.270111 loss)
I0818 17:03:46.750123  2412 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0818 17:04:16.358381  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:05:02.339426  2412 solver.cpp:357] Iteration 27400 (1.32289 iter/s, 75.592s/100 iters), loss = 0.359097
I0818 17:05:02.339813  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359097 (* 1 = 0.359097 loss)
I0818 17:05:02.339843  2412 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0818 17:06:15.744783  2412 solver.cpp:514] Iteration 27500, Testing net (#0)
I0818 17:07:02.213258  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:07:02.365764  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5858
I0818 17:07:02.365942  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.54627 (* 1 = 1.54627 loss)
I0818 17:07:02.932793  2412 solver.cpp:357] Iteration 27500 (0.829221 iter/s, 120.595s/100 iters), loss = 0.347661
I0818 17:07:02.932945  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347661 (* 1 = 0.347661 loss)
I0818 17:07:02.932974  2412 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0818 17:08:15.723333  2412 solver.cpp:357] Iteration 27600 (1.37384 iter/s, 72.7887s/100 iters), loss = 0.527045
I0818 17:08:15.730836  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.527045 (* 1 = 0.527045 loss)
I0818 17:08:15.730881  2412 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0818 17:09:29.548485  2412 solver.cpp:357] Iteration 27700 (1.35467 iter/s, 73.819s/100 iters), loss = 0.663382
I0818 17:09:29.554882  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.663382 (* 1 = 0.663382 loss)
I0818 17:09:29.554932  2412 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0818 17:09:52.455761  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:10:43.381841  2412 solver.cpp:357] Iteration 27800 (1.3545 iter/s, 73.8281s/100 iters), loss = 0.338397
I0818 17:10:43.382092  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338397 (* 1 = 0.338397 loss)
I0818 17:10:43.382122  2412 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0818 17:11:50.906517  2412 solver.cpp:357] Iteration 27900 (1.48099 iter/s, 67.5226s/100 iters), loss = 0.428818
I0818 17:11:50.910799  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428818 (* 1 = 0.428818 loss)
I0818 17:11:50.910830  2412 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0818 17:13:03.394990  2412 solver.cpp:514] Iteration 28000, Testing net (#0)
I0818 17:13:59.196518  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:13:59.550808  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6567
I0818 17:13:59.550961  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07311 (* 1 = 1.07311 loss)
I0818 17:14:00.106757  2412 solver.cpp:357] Iteration 28000 (0.774009 iter/s, 129.197s/100 iters), loss = 0.388476
I0818 17:14:00.106983  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388476 (* 1 = 0.388476 loss)
I0818 17:14:00.107030  2412 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0818 17:15:13.313026  2412 solver.cpp:357] Iteration 28100 (1.36599 iter/s, 73.2068s/100 iters), loss = 0.41939
I0818 17:15:13.313545  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41939 (* 1 = 0.41939 loss)
I0818 17:15:13.313784  2412 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0818 17:15:28.793799  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:16:24.272104  2412 solver.cpp:357] Iteration 28200 (1.40923 iter/s, 70.9607s/100 iters), loss = 0.414202
I0818 17:16:24.272617  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414202 (* 1 = 0.414202 loss)
I0818 17:16:24.272790  2412 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0818 17:17:23.993862  2412 solver.cpp:357] Iteration 28300 (1.6746 iter/s, 59.7158s/100 iters), loss = 0.367935
I0818 17:17:23.994808  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367935 (* 1 = 0.367935 loss)
I0818 17:17:23.994858  2412 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0818 17:18:36.276798  2412 solver.cpp:357] Iteration 28400 (1.38355 iter/s, 72.278s/100 iters), loss = 0.281106
I0818 17:18:36.277071  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281106 (* 1 = 0.281106 loss)
I0818 17:18:36.277132  2412 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0818 17:19:48.365183  2412 solver.cpp:514] Iteration 28500, Testing net (#0)
I0818 17:20:48.410562  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:20:48.714757  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4988
I0818 17:20:48.714913  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.75511 (* 1 = 1.75511 loss)
I0818 17:20:49.241598  2412 solver.cpp:357] Iteration 28500 (0.752124 iter/s, 132.957s/100 iters), loss = 0.382892
I0818 17:20:49.241806  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382892 (* 1 = 0.382892 loss)
I0818 17:20:49.241852  2412 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0818 17:20:57.451532  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:22:01.313761  2412 solver.cpp:357] Iteration 28600 (1.38757 iter/s, 72.0685s/100 iters), loss = 0.459039
I0818 17:22:01.318858  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.459039 (* 1 = 0.459039 loss)
I0818 17:22:01.318917  2412 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0818 17:23:05.776793  2412 solver.cpp:357] Iteration 28700 (1.55146 iter/s, 64.4555s/100 iters), loss = 0.325904
I0818 17:23:05.777115  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325904 (* 1 = 0.325904 loss)
I0818 17:23:05.777164  2412 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0818 17:24:18.902431  2412 solver.cpp:357] Iteration 28800 (1.36757 iter/s, 73.1227s/100 iters), loss = 0.268972
I0818 17:24:18.904187  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.268972 (* 1 = 0.268972 loss)
I0818 17:24:18.904239  2412 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0818 17:25:28.861127  2412 solver.cpp:357] Iteration 28900 (1.42947 iter/s, 69.956s/100 iters), loss = 0.528506
I0818 17:25:28.862921  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.528506 (* 1 = 0.528506 loss)
I0818 17:25:28.863024  2412 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0818 17:25:29.932322  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:26:35.585597  2412 solver.cpp:514] Iteration 29000, Testing net (#0)
I0818 17:27:30.553238  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:27:30.712515  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6301
I0818 17:27:30.712666  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.27256 (* 1 = 1.27256 loss)
I0818 17:27:31.312566  2412 solver.cpp:357] Iteration 29000 (0.816681 iter/s, 122.447s/100 iters), loss = 0.339797
I0818 17:27:31.312767  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339797 (* 1 = 0.339797 loss)
I0818 17:27:31.312813  2412 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0818 17:28:40.490711  2412 solver.cpp:357] Iteration 29100 (1.44564 iter/s, 69.1735s/100 iters), loss = 0.430154
I0818 17:28:40.490924  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430154 (* 1 = 0.430154 loss)
I0818 17:28:40.490955  2412 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0818 17:29:55.131901  2412 solver.cpp:357] Iteration 29200 (1.33977 iter/s, 74.6396s/100 iters), loss = 0.382916
I0818 17:29:55.134922  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382916 (* 1 = 0.382916 loss)
I0818 17:29:55.135058  2412 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0818 17:31:04.721287  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:31:10.251739  2412 solver.cpp:357] Iteration 29300 (1.33125 iter/s, 75.1172s/100 iters), loss = 0.393625
I0818 17:31:10.252009  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393625 (* 1 = 0.393625 loss)
I0818 17:31:10.252059  2412 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0818 17:32:22.661968  2412 solver.cpp:357] Iteration 29400 (1.38109 iter/s, 72.4064s/100 iters), loss = 0.341433
I0818 17:32:22.662309  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341433 (* 1 = 0.341433 loss)
I0818 17:32:22.662369  2412 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0818 17:33:25.605939  2412 solver.cpp:514] Iteration 29500, Testing net (#0)
I0818 17:34:22.611542  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:34:22.844070  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6352
I0818 17:34:22.844446  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30794 (* 1 = 1.30794 loss)
I0818 17:34:23.473388  2412 solver.cpp:357] Iteration 29500 (0.827744 iter/s, 120.81s/100 iters), loss = 0.346397
I0818 17:34:23.473804  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346397 (* 1 = 0.346397 loss)
I0818 17:34:23.473978  2412 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0818 17:35:30.078716  2412 solver.cpp:357] Iteration 29600 (1.50147 iter/s, 66.6013s/100 iters), loss = 0.343208
I0818 17:35:30.079075  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343208 (* 1 = 0.343208 loss)
I0818 17:35:30.079105  2412 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0818 17:36:29.167121  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:36:41.462244  2412 solver.cpp:357] Iteration 29700 (1.40088 iter/s, 71.3838s/100 iters), loss = 0.293209
I0818 17:36:41.462416  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.293209 (* 1 = 0.293209 loss)
I0818 17:36:41.462445  2412 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0818 17:37:54.690327  2412 solver.cpp:357] Iteration 29800 (1.36562 iter/s, 73.2267s/100 iters), loss = 0.247042
I0818 17:37:54.690819  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.247042 (* 1 = 0.247042 loss)
I0818 17:37:54.690851  2412 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0818 17:39:02.944147  2412 solver.cpp:357] Iteration 29900 (1.46515 iter/s, 68.2525s/100 iters), loss = 0.377696
I0818 17:39:02.947010  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377696 (* 1 = 0.377696 loss)
I0818 17:39:02.947196  2412 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0818 17:40:16.566162  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_30000.caffemodel
I0818 17:40:16.603260  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_30000.solverstate
I0818 17:40:16.610944  2412 solver.cpp:514] Iteration 30000, Testing net (#0)
I0818 17:41:14.344810  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:41:14.668244  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.617101
I0818 17:41:14.674799  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.28423 (* 1 = 1.28423 loss)
I0818 17:41:15.314714  2412 solver.cpp:357] Iteration 30000 (0.755473 iter/s, 132.367s/100 iters), loss = 0.273662
I0818 17:41:15.314916  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.273662 (* 1 = 0.273662 loss)
I0818 17:41:15.314963  2412 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0818 17:42:08.831094  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:42:28.305900  2412 solver.cpp:357] Iteration 30100 (1.37005 iter/s, 72.99s/100 iters), loss = 0.371421
I0818 17:42:28.310782  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371421 (* 1 = 0.371421 loss)
I0818 17:42:28.310842  2412 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0818 17:43:37.253047  2412 solver.cpp:357] Iteration 30200 (1.4505 iter/s, 68.9416s/100 iters), loss = 0.509144
I0818 17:43:37.253304  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509144 (* 1 = 0.509144 loss)
I0818 17:43:37.253352  2412 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0818 17:44:43.744668  2412 solver.cpp:357] Iteration 30300 (1.50402 iter/s, 66.4884s/100 iters), loss = 0.374863
I0818 17:44:43.750955  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374863 (* 1 = 0.374863 loss)
I0818 17:44:43.751060  2412 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0818 17:45:55.085198  2412 solver.cpp:357] Iteration 30400 (1.40185 iter/s, 71.3342s/100 iters), loss = 0.29604
I0818 17:45:55.090930  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29604 (* 1 = 0.29604 loss)
I0818 17:45:55.091035  2412 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0818 17:46:42.127331  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:47:08.448379  2412 solver.cpp:514] Iteration 30500, Testing net (#0)
I0818 17:48:04.466027  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:48:04.619060  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7086
I0818 17:48:04.619114  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.919586 (* 1 = 0.919586 loss)
I0818 17:48:05.170259  2412 solver.cpp:357] Iteration 30500 (0.768767 iter/s, 130.079s/100 iters), loss = 0.321897
I0818 17:48:05.170334  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.321897 (* 1 = 0.321897 loss)
I0818 17:48:05.170346  2412 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0818 17:49:13.875762  2412 solver.cpp:357] Iteration 30600 (1.45551 iter/s, 68.7044s/100 iters), loss = 0.313852
I0818 17:49:13.875919  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313852 (* 1 = 0.313852 loss)
I0818 17:49:13.875931  2412 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0818 17:50:29.396775  2412 solver.cpp:357] Iteration 30700 (1.32412 iter/s, 75.5221s/100 iters), loss = 0.347418
I0818 17:50:29.396924  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347418 (* 1 = 0.347418 loss)
I0818 17:50:29.396935  2412 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0818 17:51:45.031172  2412 solver.cpp:357] Iteration 30800 (1.32215 iter/s, 75.6342s/100 iters), loss = 0.271796
I0818 17:51:45.031318  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.271797 (* 1 = 0.271797 loss)
I0818 17:51:45.031332  2412 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0818 17:52:26.879395  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:53:00.781677  2412 solver.cpp:357] Iteration 30900 (1.32012 iter/s, 75.7505s/100 iters), loss = 0.512962
I0818 17:53:00.781793  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.512962 (* 1 = 0.512962 loss)
I0818 17:53:00.781805  2412 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0818 17:54:11.469115  2412 solver.cpp:514] Iteration 31000, Testing net (#0)
I0818 17:54:59.178104  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:54:59.281675  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6754
I0818 17:54:59.281736  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02989 (* 1 = 1.02989 loss)
I0818 17:54:59.922672  2412 solver.cpp:357] Iteration 31000 (0.839349 iter/s, 119.14s/100 iters), loss = 0.444583
I0818 17:54:59.922746  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444583 (* 1 = 0.444583 loss)
I0818 17:54:59.922758  2412 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0818 17:56:15.767107  2412 solver.cpp:357] Iteration 31100 (1.31853 iter/s, 75.8422s/100 iters), loss = 0.446432
I0818 17:56:15.767284  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446432 (* 1 = 0.446432 loss)
I0818 17:56:15.767311  2412 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0818 17:57:31.455938  2412 solver.cpp:357] Iteration 31200 (1.32124 iter/s, 75.6863s/100 iters), loss = 0.253462
I0818 17:57:31.456255  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.253462 (* 1 = 0.253462 loss)
I0818 17:57:31.456320  2412 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0818 17:58:05.557596  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:58:46.975131  2412 solver.cpp:357] Iteration 31300 (1.32421 iter/s, 75.5167s/100 iters), loss = 0.35906
I0818 17:58:46.975268  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.35906 (* 1 = 0.35906 loss)
I0818 17:58:46.975280  2412 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0818 17:59:55.205111  2412 solver.cpp:357] Iteration 31400 (1.46564 iter/s, 68.2293s/100 iters), loss = 0.336145
I0818 17:59:55.205227  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.336145 (* 1 = 0.336145 loss)
I0818 17:59:55.205240  2412 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0818 18:01:10.090621  2412 solver.cpp:514] Iteration 31500, Testing net (#0)
I0818 18:02:05.961822  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:02:06.245672  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6887
I0818 18:02:06.245731  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.992305 (* 1 = 0.992305 loss)
I0818 18:02:06.742913  2412 solver.cpp:357] Iteration 31500 (0.760245 iter/s, 131.537s/100 iters), loss = 0.27901
I0818 18:02:06.742981  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.27901 (* 1 = 0.27901 loss)
I0818 18:02:06.742993  2412 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0818 18:03:22.516757  2412 solver.cpp:357] Iteration 31600 (1.31976 iter/s, 75.7712s/100 iters), loss = 0.355837
I0818 18:03:22.516942  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355837 (* 1 = 0.355837 loss)
I0818 18:03:22.516953  2412 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0818 18:03:48.844872  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:04:31.982054  2412 solver.cpp:357] Iteration 31700 (1.43954 iter/s, 69.4665s/100 iters), loss = 0.400804
I0818 18:04:31.982226  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400804 (* 1 = 0.400804 loss)
I0818 18:04:31.982257  2412 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0818 18:05:41.795949  2412 solver.cpp:357] Iteration 31800 (1.43235 iter/s, 69.8152s/100 iters), loss = 0.351923
I0818 18:05:41.796146  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351923 (* 1 = 0.351923 loss)
I0818 18:05:41.796175  2412 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0818 18:06:57.480015  2412 solver.cpp:357] Iteration 31900 (1.32129 iter/s, 75.6835s/100 iters), loss = 0.463347
I0818 18:06:57.480135  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463347 (* 1 = 0.463347 loss)
I0818 18:06:57.480147  2412 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0818 18:08:12.575083  2412 solver.cpp:514] Iteration 32000, Testing net (#0)
I0818 18:09:08.480127  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:09:08.777997  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5356
I0818 18:09:08.778057  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.72382 (* 1 = 1.72382 loss)
I0818 18:09:09.331320  2412 solver.cpp:357] Iteration 32000 (0.758427 iter/s, 131.852s/100 iters), loss = 0.373697
I0818 18:09:09.331383  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373697 (* 1 = 0.373697 loss)
I0818 18:09:09.331393  2412 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0818 18:09:09.331400  2412 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0818 18:09:30.035493  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:10:18.669104  2412 solver.cpp:357] Iteration 32100 (1.44223 iter/s, 69.337s/100 iters), loss = 0.254367
I0818 18:10:18.669257  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.254367 (* 1 = 0.254367 loss)
I0818 18:10:18.669270  2412 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0818 18:11:33.701319  2412 solver.cpp:357] Iteration 32200 (1.33277 iter/s, 75.0316s/100 iters), loss = 0.352248
I0818 18:11:33.701441  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352248 (* 1 = 0.352248 loss)
I0818 18:11:33.701452  2412 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0818 18:12:49.389780  2412 solver.cpp:357] Iteration 32300 (1.32123 iter/s, 75.6873s/100 iters), loss = 0.160148
I0818 18:12:49.389926  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160148 (* 1 = 0.160148 loss)
I0818 18:12:49.389940  2412 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0818 18:14:01.735702  2412 solver.cpp:357] Iteration 32400 (1.3823 iter/s, 72.3431s/100 iters), loss = 0.18322
I0818 18:14:01.735906  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.18322 (* 1 = 0.18322 loss)
I0818 18:14:01.735937  2412 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0818 18:14:13.946327  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:15:12.623389  2412 solver.cpp:514] Iteration 32500, Testing net (#0)
I0818 18:16:03.422869  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:16:03.551542  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.850301
I0818 18:16:03.551591  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.461449 (* 1 = 0.461449 loss)
I0818 18:16:04.188050  2412 solver.cpp:357] Iteration 32500 (0.816643 iter/s, 122.453s/100 iters), loss = 0.189396
I0818 18:16:04.188130  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.189396 (* 1 = 0.189396 loss)
I0818 18:16:04.188143  2412 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0818 18:17:19.825620  2412 solver.cpp:357] Iteration 32600 (1.32211 iter/s, 75.6369s/100 iters), loss = 0.184967
I0818 18:17:19.825803  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.184967 (* 1 = 0.184967 loss)
I0818 18:17:19.825814  2412 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0818 18:18:35.439927  2412 solver.cpp:357] Iteration 32700 (1.32252 iter/s, 75.6131s/100 iters), loss = 0.180478
I0818 18:18:35.440101  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.180478 (* 1 = 0.180478 loss)
I0818 18:18:35.440114  2412 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0818 18:19:50.047796  2412 solver.cpp:357] Iteration 32800 (1.34039 iter/s, 74.605s/100 iters), loss = 0.146549
I0818 18:19:50.050909  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.146549 (* 1 = 0.146549 loss)
I0818 18:19:50.051014  2412 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0818 18:19:55.829473  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:20:58.941787  2412 solver.cpp:357] Iteration 32900 (1.45157 iter/s, 68.891s/100 iters), loss = 0.120651
I0818 18:20:58.946846  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120651 (* 1 = 0.120651 loss)
I0818 18:20:58.946890  2412 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0818 18:22:12.708521  2412 solver.cpp:514] Iteration 33000, Testing net (#0)
I0818 18:23:07.865885  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:23:08.060950  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.848101
I0818 18:23:08.061130  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.445596 (* 1 = 0.445596 loss)
I0818 18:23:08.636291  2412 solver.cpp:357] Iteration 33000 (0.771058 iter/s, 129.692s/100 iters), loss = 0.141228
I0818 18:23:08.636483  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.141228 (* 1 = 0.141228 loss)
I0818 18:23:08.636530  2412 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0818 18:24:19.346608  2412 solver.cpp:357] Iteration 33100 (1.41424 iter/s, 70.7093s/100 iters), loss = 0.147274
I0818 18:24:19.348187  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.147274 (* 1 = 0.147274 loss)
I0818 18:24:19.348219  2412 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0818 18:25:31.480638  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:25:31.991209  2412 solver.cpp:357] Iteration 33200 (1.37653 iter/s, 72.6465s/100 iters), loss = 0.131638
I0818 18:25:31.991588  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.131638 (* 1 = 0.131638 loss)
I0818 18:25:31.991761  2412 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0818 18:26:40.750965  2412 solver.cpp:357] Iteration 33300 (1.4543 iter/s, 68.7615s/100 iters), loss = 0.129592
I0818 18:26:40.754812  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129592 (* 1 = 0.129592 loss)
I0818 18:26:40.754842  2412 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0818 18:27:53.927911  2412 solver.cpp:357] Iteration 33400 (1.36653 iter/s, 73.1779s/100 iters), loss = 0.122192
I0818 18:27:53.928047  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122192 (* 1 = 0.122192 loss)
I0818 18:27:53.928058  2412 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0818 18:29:09.065657  2412 solver.cpp:514] Iteration 33500, Testing net (#0)
I0818 18:30:05.292773  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:30:05.469502  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.836101
I0818 18:30:05.469553  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.488199 (* 1 = 0.488199 loss)
I0818 18:30:06.123946  2412 solver.cpp:357] Iteration 33500 (0.756411 iter/s, 132.203s/100 iters), loss = 0.0817892
I0818 18:30:06.124017  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0817894 (* 1 = 0.0817894 loss)
I0818 18:30:06.124029  2412 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0818 18:31:09.657207  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:31:16.795657  2412 solver.cpp:357] Iteration 33600 (1.41495 iter/s, 70.674s/100 iters), loss = 0.230731
I0818 18:31:16.795724  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.230731 (* 1 = 0.230731 loss)
I0818 18:31:16.795734  2412 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0818 18:32:30.306978  2412 solver.cpp:357] Iteration 33700 (1.36026 iter/s, 73.5156s/100 iters), loss = 0.145399
I0818 18:32:30.307140  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145399 (* 1 = 0.145399 loss)
I0818 18:32:30.307153  2412 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0818 18:33:41.004005  2412 solver.cpp:357] Iteration 33800 (1.4145 iter/s, 70.6966s/100 iters), loss = 0.117238
I0818 18:33:41.004118  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117238 (* 1 = 0.117238 loss)
I0818 18:33:41.004130  2412 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0818 18:34:56.378226  2412 solver.cpp:357] Iteration 33900 (1.32672 iter/s, 75.3739s/100 iters), loss = 0.15774
I0818 18:34:56.378430  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.15774 (* 1 = 0.15774 loss)
I0818 18:34:56.378460  2412 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0818 18:35:57.008705  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:36:10.589085  2412 solver.cpp:514] Iteration 34000, Testing net (#0)
I0818 18:36:59.618135  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:36:59.889942  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7848
I0818 18:36:59.889993  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.672537 (* 1 = 0.672537 loss)
I0818 18:37:00.402524  2412 solver.cpp:357] Iteration 34000 (0.806283 iter/s, 124.026s/100 iters), loss = 0.108403
I0818 18:37:00.402593  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108403 (* 1 = 0.108403 loss)
I0818 18:37:00.402606  2412 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0818 18:38:16.024492  2412 solver.cpp:357] Iteration 34100 (1.32238 iter/s, 75.6211s/100 iters), loss = 0.149755
I0818 18:38:16.024650  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.149755 (* 1 = 0.149755 loss)
I0818 18:38:16.024662  2412 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0818 18:39:29.415225  2412 solver.cpp:357] Iteration 34200 (1.36251 iter/s, 73.3937s/100 iters), loss = 0.0642237
I0818 18:39:29.415508  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0642239 (* 1 = 0.0642239 loss)
I0818 18:39:29.415557  2412 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0818 18:40:42.381754  2412 solver.cpp:357] Iteration 34300 (1.37052 iter/s, 72.9653s/100 iters), loss = 0.0924033
I0818 18:40:42.381892  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0924035 (* 1 = 0.0924035 loss)
I0818 18:40:42.381904  2412 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0818 18:41:34.227429  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:41:53.648903  2412 solver.cpp:357] Iteration 34400 (1.40316 iter/s, 71.2678s/100 iters), loss = 0.238067
I0818 18:41:53.648984  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.238067 (* 1 = 0.238067 loss)
I0818 18:41:53.648995  2412 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0818 18:42:56.895056  2412 solver.cpp:514] Iteration 34500, Testing net (#0)
I0818 18:43:51.287678  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:43:51.496417  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7926
I0818 18:43:51.496479  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.630727 (* 1 = 0.630727 loss)
I0818 18:43:52.050480  2412 solver.cpp:357] Iteration 34500 (0.844566 iter/s, 118.404s/100 iters), loss = 0.092911
I0818 18:43:52.050549  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0929112 (* 1 = 0.0929112 loss)
I0818 18:43:52.050559  2412 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0818 18:45:07.947883  2412 solver.cpp:357] Iteration 34600 (1.31753 iter/s, 75.8994s/100 iters), loss = 0.194251
I0818 18:45:07.948101  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.194252 (* 1 = 0.194252 loss)
I0818 18:45:07.948127  2412 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0818 18:46:23.333243  2412 solver.cpp:357] Iteration 34700 (1.32647 iter/s, 75.3878s/100 iters), loss = 0.0745533
I0818 18:46:23.333407  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0745535 (* 1 = 0.0745535 loss)
I0818 18:46:23.333420  2412 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0818 18:47:05.909709  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:47:32.045677  2412 solver.cpp:357] Iteration 34800 (1.45538 iter/s, 68.7106s/100 iters), loss = 0.0932021
I0818 18:47:32.045753  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0932023 (* 1 = 0.0932023 loss)
I0818 18:47:32.045765  2412 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0818 18:48:47.572381  2412 solver.cpp:357] Iteration 34900 (1.32406 iter/s, 75.5251s/100 iters), loss = 0.124944
I0818 18:48:47.572533  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.124944 (* 1 = 0.124944 loss)
I0818 18:48:47.572546  2412 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0818 18:50:02.452277  2412 solver.cpp:514] Iteration 35000, Testing net (#0)
I0818 18:50:58.258886  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:50:58.467494  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8033
I0818 18:50:58.467646  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.607571 (* 1 = 0.607571 loss)
I0818 18:50:59.118222  2412 solver.cpp:357] Iteration 35000 (0.76019 iter/s, 131.546s/100 iters), loss = 0.111608
I0818 18:50:59.118396  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.111608 (* 1 = 0.111608 loss)
I0818 18:50:59.118441  2412 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0818 18:52:11.478343  2412 solver.cpp:357] Iteration 35100 (1.38201 iter/s, 72.3583s/100 iters), loss = 0.097142
I0818 18:52:11.478485  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0971422 (* 1 = 0.0971422 loss)
I0818 18:52:11.478497  2412 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0818 18:52:43.431270  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:53:18.689994  2412 solver.cpp:357] Iteration 35200 (1.48788 iter/s, 67.2096s/100 iters), loss = 0.16639
I0818 18:53:18.690141  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.16639 (* 1 = 0.16639 loss)
I0818 18:53:18.690155  2412 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0818 18:54:32.794322  2412 solver.cpp:357] Iteration 35300 (1.34948 iter/s, 74.1024s/100 iters), loss = 0.13183
I0818 18:54:32.798934  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13183 (* 1 = 0.13183 loss)
I0818 18:54:32.799191  2412 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0818 18:55:45.482295  2412 solver.cpp:357] Iteration 35400 (1.37584 iter/s, 72.6829s/100 iters), loss = 0.092
I0818 18:55:45.486934  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0920002 (* 1 = 0.0920002 loss)
I0818 18:55:45.487067  2412 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0818 18:56:58.517666  2412 solver.cpp:514] Iteration 35500, Testing net (#0)
I0818 18:57:50.783474  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:57:51.023263  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7726
I0818 18:57:51.023455  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.75361 (* 1 = 0.75361 loss)
I0818 18:57:51.530709  2412 solver.cpp:357] Iteration 35500 (0.793388 iter/s, 126.042s/100 iters), loss = 0.110972
I0818 18:57:51.530921  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110972 (* 1 = 0.110972 loss)
I0818 18:57:51.530968  2412 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0818 18:58:22.387228  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:59:04.405783  2412 solver.cpp:357] Iteration 35600 (1.37217 iter/s, 72.877s/100 iters), loss = 0.145976
I0818 18:59:04.410791  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145977 (* 1 = 0.145977 loss)
I0818 18:59:04.410826  2412 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0818 19:00:16.999892  2412 solver.cpp:357] Iteration 35700 (1.37774 iter/s, 72.5825s/100 iters), loss = 0.104889
I0818 19:00:17.002802  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104889 (* 1 = 0.104889 loss)
I0818 19:00:17.002835  2412 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0818 19:01:30.552934  2412 solver.cpp:357] Iteration 35800 (1.35973 iter/s, 73.5442s/100 iters), loss = 0.0864375
I0818 19:01:30.553143  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0864376 (* 1 = 0.0864376 loss)
I0818 19:01:30.553194  2412 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0818 19:02:37.023524  2412 solver.cpp:357] Iteration 35900 (1.50451 iter/s, 66.4667s/100 iters), loss = 0.150329
I0818 19:02:37.026932  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150329 (* 1 = 0.150329 loss)
I0818 19:02:37.027034  2412 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0818 19:02:56.680130  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:03:43.335237  2412 solver.cpp:514] Iteration 36000, Testing net (#0)
I0818 19:04:38.955708  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:04:39.140393  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.801
I0818 19:04:39.140602  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.64104 (* 1 = 0.64104 loss)
I0818 19:04:39.786991  2412 solver.cpp:357] Iteration 36000 (0.814616 iter/s, 122.757s/100 iters), loss = 0.157677
I0818 19:04:39.787240  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.157677 (* 1 = 0.157677 loss)
I0818 19:04:39.787290  2412 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0818 19:05:52.054739  2412 solver.cpp:357] Iteration 36100 (1.38384 iter/s, 72.2626s/100 iters), loss = 0.11712
I0818 19:05:52.058904  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11712 (* 1 = 0.11712 loss)
I0818 19:05:52.058971  2412 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0818 19:07:04.418728  2412 solver.cpp:357] Iteration 36200 (1.38202 iter/s, 72.3581s/100 iters), loss = 0.0569507
I0818 19:07:04.418947  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0569509 (* 1 = 0.0569509 loss)
I0818 19:07:04.418975  2412 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0818 19:08:15.132735  2412 solver.cpp:357] Iteration 36300 (1.41422 iter/s, 70.7106s/100 iters), loss = 0.137105
I0818 19:08:15.133149  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137105 (* 1 = 0.137105 loss)
I0818 19:08:15.133235  2412 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0818 19:08:27.917654  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:09:22.795418  2412 solver.cpp:357] Iteration 36400 (1.47801 iter/s, 67.6585s/100 iters), loss = 0.0863914
I0818 19:09:22.795626  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0863916 (* 1 = 0.0863916 loss)
I0818 19:09:22.795661  2412 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0818 19:10:35.089200  2412 solver.cpp:514] Iteration 36500, Testing net (#0)
I0818 19:11:26.428092  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:11:26.628995  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.824701
I0818 19:11:26.629123  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.546912 (* 1 = 0.546912 loss)
I0818 19:11:27.213712  2412 solver.cpp:357] Iteration 36500 (0.803764 iter/s, 124.415s/100 iters), loss = 0.105398
I0818 19:11:27.218819  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.105398 (* 1 = 0.105398 loss)
I0818 19:11:27.218911  2412 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0818 19:12:39.369596  2412 solver.cpp:357] Iteration 36600 (1.38598 iter/s, 72.1513s/100 iters), loss = 0.0501477
I0818 19:12:39.375039  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0501479 (* 1 = 0.0501479 loss)
I0818 19:12:39.375221  2412 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0818 19:13:46.458760  2412 solver.cpp:357] Iteration 36700 (1.49066 iter/s, 67.0844s/100 iters), loss = 0.112096
I0818 19:13:46.458986  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112096 (* 1 = 0.112096 loss)
I0818 19:13:46.459013  2412 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0818 19:13:57.500982  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:14:58.953486  2412 solver.cpp:357] Iteration 36800 (1.37944 iter/s, 72.4934s/100 iters), loss = 0.114118
I0818 19:14:58.953743  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114118 (* 1 = 0.114118 loss)
I0818 19:14:58.953778  2412 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0818 19:16:11.570647  2412 solver.cpp:357] Iteration 36900 (1.37715 iter/s, 72.6138s/100 iters), loss = 0.115061
I0818 19:16:11.570860  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.115061 (* 1 = 0.115061 loss)
I0818 19:16:11.570888  2412 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0818 19:17:23.847226  2412 solver.cpp:514] Iteration 37000, Testing net (#0)
I0818 19:18:20.767446  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:18:20.984894  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.854201
I0818 19:18:20.985071  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.458307 (* 1 = 0.458307 loss)
I0818 19:18:21.469766  2412 solver.cpp:357] Iteration 37000 (0.76983 iter/s, 129.899s/100 iters), loss = 0.134632
I0818 19:18:21.474858  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.134632 (* 1 = 0.134632 loss)
I0818 19:18:21.474930  2412 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0818 19:19:24.647322  2412 solver.cpp:357] Iteration 37100 (1.583 iter/s, 63.1711s/100 iters), loss = 0.126594
I0818 19:19:24.647625  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126594 (* 1 = 0.126594 loss)
I0818 19:19:24.647673  2412 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0818 19:19:28.674214  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:20:37.362802  2412 solver.cpp:357] Iteration 37200 (1.37529 iter/s, 72.712s/100 iters), loss = 0.122939
I0818 19:20:37.363009  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12294 (* 1 = 0.12294 loss)
I0818 19:20:37.363035  2412 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0818 19:21:44.597115  2412 solver.cpp:357] Iteration 37300 (1.48734 iter/s, 67.2342s/100 iters), loss = 0.0882185
I0818 19:21:44.602957  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0882187 (* 1 = 0.0882187 loss)
I0818 19:21:44.603214  2412 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0818 19:22:58.462049  2412 solver.cpp:357] Iteration 37400 (1.35395 iter/s, 73.8577s/100 iters), loss = 0.127826
I0818 19:22:58.463634  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127826 (* 1 = 0.127826 loss)
I0818 19:22:58.463820  2412 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0818 19:24:05.189579  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:24:07.652288  2412 solver.cpp:514] Iteration 37500, Testing net (#0)
I0818 19:25:01.003878  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:25:01.304467  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.803501
I0818 19:25:01.304679  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.644712 (* 1 = 0.644712 loss)
I0818 19:25:01.869343  2412 solver.cpp:357] Iteration 37500 (0.810367 iter/s, 123.401s/100 iters), loss = 0.0815524
I0818 19:25:01.869524  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0815526 (* 1 = 0.0815526 loss)
I0818 19:25:01.869570  2412 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0818 19:26:15.938712  2412 solver.cpp:357] Iteration 37600 (1.3501 iter/s, 74.0686s/100 iters), loss = 0.0829628
I0818 19:26:15.939272  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.082963 (* 1 = 0.082963 loss)
I0818 19:26:15.939371  2412 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0818 19:27:28.733063  2412 solver.cpp:357] Iteration 37700 (1.37375 iter/s, 72.7936s/100 iters), loss = 0.0977398
I0818 19:27:28.738939  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0977399 (* 1 = 0.0977399 loss)
I0818 19:27:28.739044  2412 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0818 19:28:42.125301  2412 solver.cpp:357] Iteration 37800 (1.36268 iter/s, 73.3851s/100 iters), loss = 0.100669
I0818 19:28:42.130920  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100669 (* 1 = 0.100669 loss)
I0818 19:28:42.131021  2412 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0818 19:29:42.132448  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:29:51.489218  2412 solver.cpp:357] Iteration 37900 (1.44176 iter/s, 69.3597s/100 iters), loss = 0.108372
I0818 19:29:51.489631  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108372 (* 1 = 0.108372 loss)
I0818 19:29:51.489804  2412 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0818 19:31:00.592926  2412 solver.cpp:514] Iteration 38000, Testing net (#0)
I0818 19:31:54.600100  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:31:54.835198  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7952
I0818 19:31:54.835386  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.720823 (* 1 = 0.720823 loss)
I0818 19:31:55.500351  2412 solver.cpp:357] Iteration 38000 (0.80639 iter/s, 124.009s/100 iters), loss = 0.0397279
I0818 19:31:55.500609  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0397281 (* 1 = 0.0397281 loss)
I0818 19:31:55.500658  2412 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0818 19:33:08.711227  2412 solver.cpp:357] Iteration 38100 (1.36597 iter/s, 73.2081s/100 iters), loss = 0.0486296
I0818 19:33:08.711374  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0486298 (* 1 = 0.0486298 loss)
I0818 19:33:08.711387  2412 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0818 19:34:22.740866  2412 solver.cpp:357] Iteration 38200 (1.3509 iter/s, 74.025s/100 iters), loss = 0.0704436
I0818 19:34:22.741016  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0704438 (* 1 = 0.0704438 loss)
I0818 19:34:22.741029  2412 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0818 19:35:15.387233  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:35:32.683032  2412 solver.cpp:357] Iteration 38300 (1.4298 iter/s, 69.9398s/100 iters), loss = 0.16435
I0818 19:35:32.683107  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.16435 (* 1 = 0.16435 loss)
I0818 19:35:32.683118  2412 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0818 19:36:48.131950  2412 solver.cpp:357] Iteration 38400 (1.32547 iter/s, 75.4447s/100 iters), loss = 0.104631
I0818 19:36:48.132061  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104632 (* 1 = 0.104632 loss)
I0818 19:36:48.132072  2412 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0818 19:38:03.125510  2412 solver.cpp:514] Iteration 38500, Testing net (#0)
I0818 19:38:59.280480  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:38:59.480311  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7929
I0818 19:38:59.480374  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.740715 (* 1 = 0.740715 loss)
I0818 19:39:00.114501  2412 solver.cpp:357] Iteration 38500 (0.757674 iter/s, 131.983s/100 iters), loss = 0.107192
I0818 19:39:00.114579  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107192 (* 1 = 0.107192 loss)
I0818 19:39:00.114593  2412 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0818 19:40:07.298660  2412 solver.cpp:357] Iteration 38600 (1.48853 iter/s, 67.1803s/100 iters), loss = 0.0693068
I0818 19:40:07.298853  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.069307 (* 1 = 0.069307 loss)
I0818 19:40:07.298866  2412 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0818 19:40:54.454541  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:41:19.373566  2412 solver.cpp:357] Iteration 38700 (1.38744 iter/s, 72.0752s/100 iters), loss = 0.0771064
I0818 19:41:19.373634  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0771066 (* 1 = 0.0771066 loss)
I0818 19:41:19.373644  2412 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0818 19:42:34.906196  2412 solver.cpp:357] Iteration 38800 (1.32392 iter/s, 75.5332s/100 iters), loss = 0.0728685
I0818 19:42:34.906352  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0728686 (* 1 = 0.0728686 loss)
I0818 19:42:34.906365  2412 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0818 19:43:50.497360  2412 solver.cpp:357] Iteration 38900 (1.32297 iter/s, 75.5877s/100 iters), loss = 0.0494485
I0818 19:43:50.497506  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0494487 (* 1 = 0.0494487 loss)
I0818 19:43:50.497519  2412 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0818 19:45:03.325304  2412 solver.cpp:514] Iteration 39000, Testing net (#0)
I0818 19:45:53.878403  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:45:54.075099  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.843002
I0818 19:45:54.075158  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.512391 (* 1 = 0.512391 loss)
I0818 19:45:54.623911  2412 solver.cpp:357] Iteration 39000 (0.805634 iter/s, 124.126s/100 iters), loss = 0.0408212
I0818 19:45:54.623980  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0408214 (* 1 = 0.0408214 loss)
I0818 19:45:54.623991  2412 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0818 19:46:38.578287  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:47:10.076155  2412 solver.cpp:357] Iteration 39100 (1.32533 iter/s, 75.453s/100 iters), loss = 0.121539
I0818 19:47:10.076321  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121539 (* 1 = 0.121539 loss)
I0818 19:47:10.076333  2412 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0818 19:48:25.716981  2412 solver.cpp:357] Iteration 39200 (1.32209 iter/s, 75.6376s/100 iters), loss = 0.104447
I0818 19:48:25.717144  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104447 (* 1 = 0.104447 loss)
I0818 19:48:25.717156  2412 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0818 19:49:40.576483  2412 solver.cpp:357] Iteration 39300 (1.33589 iter/s, 74.8563s/100 iters), loss = 0.129046
I0818 19:49:40.576628  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129046 (* 1 = 0.129046 loss)
I0818 19:49:40.576639  2412 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0818 19:50:46.175834  2412 solver.cpp:357] Iteration 39400 (1.52439 iter/s, 65.6002s/100 iters), loss = 0.111056
I0818 19:50:46.175981  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.111056 (* 1 = 0.111056 loss)
I0818 19:50:46.175993  2412 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0818 19:51:22.780288  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:52:00.489115  2412 solver.cpp:514] Iteration 39500, Testing net (#0)
I0818 19:52:56.584424  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:52:56.809442  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.835101
I0818 19:52:56.809535  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.565041 (* 1 = 0.565041 loss)
I0818 19:52:57.421236  2412 solver.cpp:357] Iteration 39500 (0.761944 iter/s, 131.243s/100 iters), loss = 0.181218
I0818 19:52:57.421309  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.181218 (* 1 = 0.181218 loss)
I0818 19:52:57.421319  2412 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0818 19:54:13.204957  2412 solver.cpp:357] Iteration 39600 (1.31952 iter/s, 75.7849s/100 iters), loss = 0.0489907
I0818 19:54:13.205237  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0489908 (* 1 = 0.0489908 loss)
I0818 19:54:13.205269  2412 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0818 19:55:28.275212  2412 solver.cpp:357] Iteration 39700 (1.33214 iter/s, 75.0672s/100 iters), loss = 0.0677475
I0818 19:55:28.275424  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0677476 (* 1 = 0.0677476 loss)
I0818 19:55:28.275440  2412 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0818 19:56:37.674960  2412 solver.cpp:357] Iteration 39800 (1.44097 iter/s, 69.3975s/100 iters), loss = 0.112006
I0818 19:56:37.675148  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112006 (* 1 = 0.112006 loss)
I0818 19:56:37.675179  2412 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0818 19:57:07.258802  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:57:53.338649  2412 solver.cpp:357] Iteration 39900 (1.32169 iter/s, 75.6607s/100 iters), loss = 0.0879262
I0818 19:57:53.338814  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0879264 (* 1 = 0.0879264 loss)
I0818 19:57:53.338825  2412 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0818 19:59:08.320230  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_40000.caffemodel
I0818 19:59:08.345244  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_40000.solverstate
I0818 19:59:08.352141  2412 solver.cpp:514] Iteration 40000, Testing net (#0)
I0818 20:00:00.043211  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:00:00.203795  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.859501
I0818 20:00:00.203858  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.466461 (* 1 = 0.466461 loss)
I0818 20:00:00.763891  2412 solver.cpp:357] Iteration 40000 (0.784773 iter/s, 127.425s/100 iters), loss = 0.118483
I0818 20:00:00.763965  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118483 (* 1 = 0.118483 loss)
I0818 20:00:00.763978  2412 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0818 20:01:10.641386  2412 solver.cpp:357] Iteration 40100 (1.4311 iter/s, 69.8765s/100 iters), loss = 0.100936
I0818 20:01:10.641535  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100937 (* 1 = 0.100937 loss)
I0818 20:01:10.641547  2412 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0818 20:02:24.542544  2412 solver.cpp:357] Iteration 40200 (1.35317 iter/s, 73.9003s/100 iters), loss = 0.0814036
I0818 20:02:24.542738  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0814038 (* 1 = 0.0814038 loss)
I0818 20:02:24.542765  2412 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0818 20:02:47.450954  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:03:40.246466  2412 solver.cpp:357] Iteration 40300 (1.32095 iter/s, 75.7031s/100 iters), loss = 0.0588121
I0818 20:03:40.246626  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0588123 (* 1 = 0.0588123 loss)
I0818 20:03:40.246639  2412 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0818 20:04:54.091902  2412 solver.cpp:357] Iteration 40400 (1.3542 iter/s, 73.8446s/100 iters), loss = 0.0593847
I0818 20:04:54.092022  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0593849 (* 1 = 0.0593849 loss)
I0818 20:04:54.092034  2412 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0818 20:06:07.738500  2412 solver.cpp:514] Iteration 40500, Testing net (#0)
I0818 20:06:57.299100  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:06:57.573618  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.828301
I0818 20:06:57.573691  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.608444 (* 1 = 0.608444 loss)
I0818 20:06:58.135643  2412 solver.cpp:357] Iteration 40500 (0.806158 iter/s, 124.045s/100 iters), loss = 0.0958521
I0818 20:06:58.135720  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0958523 (* 1 = 0.0958523 loss)
I0818 20:06:58.135731  2412 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0818 20:08:13.793891  2412 solver.cpp:357] Iteration 40600 (1.32182 iter/s, 75.6534s/100 iters), loss = 0.0586792
I0818 20:08:13.794231  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0586794 (* 1 = 0.0586794 loss)
I0818 20:08:13.794245  2412 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0818 20:08:29.253962  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:09:26.290930  2412 solver.cpp:357] Iteration 40700 (1.37952 iter/s, 72.4892s/100 iters), loss = 0.0779215
I0818 20:09:26.291056  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0779217 (* 1 = 0.0779217 loss)
I0818 20:09:26.291067  2412 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0818 20:10:40.277837  2412 solver.cpp:357] Iteration 40800 (1.35172 iter/s, 73.9795s/100 iters), loss = 0.101689
I0818 20:10:40.277977  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101689 (* 1 = 0.101689 loss)
I0818 20:10:40.277988  2412 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0818 20:11:50.353298  2412 solver.cpp:357] Iteration 40900 (1.42713 iter/s, 70.0708s/100 iters), loss = 0.16114
I0818 20:11:50.353436  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.16114 (* 1 = 0.16114 loss)
I0818 20:11:50.353448  2412 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0818 20:13:03.718487  2412 solver.cpp:514] Iteration 41000, Testing net (#0)
I0818 20:13:59.357990  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:13:59.527724  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8115
I0818 20:13:59.527776  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.655512 (* 1 = 0.655512 loss)
I0818 20:14:00.179399  2412 solver.cpp:357] Iteration 41000 (0.770296 iter/s, 129.82s/100 iters), loss = 0.0336823
I0818 20:14:00.179474  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0336825 (* 1 = 0.0336825 loss)
I0818 20:14:00.179487  2412 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0818 20:14:08.622393  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:15:15.733603  2412 solver.cpp:357] Iteration 41100 (1.32366 iter/s, 75.5483s/100 iters), loss = 0.12265
I0818 20:15:15.733707  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12265 (* 1 = 0.12265 loss)
I0818 20:15:15.733717  2412 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0818 20:16:29.421290  2412 solver.cpp:357] Iteration 41200 (1.35712 iter/s, 73.6856s/100 iters), loss = 0.0882808
I0818 20:16:29.421404  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0882809 (* 1 = 0.0882809 loss)
I0818 20:16:29.421416  2412 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0818 20:17:38.093158  2412 solver.cpp:357] Iteration 41300 (1.45627 iter/s, 68.6686s/100 iters), loss = 0.0748818
I0818 20:17:38.093281  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.074882 (* 1 = 0.074882 loss)
I0818 20:17:38.093292  2412 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0818 20:18:51.747809  2412 solver.cpp:357] Iteration 41400 (1.35778 iter/s, 73.6494s/100 iters), loss = 0.0597219
I0818 20:18:51.747974  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0597221 (* 1 = 0.0597221 loss)
I0818 20:18:51.748016  2412 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0818 20:18:53.347229  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:20:03.626370  2412 solver.cpp:514] Iteration 41500, Testing net (#0)
I0818 20:20:59.791735  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:21:00.093014  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.849301
I0818 20:21:00.093066  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.524809 (* 1 = 0.524809 loss)
I0818 20:21:00.677958  2412 solver.cpp:357] Iteration 41500 (0.775622 iter/s, 128.929s/100 iters), loss = 0.0469101
I0818 20:21:00.678025  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0469102 (* 1 = 0.0469102 loss)
I0818 20:21:00.678035  2412 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0818 20:22:11.878249  2412 solver.cpp:357] Iteration 41600 (1.40454 iter/s, 71.1976s/100 iters), loss = 0.0747473
I0818 20:22:11.878509  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0747474 (* 1 = 0.0747474 loss)
I0818 20:22:11.878522  2412 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0818 20:23:24.284373  2412 solver.cpp:357] Iteration 41700 (1.38116 iter/s, 72.403s/100 iters), loss = 0.11641
I0818 20:23:24.284597  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.116411 (* 1 = 0.116411 loss)
I0818 20:23:24.284627  2412 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0818 20:24:34.078964  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:24:39.893049  2412 solver.cpp:357] Iteration 41800 (1.32264 iter/s, 75.6062s/100 iters), loss = 0.0891879
I0818 20:24:39.893224  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0891881 (* 1 = 0.0891881 loss)
I0818 20:24:39.893261  2412 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0818 20:25:53.195562  2412 solver.cpp:357] Iteration 41900 (1.36429 iter/s, 73.2981s/100 iters), loss = 0.0661874
I0818 20:25:53.195705  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0661876 (* 1 = 0.0661876 loss)
I0818 20:25:53.195715  2412 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0818 20:27:08.144778  2412 solver.cpp:514] Iteration 42000, Testing net (#0)
I0818 20:27:56.724544  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:27:56.942941  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.871301
I0818 20:27:56.942999  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.411591 (* 1 = 0.411591 loss)
I0818 20:27:57.440691  2412 solver.cpp:357] Iteration 42000 (0.804861 iter/s, 124.245s/100 iters), loss = 0.0761153
I0818 20:27:57.440770  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0761155 (* 1 = 0.0761155 loss)
I0818 20:27:57.440781  2412 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0818 20:29:08.259258  2412 solver.cpp:357] Iteration 42100 (1.41214 iter/s, 70.8143s/100 iters), loss = 0.0458892
I0818 20:29:08.259435  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0458894 (* 1 = 0.0458894 loss)
I0818 20:29:08.259462  2412 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0818 20:30:11.214107  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:30:23.741039  2412 solver.cpp:357] Iteration 42200 (1.32482 iter/s, 75.4817s/100 iters), loss = 0.0930136
I0818 20:30:23.741122  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0930137 (* 1 = 0.0930137 loss)
I0818 20:30:23.741134  2412 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0818 20:31:39.519330  2412 solver.cpp:357] Iteration 42300 (1.31971 iter/s, 75.7741s/100 iters), loss = 0.0708454
I0818 20:31:39.519482  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0708455 (* 1 = 0.0708455 loss)
I0818 20:31:39.519493  2412 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0818 20:32:49.102267  2412 solver.cpp:357] Iteration 42400 (1.43713 iter/s, 69.583s/100 iters), loss = 0.114467
I0818 20:32:49.102417  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114467 (* 1 = 0.114467 loss)
I0818 20:32:49.102429  2412 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0818 20:34:01.222867  2412 solver.cpp:514] Iteration 42500, Testing net (#0)
I0818 20:34:57.218021  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:34:57.317885  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.854801
I0818 20:34:57.317977  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.502081 (* 1 = 0.502081 loss)
I0818 20:34:57.979039  2412 solver.cpp:357] Iteration 42500 (0.775958 iter/s, 128.873s/100 iters), loss = 0.0558531
I0818 20:34:57.979115  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0558532 (* 1 = 0.0558532 loss)
I0818 20:34:57.979125  2412 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0818 20:35:53.987403  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:36:13.427408  2412 solver.cpp:357] Iteration 42600 (1.32544 iter/s, 75.4465s/100 iters), loss = 0.0407072
I0818 20:36:13.427487  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0407073 (* 1 = 0.0407073 loss)
I0818 20:36:13.427500  2412 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0818 20:37:29.198562  2412 solver.cpp:357] Iteration 42700 (1.31983 iter/s, 75.7672s/100 iters), loss = 0.13437
I0818 20:37:29.198766  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13437 (* 1 = 0.13437 loss)
I0818 20:37:29.198796  2412 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0818 20:38:33.996981  2412 solver.cpp:357] Iteration 42800 (1.54334 iter/s, 64.7944s/100 iters), loss = 0.100601
I0818 20:38:33.997092  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100602 (* 1 = 0.100602 loss)
I0818 20:38:33.997102  2412 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0818 20:39:48.343901  2412 solver.cpp:357] Iteration 42900 (1.34506 iter/s, 74.3459s/100 iters), loss = 0.112355
I0818 20:39:48.344208  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112355 (* 1 = 0.112355 loss)
I0818 20:39:48.344271  2412 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0818 20:40:36.962311  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:41:01.530213  2412 solver.cpp:514] Iteration 43000, Testing net (#0)
I0818 20:41:57.826813  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:41:58.001238  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.872601
I0818 20:41:58.001281  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.45204 (* 1 = 0.45204 loss)
I0818 20:41:58.665760  2412 solver.cpp:357] Iteration 43000 (0.767334 iter/s, 130.321s/100 iters), loss = 0.0610271
I0818 20:41:58.665832  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0610272 (* 1 = 0.0610272 loss)
I0818 20:41:58.665844  2412 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0818 20:43:11.430526  2412 solver.cpp:357] Iteration 43100 (1.37428 iter/s, 72.7652s/100 iters), loss = 0.0886649
I0818 20:43:11.430721  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0886651 (* 1 = 0.0886651 loss)
I0818 20:43:11.430734  2412 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0818 20:44:22.809909  2412 solver.cpp:357] Iteration 43200 (1.40096 iter/s, 71.3795s/100 iters), loss = 0.0697066
I0818 20:44:22.810088  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0697067 (* 1 = 0.0697067 loss)
I0818 20:44:22.810101  2412 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0818 20:45:38.594689  2412 solver.cpp:357] Iteration 43300 (1.31953 iter/s, 75.7848s/100 iters), loss = 0.133921
I0818 20:45:38.594810  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.133921 (* 1 = 0.133921 loss)
I0818 20:45:38.594821  2412 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0818 20:46:20.245309  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:46:54.028416  2412 solver.cpp:357] Iteration 43400 (1.32567 iter/s, 75.4336s/100 iters), loss = 0.167508
I0818 20:46:54.028745  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.167508 (* 1 = 0.167508 loss)
I0818 20:46:54.028808  2412 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0818 20:48:06.696857  2412 solver.cpp:514] Iteration 43500, Testing net (#0)
I0818 20:48:46.253767  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:48:46.491621  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.884401
I0818 20:48:46.491797  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.375015 (* 1 = 0.375015 loss)
I0818 20:48:47.138902  2412 solver.cpp:357] Iteration 43500 (0.884087 iter/s, 113.111s/100 iters), loss = 0.0658249
I0818 20:48:47.139081  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0658251 (* 1 = 0.0658251 loss)
I0818 20:48:47.139123  2412 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0818 20:50:02.524343  2412 solver.cpp:357] Iteration 43600 (1.3265 iter/s, 75.3865s/100 iters), loss = 0.0979662
I0818 20:50:02.524572  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0979664 (* 1 = 0.0979664 loss)
I0818 20:50:02.524585  2412 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0818 20:51:15.803654  2412 solver.cpp:357] Iteration 43700 (1.36466 iter/s, 73.2786s/100 iters), loss = 0.0436722
I0818 20:51:15.806788  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0436724 (* 1 = 0.0436724 loss)
I0818 20:51:15.806823  2412 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0818 20:51:48.995059  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:52:29.342768  2412 solver.cpp:357] Iteration 43800 (1.35987 iter/s, 73.5362s/100 iters), loss = 0.0537762
I0818 20:52:29.346881  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0537763 (* 1 = 0.0537763 loss)
I0818 20:52:29.346926  2412 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0818 20:53:40.853087  2412 solver.cpp:357] Iteration 43900 (1.39847 iter/s, 71.5066s/100 iters), loss = 0.13093
I0818 20:53:40.853317  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13093 (* 1 = 0.13093 loss)
I0818 20:53:40.853364  2412 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0818 20:54:52.776482  2412 solver.cpp:514] Iteration 44000, Testing net (#0)
I0818 20:55:47.841739  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:55:48.129626  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.872801
I0818 20:55:48.129792  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.44229 (* 1 = 0.44229 loss)
I0818 20:55:48.730793  2412 solver.cpp:357] Iteration 44000 (0.781997 iter/s, 127.878s/100 iters), loss = 0.0693746
I0818 20:55:48.730967  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0693747 (* 1 = 0.0693747 loss)
I0818 20:55:48.731014  2412 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0818 20:57:01.953577  2412 solver.cpp:357] Iteration 44100 (1.36572 iter/s, 73.2217s/100 iters), loss = 0.0874892
I0818 20:57:01.958488  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0874893 (* 1 = 0.0874893 loss)
I0818 20:57:01.958549  2412 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0818 20:57:28.147692  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:58:08.074007  2412 solver.cpp:357] Iteration 44200 (1.51252 iter/s, 66.1148s/100 iters), loss = 0.0583585
I0818 20:58:08.074260  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0583586 (* 1 = 0.0583586 loss)
I0818 20:58:08.074306  2412 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0818 20:59:16.746233  2412 solver.cpp:357] Iteration 44300 (1.45625 iter/s, 68.6694s/100 iters), loss = 0.0740225
I0818 20:59:16.746366  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0740227 (* 1 = 0.0740227 loss)
I0818 20:59:16.746376  2412 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0818 21:00:32.032299  2412 solver.cpp:357] Iteration 44400 (1.32825 iter/s, 75.2869s/100 iters), loss = 0.0894562
I0818 21:00:32.032475  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0894564 (* 1 = 0.0894564 loss)
I0818 21:00:32.032487  2412 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0818 21:01:46.886598  2412 solver.cpp:514] Iteration 44500, Testing net (#0)
I0818 21:02:41.750545  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:02:42.043356  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880502
I0818 21:02:42.043556  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.409305 (* 1 = 0.409305 loss)
I0818 21:02:42.559298  2412 solver.cpp:357] Iteration 44500 (0.766128 iter/s, 130.527s/100 iters), loss = 0.146584
I0818 21:02:42.559495  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.146584 (* 1 = 0.146584 loss)
I0818 21:02:42.559541  2412 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0818 21:03:02.658638  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:03:54.959758  2412 solver.cpp:357] Iteration 44600 (1.38123 iter/s, 72.3992s/100 iters), loss = 0.121262
I0818 21:03:54.962793  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121263 (* 1 = 0.121263 loss)
I0818 21:03:54.962829  2412 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0818 21:05:01.938184  2412 solver.cpp:357] Iteration 44700 (1.49307 iter/s, 66.9759s/100 iters), loss = 0.117119
I0818 21:05:01.942811  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117119 (* 1 = 0.117119 loss)
I0818 21:05:01.942852  2412 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0818 21:06:15.054436  2412 solver.cpp:357] Iteration 44800 (1.36779 iter/s, 73.1106s/100 iters), loss = 0.100071
I0818 21:06:15.054677  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100071 (* 1 = 0.100071 loss)
I0818 21:06:15.054724  2412 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0818 21:07:20.934177  2412 solver.cpp:357] Iteration 44900 (1.51795 iter/s, 65.8783s/100 iters), loss = 0.060422
I0818 21:07:20.934496  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0604221 (* 1 = 0.0604221 loss)
I0818 21:07:20.934545  2412 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0818 21:07:33.477133  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:08:33.403527  2412 solver.cpp:514] Iteration 45000, Testing net (#0)
I0818 21:09:26.756590  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:09:26.988696  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8424
I0818 21:09:26.988885  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.581855 (* 1 = 0.581855 loss)
I0818 21:09:27.548665  2412 solver.cpp:357] Iteration 45000 (0.789817 iter/s, 126.612s/100 iters), loss = 0.0384841
I0818 21:09:27.548861  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0384842 (* 1 = 0.0384842 loss)
I0818 21:09:27.548912  2412 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0818 21:10:34.899619  2412 solver.cpp:357] Iteration 45100 (1.48479 iter/s, 67.3495s/100 iters), loss = 0.102603
I0818 21:10:34.899869  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102603 (* 1 = 0.102603 loss)
I0818 21:10:34.899915  2412 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0818 21:11:48.257181  2412 solver.cpp:357] Iteration 45200 (1.36325 iter/s, 73.354s/100 iters), loss = 0.0891941
I0818 21:11:48.262744  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0891942 (* 1 = 0.0891942 loss)
I0818 21:11:48.262773  2412 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0818 21:13:01.801193  2412 solver.cpp:357] Iteration 45300 (1.35987 iter/s, 73.5362s/100 iters), loss = 0.101664
I0818 21:13:01.801441  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101665 (* 1 = 0.101665 loss)
I0818 21:13:01.801518  2412 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0818 21:13:08.220144  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:14:15.203737  2412 solver.cpp:357] Iteration 45400 (1.36234 iter/s, 73.4029s/100 iters), loss = 0.088709
I0818 21:14:15.210875  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0887091 (* 1 = 0.0887091 loss)
I0818 21:14:15.210942  2412 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0818 21:15:20.924404  2412 solver.cpp:514] Iteration 45500, Testing net (#0)
I0818 21:16:14.026968  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:16:14.267508  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.871701
I0818 21:16:14.267663  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.451143 (* 1 = 0.451143 loss)
I0818 21:16:14.861703  2412 solver.cpp:357] Iteration 45500 (0.835735 iter/s, 119.655s/100 iters), loss = 0.0987306
I0818 21:16:14.861863  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0987307 (* 1 = 0.0987307 loss)
I0818 21:16:14.861907  2412 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0818 21:17:26.071919  2412 solver.cpp:357] Iteration 45600 (1.40412 iter/s, 71.219s/100 iters), loss = 0.0869579
I0818 21:17:26.072237  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.086958 (* 1 = 0.086958 loss)
I0818 21:17:26.072286  2412 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0818 21:18:38.962096  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:18:39.531060  2412 solver.cpp:357] Iteration 45700 (1.36122 iter/s, 73.4638s/100 iters), loss = 0.0905033
I0818 21:18:39.531308  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0905034 (* 1 = 0.0905034 loss)
I0818 21:18:39.531533  2412 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0818 21:19:52.525255  2412 solver.cpp:357] Iteration 45800 (1.36984 iter/s, 73.0014s/100 iters), loss = 0.127976
I0818 21:19:52.530901  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127976 (* 1 = 0.127976 loss)
I0818 21:19:52.531002  2412 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0818 21:21:00.174752  2412 solver.cpp:357] Iteration 45900 (1.47819 iter/s, 67.6502s/100 iters), loss = 0.0428412
I0818 21:21:00.178777  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0428412 (* 1 = 0.0428412 loss)
I0818 21:21:00.178812  2412 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0818 21:22:12.466784  2412 solver.cpp:514] Iteration 46000, Testing net (#0)
I0818 21:23:07.863554  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:23:08.078295  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.856101
I0818 21:23:08.078485  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.510694 (* 1 = 0.510694 loss)
I0818 21:23:08.644256  2412 solver.cpp:357] Iteration 46000 (0.778372 iter/s, 128.473s/100 iters), loss = 0.0722241
I0818 21:23:08.644382  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0722242 (* 1 = 0.0722242 loss)
I0818 21:23:08.644408  2412 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0818 21:24:13.735781  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:24:21.955490  2412 solver.cpp:357] Iteration 46100 (1.36395 iter/s, 73.3165s/100 iters), loss = 0.043039
I0818 21:24:21.955569  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.043039 (* 1 = 0.043039 loss)
I0818 21:24:21.955580  2412 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0818 21:25:32.705932  2412 solver.cpp:357] Iteration 46200 (1.41337 iter/s, 70.7531s/100 iters), loss = 0.10712
I0818 21:25:32.706086  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10712 (* 1 = 0.10712 loss)
I0818 21:25:32.706113  2412 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0818 21:26:41.151173  2412 solver.cpp:357] Iteration 46300 (1.46097 iter/s, 68.4478s/100 iters), loss = 0.0724699
I0818 21:26:41.151299  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.07247 (* 1 = 0.07247 loss)
I0818 21:26:41.151311  2412 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0818 21:27:56.737869  2412 solver.cpp:357] Iteration 46400 (1.32298 iter/s, 75.5869s/100 iters), loss = 0.109514
I0818 21:27:56.738030  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.109514 (* 1 = 0.109514 loss)
I0818 21:27:56.738052  2412 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0818 21:28:55.131716  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:29:09.367741  2412 solver.cpp:514] Iteration 46500, Testing net (#0)
I0818 21:30:04.392484  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:30:04.660606  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.858001
I0818 21:30:04.660781  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.485194 (* 1 = 0.485194 loss)
I0818 21:30:05.188251  2412 solver.cpp:357] Iteration 46500 (0.77847 iter/s, 128.457s/100 iters), loss = 0.0533078
I0818 21:30:05.188436  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0533079 (* 1 = 0.0533079 loss)
I0818 21:30:05.188482  2412 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0818 21:31:11.748842  2412 solver.cpp:357] Iteration 46600 (1.50241 iter/s, 66.5596s/100 iters), loss = 0.16047
I0818 21:31:11.749090  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.16047 (* 1 = 0.16047 loss)
I0818 21:31:11.749178  2412 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0818 21:32:24.415714  2412 solver.cpp:357] Iteration 46700 (1.37612 iter/s, 72.6681s/100 iters), loss = 0.0519147
I0818 21:32:24.422844  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0519148 (* 1 = 0.0519148 loss)
I0818 21:32:24.422888  2412 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0818 21:33:39.952726  2412 solver.cpp:357] Iteration 46800 (1.32395 iter/s, 75.5317s/100 iters), loss = 0.0832358
I0818 21:33:39.952872  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.083236 (* 1 = 0.083236 loss)
I0818 21:33:39.952883  2412 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0818 21:34:33.127216  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:34:55.430912  2412 solver.cpp:357] Iteration 46900 (1.32487 iter/s, 75.4793s/100 iters), loss = 0.117881
I0818 21:34:55.431074  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117881 (* 1 = 0.117881 loss)
I0818 21:34:55.431107  2412 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0818 21:35:59.768431  2412 solver.cpp:514] Iteration 47000, Testing net (#0)
I0818 21:36:51.386570  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:36:51.552855  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.881602
I0818 21:36:51.553027  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.413999 (* 1 = 0.413999 loss)
I0818 21:36:52.188495  2412 solver.cpp:357] Iteration 47000 (0.856456 iter/s, 116.76s/100 iters), loss = 0.0548322
I0818 21:36:52.188678  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0548324 (* 1 = 0.0548324 loss)
I0818 21:36:52.188726  2412 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0818 21:38:05.623529  2412 solver.cpp:357] Iteration 47100 (1.36177 iter/s, 73.4336s/100 iters), loss = 0.0654292
I0818 21:38:05.624606  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0654294 (* 1 = 0.0654294 loss)
I0818 21:38:05.624660  2412 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0818 21:39:19.743975  2412 solver.cpp:357] Iteration 47200 (1.34914 iter/s, 74.1211s/100 iters), loss = 0.0654541
I0818 21:39:19.744129  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0654542 (* 1 = 0.0654542 loss)
I0818 21:39:19.744141  2412 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0818 21:40:05.919466  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:40:35.149907  2412 solver.cpp:357] Iteration 47300 (1.32615 iter/s, 75.4065s/100 iters), loss = 0.0575208
I0818 21:40:35.150053  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0575209 (* 1 = 0.0575209 loss)
I0818 21:40:35.150084  2412 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0818 21:41:43.524828  2412 solver.cpp:357] Iteration 47400 (1.46256 iter/s, 68.3731s/100 iters), loss = 0.0897878
I0818 21:41:43.524992  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0897879 (* 1 = 0.0897879 loss)
I0818 21:41:43.525005  2412 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0818 21:42:58.369022  2412 solver.cpp:514] Iteration 47500, Testing net (#0)
I0818 21:43:54.421398  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:43:54.715997  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.825601
I0818 21:43:54.716054  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.703682 (* 1 = 0.703682 loss)
I0818 21:43:55.204234  2412 solver.cpp:357] Iteration 47500 (0.759406 iter/s, 131.682s/100 iters), loss = 0.0675638
I0818 21:43:55.204309  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.067564 (* 1 = 0.067564 loss)
I0818 21:43:55.204321  2412 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0818 21:45:08.684473  2412 solver.cpp:357] Iteration 47600 (1.36094 iter/s, 73.4785s/100 iters), loss = 0.0511746
I0818 21:45:08.684751  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0511747 (* 1 = 0.0511747 loss)
I0818 21:45:08.684783  2412 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0818 21:45:45.496371  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:46:19.641083  2412 solver.cpp:357] Iteration 47700 (1.40931 iter/s, 70.9568s/100 iters), loss = 0.0602836
I0818 21:46:19.641193  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0602837 (* 1 = 0.0602837 loss)
I0818 21:46:19.641206  2412 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0818 21:47:18.332679  2412 solver.cpp:357] Iteration 47800 (1.70382 iter/s, 58.6917s/100 iters), loss = 0.107583
I0818 21:47:18.332844  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107583 (* 1 = 0.107583 loss)
I0818 21:47:18.332855  2412 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0818 21:48:08.725364  2412 solver.cpp:357] Iteration 47900 (1.98443 iter/s, 50.3922s/100 iters), loss = 0.0388185
I0818 21:48:08.725507  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0388186 (* 1 = 0.0388186 loss)
I0818 21:48:08.725520  2412 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0818 21:48:58.647301  2412 solver.cpp:514] Iteration 48000, Testing net (#0)
I0818 21:49:36.016844  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:49:36.196522  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888601
I0818 21:49:36.196601  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.405874 (* 1 = 0.405874 loss)
I0818 21:49:36.633255  2412 solver.cpp:357] Iteration 48000 (1.13754 iter/s, 87.9087s/100 iters), loss = 0.0960946
I0818 21:49:36.633368  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0960947 (* 1 = 0.0960947 loss)
I0818 21:49:36.633391  2412 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0818 21:49:36.633409  2412 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0818 21:49:57.955332  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:50:27.086457  2412 solver.cpp:357] Iteration 48100 (1.982 iter/s, 50.4542s/100 iters), loss = 0.0296213
I0818 21:50:27.087011  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0296214 (* 1 = 0.0296214 loss)
I0818 21:50:27.087025  2412 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0818 21:51:17.331943  2412 solver.cpp:357] Iteration 48200 (1.99028 iter/s, 50.2443s/100 iters), loss = 0.0261704
I0818 21:51:17.332121  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0261705 (* 1 = 0.0261705 loss)
I0818 21:51:17.332135  2412 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0818 21:52:05.801719  2412 solver.cpp:357] Iteration 48300 (2.06319 iter/s, 48.4686s/100 iters), loss = 0.0451327
I0818 21:52:05.801863  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0451328 (* 1 = 0.0451328 loss)
I0818 21:52:05.801875  2412 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0818 21:52:48.279770  2412 solver.cpp:357] Iteration 48400 (2.35412 iter/s, 42.4788s/100 iters), loss = 0.0360601
I0818 21:52:48.280000  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0360603 (* 1 = 0.0360603 loss)
I0818 21:52:48.280033  2412 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0818 21:53:05.002879  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:53:38.215831  2412 solver.cpp:514] Iteration 48500, Testing net (#0)
I0818 21:54:16.509033  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:54:16.689863  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920502
I0818 21:54:16.689926  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.261358 (* 1 = 0.261358 loss)
I0818 21:54:17.177125  2412 solver.cpp:357] Iteration 48500 (1.12489 iter/s, 88.8972s/100 iters), loss = 0.0503242
I0818 21:54:17.177201  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0503243 (* 1 = 0.0503243 loss)
I0818 21:54:17.177212  2412 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0818 21:55:08.121085  2412 solver.cpp:357] Iteration 48600 (1.9629 iter/s, 50.9451s/100 iters), loss = 0.052233
I0818 21:55:08.121369  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0522331 (* 1 = 0.0522331 loss)
I0818 21:55:08.121402  2412 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0818 21:55:55.591341  2412 solver.cpp:357] Iteration 48700 (2.10655 iter/s, 47.4711s/100 iters), loss = 0.0153706
I0818 21:55:55.594851  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0153707 (* 1 = 0.0153707 loss)
I0818 21:55:55.594897  2412 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0818 21:56:43.994858  2412 solver.cpp:357] Iteration 48800 (2.0661 iter/s, 48.4004s/100 iters), loss = 0.0441252
I0818 21:56:43.995226  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0441254 (* 1 = 0.0441254 loss)
I0818 21:56:43.995318  2412 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0818 21:56:55.033262  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:57:33.720285  2412 solver.cpp:357] Iteration 48900 (2.01101 iter/s, 49.7263s/100 iters), loss = 0.0287437
I0818 21:57:33.726925  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0287439 (* 1 = 0.0287439 loss)
I0818 21:57:33.726981  2412 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0818 21:58:23.560397  2412 solver.cpp:514] Iteration 49000, Testing net (#0)
I0818 21:58:54.326354  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:58:54.463696  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926802
I0818 21:58:54.464241  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.24454 (* 1 = 0.24454 loss)
I0818 21:58:54.844116  2412 solver.cpp:357] Iteration 49000 (1.23276 iter/s, 81.1189s/100 iters), loss = 0.0243586
I0818 21:58:54.844208  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0243587 (* 1 = 0.0243587 loss)
I0818 21:58:54.844220  2412 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0818 21:59:45.226732  2412 solver.cpp:357] Iteration 49100 (1.98476 iter/s, 50.3838s/100 iters), loss = 0.0558943
I0818 21:59:45.226928  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0558944 (* 1 = 0.0558944 loss)
I0818 21:59:45.226955  2412 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0818 22:00:31.706851  2412 solver.cpp:357] Iteration 49200 (2.1515 iter/s, 46.4792s/100 iters), loss = 0.0261577
I0818 22:00:31.710953  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0261578 (* 1 = 0.0261578 loss)
I0818 22:00:31.711020  2412 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0818 22:00:38.850687  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:01:22.360306  2412 solver.cpp:357] Iteration 49300 (1.97431 iter/s, 50.6505s/100 iters), loss = 0.0724984
I0818 22:01:22.366878  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0724985 (* 1 = 0.0724985 loss)
I0818 22:01:22.366933  2412 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0818 22:02:10.073668  2412 solver.cpp:357] Iteration 49400 (2.0961 iter/s, 47.7076s/100 iters), loss = 0.0352074
I0818 22:02:10.078899  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0352075 (* 1 = 0.0352075 loss)
I0818 22:02:10.078960  2412 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0818 22:03:01.369493  2412 solver.cpp:514] Iteration 49500, Testing net (#0)
I0818 22:03:37.053961  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:03:37.241649  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928302
I0818 22:03:37.241880  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.246016 (* 1 = 0.246016 loss)
I0818 22:03:37.623929  2412 solver.cpp:357] Iteration 49500 (1.14226 iter/s, 87.5457s/100 iters), loss = 0.0141301
I0818 22:03:37.624315  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0141302 (* 1 = 0.0141302 loss)
I0818 22:03:37.624486  2412 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0818 22:04:27.599819  2412 solver.cpp:357] Iteration 49600 (2.00092 iter/s, 49.9769s/100 iters), loss = 0.0227178
I0818 22:04:27.602803  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0227179 (* 1 = 0.0227179 loss)
I0818 22:04:27.602844  2412 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0818 22:04:30.297938  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:05:09.649991  2412 solver.cpp:357] Iteration 49700 (2.37829 iter/s, 42.0471s/100 iters), loss = 0.034785
I0818 22:05:09.650368  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0347852 (* 1 = 0.0347852 loss)
I0818 22:05:09.650460  2412 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0818 22:05:52.891158  2412 solver.cpp:357] Iteration 49800 (2.31257 iter/s, 43.242s/100 iters), loss = 0.03296
I0818 22:05:52.892364  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0329601 (* 1 = 0.0329601 loss)
I0818 22:05:52.892393  2412 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0818 22:06:43.629448  2412 solver.cpp:357] Iteration 49900 (1.97093 iter/s, 50.7376s/100 iters), loss = 0.0165882
I0818 22:06:43.630026  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0165884 (* 1 = 0.0165884 loss)
I0818 22:06:43.630203  2412 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0818 22:07:27.803129  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:07:29.859776  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_50000.caffemodel
I0818 22:07:29.885720  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_50000.solverstate
I0818 22:07:29.892891  2412 solver.cpp:514] Iteration 50000, Testing net (#0)
I0818 22:08:07.897917  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:08:08.087204  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929602
I0818 22:08:08.090700  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.246582 (* 1 = 0.246582 loss)
I0818 22:08:08.524406  2412 solver.cpp:357] Iteration 50000 (1.17792 iter/s, 84.8952s/100 iters), loss = 0.0101332
I0818 22:08:08.524546  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0101333 (* 1 = 0.0101333 loss)
I0818 22:08:08.524574  2412 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0818 22:08:55.363663  2412 solver.cpp:357] Iteration 50100 (2.13491 iter/s, 46.8404s/100 iters), loss = 0.0245857
I0818 22:08:55.366915  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0245858 (* 1 = 0.0245858 loss)
I0818 22:08:55.366977  2412 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0818 22:09:46.408989  2412 solver.cpp:357] Iteration 50200 (1.95915 iter/s, 51.0425s/100 iters), loss = 0.0174709
I0818 22:09:46.414858  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.017471 (* 1 = 0.017471 loss)
I0818 22:09:46.414902  2412 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0818 22:10:33.933257  2412 solver.cpp:357] Iteration 50300 (2.10441 iter/s, 47.5193s/100 iters), loss = 0.0112232
I0818 22:10:33.933442  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112234 (* 1 = 0.0112234 loss)
I0818 22:10:33.933455  2412 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0818 22:11:15.318967  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:11:21.046556  2412 solver.cpp:357] Iteration 50400 (2.12258 iter/s, 47.1125s/100 iters), loss = 0.0103674
I0818 22:11:21.046638  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0103676 (* 1 = 0.0103676 loss)
I0818 22:11:21.046658  2412 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0818 22:12:03.837975  2412 solver.cpp:514] Iteration 50500, Testing net (#0)
I0818 22:12:40.260746  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:12:40.398190  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929202
I0818 22:12:40.398432  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.24866 (* 1 = 0.24866 loss)
I0818 22:12:40.834816  2412 solver.cpp:357] Iteration 50500 (1.25328 iter/s, 79.7905s/100 iters), loss = 0.00982439
I0818 22:12:40.835122  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00982451 (* 1 = 0.00982451 loss)
I0818 22:12:40.835216  2412 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0818 22:13:29.605576  2412 solver.cpp:357] Iteration 50600 (2.05044 iter/s, 48.77s/100 iters), loss = 0.0118117
I0818 22:13:29.610857  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0118118 (* 1 = 0.0118118 loss)
I0818 22:13:29.610908  2412 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0818 22:14:21.061764  2412 solver.cpp:357] Iteration 50700 (1.94359 iter/s, 51.4513s/100 iters), loss = 0.0125458
I0818 22:14:21.066905  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0125459 (* 1 = 0.0125459 loss)
I0818 22:14:21.067008  2412 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0818 22:14:58.283406  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:15:09.662428  2412 solver.cpp:357] Iteration 50800 (2.05774 iter/s, 48.597s/100 iters), loss = 0.0341775
I0818 22:15:09.662552  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0341776 (* 1 = 0.0341776 loss)
I0818 22:15:09.662578  2412 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0818 22:16:00.348407  2412 solver.cpp:357] Iteration 50900 (1.97288 iter/s, 50.6873s/100 iters), loss = 0.0182885
I0818 22:16:00.348546  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0182886 (* 1 = 0.0182886 loss)
I0818 22:16:00.348559  2412 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0818 22:16:50.134137  2412 solver.cpp:514] Iteration 51000, Testing net (#0)
I0818 22:17:26.980324  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:17:27.082947  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928202
I0818 22:17:27.082995  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.250379 (* 1 = 0.250379 loss)
I0818 22:17:27.469283  2412 solver.cpp:357] Iteration 51000 (1.1478 iter/s, 87.1233s/100 iters), loss = 0.031359
I0818 22:17:27.469343  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0313591 (* 1 = 0.0313591 loss)
I0818 22:17:27.469358  2412 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0818 22:18:11.252471  2412 solver.cpp:357] Iteration 51100 (2.28392 iter/s, 43.7844s/100 iters), loss = 0.00721386
I0818 22:18:11.252560  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00721396 (* 1 = 0.00721396 loss)
I0818 22:18:11.252573  2412 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0818 22:18:45.205456  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:19:01.779767  2412 solver.cpp:357] Iteration 51200 (1.97907 iter/s, 50.5287s/100 iters), loss = 0.0355444
I0818 22:19:01.779839  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0355445 (* 1 = 0.0355445 loss)
I0818 22:19:01.779851  2412 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0818 22:19:52.175151  2412 solver.cpp:357] Iteration 51300 (1.98434 iter/s, 50.3947s/100 iters), loss = 0.00916954
I0818 22:19:52.175329  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00916964 (* 1 = 0.00916964 loss)
I0818 22:19:52.175343  2412 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0818 22:20:42.416734  2412 solver.cpp:357] Iteration 51400 (1.99041 iter/s, 50.2409s/100 iters), loss = 0.0360118
I0818 22:20:42.416877  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0360119 (* 1 = 0.0360119 loss)
I0818 22:20:42.416895  2412 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0818 22:21:32.479712  2412 solver.cpp:514] Iteration 51500, Testing net (#0)
I0818 22:22:09.342936  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:22:09.507547  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930602
I0818 22:22:09.507604  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.254199 (* 1 = 0.254199 loss)
I0818 22:22:09.876129  2412 solver.cpp:357] Iteration 51500 (1.14338 iter/s, 87.4599s/100 iters), loss = 0.0118728
I0818 22:22:09.876199  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0118729 (* 1 = 0.0118729 loss)
I0818 22:22:09.876209  2412 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0818 22:22:39.464073  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:23:00.523059  2412 solver.cpp:357] Iteration 51600 (1.9744 iter/s, 50.6483s/100 iters), loss = 0.00877769
I0818 22:23:00.523136  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00877779 (* 1 = 0.00877779 loss)
I0818 22:23:00.523149  2412 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0818 22:23:50.806812  2412 solver.cpp:357] Iteration 51700 (1.98874 iter/s, 50.2831s/100 iters), loss = 0.038025
I0818 22:23:50.806957  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0380251 (* 1 = 0.0380251 loss)
I0818 22:23:50.806968  2412 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0818 22:24:35.736349  2412 solver.cpp:357] Iteration 51800 (2.22578 iter/s, 44.928s/100 iters), loss = 0.00749896
I0818 22:24:35.736795  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00749905 (* 1 = 0.00749905 loss)
I0818 22:24:35.736809  2412 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0818 22:25:24.625139  2412 solver.cpp:357] Iteration 51900 (2.04545 iter/s, 48.889s/100 iters), loss = 0.0289913
I0818 22:25:24.625300  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0289914 (* 1 = 0.0289914 loss)
I0818 22:25:24.625313  2412 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0818 22:25:49.462025  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:26:14.540673  2412 solver.cpp:514] Iteration 52000, Testing net (#0)
I0818 22:26:51.364758  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:26:51.544582  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932102
I0818 22:26:51.544636  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.25306 (* 1 = 0.25306 loss)
I0818 22:26:51.979195  2412 solver.cpp:357] Iteration 52000 (1.14478 iter/s, 87.3532s/100 iters), loss = 0.0148134
I0818 22:26:51.979257  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0148135 (* 1 = 0.0148135 loss)
I0818 22:26:51.979272  2412 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0818 22:27:42.352972  2412 solver.cpp:357] Iteration 52100 (1.98513 iter/s, 50.3745s/100 iters), loss = 0.0130886
I0818 22:27:42.353318  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0130887 (* 1 = 0.0130887 loss)
I0818 22:27:42.353382  2412 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0818 22:28:32.780282  2412 solver.cpp:357] Iteration 52200 (1.9831 iter/s, 50.426s/100 iters), loss = 0.013535
I0818 22:28:32.780423  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0135351 (* 1 = 0.0135351 loss)
I0818 22:28:32.780436  2412 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0818 22:29:23.310559  2412 solver.cpp:357] Iteration 52300 (1.97906 iter/s, 50.529s/100 iters), loss = 0.0262534
I0818 22:29:23.310708  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0262535 (* 1 = 0.0262535 loss)
I0818 22:29:23.310725  2412 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0818 22:29:43.034718  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:30:13.640970  2412 solver.cpp:357] Iteration 52400 (1.98692 iter/s, 50.3292s/100 iters), loss = 0.0223648
I0818 22:30:13.641104  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0223649 (* 1 = 0.0223649 loss)
I0818 22:30:13.641115  2412 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0818 22:30:59.849572  2412 solver.cpp:514] Iteration 52500, Testing net (#0)
I0818 22:31:33.498478  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:31:33.625771  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931002
I0818 22:31:33.625815  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.253317 (* 1 = 0.253317 loss)
I0818 22:31:34.020767  2412 solver.cpp:357] Iteration 52500 (1.2441 iter/s, 80.3793s/100 iters), loss = 0.0255376
I0818 22:31:34.020836  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0255377 (* 1 = 0.0255377 loss)
I0818 22:31:34.020848  2412 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0818 22:32:24.395170  2412 solver.cpp:357] Iteration 52600 (1.98518 iter/s, 50.3733s/100 iters), loss = 0.0215966
I0818 22:32:24.395304  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0215967 (* 1 = 0.0215967 loss)
I0818 22:32:24.395316  2412 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0818 22:33:14.742904  2412 solver.cpp:357] Iteration 52700 (1.98623 iter/s, 50.3467s/100 iters), loss = 0.0273636
I0818 22:33:14.743070  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0273637 (* 1 = 0.0273637 loss)
I0818 22:33:14.743083  2412 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0818 22:33:29.951908  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:34:05.281266  2412 solver.cpp:357] Iteration 52800 (1.97874 iter/s, 50.5373s/100 iters), loss = 0.0180905
I0818 22:34:05.281428  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0180906 (* 1 = 0.0180906 loss)
I0818 22:34:05.281440  2412 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0818 22:34:55.836447  2412 solver.cpp:357] Iteration 52900 (1.97808 iter/s, 50.5542s/100 iters), loss = 0.0176462
I0818 22:34:55.836593  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0176463 (* 1 = 0.0176463 loss)
I0818 22:34:55.836606  2412 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0818 22:35:45.616108  2412 solver.cpp:514] Iteration 53000, Testing net (#0)
I0818 22:36:22.315963  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:36:22.482476  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932802
I0818 22:36:22.482537  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.254207 (* 1 = 0.254207 loss)
I0818 22:36:22.855347  2412 solver.cpp:357] Iteration 53000 (1.14918 iter/s, 87.0188s/100 iters), loss = 0.0279706
I0818 22:36:22.855412  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0279706 (* 1 = 0.0279706 loss)
I0818 22:36:22.855422  2412 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0818 22:37:11.577463  2412 solver.cpp:357] Iteration 53100 (2.05241 iter/s, 48.7232s/100 iters), loss = 0.00408284
I0818 22:37:11.577608  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00408292 (* 1 = 0.00408292 loss)
I0818 22:37:11.577620  2412 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0818 22:37:20.367738  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:37:56.676287  2412 solver.cpp:357] Iteration 53200 (2.21731 iter/s, 45.0997s/100 iters), loss = 0.0176546
I0818 22:37:56.676426  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0176546 (* 1 = 0.0176546 loss)
I0818 22:37:56.676439  2412 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0818 22:38:47.012776  2412 solver.cpp:357] Iteration 53300 (1.98667 iter/s, 50.3356s/100 iters), loss = 0.00584798
I0818 22:38:47.013113  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00584806 (* 1 = 0.00584806 loss)
I0818 22:38:47.013178  2412 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0818 22:39:37.540948  2412 solver.cpp:357] Iteration 53400 (1.97913 iter/s, 50.5272s/100 iters), loss = 0.0090698
I0818 22:39:37.541123  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00906988 (* 1 = 0.00906988 loss)
I0818 22:39:37.541136  2412 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0818 22:40:27.411142  2412 solver.cpp:514] Iteration 53500, Testing net (#0)
I0818 22:41:04.113495  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:41:04.282075  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930102
I0818 22:41:04.282126  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.262491 (* 1 = 0.262491 loss)
I0818 22:41:04.757819  2412 solver.cpp:357] Iteration 53500 (1.14657 iter/s, 87.2169s/100 iters), loss = 0.0251256
I0818 22:41:04.757881  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0251257 (* 1 = 0.0251257 loss)
I0818 22:41:04.757891  2412 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0818 22:41:10.418807  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:41:55.108011  2412 solver.cpp:357] Iteration 53600 (1.98604 iter/s, 50.3514s/100 iters), loss = 0.0243012
I0818 22:41:55.108255  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0243013 (* 1 = 0.0243013 loss)
I0818 22:41:55.108268  2412 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0818 22:42:45.552587  2412 solver.cpp:357] Iteration 53700 (1.98233 iter/s, 50.4456s/100 iters), loss = 0.0115527
I0818 22:42:45.552775  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0115528 (* 1 = 0.0115528 loss)
I0818 22:42:45.552788  2412 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0818 22:43:35.832219  2412 solver.cpp:357] Iteration 53800 (1.98891 iter/s, 50.2788s/100 iters), loss = 0.0112134
I0818 22:43:35.832366  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112135 (* 1 = 0.0112135 loss)
I0818 22:43:35.832377  2412 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0818 22:44:19.036599  2412 solver.cpp:357] Iteration 53900 (2.31453 iter/s, 43.2053s/100 iters), loss = 0.0122627
I0818 22:44:19.036758  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122628 (* 1 = 0.0122628 loss)
I0818 22:44:19.036772  2412 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0818 22:44:19.980172  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:45:08.799625  2412 solver.cpp:514] Iteration 54000, Testing net (#0)
I0818 22:45:45.371739  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:45:45.549321  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931902
I0818 22:45:45.549371  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.256249 (* 1 = 0.256249 loss)
I0818 22:45:45.927963  2412 solver.cpp:357] Iteration 54000 (1.15086 iter/s, 86.8915s/100 iters), loss = 0.00604888
I0818 22:45:45.928030  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00604895 (* 1 = 0.00604895 loss)
I0818 22:45:45.928040  2412 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0818 22:46:36.445580  2412 solver.cpp:357] Iteration 54100 (1.97946 iter/s, 50.5188s/100 iters), loss = 0.0141845
I0818 22:46:36.445703  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0141845 (* 1 = 0.0141845 loss)
I0818 22:46:36.445716  2412 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0818 22:47:26.849179  2412 solver.cpp:357] Iteration 54200 (1.98402 iter/s, 50.4028s/100 iters), loss = 0.0288855
I0818 22:47:26.849349  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288855 (* 1 = 0.0288855 loss)
I0818 22:47:26.849361  2412 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0818 22:48:13.236717  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:48:17.254499  2412 solver.cpp:357] Iteration 54300 (1.98395 iter/s, 50.4045s/100 iters), loss = 0.0112746
I0818 22:48:17.254580  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112747 (* 1 = 0.0112747 loss)
I0818 22:48:17.254591  2412 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0818 22:49:07.449935  2412 solver.cpp:357] Iteration 54400 (1.99223 iter/s, 50.1951s/100 iters), loss = 0.022204
I0818 22:49:07.450116  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222041 (* 1 = 0.0222041 loss)
I0818 22:49:07.450130  2412 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0818 22:49:57.298291  2412 solver.cpp:514] Iteration 54500, Testing net (#0)
I0818 22:50:29.113066  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:50:29.231051  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930402
I0818 22:50:29.231099  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.258785 (* 1 = 0.258785 loss)
I0818 22:50:29.615532  2412 solver.cpp:357] Iteration 54500 (1.21705 iter/s, 82.1656s/100 iters), loss = 0.0118923
I0818 22:50:29.615595  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0118924 (* 1 = 0.0118924 loss)
I0818 22:50:29.615607  2412 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0818 22:51:17.214501  2412 solver.cpp:357] Iteration 54600 (2.10093 iter/s, 47.5981s/100 iters), loss = 0.00300455
I0818 22:51:17.214646  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00300463 (* 1 = 0.00300463 loss)
I0818 22:51:17.214679  2412 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0818 22:51:59.117565  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:52:07.628549  2412 solver.cpp:357] Iteration 54700 (1.98361 iter/s, 50.4132s/100 iters), loss = 0.00984707
I0818 22:52:07.628629  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00984714 (* 1 = 0.00984714 loss)
I0818 22:52:07.628643  2412 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0818 22:52:57.973754  2412 solver.cpp:357] Iteration 54800 (1.98632 iter/s, 50.3444s/100 iters), loss = 0.00989944
I0818 22:52:57.973906  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00989951 (* 1 = 0.00989951 loss)
I0818 22:52:57.973915  2412 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0818 22:53:48.348695  2412 solver.cpp:357] Iteration 54900 (1.98507 iter/s, 50.3761s/100 iters), loss = 0.0233796
I0818 22:53:48.348907  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0233796 (* 1 = 0.0233796 loss)
I0818 22:53:48.348922  2412 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0818 22:54:38.138970  2412 solver.cpp:514] Iteration 55000, Testing net (#0)
I0818 22:55:14.888377  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:55:15.070392  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0818 22:55:15.070446  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.261975 (* 1 = 0.261975 loss)
I0818 22:55:15.503587  2412 solver.cpp:357] Iteration 55000 (1.14738 iter/s, 87.1551s/100 iters), loss = 0.0186157
I0818 22:55:15.503649  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186158 (* 1 = 0.0186158 loss)
I0818 22:55:15.503659  2412 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0818 22:55:52.860034  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:56:05.889581  2412 solver.cpp:357] Iteration 55100 (1.98463 iter/s, 50.3873s/100 iters), loss = 0.00768425
I0818 22:56:05.889649  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00768433 (* 1 = 0.00768433 loss)
I0818 22:56:05.889659  2412 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0818 22:56:53.696770  2412 solver.cpp:357] Iteration 55200 (2.09168 iter/s, 47.8084s/100 iters), loss = 0.0196452
I0818 22:56:53.696918  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0196453 (* 1 = 0.0196453 loss)
I0818 22:56:53.696929  2412 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0818 22:57:39.666177  2412 solver.cpp:357] Iteration 55300 (2.17531 iter/s, 45.9705s/100 iters), loss = 0.0181859
I0818 22:57:39.666332  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.018186 (* 1 = 0.018186 loss)
I0818 22:57:39.666342  2412 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0818 22:58:30.031042  2412 solver.cpp:357] Iteration 55400 (1.9854 iter/s, 50.3678s/100 iters), loss = 0.0129644
I0818 22:58:30.031152  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0129645 (* 1 = 0.0129645 loss)
I0818 22:58:30.031162  2412 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0818 22:59:02.324359  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:59:19.957128  2412 solver.cpp:514] Iteration 55500, Testing net (#0)
I0818 22:59:56.735795  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 22:59:56.873703  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932002
I0818 22:59:56.873760  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.262432 (* 1 = 0.262432 loss)
I0818 22:59:57.370582  2412 solver.cpp:357] Iteration 55500 (1.14486 iter/s, 87.3469s/100 iters), loss = 0.00796101
I0818 22:59:57.370656  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00796109 (* 1 = 0.00796109 loss)
I0818 22:59:57.370668  2412 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0818 23:00:47.622921  2412 solver.cpp:357] Iteration 55600 (1.98988 iter/s, 50.2542s/100 iters), loss = 0.00714125
I0818 23:00:47.623100  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00714134 (* 1 = 0.00714134 loss)
I0818 23:00:47.623113  2412 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0818 23:01:37.965992  2412 solver.cpp:357] Iteration 55700 (1.9863 iter/s, 50.3447s/100 iters), loss = 0.00790014
I0818 23:01:37.966115  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00790023 (* 1 = 0.00790023 loss)
I0818 23:01:37.966127  2412 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0818 23:02:28.398083  2412 solver.cpp:357] Iteration 55800 (1.98273 iter/s, 50.4356s/100 iters), loss = 0.0142112
I0818 23:02:28.398248  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0142113 (* 1 = 0.0142113 loss)
I0818 23:02:28.398262  2412 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0818 23:02:56.027701  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:03:17.433727  2412 solver.cpp:357] Iteration 55900 (2.0392 iter/s, 49.0388s/100 iters), loss = 0.0174014
I0818 23:03:17.433900  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0174015 (* 1 = 0.0174015 loss)
I0818 23:03:17.433928  2412 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0818 23:03:58.291638  2412 solver.cpp:514] Iteration 56000, Testing net (#0)
I0818 23:04:35.250195  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:04:35.426189  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932402
I0818 23:04:35.426276  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.2613 (* 1 = 0.2613 loss)
I0818 23:04:35.763001  2412 solver.cpp:357] Iteration 56000 (1.27658 iter/s, 78.3342s/100 iters), loss = 0.0230718
I0818 23:04:35.763083  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0230719 (* 1 = 0.0230719 loss)
I0818 23:04:35.763098  2412 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0818 23:05:26.200387  2412 solver.cpp:357] Iteration 56100 (1.98262 iter/s, 50.4384s/100 iters), loss = 0.0134348
I0818 23:05:26.200558  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0134349 (* 1 = 0.0134349 loss)
I0818 23:05:26.200570  2412 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0818 23:06:16.222690  2412 solver.cpp:357] Iteration 56200 (1.999 iter/s, 50.0251s/100 iters), loss = 0.00718282
I0818 23:06:16.222903  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0071829 (* 1 = 0.0071829 loss)
I0818 23:06:16.222920  2412 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0818 23:06:38.983779  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:07:06.370482  2412 solver.cpp:357] Iteration 56300 (1.99408 iter/s, 50.1485s/100 iters), loss = 0.00205545
I0818 23:07:06.370616  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00205552 (* 1 = 0.00205552 loss)
I0818 23:07:06.370630  2412 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0818 23:07:56.766623  2412 solver.cpp:357] Iteration 56400 (1.98425 iter/s, 50.3968s/100 iters), loss = 0.0130526
I0818 23:07:56.766805  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0130527 (* 1 = 0.0130527 loss)
I0818 23:07:56.766819  2412 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0818 23:08:46.686048  2412 solver.cpp:514] Iteration 56500, Testing net (#0)
I0818 23:09:23.634969  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:09:23.804349  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933202
I0818 23:09:23.804400  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.262347 (* 1 = 0.262347 loss)
I0818 23:09:24.144434  2412 solver.cpp:357] Iteration 56500 (1.14442 iter/s, 87.3803s/100 iters), loss = 0.00683971
I0818 23:09:24.144501  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00683978 (* 1 = 0.00683978 loss)
I0818 23:09:24.144511  2412 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0818 23:10:09.927232  2412 solver.cpp:357] Iteration 56600 (2.18421 iter/s, 45.7831s/100 iters), loss = 0.00611236
I0818 23:10:09.927440  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00611243 (* 1 = 0.00611243 loss)
I0818 23:10:09.927453  2412 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0818 23:10:25.656291  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:10:57.828305  2412 solver.cpp:357] Iteration 56700 (2.08754 iter/s, 47.9032s/100 iters), loss = 0.00932902
I0818 23:10:57.828454  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00932909 (* 1 = 0.00932909 loss)
I0818 23:10:57.828467  2412 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0818 23:11:47.991200  2412 solver.cpp:357] Iteration 56800 (1.99341 iter/s, 50.1652s/100 iters), loss = 0.0395736
I0818 23:11:47.991325  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0395736 (* 1 = 0.0395736 loss)
I0818 23:11:47.991338  2412 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0818 23:12:38.430248  2412 solver.cpp:357] Iteration 56900 (1.98258 iter/s, 50.4393s/100 iters), loss = 0.0163959
I0818 23:12:38.430382  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.016396 (* 1 = 0.016396 loss)
I0818 23:12:38.430392  2412 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0818 23:13:28.265846  2412 solver.cpp:514] Iteration 57000, Testing net (#0)
I0818 23:14:05.671860  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:14:05.852619  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932702
I0818 23:14:05.852699  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.264867 (* 1 = 0.264867 loss)
I0818 23:14:06.237221  2412 solver.cpp:357] Iteration 57000 (1.13881 iter/s, 87.8109s/100 iters), loss = 0.00487698
I0818 23:14:06.237308  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00487706 (* 1 = 0.00487706 loss)
I0818 23:14:06.237321  2412 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0818 23:14:19.930670  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:14:56.640220  2412 solver.cpp:357] Iteration 57100 (1.98392 iter/s, 50.4052s/100 iters), loss = 0.0298477
I0818 23:14:56.640370  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0298478 (* 1 = 0.0298478 loss)
I0818 23:14:56.640383  2412 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0818 23:15:47.053812  2412 solver.cpp:357] Iteration 57200 (1.98351 iter/s, 50.4156s/100 iters), loss = 0.00941906
I0818 23:15:47.053939  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00941914 (* 1 = 0.00941914 loss)
I0818 23:15:47.053951  2412 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0818 23:16:34.720468  2412 solver.cpp:357] Iteration 57300 (2.09791 iter/s, 47.6666s/100 iters), loss = 0.00681961
I0818 23:16:34.720580  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0068197 (* 1 = 0.0068197 loss)
I0818 23:16:34.720595  2412 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0818 23:17:21.341727  2412 solver.cpp:357] Iteration 57400 (2.14486 iter/s, 46.6232s/100 iters), loss = 0.00933452
I0818 23:17:21.341848  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0093346 (* 1 = 0.0093346 loss)
I0818 23:17:21.341862  2412 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0818 23:17:30.098624  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:18:11.123725  2412 solver.cpp:514] Iteration 57500, Testing net (#0)
I0818 23:18:49.792783  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:18:49.893604  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931502
I0818 23:18:49.893702  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265601 (* 1 = 0.265601 loss)
I0818 23:18:50.379748  2412 solver.cpp:357] Iteration 57500 (1.12309 iter/s, 89.0397s/100 iters), loss = 0.0280441
I0818 23:18:50.379832  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0280442 (* 1 = 0.0280442 loss)
I0818 23:18:50.379847  2412 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0818 23:19:40.735445  2412 solver.cpp:357] Iteration 57600 (1.98587 iter/s, 50.3557s/100 iters), loss = 0.0164323
I0818 23:19:40.735617  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0164324 (* 1 = 0.0164324 loss)
I0818 23:19:40.735630  2412 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0818 23:20:31.068881  2412 solver.cpp:357] Iteration 57700 (1.98675 iter/s, 50.3334s/100 iters), loss = 0.0260756
I0818 23:20:31.068989  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0260757 (* 1 = 0.0260757 loss)
I0818 23:20:31.069000  2412 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0818 23:21:21.538904  2412 solver.cpp:357] Iteration 57800 (1.9813 iter/s, 50.472s/100 iters), loss = 0.00408273
I0818 23:21:21.539037  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00408281 (* 1 = 0.00408281 loss)
I0818 23:21:21.539049  2412 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0818 23:21:25.695031  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:22:11.977200  2412 solver.cpp:357] Iteration 57900 (1.98263 iter/s, 50.4382s/100 iters), loss = 0.0157958
I0818 23:22:11.977352  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157959 (* 1 = 0.0157959 loss)
I0818 23:22:11.977362  2412 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0818 23:23:00.291155  2412 solver.cpp:514] Iteration 58000, Testing net (#0)
I0818 23:23:30.887682  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:23:31.061674  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933902
I0818 23:23:31.061725  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.264042 (* 1 = 0.264042 loss)
I0818 23:23:31.395939  2412 solver.cpp:357] Iteration 58000 (1.2591 iter/s, 79.4218s/100 iters), loss = 0.0045184
I0818 23:23:31.395999  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00451848 (* 1 = 0.00451848 loss)
I0818 23:23:31.396009  2412 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0818 23:24:21.751935  2412 solver.cpp:357] Iteration 58100 (1.98578 iter/s, 50.3579s/100 iters), loss = 0.00599101
I0818 23:24:21.752259  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00599108 (* 1 = 0.00599108 loss)
I0818 23:24:21.752323  2412 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0818 23:25:11.645087  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:25:12.040285  2412 solver.cpp:357] Iteration 58200 (1.98854 iter/s, 50.2882s/100 iters), loss = 0.0163902
I0818 23:25:12.040343  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0163903 (* 1 = 0.0163903 loss)
I0818 23:25:12.040354  2412 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0818 23:26:02.385931  2412 solver.cpp:357] Iteration 58300 (1.98619 iter/s, 50.3476s/100 iters), loss = 0.00403697
I0818 23:26:02.386050  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00403704 (* 1 = 0.00403704 loss)
I0818 23:26:02.386062  2412 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0818 23:26:52.617600  2412 solver.cpp:357] Iteration 58400 (1.99078 iter/s, 50.2315s/100 iters), loss = 0.00792877
I0818 23:26:52.617790  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00792884 (* 1 = 0.00792884 loss)
I0818 23:26:52.617802  2412 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0818 23:27:42.587412  2412 solver.cpp:514] Iteration 58500, Testing net (#0)
I0818 23:28:19.695885  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:28:19.792254  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931502
I0818 23:28:19.792312  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271699 (* 1 = 0.271699 loss)
I0818 23:28:20.275588  2412 solver.cpp:357] Iteration 58500 (1.14078 iter/s, 87.6593s/100 iters), loss = 0.00418946
I0818 23:28:20.275663  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00418953 (* 1 = 0.00418953 loss)
I0818 23:28:20.275676  2412 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0818 23:29:05.222513  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:29:10.663008  2412 solver.cpp:357] Iteration 58600 (1.98463 iter/s, 50.3872s/100 iters), loss = 0.00881326
I0818 23:29:10.663075  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00881333 (* 1 = 0.00881333 loss)
I0818 23:29:10.663085  2412 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0818 23:29:54.792465  2412 solver.cpp:357] Iteration 58700 (2.26598 iter/s, 44.1311s/100 iters), loss = 0.0119675
I0818 23:29:54.792637  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0119676 (* 1 = 0.0119676 loss)
I0818 23:29:54.792650  2412 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0818 23:30:44.063328  2412 solver.cpp:357] Iteration 58800 (2.02961 iter/s, 49.2706s/100 iters), loss = 0.00399485
I0818 23:30:44.063439  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00399491 (* 1 = 0.00399491 loss)
I0818 23:30:44.063450  2412 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0818 23:31:34.472167  2412 solver.cpp:357] Iteration 58900 (1.98371 iter/s, 50.4107s/100 iters), loss = 0.00715256
I0818 23:31:34.472328  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00715262 (* 1 = 0.00715262 loss)
I0818 23:31:34.472345  2412 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0818 23:32:15.075264  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:32:24.509735  2412 solver.cpp:514] Iteration 59000, Testing net (#0)
I0818 23:33:01.930522  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:33:01.990489  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932902
I0818 23:33:01.990540  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.269201 (* 1 = 0.269201 loss)
I0818 23:33:02.472342  2412 solver.cpp:357] Iteration 59000 (1.13635 iter/s, 88.0014s/100 iters), loss = 0.00211106
I0818 23:33:02.472409  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00211112 (* 1 = 0.00211112 loss)
I0818 23:33:02.472420  2412 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0818 23:33:53.011085  2412 solver.cpp:357] Iteration 59100 (1.97869 iter/s, 50.5385s/100 iters), loss = 0.0104466
I0818 23:33:53.011242  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0104467 (* 1 = 0.0104467 loss)
I0818 23:33:53.011253  2412 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0818 23:34:43.593883  2412 solver.cpp:357] Iteration 59200 (1.97689 iter/s, 50.5846s/100 iters), loss = 0.00227111
I0818 23:34:43.594017  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00227117 (* 1 = 0.00227117 loss)
I0818 23:34:43.594030  2412 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0818 23:35:34.050110  2412 solver.cpp:357] Iteration 59300 (1.98192 iter/s, 50.456s/100 iters), loss = 0.00400964
I0818 23:35:34.050238  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0040097 (* 1 = 0.0040097 loss)
I0818 23:35:34.050251  2412 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0818 23:36:06.842555  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:36:19.859200  2412 solver.cpp:357] Iteration 59400 (2.18299 iter/s, 45.8087s/100 iters), loss = 0.0098455
I0818 23:36:19.859268  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00984556 (* 1 = 0.00984556 loss)
I0818 23:36:19.859280  2412 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0818 23:37:07.346135  2412 solver.cpp:514] Iteration 59500, Testing net (#0)
I0818 23:37:44.468891  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:37:44.653589  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932402
I0818 23:37:44.653647  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.268121 (* 1 = 0.268121 loss)
I0818 23:37:45.058554  2412 solver.cpp:357] Iteration 59500 (1.17367 iter/s, 85.2025s/100 iters), loss = 0.00264438
I0818 23:37:45.058614  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00264443 (* 1 = 0.00264443 loss)
I0818 23:37:45.058624  2412 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0818 23:38:35.722048  2412 solver.cpp:357] Iteration 59600 (1.97374 iter/s, 50.6653s/100 iters), loss = 0.00306877
I0818 23:38:35.722231  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00306882 (* 1 = 0.00306882 loss)
I0818 23:38:35.722244  2412 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0818 23:39:26.173106  2412 solver.cpp:357] Iteration 59700 (1.98213 iter/s, 50.4508s/100 iters), loss = 0.0301673
I0818 23:39:26.173285  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0301673 (* 1 = 0.0301673 loss)
I0818 23:39:26.173298  2412 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0818 23:39:57.018610  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:40:16.636590  2412 solver.cpp:357] Iteration 59800 (1.98156 iter/s, 50.4652s/100 iters), loss = 0.00498392
I0818 23:40:16.636672  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00498397 (* 1 = 0.00498397 loss)
I0818 23:40:16.636683  2412 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0818 23:41:07.089720  2412 solver.cpp:357] Iteration 59900 (1.98205 iter/s, 50.4529s/100 iters), loss = 0.00996095
I0818 23:41:07.090044  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.009961 (* 1 = 0.009961 loss)
I0818 23:41:07.090057  2412 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0818 23:41:57.082772  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_60000.caffemodel
I0818 23:41:57.104804  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_60000.solverstate
I0818 23:41:57.111908  2412 solver.cpp:514] Iteration 60000, Testing net (#0)
I0818 23:42:31.258677  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:42:31.368299  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930902
I0818 23:42:31.368351  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275809 (* 1 = 0.275809 loss)
I0818 23:42:31.754633  2412 solver.cpp:357] Iteration 60000 (1.18111 iter/s, 84.666s/100 iters), loss = 0.00812819
I0818 23:42:31.754698  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00812824 (* 1 = 0.00812824 loss)
I0818 23:42:31.754714  2412 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0818 23:43:17.611835  2412 solver.cpp:357] Iteration 60100 (2.18061 iter/s, 45.8588s/100 iters), loss = 0.00581632
I0818 23:43:17.612002  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00581638 (* 1 = 0.00581638 loss)
I0818 23:43:17.612013  2412 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0818 23:43:43.890961  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:44:08.086433  2412 solver.cpp:357] Iteration 60200 (1.98113 iter/s, 50.4763s/100 iters), loss = 0.00263089
I0818 23:44:08.086606  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00263094 (* 1 = 0.00263094 loss)
I0818 23:44:08.086618  2412 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0818 23:44:58.504767  2412 solver.cpp:357] Iteration 60300 (1.98342 iter/s, 50.4181s/100 iters), loss = 0.0032985
I0818 23:44:58.504940  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00329856 (* 1 = 0.00329856 loss)
I0818 23:44:58.504953  2412 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0818 23:45:48.893095  2412 solver.cpp:357] Iteration 60400 (1.9846 iter/s, 50.3881s/100 iters), loss = 0.00703863
I0818 23:45:48.893216  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00703869 (* 1 = 0.00703869 loss)
I0818 23:45:48.893229  2412 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0818 23:46:38.942095  2412 solver.cpp:514] Iteration 60500, Testing net (#0)
I0818 23:47:15.872018  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:47:16.053102  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931902
I0818 23:47:16.053167  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275167 (* 1 = 0.275167 loss)
I0818 23:47:16.458226  2412 solver.cpp:357] Iteration 60500 (1.14199 iter/s, 87.5663s/100 iters), loss = 0.00705753
I0818 23:47:16.458290  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00705758 (* 1 = 0.00705758 loss)
I0818 23:47:16.458300  2412 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0818 23:47:37.829946  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:48:07.116503  2412 solver.cpp:357] Iteration 60600 (1.97394 iter/s, 50.6601s/100 iters), loss = 0.00471165
I0818 23:48:07.116616  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00471171 (* 1 = 0.00471171 loss)
I0818 23:48:07.116631  2412 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0818 23:48:56.577841  2412 solver.cpp:357] Iteration 60700 (2.02179 iter/s, 49.4611s/100 iters), loss = 0.00811904
I0818 23:48:56.578032  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0081191 (* 1 = 0.0081191 loss)
I0818 23:48:56.578048  2412 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0818 23:49:40.621728  2412 solver.cpp:357] Iteration 60800 (2.2708 iter/s, 44.0374s/100 iters), loss = 0.0051087
I0818 23:49:40.621877  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00510876 (* 1 = 0.00510876 loss)
I0818 23:49:40.621891  2412 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0818 23:50:31.165086  2412 solver.cpp:357] Iteration 60900 (1.97907 iter/s, 50.5288s/100 iters), loss = 0.00504089
I0818 23:50:31.165241  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00504095 (* 1 = 0.00504095 loss)
I0818 23:50:31.165254  2412 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0818 23:50:47.815174  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:51:21.176931  2412 solver.cpp:514] Iteration 61000, Testing net (#0)
I0818 23:51:58.971559  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:51:59.140198  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932102
I0818 23:51:59.140257  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271027 (* 1 = 0.271027 loss)
I0818 23:51:59.510486  2412 solver.cpp:357] Iteration 61000 (1.13216 iter/s, 88.3266s/100 iters), loss = 0.00377488
I0818 23:51:59.510550  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00377495 (* 1 = 0.00377495 loss)
I0818 23:51:59.510561  2412 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0818 23:52:49.831105  2412 solver.cpp:357] Iteration 61100 (1.98755 iter/s, 50.3132s/100 iters), loss = 0.00481491
I0818 23:52:49.831233  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00481496 (* 1 = 0.00481496 loss)
I0818 23:52:49.831243  2412 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0818 23:53:40.132138  2412 solver.cpp:357] Iteration 61200 (1.98827 iter/s, 50.2949s/100 iters), loss = 0.00221432
I0818 23:53:40.132282  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00221438 (* 1 = 0.00221438 loss)
I0818 23:53:40.132293  2412 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0818 23:54:30.522533  2412 solver.cpp:357] Iteration 61300 (1.98471 iter/s, 50.3852s/100 iters), loss = 0.00910036
I0818 23:54:30.522732  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00910041 (* 1 = 0.00910041 loss)
I0818 23:54:30.522744  2412 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0818 23:54:42.797605  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:55:20.904130  2412 solver.cpp:357] Iteration 61400 (1.9851 iter/s, 50.3753s/100 iters), loss = 0.00664292
I0818 23:55:20.904306  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00664298 (* 1 = 0.00664298 loss)
I0818 23:55:20.904319  2412 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0818 23:56:04.589262  2412 solver.cpp:514] Iteration 61500, Testing net (#0)
I0818 23:56:41.180841  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:56:41.324529  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932402
I0818 23:56:41.324604  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273219 (* 1 = 0.273219 loss)
I0818 23:56:41.804433  2412 solver.cpp:357] Iteration 61500 (1.2362 iter/s, 80.8929s/100 iters), loss = 0.00789763
I0818 23:56:41.804533  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00789769 (* 1 = 0.00789769 loss)
I0818 23:56:41.804546  2412 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0818 23:57:32.037416  2412 solver.cpp:357] Iteration 61600 (1.99091 iter/s, 50.2282s/100 iters), loss = 0.00274228
I0818 23:57:32.037583  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00274233 (* 1 = 0.00274233 loss)
I0818 23:57:32.037597  2412 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0818 23:58:22.239349  2412 solver.cpp:357] Iteration 61700 (1.99205 iter/s, 50.1996s/100 iters), loss = 0.00267082
I0818 23:58:22.239536  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00267088 (* 1 = 0.00267088 loss)
I0818 23:58:22.239550  2412 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0818 23:58:29.343451  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0818 23:59:12.287134  2412 solver.cpp:357] Iteration 61800 (1.99825 iter/s, 50.0438s/100 iters), loss = 0.0102339
I0818 23:59:12.287303  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0102339 (* 1 = 0.0102339 loss)
I0818 23:59:12.287317  2412 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0819 00:00:02.385288  2412 solver.cpp:357] Iteration 61900 (1.99616 iter/s, 50.0962s/100 iters), loss = 0.00286759
I0819 00:00:02.385432  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00286765 (* 1 = 0.00286765 loss)
I0819 00:00:02.385445  2412 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0819 00:00:51.961778  2412 solver.cpp:514] Iteration 62000, Testing net (#0)
I0819 00:01:29.461393  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:01:29.633549  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931602
I0819 00:01:29.633599  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.278766 (* 1 = 0.278766 loss)
I0819 00:01:29.966342  2412 solver.cpp:357] Iteration 62000 (1.14183 iter/s, 87.5787s/100 iters), loss = 0.00729497
I0819 00:01:29.966411  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00729502 (* 1 = 0.00729502 loss)
I0819 00:01:29.966423  2412 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0819 00:02:16.347872  2412 solver.cpp:357] Iteration 62100 (2.15617 iter/s, 46.3784s/100 iters), loss = 0.00546045
I0819 00:02:16.348019  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0054605 (* 1 = 0.0054605 loss)
I0819 00:02:16.348031  2412 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0819 00:02:18.583226  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:03:03.646242  2412 solver.cpp:357] Iteration 62200 (2.11428 iter/s, 47.2974s/100 iters), loss = 0.00637705
I0819 00:03:03.646390  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0063771 (* 1 = 0.0063771 loss)
I0819 00:03:03.646401  2412 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0819 00:03:53.940820  2412 solver.cpp:357] Iteration 62300 (1.9884 iter/s, 50.2916s/100 iters), loss = 0.00520593
I0819 00:03:53.940927  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00520598 (* 1 = 0.00520598 loss)
I0819 00:03:53.940938  2412 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0819 00:04:44.397408  2412 solver.cpp:357] Iteration 62400 (1.98193 iter/s, 50.4558s/100 iters), loss = 0.00957764
I0819 00:04:44.397572  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00957769 (* 1 = 0.00957769 loss)
I0819 00:04:44.397583  2412 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0819 00:05:32.419435  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:05:34.498077  2412 solver.cpp:514] Iteration 62500, Testing net (#0)
I0819 00:06:11.468858  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:06:11.654770  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931302
I0819 00:06:11.654820  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.278535 (* 1 = 0.278535 loss)
I0819 00:06:11.989635  2412 solver.cpp:357] Iteration 62500 (1.14169 iter/s, 87.5891s/100 iters), loss = 0.0136165
I0819 00:06:11.989698  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0136165 (* 1 = 0.0136165 loss)
I0819 00:06:11.989711  2412 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0819 00:07:02.550158  2412 solver.cpp:357] Iteration 62600 (1.97797 iter/s, 50.5569s/100 iters), loss = 0.00350599
I0819 00:07:02.550278  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00350604 (* 1 = 0.00350604 loss)
I0819 00:07:02.550292  2412 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0819 00:07:53.033210  2412 solver.cpp:357] Iteration 62700 (1.98101 iter/s, 50.4793s/100 iters), loss = 0.0115494
I0819 00:07:53.033339  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0115495 (* 1 = 0.0115495 loss)
I0819 00:07:53.033349  2412 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0819 00:08:40.947034  2412 solver.cpp:357] Iteration 62800 (2.08715 iter/s, 47.9123s/100 iters), loss = 0.00453428
I0819 00:08:40.947183  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00453432 (* 1 = 0.00453432 loss)
I0819 00:08:40.947196  2412 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0819 00:09:19.818768  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:09:26.709024  2412 solver.cpp:357] Iteration 62900 (2.18529 iter/s, 45.7606s/100 iters), loss = 0.0132473
I0819 00:09:26.709106  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0132473 (* 1 = 0.0132473 loss)
I0819 00:09:26.709116  2412 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0819 00:10:16.660910  2412 solver.cpp:514] Iteration 63000, Testing net (#0)
I0819 00:10:53.498944  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:10:53.583825  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932902
I0819 00:10:53.583869  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271256 (* 1 = 0.271256 loss)
I0819 00:10:54.024585  2412 solver.cpp:357] Iteration 63000 (1.1453 iter/s, 87.3133s/100 iters), loss = 0.0104273
I0819 00:10:54.024657  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0104273 (* 1 = 0.0104273 loss)
I0819 00:10:54.024670  2412 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0819 00:11:44.535212  2412 solver.cpp:357] Iteration 63100 (1.97991 iter/s, 50.5073s/100 iters), loss = 0.00329567
I0819 00:11:44.535358  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00329571 (* 1 = 0.00329571 loss)
I0819 00:11:44.535372  2412 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0819 00:12:34.776897  2412 solver.cpp:357] Iteration 63200 (1.99051 iter/s, 50.2385s/100 iters), loss = 0.00273051
I0819 00:12:34.777061  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00273055 (* 1 = 0.00273055 loss)
I0819 00:12:34.777072  2412 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0819 00:13:13.983415  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:13:25.397783  2412 solver.cpp:357] Iteration 63300 (1.97552 iter/s, 50.6197s/100 iters), loss = 0.0153305
I0819 00:13:25.397857  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0153305 (* 1 = 0.0153305 loss)
I0819 00:13:25.397868  2412 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0819 00:14:15.794670  2412 solver.cpp:357] Iteration 63400 (1.98437 iter/s, 50.3938s/100 iters), loss = 0.00572776
I0819 00:14:15.794854  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0057278 (* 1 = 0.0057278 loss)
I0819 00:14:15.794867  2412 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0819 00:15:04.996898  2412 solver.cpp:514] Iteration 63500, Testing net (#0)
I0819 00:15:34.650322  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:15:34.763106  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933602
I0819 00:15:34.763152  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271774 (* 1 = 0.271774 loss)
I0819 00:15:35.144114  2412 solver.cpp:357] Iteration 63500 (1.2603 iter/s, 79.3459s/100 iters), loss = 0.0110962
I0819 00:15:35.144222  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0110962 (* 1 = 0.0110962 loss)
I0819 00:15:35.144235  2412 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0819 00:16:25.244124  2412 solver.cpp:357] Iteration 63600 (1.99605 iter/s, 50.099s/100 iters), loss = 0.00386688
I0819 00:16:25.244248  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00386692 (* 1 = 0.00386692 loss)
I0819 00:16:25.244261  2412 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0819 00:16:59.091908  2417 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:17:15.547878  2412 solver.cpp:357] Iteration 63700 (1.98804 iter/s, 50.3008s/100 iters), loss = 0.00694676
I0819 00:17:15.547955  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00694679 (* 1 = 0.00694679 loss)
I0819 00:17:15.547966  2412 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0819 00:18:05.934515  2412 solver.cpp:357] Iteration 63800 (1.98477 iter/s, 50.3837s/100 iters), loss = 0.00840469
I0819 00:18:05.934710  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00840473 (* 1 = 0.00840473 loss)
I0819 00:18:05.934742  2412 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0819 00:18:56.364326  2412 solver.cpp:357] Iteration 63900 (1.98307 iter/s, 50.4269s/100 iters), loss = 0.00370465
I0819 00:18:56.364496  2412 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00370468 (* 1 = 0.00370468 loss)
I0819 00:18:56.364509  2412 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0819 00:19:46.233793  2412 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_64000.caffemodel
I0819 00:19:46.256207  2412 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_64000.solverstate
I0819 00:19:46.389837  2412 solver.cpp:472] Iteration 64000, loss = 0.00450019
I0819 00:19:46.389904  2412 solver.cpp:514] Iteration 64000, Testing net (#0)
I0819 00:20:25.203554  2418 data_layer.cpp:73] Restarting data prefetching from start.
I0819 00:20:25.342617  2412 solver.cpp:580]     Test net output #0: Accuracy1 = 0.933002
I0819 00:20:25.342689  2412 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.280935 (* 1 = 0.280935 loss)
I0819 00:20:25.342696  2412 solver.cpp:479] Optimization Done.
I0819 00:20:25.342712  2412 caffe.cpp:326] Optimization Done.
