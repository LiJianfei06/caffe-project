WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 11:11:32.424672  1899 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0825 11:11:32.424818  1899 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0825 11:11:32.424823  1899 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0825 11:11:32.424826  1899 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0825 11:11:32.424830  1899 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0825 11:11:32.424834  1899 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0825 11:11:32.424896  1899 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0825 11:11:32.425128  1899 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0825 11:11:32.447859  1899 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0825 11:11:32.447988  1899 caffe.cpp:269] Using GPUs 0
I0825 11:11:32.460855  1899 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0825 11:11:33.191905  1899 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0825 11:11:33.191952  1899 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0825 11:11:33.307967  1899 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_44.prototxt"
test_net: "./test_ResNet_44.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_44"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 2
type: "Nesterov"
I0825 11:11:33.308301  1899 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_44.prototxt
I0825 11:11:33.309933  1899 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_44.prototxt
I0825 11:11:33.309962  1899 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:11:33.310283  1899 net.cpp:390] layer_param.include_size():1
I0825 11:11:33.310298  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310307  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310358  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310369  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310372  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310376  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310380  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310384  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310389  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310411  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310425  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310442  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310458  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310475  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310483  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310487  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310492  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310497  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310500  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310523  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310536  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310552  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310559  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310564  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310569  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310572  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310576  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310598  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310626  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310634  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310639  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310643  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310647  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310668  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310685  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310701  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310709  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310714  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310750  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310758  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310762  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310766  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310770  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310775  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310796  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310809  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310825  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310842  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310858  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310874  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310881  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310886  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310890  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310895  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310899  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310920  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310935  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310950  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310957  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310962  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310966  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310971  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.310973  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.310994  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311007  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311024  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311033  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311036  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311040  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311044  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311048  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311069  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311082  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311100  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311106  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311110  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311115  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311118  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311121  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311142  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311156  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311172  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311179  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311184  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311187  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311192  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311213  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311233  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311240  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311245  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311249  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311254  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311273  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311290  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311305  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311313  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311317  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311345  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311362  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311378  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311385  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311390  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311394  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311398  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311401  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311424  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311437  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311453  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311468  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311486  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311493  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311497  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311501  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311506  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311509  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311530  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311544  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311560  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311575  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311592  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311599  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311604  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311607  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311611  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311615  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311636  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311650  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311666  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311681  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311698  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311705  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311709  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311713  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311717  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311722  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311743  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311755  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311772  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311789  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311805  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311811  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311816  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311820  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311825  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311828  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311853  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311870  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311887  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311895  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311899  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311903  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311908  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311928  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311945  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311961  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311975  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.311980  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.311985  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312005  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312022  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312031  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312034  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312038  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312042  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312047  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312067  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312081  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312098  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312105  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312110  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312114  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312117  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312121  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312141  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312155  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312170  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312186  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312202  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312209  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312214  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312217  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312222  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312225  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312245  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312258  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312274  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312289  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312307  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312314  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312319  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312322  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312326  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312330  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312350  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312364  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312381  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312387  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312393  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312397  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312402  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312405  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312425  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312438  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312453  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312474  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312481  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312485  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312489  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312494  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312497  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312516  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312530  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312546  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312561  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312574  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312579  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312583  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312587  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312592  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312613  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312625  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312642  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312659  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312674  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312681  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312686  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312690  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312693  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312697  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312717  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312731  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312747  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312762  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312779  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312786  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312791  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312794  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312798  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312803  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312824  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312836  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312853  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312860  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312865  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312870  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312873  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312877  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312898  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312911  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312928  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312935  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312940  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312944  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312948  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312952  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.312974  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.312988  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313004  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313010  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313015  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313019  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313024  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313026  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313031  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313035  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313038  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313042  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313046  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313050  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313055  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313076  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313089  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313109  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313123  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313140  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313156  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313164  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313169  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313172  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313176  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313197  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313215  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313230  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313247  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313254  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313258  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313262  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313266  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313271  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313292  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313304  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313320  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313338  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313354  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313361  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313365  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313369  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313374  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313377  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313398  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313411  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313428  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313436  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313441  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313444  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313448  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313469  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313483  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313499  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313516  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313532  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313549  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313555  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313560  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313563  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313567  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313571  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313591  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313603  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313619  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313635  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313655  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313663  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313668  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313671  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313675  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313679  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313700  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313714  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313730  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313738  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313750  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313769  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313786  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313792  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313797  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313800  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313805  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313809  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313829  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313843  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313859  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313866  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313870  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313874  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313879  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313899  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313913  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313930  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313946  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313961  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313977  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313984  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313989  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.313992  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.313997  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314000  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314021  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314035  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314051  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314059  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314062  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314066  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314070  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314074  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314079  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314098  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314112  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314127  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314143  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314152  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314155  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314159  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314163  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314167  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314190  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314204  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314220  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314236  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314257  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314265  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314270  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314272  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314278  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314281  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314302  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314316  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314332  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314347  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314363  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314386  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314404  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314410  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314414  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314419  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314422  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314441  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314456  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314471  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314479  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314483  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314487  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314507  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.314524  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:33.314540  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:33.315872  1899 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
I0825 11:11:33.316799  1899 layer_factory.hpp:77] Creating layer Data1
I0825 11:11:33.316988  1899 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0825 11:11:33.317046  1899 net.cpp:128] Creating Layer Data1
I0825 11:11:33.317057  1899 net.cpp:522] Data1 -> Data1
I0825 11:11:33.317092  1899 net.cpp:522] Data1 -> Data2
I0825 11:11:33.318965  1899 data_layer.cpp:45] output data size: 64,3,32,32
I0825 11:11:33.331548  1899 net.cpp:172] Setting up Data1
I0825 11:11:33.331631  1899 net.cpp:186] Top shape: 64 3 32 32 (196608)
I0825 11:11:33.331651  1899 net.cpp:186] Top shape: 64 (64)
I0825 11:11:33.331666  1899 net.cpp:194] Memory required for data: 786688
I0825 11:11:33.331693  1899 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:11:33.331743  1899 net.cpp:128] Creating Layer Convolution1
I0825 11:11:33.331763  1899 net.cpp:558] Convolution1 <- Data1
I0825 11:11:33.331794  1899 net.cpp:522] Convolution1 -> Convolution1
I0825 11:11:34.409482  1899 net.cpp:172] Setting up Convolution1
I0825 11:11:34.409539  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.409544  1899 net.cpp:194] Memory required for data: 4980992
I0825 11:11:34.409586  1899 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:11:34.409605  1899 net.cpp:128] Creating Layer BatchNorm1
I0825 11:11:34.409664  1899 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:11:34.409692  1899 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:11:34.409938  1899 net.cpp:172] Setting up BatchNorm1
I0825 11:11:34.409950  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.409956  1899 net.cpp:194] Memory required for data: 9175296
I0825 11:11:34.409996  1899 layer_factory.hpp:77] Creating layer Scale1
I0825 11:11:34.410039  1899 net.cpp:128] Creating Layer Scale1
I0825 11:11:34.410055  1899 net.cpp:558] Scale1 <- Convolution1
I0825 11:11:34.410073  1899 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:11:34.410140  1899 layer_factory.hpp:77] Creating layer Scale1
I0825 11:11:34.410284  1899 net.cpp:172] Setting up Scale1
I0825 11:11:34.410295  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.410300  1899 net.cpp:194] Memory required for data: 13369600
I0825 11:11:34.410308  1899 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:11:34.410318  1899 net.cpp:128] Creating Layer ReLU1
I0825 11:11:34.410323  1899 net.cpp:558] ReLU1 <- Convolution1
I0825 11:11:34.410329  1899 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:11:34.411243  1899 net.cpp:172] Setting up ReLU1
I0825 11:11:34.411260  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.411265  1899 net.cpp:194] Memory required for data: 17563904
I0825 11:11:34.411270  1899 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:11:34.411281  1899 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:11:34.411285  1899 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:11:34.411293  1899 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:11:34.411300  1899 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:11:34.411342  1899 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:11:34.411350  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.411355  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.411360  1899 net.cpp:194] Memory required for data: 25952512
I0825 11:11:34.411365  1899 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:11:34.411378  1899 net.cpp:128] Creating Layer Convolution2
I0825 11:11:34.411383  1899 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:11:34.411389  1899 net.cpp:522] Convolution2 -> Convolution2
I0825 11:11:34.418064  1899 net.cpp:172] Setting up Convolution2
I0825 11:11:34.418090  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.418095  1899 net.cpp:194] Memory required for data: 30146816
I0825 11:11:34.418108  1899 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:11:34.418118  1899 net.cpp:128] Creating Layer BatchNorm2
I0825 11:11:34.418160  1899 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:11:34.418174  1899 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:11:34.418413  1899 net.cpp:172] Setting up BatchNorm2
I0825 11:11:34.418428  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.418433  1899 net.cpp:194] Memory required for data: 34341120
I0825 11:11:34.418444  1899 layer_factory.hpp:77] Creating layer Scale2
I0825 11:11:34.418453  1899 net.cpp:128] Creating Layer Scale2
I0825 11:11:34.418457  1899 net.cpp:558] Scale2 <- Convolution2
I0825 11:11:34.418464  1899 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:11:34.418507  1899 layer_factory.hpp:77] Creating layer Scale2
I0825 11:11:34.418664  1899 net.cpp:172] Setting up Scale2
I0825 11:11:34.418687  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.418702  1899 net.cpp:194] Memory required for data: 38535424
I0825 11:11:34.418733  1899 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:11:34.418751  1899 net.cpp:128] Creating Layer ReLU2
I0825 11:11:34.418771  1899 net.cpp:558] ReLU2 <- Convolution2
I0825 11:11:34.418787  1899 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:11:34.420192  1899 net.cpp:172] Setting up ReLU2
I0825 11:11:34.420210  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.420217  1899 net.cpp:194] Memory required for data: 42729728
I0825 11:11:34.420220  1899 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:11:34.420233  1899 net.cpp:128] Creating Layer Convolution3
I0825 11:11:34.420238  1899 net.cpp:558] Convolution3 <- Convolution2
I0825 11:11:34.420285  1899 net.cpp:522] Convolution3 -> Convolution3
I0825 11:11:34.424302  1899 net.cpp:172] Setting up Convolution3
I0825 11:11:34.424329  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.424335  1899 net.cpp:194] Memory required for data: 46924032
I0825 11:11:34.424345  1899 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:11:34.424355  1899 net.cpp:128] Creating Layer BatchNorm3
I0825 11:11:34.424360  1899 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:11:34.424401  1899 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:11:34.424641  1899 net.cpp:172] Setting up BatchNorm3
I0825 11:11:34.424652  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.424657  1899 net.cpp:194] Memory required for data: 51118336
I0825 11:11:34.424670  1899 layer_factory.hpp:77] Creating layer Scale3
I0825 11:11:34.424679  1899 net.cpp:128] Creating Layer Scale3
I0825 11:11:34.424684  1899 net.cpp:558] Scale3 <- Convolution3
I0825 11:11:34.424690  1899 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:11:34.424731  1899 layer_factory.hpp:77] Creating layer Scale3
I0825 11:11:34.424886  1899 net.cpp:172] Setting up Scale3
I0825 11:11:34.424909  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.424927  1899 net.cpp:194] Memory required for data: 55312640
I0825 11:11:34.424947  1899 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:11:34.424970  1899 net.cpp:128] Creating Layer Eltwise1
I0825 11:11:34.424984  1899 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:11:34.425000  1899 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:11:34.425021  1899 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:11:34.425065  1899 net.cpp:172] Setting up Eltwise1
I0825 11:11:34.425083  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.425101  1899 net.cpp:194] Memory required for data: 59506944
I0825 11:11:34.425115  1899 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:11:34.425132  1899 net.cpp:128] Creating Layer ReLU3
I0825 11:11:34.425146  1899 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:11:34.425166  1899 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:11:34.428544  1899 net.cpp:172] Setting up ReLU3
I0825 11:11:34.428570  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.428575  1899 net.cpp:194] Memory required for data: 63701248
I0825 11:11:34.428580  1899 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:11:34.428589  1899 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:11:34.428593  1899 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:11:34.428628  1899 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:11:34.428650  1899 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:11:34.428714  1899 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:11:34.428725  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.428732  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.428736  1899 net.cpp:194] Memory required for data: 72089856
I0825 11:11:34.428757  1899 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:11:34.428783  1899 net.cpp:128] Creating Layer Convolution4
I0825 11:11:34.428791  1899 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:11:34.428799  1899 net.cpp:522] Convolution4 -> Convolution4
I0825 11:11:34.435818  1899 net.cpp:172] Setting up Convolution4
I0825 11:11:34.435847  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.435850  1899 net.cpp:194] Memory required for data: 76284160
I0825 11:11:34.435863  1899 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:11:34.435871  1899 net.cpp:128] Creating Layer BatchNorm4
I0825 11:11:34.435904  1899 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:11:34.435922  1899 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:11:34.436177  1899 net.cpp:172] Setting up BatchNorm4
I0825 11:11:34.436188  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.436192  1899 net.cpp:194] Memory required for data: 80478464
I0825 11:11:34.436236  1899 layer_factory.hpp:77] Creating layer Scale4
I0825 11:11:34.436249  1899 net.cpp:128] Creating Layer Scale4
I0825 11:11:34.436254  1899 net.cpp:558] Scale4 <- Convolution4
I0825 11:11:34.436261  1899 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:11:34.436329  1899 layer_factory.hpp:77] Creating layer Scale4
I0825 11:11:34.436465  1899 net.cpp:172] Setting up Scale4
I0825 11:11:34.436475  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.436480  1899 net.cpp:194] Memory required for data: 84672768
I0825 11:11:34.436488  1899 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:11:34.436516  1899 net.cpp:128] Creating Layer ReLU4
I0825 11:11:34.436532  1899 net.cpp:558] ReLU4 <- Convolution4
I0825 11:11:34.436550  1899 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:11:34.437937  1899 net.cpp:172] Setting up ReLU4
I0825 11:11:34.437955  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.437960  1899 net.cpp:194] Memory required for data: 88867072
I0825 11:11:34.437964  1899 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:11:34.437984  1899 net.cpp:128] Creating Layer Convolution5
I0825 11:11:34.438014  1899 net.cpp:558] Convolution5 <- Convolution4
I0825 11:11:34.438037  1899 net.cpp:522] Convolution5 -> Convolution5
I0825 11:11:34.444789  1899 net.cpp:172] Setting up Convolution5
I0825 11:11:34.444813  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.444816  1899 net.cpp:194] Memory required for data: 93061376
I0825 11:11:34.444826  1899 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:11:34.444838  1899 net.cpp:128] Creating Layer BatchNorm5
I0825 11:11:34.444875  1899 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:11:34.444898  1899 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:11:34.445143  1899 net.cpp:172] Setting up BatchNorm5
I0825 11:11:34.445154  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.445178  1899 net.cpp:194] Memory required for data: 97255680
I0825 11:11:34.445199  1899 layer_factory.hpp:77] Creating layer Scale5
I0825 11:11:34.445222  1899 net.cpp:128] Creating Layer Scale5
I0825 11:11:34.445230  1899 net.cpp:558] Scale5 <- Convolution5
I0825 11:11:34.445236  1899 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:11:34.445286  1899 layer_factory.hpp:77] Creating layer Scale5
I0825 11:11:34.445416  1899 net.cpp:172] Setting up Scale5
I0825 11:11:34.445427  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.445431  1899 net.cpp:194] Memory required for data: 101449984
I0825 11:11:34.445458  1899 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:11:34.445473  1899 net.cpp:128] Creating Layer Eltwise2
I0825 11:11:34.445497  1899 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:11:34.445505  1899 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:11:34.445514  1899 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:11:34.445561  1899 net.cpp:172] Setting up Eltwise2
I0825 11:11:34.445571  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.445575  1899 net.cpp:194] Memory required for data: 105644288
I0825 11:11:34.445580  1899 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:11:34.445605  1899 net.cpp:128] Creating Layer ReLU5
I0825 11:11:34.445622  1899 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:11:34.445634  1899 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:11:34.446877  1899 net.cpp:172] Setting up ReLU5
I0825 11:11:34.446895  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.446900  1899 net.cpp:194] Memory required for data: 109838592
I0825 11:11:34.446904  1899 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:11:34.446915  1899 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:11:34.446943  1899 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:11:34.446964  1899 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:11:34.446990  1899 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:11:34.447054  1899 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:11:34.447077  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.447082  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.447087  1899 net.cpp:194] Memory required for data: 118227200
I0825 11:11:34.447090  1899 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:11:34.447108  1899 net.cpp:128] Creating Layer Convolution6
I0825 11:11:34.447132  1899 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:11:34.447154  1899 net.cpp:522] Convolution6 -> Convolution6
I0825 11:11:34.454583  1899 net.cpp:172] Setting up Convolution6
I0825 11:11:34.454607  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.454612  1899 net.cpp:194] Memory required for data: 122421504
I0825 11:11:34.454622  1899 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:11:34.454632  1899 net.cpp:128] Creating Layer BatchNorm6
I0825 11:11:34.454665  1899 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:11:34.454684  1899 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:11:34.454938  1899 net.cpp:172] Setting up BatchNorm6
I0825 11:11:34.454949  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.454954  1899 net.cpp:194] Memory required for data: 126615808
I0825 11:11:34.454964  1899 layer_factory.hpp:77] Creating layer Scale6
I0825 11:11:34.454990  1899 net.cpp:128] Creating Layer Scale6
I0825 11:11:34.455008  1899 net.cpp:558] Scale6 <- Convolution6
I0825 11:11:34.455026  1899 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:11:34.455088  1899 layer_factory.hpp:77] Creating layer Scale6
I0825 11:11:34.455224  1899 net.cpp:172] Setting up Scale6
I0825 11:11:34.455235  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.455240  1899 net.cpp:194] Memory required for data: 130810112
I0825 11:11:34.455247  1899 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:11:34.455273  1899 net.cpp:128] Creating Layer ReLU6
I0825 11:11:34.455291  1899 net.cpp:558] ReLU6 <- Convolution6
I0825 11:11:34.455312  1899 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:11:34.456732  1899 net.cpp:172] Setting up ReLU6
I0825 11:11:34.456756  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.456761  1899 net.cpp:194] Memory required for data: 135004416
I0825 11:11:34.456766  1899 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:11:34.456780  1899 net.cpp:128] Creating Layer Convolution7
I0825 11:11:34.456815  1899 net.cpp:558] Convolution7 <- Convolution6
I0825 11:11:34.456835  1899 net.cpp:522] Convolution7 -> Convolution7
I0825 11:11:34.459430  1899 net.cpp:172] Setting up Convolution7
I0825 11:11:34.459458  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.459463  1899 net.cpp:194] Memory required for data: 139198720
I0825 11:11:34.459473  1899 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:11:34.459483  1899 net.cpp:128] Creating Layer BatchNorm7
I0825 11:11:34.459519  1899 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:11:34.459537  1899 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:11:34.459800  1899 net.cpp:172] Setting up BatchNorm7
I0825 11:11:34.459812  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.459816  1899 net.cpp:194] Memory required for data: 143393024
I0825 11:11:34.459827  1899 layer_factory.hpp:77] Creating layer Scale7
I0825 11:11:34.459861  1899 net.cpp:128] Creating Layer Scale7
I0825 11:11:34.459880  1899 net.cpp:558] Scale7 <- Convolution7
I0825 11:11:34.459899  1899 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:11:34.459966  1899 layer_factory.hpp:77] Creating layer Scale7
I0825 11:11:34.460108  1899 net.cpp:172] Setting up Scale7
I0825 11:11:34.460117  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.460121  1899 net.cpp:194] Memory required for data: 147587328
I0825 11:11:34.460129  1899 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:11:34.460157  1899 net.cpp:128] Creating Layer Eltwise3
I0825 11:11:34.460175  1899 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:11:34.460192  1899 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:11:34.460204  1899 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:11:34.460253  1899 net.cpp:172] Setting up Eltwise3
I0825 11:11:34.460264  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.460268  1899 net.cpp:194] Memory required for data: 151781632
I0825 11:11:34.460273  1899 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:11:34.460280  1899 net.cpp:128] Creating Layer ReLU7
I0825 11:11:34.460302  1899 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:11:34.460320  1899 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:11:34.461446  1899 net.cpp:172] Setting up ReLU7
I0825 11:11:34.461460  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.461464  1899 net.cpp:194] Memory required for data: 155975936
I0825 11:11:34.461469  1899 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:11:34.461478  1899 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:11:34.461483  1899 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:11:34.461489  1899 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:11:34.461498  1899 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:11:34.461572  1899 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:11:34.461593  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.461611  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.461628  1899 net.cpp:194] Memory required for data: 164364544
I0825 11:11:34.461645  1899 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:11:34.461671  1899 net.cpp:128] Creating Layer Convolution8
I0825 11:11:34.461679  1899 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:11:34.461689  1899 net.cpp:522] Convolution8 -> Convolution8
I0825 11:11:34.468313  1899 net.cpp:172] Setting up Convolution8
I0825 11:11:34.468359  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.468379  1899 net.cpp:194] Memory required for data: 168558848
I0825 11:11:34.468402  1899 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:11:34.468425  1899 net.cpp:128] Creating Layer BatchNorm8
I0825 11:11:34.468443  1899 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:11:34.468463  1899 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:11:34.468734  1899 net.cpp:172] Setting up BatchNorm8
I0825 11:11:34.468757  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.468773  1899 net.cpp:194] Memory required for data: 172753152
I0825 11:11:34.468796  1899 layer_factory.hpp:77] Creating layer Scale8
I0825 11:11:34.468817  1899 net.cpp:128] Creating Layer Scale8
I0825 11:11:34.468834  1899 net.cpp:558] Scale8 <- Convolution8
I0825 11:11:34.468852  1899 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:11:34.468920  1899 layer_factory.hpp:77] Creating layer Scale8
I0825 11:11:34.469077  1899 net.cpp:172] Setting up Scale8
I0825 11:11:34.469099  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.469115  1899 net.cpp:194] Memory required for data: 176947456
I0825 11:11:34.469141  1899 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:11:34.469161  1899 net.cpp:128] Creating Layer ReLU8
I0825 11:11:34.469177  1899 net.cpp:558] ReLU8 <- Convolution8
I0825 11:11:34.469198  1899 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:11:34.470397  1899 net.cpp:172] Setting up ReLU8
I0825 11:11:34.470414  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.470418  1899 net.cpp:194] Memory required for data: 181141760
I0825 11:11:34.470423  1899 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:11:34.470438  1899 net.cpp:128] Creating Layer Convolution9
I0825 11:11:34.470464  1899 net.cpp:558] Convolution9 <- Convolution8
I0825 11:11:34.470486  1899 net.cpp:522] Convolution9 -> Convolution9
I0825 11:11:34.483178  1899 net.cpp:172] Setting up Convolution9
I0825 11:11:34.483206  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.483211  1899 net.cpp:194] Memory required for data: 185336064
I0825 11:11:34.483237  1899 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:11:34.483278  1899 net.cpp:128] Creating Layer BatchNorm9
I0825 11:11:34.483295  1899 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:11:34.483321  1899 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:11:34.483580  1899 net.cpp:172] Setting up BatchNorm9
I0825 11:11:34.483590  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.483594  1899 net.cpp:194] Memory required for data: 189530368
I0825 11:11:34.483605  1899 layer_factory.hpp:77] Creating layer Scale9
I0825 11:11:34.483635  1899 net.cpp:128] Creating Layer Scale9
I0825 11:11:34.483652  1899 net.cpp:558] Scale9 <- Convolution9
I0825 11:11:34.483671  1899 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:11:34.483736  1899 layer_factory.hpp:77] Creating layer Scale9
I0825 11:11:34.483880  1899 net.cpp:172] Setting up Scale9
I0825 11:11:34.483891  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.483896  1899 net.cpp:194] Memory required for data: 193724672
I0825 11:11:34.483904  1899 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:11:34.483935  1899 net.cpp:128] Creating Layer Eltwise4
I0825 11:11:34.483952  1899 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:11:34.483971  1899 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:11:34.483989  1899 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:11:34.484036  1899 net.cpp:172] Setting up Eltwise4
I0825 11:11:34.484046  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.484051  1899 net.cpp:194] Memory required for data: 197918976
I0825 11:11:34.484055  1899 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:11:34.484062  1899 net.cpp:128] Creating Layer ReLU9
I0825 11:11:34.484084  1899 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:11:34.484103  1899 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:11:34.485273  1899 net.cpp:172] Setting up ReLU9
I0825 11:11:34.485287  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.485291  1899 net.cpp:194] Memory required for data: 202113280
I0825 11:11:34.485296  1899 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:11:34.485304  1899 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:11:34.485308  1899 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:11:34.485316  1899 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:11:34.485325  1899 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:11:34.485404  1899 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:11:34.485425  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.485445  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.485460  1899 net.cpp:194] Memory required for data: 210501888
I0825 11:11:34.485477  1899 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:11:34.485502  1899 net.cpp:128] Creating Layer Convolution10
I0825 11:11:34.485512  1899 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:11:34.485522  1899 net.cpp:522] Convolution10 -> Convolution10
I0825 11:11:34.492048  1899 net.cpp:172] Setting up Convolution10
I0825 11:11:34.492074  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.492079  1899 net.cpp:194] Memory required for data: 214696192
I0825 11:11:34.492100  1899 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:11:34.492110  1899 net.cpp:128] Creating Layer BatchNorm10
I0825 11:11:34.492115  1899 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:11:34.492123  1899 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:11:34.492375  1899 net.cpp:172] Setting up BatchNorm10
I0825 11:11:34.492388  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.492393  1899 net.cpp:194] Memory required for data: 218890496
I0825 11:11:34.492403  1899 layer_factory.hpp:77] Creating layer Scale10
I0825 11:11:34.492409  1899 net.cpp:128] Creating Layer Scale10
I0825 11:11:34.492413  1899 net.cpp:558] Scale10 <- Convolution10
I0825 11:11:34.492419  1899 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:11:34.492530  1899 layer_factory.hpp:77] Creating layer Scale10
I0825 11:11:34.492688  1899 net.cpp:172] Setting up Scale10
I0825 11:11:34.492702  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.492707  1899 net.cpp:194] Memory required for data: 223084800
I0825 11:11:34.492715  1899 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:11:34.492724  1899 net.cpp:128] Creating Layer ReLU10
I0825 11:11:34.492729  1899 net.cpp:558] ReLU10 <- Convolution10
I0825 11:11:34.492735  1899 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:11:34.494180  1899 net.cpp:172] Setting up ReLU10
I0825 11:11:34.494230  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.494248  1899 net.cpp:194] Memory required for data: 227279104
I0825 11:11:34.494264  1899 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:11:34.494288  1899 net.cpp:128] Creating Layer Convolution11
I0825 11:11:34.494310  1899 net.cpp:558] Convolution11 <- Convolution10
I0825 11:11:34.494331  1899 net.cpp:522] Convolution11 -> Convolution11
I0825 11:11:34.501595  1899 net.cpp:172] Setting up Convolution11
I0825 11:11:34.501621  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.501626  1899 net.cpp:194] Memory required for data: 231473408
I0825 11:11:34.501636  1899 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:11:34.501644  1899 net.cpp:128] Creating Layer BatchNorm11
I0825 11:11:34.501648  1899 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:11:34.501658  1899 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:11:34.501955  1899 net.cpp:172] Setting up BatchNorm11
I0825 11:11:34.501966  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.501971  1899 net.cpp:194] Memory required for data: 235667712
I0825 11:11:34.501981  1899 layer_factory.hpp:77] Creating layer Scale11
I0825 11:11:34.502008  1899 net.cpp:128] Creating Layer Scale11
I0825 11:11:34.502025  1899 net.cpp:558] Scale11 <- Convolution11
I0825 11:11:34.502048  1899 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:11:34.502100  1899 layer_factory.hpp:77] Creating layer Scale11
I0825 11:11:34.502243  1899 net.cpp:172] Setting up Scale11
I0825 11:11:34.502254  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.502257  1899 net.cpp:194] Memory required for data: 239862016
I0825 11:11:34.502265  1899 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:11:34.502291  1899 net.cpp:128] Creating Layer Eltwise5
I0825 11:11:34.502308  1899 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:11:34.502326  1899 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:11:34.502359  1899 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:11:34.502396  1899 net.cpp:172] Setting up Eltwise5
I0825 11:11:34.502408  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.502429  1899 net.cpp:194] Memory required for data: 244056320
I0825 11:11:34.502445  1899 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:11:34.502465  1899 net.cpp:128] Creating Layer ReLU11
I0825 11:11:34.502472  1899 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:11:34.502478  1899 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:11:34.505784  1899 net.cpp:172] Setting up ReLU11
I0825 11:11:34.505801  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.505806  1899 net.cpp:194] Memory required for data: 248250624
I0825 11:11:34.505810  1899 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:11:34.505820  1899 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:11:34.505825  1899 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:11:34.505831  1899 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:11:34.505841  1899 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:11:34.505918  1899 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:11:34.505929  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.505935  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.505967  1899 net.cpp:194] Memory required for data: 256639232
I0825 11:11:34.505975  1899 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:11:34.505993  1899 net.cpp:128] Creating Layer Convolution12
I0825 11:11:34.506014  1899 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:11:34.506026  1899 net.cpp:522] Convolution12 -> Convolution12
I0825 11:11:34.515323  1899 net.cpp:172] Setting up Convolution12
I0825 11:11:34.515349  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.515353  1899 net.cpp:194] Memory required for data: 260833536
I0825 11:11:34.515363  1899 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:11:34.515377  1899 net.cpp:128] Creating Layer BatchNorm12
I0825 11:11:34.515381  1899 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:11:34.515388  1899 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:11:34.515641  1899 net.cpp:172] Setting up BatchNorm12
I0825 11:11:34.515653  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.515658  1899 net.cpp:194] Memory required for data: 265027840
I0825 11:11:34.515668  1899 layer_factory.hpp:77] Creating layer Scale12
I0825 11:11:34.515677  1899 net.cpp:128] Creating Layer Scale12
I0825 11:11:34.515682  1899 net.cpp:558] Scale12 <- Convolution12
I0825 11:11:34.515688  1899 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:11:34.515734  1899 layer_factory.hpp:77] Creating layer Scale12
I0825 11:11:34.515916  1899 net.cpp:172] Setting up Scale12
I0825 11:11:34.515929  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.515935  1899 net.cpp:194] Memory required for data: 269222144
I0825 11:11:34.515944  1899 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:11:34.515950  1899 net.cpp:128] Creating Layer ReLU12
I0825 11:11:34.515954  1899 net.cpp:558] ReLU12 <- Convolution12
I0825 11:11:34.515962  1899 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0825 11:11:34.517429  1899 net.cpp:172] Setting up ReLU12
I0825 11:11:34.517446  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.517451  1899 net.cpp:194] Memory required for data: 273416448
I0825 11:11:34.517455  1899 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:11:34.517468  1899 net.cpp:128] Creating Layer Convolution13
I0825 11:11:34.517473  1899 net.cpp:558] Convolution13 <- Convolution12
I0825 11:11:34.517483  1899 net.cpp:522] Convolution13 -> Convolution13
I0825 11:11:34.524245  1899 net.cpp:172] Setting up Convolution13
I0825 11:11:34.524292  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.524312  1899 net.cpp:194] Memory required for data: 277610752
I0825 11:11:34.524338  1899 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:11:34.524360  1899 net.cpp:128] Creating Layer BatchNorm13
I0825 11:11:34.524379  1899 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:11:34.524404  1899 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:11:34.524680  1899 net.cpp:172] Setting up BatchNorm13
I0825 11:11:34.524693  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.524696  1899 net.cpp:194] Memory required for data: 281805056
I0825 11:11:34.524708  1899 layer_factory.hpp:77] Creating layer Scale13
I0825 11:11:34.524714  1899 net.cpp:128] Creating Layer Scale13
I0825 11:11:34.524739  1899 net.cpp:558] Scale13 <- Convolution13
I0825 11:11:34.524762  1899 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:11:34.524817  1899 layer_factory.hpp:77] Creating layer Scale13
I0825 11:11:34.524961  1899 net.cpp:172] Setting up Scale13
I0825 11:11:34.524973  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.524997  1899 net.cpp:194] Memory required for data: 285999360
I0825 11:11:34.525010  1899 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:11:34.525017  1899 net.cpp:128] Creating Layer Eltwise6
I0825 11:11:34.525038  1899 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:11:34.525058  1899 net.cpp:558] Eltwise6 <- Convolution13
I0825 11:11:34.525068  1899 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:11:34.525135  1899 net.cpp:172] Setting up Eltwise6
I0825 11:11:34.525147  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.525152  1899 net.cpp:194] Memory required for data: 290193664
I0825 11:11:34.525156  1899 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:11:34.525187  1899 net.cpp:128] Creating Layer ReLU13
I0825 11:11:34.525195  1899 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:11:34.525202  1899 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:11:34.526327  1899 net.cpp:172] Setting up ReLU13
I0825 11:11:34.526360  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.526365  1899 net.cpp:194] Memory required for data: 294387968
I0825 11:11:34.526371  1899 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:11:34.526383  1899 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:11:34.526414  1899 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:11:34.526440  1899 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:11:34.526463  1899 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:11:34.526535  1899 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:11:34.526548  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.526554  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.526576  1899 net.cpp:194] Memory required for data: 302776576
I0825 11:11:34.526592  1899 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:11:34.526610  1899 net.cpp:128] Creating Layer Convolution14
I0825 11:11:34.526630  1899 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0825 11:11:34.526641  1899 net.cpp:522] Convolution14 -> Convolution14
I0825 11:11:34.535372  1899 net.cpp:172] Setting up Convolution14
I0825 11:11:34.535398  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.535403  1899 net.cpp:194] Memory required for data: 306970880
I0825 11:11:34.535413  1899 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:11:34.535423  1899 net.cpp:128] Creating Layer BatchNorm14
I0825 11:11:34.535429  1899 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:11:34.535435  1899 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:11:34.535727  1899 net.cpp:172] Setting up BatchNorm14
I0825 11:11:34.535738  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.535742  1899 net.cpp:194] Memory required for data: 311165184
I0825 11:11:34.535753  1899 layer_factory.hpp:77] Creating layer Scale14
I0825 11:11:34.535779  1899 net.cpp:128] Creating Layer Scale14
I0825 11:11:34.535797  1899 net.cpp:558] Scale14 <- Convolution14
I0825 11:11:34.535815  1899 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:11:34.535882  1899 layer_factory.hpp:77] Creating layer Scale14
I0825 11:11:34.536026  1899 net.cpp:172] Setting up Scale14
I0825 11:11:34.536036  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.536039  1899 net.cpp:194] Memory required for data: 315359488
I0825 11:11:34.536047  1899 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:11:34.536072  1899 net.cpp:128] Creating Layer ReLU14
I0825 11:11:34.536090  1899 net.cpp:558] ReLU14 <- Convolution14
I0825 11:11:34.536110  1899 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0825 11:11:34.539589  1899 net.cpp:172] Setting up ReLU14
I0825 11:11:34.539609  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.539613  1899 net.cpp:194] Memory required for data: 319553792
I0825 11:11:34.539618  1899 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:11:34.539633  1899 net.cpp:128] Creating Layer Convolution15
I0825 11:11:34.539638  1899 net.cpp:558] Convolution15 <- Convolution14
I0825 11:11:34.539652  1899 net.cpp:522] Convolution15 -> Convolution15
I0825 11:11:34.548035  1899 net.cpp:172] Setting up Convolution15
I0825 11:11:34.548061  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.548065  1899 net.cpp:194] Memory required for data: 323748096
I0825 11:11:34.548075  1899 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:11:34.548102  1899 net.cpp:128] Creating Layer BatchNorm15
I0825 11:11:34.548107  1899 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:11:34.548116  1899 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:11:34.548378  1899 net.cpp:172] Setting up BatchNorm15
I0825 11:11:34.548390  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.548395  1899 net.cpp:194] Memory required for data: 327942400
I0825 11:11:34.548405  1899 layer_factory.hpp:77] Creating layer Scale15
I0825 11:11:34.548413  1899 net.cpp:128] Creating Layer Scale15
I0825 11:11:34.548418  1899 net.cpp:558] Scale15 <- Convolution15
I0825 11:11:34.548424  1899 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:11:34.548470  1899 layer_factory.hpp:77] Creating layer Scale15
I0825 11:11:34.548653  1899 net.cpp:172] Setting up Scale15
I0825 11:11:34.548666  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.548671  1899 net.cpp:194] Memory required for data: 332136704
I0825 11:11:34.548679  1899 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:11:34.548689  1899 net.cpp:128] Creating Layer Eltwise7
I0825 11:11:34.548694  1899 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:11:34.548699  1899 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:11:34.548707  1899 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:11:34.548734  1899 net.cpp:172] Setting up Eltwise7
I0825 11:11:34.548765  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.548779  1899 net.cpp:194] Memory required for data: 336331008
I0825 11:11:34.548794  1899 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:11:34.548813  1899 net.cpp:128] Creating Layer ReLU15
I0825 11:11:34.548828  1899 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:11:34.548843  1899 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:11:34.550138  1899 net.cpp:172] Setting up ReLU15
I0825 11:11:34.550153  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.550158  1899 net.cpp:194] Memory required for data: 340525312
I0825 11:11:34.550161  1899 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:11:34.550171  1899 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:11:34.550175  1899 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:11:34.550182  1899 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:11:34.550191  1899 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:11:34.550241  1899 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:11:34.550248  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.550254  1899 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:11:34.550257  1899 net.cpp:194] Memory required for data: 348913920
I0825 11:11:34.550262  1899 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:11:34.550274  1899 net.cpp:128] Creating Layer Convolution16
I0825 11:11:34.550279  1899 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0825 11:11:34.550288  1899 net.cpp:522] Convolution16 -> Convolution16
I0825 11:11:34.552079  1899 net.cpp:172] Setting up Convolution16
I0825 11:11:34.552132  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.552151  1899 net.cpp:194] Memory required for data: 351011072
I0825 11:11:34.552172  1899 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:11:34.552198  1899 net.cpp:128] Creating Layer BatchNorm16
I0825 11:11:34.552219  1899 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:11:34.552238  1899 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:11:34.552520  1899 net.cpp:172] Setting up BatchNorm16
I0825 11:11:34.552532  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.552538  1899 net.cpp:194] Memory required for data: 353108224
I0825 11:11:34.552548  1899 layer_factory.hpp:77] Creating layer Scale16
I0825 11:11:34.552556  1899 net.cpp:128] Creating Layer Scale16
I0825 11:11:34.552559  1899 net.cpp:558] Scale16 <- Convolution16
I0825 11:11:34.552565  1899 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:11:34.552611  1899 layer_factory.hpp:77] Creating layer Scale16
I0825 11:11:34.552793  1899 net.cpp:172] Setting up Scale16
I0825 11:11:34.552804  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.552809  1899 net.cpp:194] Memory required for data: 355205376
I0825 11:11:34.552817  1899 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:11:34.552829  1899 net.cpp:128] Creating Layer Convolution17
I0825 11:11:34.552834  1899 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_1
I0825 11:11:34.552844  1899 net.cpp:522] Convolution17 -> Convolution17
I0825 11:11:34.557602  1899 net.cpp:172] Setting up Convolution17
I0825 11:11:34.557633  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.557637  1899 net.cpp:194] Memory required for data: 357302528
I0825 11:11:34.557649  1899 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:11:34.557660  1899 net.cpp:128] Creating Layer BatchNorm17
I0825 11:11:34.557695  1899 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:11:34.557718  1899 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:11:34.557992  1899 net.cpp:172] Setting up BatchNorm17
I0825 11:11:34.558003  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.558007  1899 net.cpp:194] Memory required for data: 359399680
I0825 11:11:34.558018  1899 layer_factory.hpp:77] Creating layer Scale17
I0825 11:11:34.558046  1899 net.cpp:128] Creating Layer Scale17
I0825 11:11:34.558064  1899 net.cpp:558] Scale17 <- Convolution17
I0825 11:11:34.558082  1899 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:11:34.558147  1899 layer_factory.hpp:77] Creating layer Scale17
I0825 11:11:34.558297  1899 net.cpp:172] Setting up Scale17
I0825 11:11:34.558307  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.558312  1899 net.cpp:194] Memory required for data: 361496832
I0825 11:11:34.558320  1899 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:11:34.558362  1899 net.cpp:128] Creating Layer ReLU16
I0825 11:11:34.558382  1899 net.cpp:558] ReLU16 <- Convolution17
I0825 11:11:34.558401  1899 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0825 11:11:34.558907  1899 net.cpp:172] Setting up ReLU16
I0825 11:11:34.558928  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.558933  1899 net.cpp:194] Memory required for data: 363593984
I0825 11:11:34.558938  1899 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:11:34.558953  1899 net.cpp:128] Creating Layer Convolution18
I0825 11:11:34.558985  1899 net.cpp:558] Convolution18 <- Convolution17
I0825 11:11:34.559007  1899 net.cpp:522] Convolution18 -> Convolution18
I0825 11:11:34.562592  1899 net.cpp:172] Setting up Convolution18
I0825 11:11:34.562615  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.562619  1899 net.cpp:194] Memory required for data: 365691136
I0825 11:11:34.562630  1899 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:11:34.562642  1899 net.cpp:128] Creating Layer BatchNorm18
I0825 11:11:34.562675  1899 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:11:34.562695  1899 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:11:34.562968  1899 net.cpp:172] Setting up BatchNorm18
I0825 11:11:34.562980  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.562984  1899 net.cpp:194] Memory required for data: 367788288
I0825 11:11:34.562994  1899 layer_factory.hpp:77] Creating layer Scale18
I0825 11:11:34.563031  1899 net.cpp:128] Creating Layer Scale18
I0825 11:11:34.563040  1899 net.cpp:558] Scale18 <- Convolution18
I0825 11:11:34.563045  1899 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:11:34.563098  1899 layer_factory.hpp:77] Creating layer Scale18
I0825 11:11:34.563246  1899 net.cpp:172] Setting up Scale18
I0825 11:11:34.563254  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.563277  1899 net.cpp:194] Memory required for data: 369885440
I0825 11:11:34.563289  1899 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:11:34.563313  1899 net.cpp:128] Creating Layer Eltwise8
I0825 11:11:34.563321  1899 net.cpp:558] Eltwise8 <- Convolution16
I0825 11:11:34.563349  1899 net.cpp:558] Eltwise8 <- Convolution18
I0825 11:11:34.563362  1899 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:11:34.563406  1899 net.cpp:172] Setting up Eltwise8
I0825 11:11:34.563416  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.563421  1899 net.cpp:194] Memory required for data: 371982592
I0825 11:11:34.563424  1899 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:11:34.563446  1899 net.cpp:128] Creating Layer ReLU17
I0825 11:11:34.563462  1899 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:11:34.563474  1899 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:11:34.563727  1899 net.cpp:172] Setting up ReLU17
I0825 11:11:34.563742  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.563746  1899 net.cpp:194] Memory required for data: 374079744
I0825 11:11:34.563771  1899 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:11:34.563784  1899 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:11:34.563803  1899 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:11:34.563814  1899 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:11:34.563825  1899 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:11:34.563895  1899 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:11:34.563905  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.563911  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.563932  1899 net.cpp:194] Memory required for data: 378274048
I0825 11:11:34.563949  1899 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:11:34.563967  1899 net.cpp:128] Creating Layer Convolution19
I0825 11:11:34.563989  1899 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0825 11:11:34.564003  1899 net.cpp:522] Convolution19 -> Convolution19
I0825 11:11:34.568663  1899 net.cpp:172] Setting up Convolution19
I0825 11:11:34.568708  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.568725  1899 net.cpp:194] Memory required for data: 380371200
I0825 11:11:34.568748  1899 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:11:34.568770  1899 net.cpp:128] Creating Layer BatchNorm19
I0825 11:11:34.568789  1899 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:11:34.568809  1899 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:11:34.569087  1899 net.cpp:172] Setting up BatchNorm19
I0825 11:11:34.569109  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.569125  1899 net.cpp:194] Memory required for data: 382468352
I0825 11:11:34.569166  1899 layer_factory.hpp:77] Creating layer Scale19
I0825 11:11:34.569190  1899 net.cpp:128] Creating Layer Scale19
I0825 11:11:34.569207  1899 net.cpp:558] Scale19 <- Convolution19
I0825 11:11:34.569226  1899 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:11:34.569295  1899 layer_factory.hpp:77] Creating layer Scale19
I0825 11:11:34.569455  1899 net.cpp:172] Setting up Scale19
I0825 11:11:34.569478  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.569494  1899 net.cpp:194] Memory required for data: 384565504
I0825 11:11:34.569516  1899 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:11:34.569535  1899 net.cpp:128] Creating Layer ReLU18
I0825 11:11:34.569551  1899 net.cpp:558] ReLU18 <- Convolution19
I0825 11:11:34.569572  1899 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0825 11:11:34.571012  1899 net.cpp:172] Setting up ReLU18
I0825 11:11:34.571033  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.571038  1899 net.cpp:194] Memory required for data: 386662656
I0825 11:11:34.571043  1899 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:11:34.571058  1899 net.cpp:128] Creating Layer Convolution20
I0825 11:11:34.571063  1899 net.cpp:558] Convolution20 <- Convolution19
I0825 11:11:34.571105  1899 net.cpp:522] Convolution20 -> Convolution20
I0825 11:11:34.577898  1899 net.cpp:172] Setting up Convolution20
I0825 11:11:34.577924  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.577947  1899 net.cpp:194] Memory required for data: 388759808
I0825 11:11:34.577957  1899 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:11:34.577993  1899 net.cpp:128] Creating Layer BatchNorm20
I0825 11:11:34.578011  1899 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:11:34.578035  1899 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:11:34.578320  1899 net.cpp:172] Setting up BatchNorm20
I0825 11:11:34.578332  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.578346  1899 net.cpp:194] Memory required for data: 390856960
I0825 11:11:34.578356  1899 layer_factory.hpp:77] Creating layer Scale20
I0825 11:11:34.578387  1899 net.cpp:128] Creating Layer Scale20
I0825 11:11:34.578405  1899 net.cpp:558] Scale20 <- Convolution20
I0825 11:11:34.578428  1899 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:11:34.578496  1899 layer_factory.hpp:77] Creating layer Scale20
I0825 11:11:34.578645  1899 net.cpp:172] Setting up Scale20
I0825 11:11:34.578655  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.578660  1899 net.cpp:194] Memory required for data: 392954112
I0825 11:11:34.578667  1899 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:11:34.578696  1899 net.cpp:128] Creating Layer Eltwise9
I0825 11:11:34.578714  1899 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:11:34.578732  1899 net.cpp:558] Eltwise9 <- Convolution20
I0825 11:11:34.578755  1899 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:11:34.578788  1899 net.cpp:172] Setting up Eltwise9
I0825 11:11:34.578797  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.578801  1899 net.cpp:194] Memory required for data: 395051264
I0825 11:11:34.578805  1899 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:11:34.578812  1899 net.cpp:128] Creating Layer ReLU19
I0825 11:11:34.578816  1899 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:11:34.578825  1899 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:11:34.579980  1899 net.cpp:172] Setting up ReLU19
I0825 11:11:34.579996  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.580000  1899 net.cpp:194] Memory required for data: 397148416
I0825 11:11:34.580006  1899 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:11:34.580018  1899 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:11:34.580024  1899 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:11:34.580030  1899 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:11:34.580039  1899 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:11:34.580127  1899 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:11:34.580138  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.580144  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.580148  1899 net.cpp:194] Memory required for data: 401342720
I0825 11:11:34.580152  1899 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:11:34.580165  1899 net.cpp:128] Creating Layer Convolution21
I0825 11:11:34.580189  1899 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0825 11:11:34.580211  1899 net.cpp:522] Convolution21 -> Convolution21
I0825 11:11:34.591248  1899 net.cpp:172] Setting up Convolution21
I0825 11:11:34.591275  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.591279  1899 net.cpp:194] Memory required for data: 403439872
I0825 11:11:34.591290  1899 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:11:34.591301  1899 net.cpp:128] Creating Layer BatchNorm21
I0825 11:11:34.591306  1899 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:11:34.591313  1899 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:11:34.591584  1899 net.cpp:172] Setting up BatchNorm21
I0825 11:11:34.591596  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.591601  1899 net.cpp:194] Memory required for data: 405537024
I0825 11:11:34.591611  1899 layer_factory.hpp:77] Creating layer Scale21
I0825 11:11:34.591619  1899 net.cpp:128] Creating Layer Scale21
I0825 11:11:34.591622  1899 net.cpp:558] Scale21 <- Convolution21
I0825 11:11:34.591645  1899 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:11:34.591696  1899 layer_factory.hpp:77] Creating layer Scale21
I0825 11:11:34.591882  1899 net.cpp:172] Setting up Scale21
I0825 11:11:34.591894  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.591898  1899 net.cpp:194] Memory required for data: 407634176
I0825 11:11:34.591907  1899 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:11:34.591914  1899 net.cpp:128] Creating Layer ReLU20
I0825 11:11:34.591918  1899 net.cpp:558] ReLU20 <- Convolution21
I0825 11:11:34.591926  1899 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:11:34.594772  1899 net.cpp:172] Setting up ReLU20
I0825 11:11:34.594794  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.594797  1899 net.cpp:194] Memory required for data: 409731328
I0825 11:11:34.594802  1899 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:11:34.594820  1899 net.cpp:128] Creating Layer Convolution22
I0825 11:11:34.594825  1899 net.cpp:558] Convolution22 <- Convolution21
I0825 11:11:34.594835  1899 net.cpp:522] Convolution22 -> Convolution22
I0825 11:11:34.601588  1899 net.cpp:172] Setting up Convolution22
I0825 11:11:34.601613  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.601619  1899 net.cpp:194] Memory required for data: 411828480
I0825 11:11:34.601627  1899 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:11:34.601636  1899 net.cpp:128] Creating Layer BatchNorm22
I0825 11:11:34.601641  1899 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:11:34.601650  1899 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:11:34.601909  1899 net.cpp:172] Setting up BatchNorm22
I0825 11:11:34.601920  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.601925  1899 net.cpp:194] Memory required for data: 413925632
I0825 11:11:34.601934  1899 layer_factory.hpp:77] Creating layer Scale22
I0825 11:11:34.601943  1899 net.cpp:128] Creating Layer Scale22
I0825 11:11:34.601946  1899 net.cpp:558] Scale22 <- Convolution22
I0825 11:11:34.601954  1899 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:11:34.602000  1899 layer_factory.hpp:77] Creating layer Scale22
I0825 11:11:34.602188  1899 net.cpp:172] Setting up Scale22
I0825 11:11:34.602200  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.602206  1899 net.cpp:194] Memory required for data: 416022784
I0825 11:11:34.602214  1899 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:11:34.602221  1899 net.cpp:128] Creating Layer Eltwise10
I0825 11:11:34.602227  1899 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0825 11:11:34.602232  1899 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:11:34.602242  1899 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:11:34.602264  1899 net.cpp:172] Setting up Eltwise10
I0825 11:11:34.602294  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.602308  1899 net.cpp:194] Memory required for data: 418119936
I0825 11:11:34.602322  1899 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:11:34.602346  1899 net.cpp:128] Creating Layer ReLU21
I0825 11:11:34.602362  1899 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:11:34.602381  1899 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:11:34.602638  1899 net.cpp:172] Setting up ReLU21
I0825 11:11:34.602654  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.602660  1899 net.cpp:194] Memory required for data: 420217088
I0825 11:11:34.602665  1899 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:11:34.602672  1899 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:11:34.602679  1899 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:11:34.602686  1899 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:11:34.602695  1899 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:11:34.602748  1899 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:11:34.602784  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.602813  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.602828  1899 net.cpp:194] Memory required for data: 424411392
I0825 11:11:34.602841  1899 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:11:34.602865  1899 net.cpp:128] Creating Layer Convolution23
I0825 11:11:34.602880  1899 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:11:34.602901  1899 net.cpp:522] Convolution23 -> Convolution23
I0825 11:11:34.614022  1899 net.cpp:172] Setting up Convolution23
I0825 11:11:34.614048  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.614053  1899 net.cpp:194] Memory required for data: 426508544
I0825 11:11:34.614064  1899 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:11:34.614078  1899 net.cpp:128] Creating Layer BatchNorm23
I0825 11:11:34.614084  1899 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:11:34.614091  1899 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:11:34.614365  1899 net.cpp:172] Setting up BatchNorm23
I0825 11:11:34.614378  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.614382  1899 net.cpp:194] Memory required for data: 428605696
I0825 11:11:34.614392  1899 layer_factory.hpp:77] Creating layer Scale23
I0825 11:11:34.614400  1899 net.cpp:128] Creating Layer Scale23
I0825 11:11:34.614404  1899 net.cpp:558] Scale23 <- Convolution23
I0825 11:11:34.614413  1899 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:11:34.614460  1899 layer_factory.hpp:77] Creating layer Scale23
I0825 11:11:34.614650  1899 net.cpp:172] Setting up Scale23
I0825 11:11:34.614662  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.614667  1899 net.cpp:194] Memory required for data: 430702848
I0825 11:11:34.614676  1899 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:11:34.614683  1899 net.cpp:128] Creating Layer ReLU22
I0825 11:11:34.614688  1899 net.cpp:558] ReLU22 <- Convolution23
I0825 11:11:34.614697  1899 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0825 11:11:34.616104  1899 net.cpp:172] Setting up ReLU22
I0825 11:11:34.616122  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.616125  1899 net.cpp:194] Memory required for data: 432800000
I0825 11:11:34.616130  1899 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:11:34.616143  1899 net.cpp:128] Creating Layer Convolution24
I0825 11:11:34.616148  1899 net.cpp:558] Convolution24 <- Convolution23
I0825 11:11:34.616159  1899 net.cpp:522] Convolution24 -> Convolution24
I0825 11:11:34.622910  1899 net.cpp:172] Setting up Convolution24
I0825 11:11:34.622939  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.622943  1899 net.cpp:194] Memory required for data: 434897152
I0825 11:11:34.622953  1899 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:11:34.622962  1899 net.cpp:128] Creating Layer BatchNorm24
I0825 11:11:34.622967  1899 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:11:34.622977  1899 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:11:34.623246  1899 net.cpp:172] Setting up BatchNorm24
I0825 11:11:34.623257  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.623263  1899 net.cpp:194] Memory required for data: 436994304
I0825 11:11:34.623273  1899 layer_factory.hpp:77] Creating layer Scale24
I0825 11:11:34.623281  1899 net.cpp:128] Creating Layer Scale24
I0825 11:11:34.623286  1899 net.cpp:558] Scale24 <- Convolution24
I0825 11:11:34.623291  1899 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:11:34.623340  1899 layer_factory.hpp:77] Creating layer Scale24
I0825 11:11:34.623533  1899 net.cpp:172] Setting up Scale24
I0825 11:11:34.623548  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.623553  1899 net.cpp:194] Memory required for data: 439091456
I0825 11:11:34.623561  1899 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:11:34.623569  1899 net.cpp:128] Creating Layer Eltwise11
I0825 11:11:34.623574  1899 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0825 11:11:34.623579  1899 net.cpp:558] Eltwise11 <- Convolution24
I0825 11:11:34.623600  1899 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:11:34.623626  1899 net.cpp:172] Setting up Eltwise11
I0825 11:11:34.623656  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.623672  1899 net.cpp:194] Memory required for data: 441188608
I0825 11:11:34.623685  1899 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:11:34.623702  1899 net.cpp:128] Creating Layer ReLU23
I0825 11:11:34.623716  1899 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:11:34.623734  1899 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:11:34.625033  1899 net.cpp:172] Setting up ReLU23
I0825 11:11:34.625059  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.625063  1899 net.cpp:194] Memory required for data: 443285760
I0825 11:11:34.625068  1899 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:11:34.625077  1899 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:11:34.625082  1899 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:11:34.625090  1899 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:11:34.625100  1899 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:11:34.625157  1899 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:11:34.625198  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.625216  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.625229  1899 net.cpp:194] Memory required for data: 447480064
I0825 11:11:34.625243  1899 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:11:34.625267  1899 net.cpp:128] Creating Layer Convolution25
I0825 11:11:34.625282  1899 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0825 11:11:34.625300  1899 net.cpp:522] Convolution25 -> Convolution25
I0825 11:11:34.637719  1899 net.cpp:172] Setting up Convolution25
I0825 11:11:34.637745  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.637749  1899 net.cpp:194] Memory required for data: 449577216
I0825 11:11:34.637759  1899 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:11:34.637771  1899 net.cpp:128] Creating Layer BatchNorm25
I0825 11:11:34.637776  1899 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:11:34.637784  1899 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:11:34.638048  1899 net.cpp:172] Setting up BatchNorm25
I0825 11:11:34.638061  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.638064  1899 net.cpp:194] Memory required for data: 451674368
I0825 11:11:34.638074  1899 layer_factory.hpp:77] Creating layer Scale25
I0825 11:11:34.638087  1899 net.cpp:128] Creating Layer Scale25
I0825 11:11:34.638092  1899 net.cpp:558] Scale25 <- Convolution25
I0825 11:11:34.638098  1899 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:11:34.638146  1899 layer_factory.hpp:77] Creating layer Scale25
I0825 11:11:34.638350  1899 net.cpp:172] Setting up Scale25
I0825 11:11:34.638363  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.638367  1899 net.cpp:194] Memory required for data: 453771520
I0825 11:11:34.638376  1899 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:11:34.638386  1899 net.cpp:128] Creating Layer ReLU24
I0825 11:11:34.638391  1899 net.cpp:558] ReLU24 <- Convolution25
I0825 11:11:34.638398  1899 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0825 11:11:34.639786  1899 net.cpp:172] Setting up ReLU24
I0825 11:11:34.639802  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.639807  1899 net.cpp:194] Memory required for data: 455868672
I0825 11:11:34.639811  1899 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:11:34.639828  1899 net.cpp:128] Creating Layer Convolution26
I0825 11:11:34.639834  1899 net.cpp:558] Convolution26 <- Convolution25
I0825 11:11:34.639843  1899 net.cpp:522] Convolution26 -> Convolution26
I0825 11:11:34.646651  1899 net.cpp:172] Setting up Convolution26
I0825 11:11:34.646673  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.646678  1899 net.cpp:194] Memory required for data: 457965824
I0825 11:11:34.646704  1899 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:11:34.646716  1899 net.cpp:128] Creating Layer BatchNorm26
I0825 11:11:34.646721  1899 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:11:34.646733  1899 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:11:34.647002  1899 net.cpp:172] Setting up BatchNorm26
I0825 11:11:34.647011  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.647014  1899 net.cpp:194] Memory required for data: 460062976
I0825 11:11:34.647024  1899 layer_factory.hpp:77] Creating layer Scale26
I0825 11:11:34.647033  1899 net.cpp:128] Creating Layer Scale26
I0825 11:11:34.647038  1899 net.cpp:558] Scale26 <- Convolution26
I0825 11:11:34.647043  1899 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:11:34.647091  1899 layer_factory.hpp:77] Creating layer Scale26
I0825 11:11:34.647245  1899 net.cpp:172] Setting up Scale26
I0825 11:11:34.647253  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.647256  1899 net.cpp:194] Memory required for data: 462160128
I0825 11:11:34.647264  1899 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:11:34.647274  1899 net.cpp:128] Creating Layer Eltwise12
I0825 11:11:34.647279  1899 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:11:34.647284  1899 net.cpp:558] Eltwise12 <- Convolution26
I0825 11:11:34.647289  1899 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:11:34.647315  1899 net.cpp:172] Setting up Eltwise12
I0825 11:11:34.647321  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.647325  1899 net.cpp:194] Memory required for data: 464257280
I0825 11:11:34.647330  1899 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:11:34.647336  1899 net.cpp:128] Creating Layer ReLU25
I0825 11:11:34.647339  1899 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:11:34.647346  1899 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:11:34.648758  1899 net.cpp:172] Setting up ReLU25
I0825 11:11:34.648777  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.648782  1899 net.cpp:194] Memory required for data: 466354432
I0825 11:11:34.648787  1899 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:11:34.648807  1899 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:11:34.648811  1899 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:11:34.648818  1899 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:11:34.648834  1899 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:11:34.648885  1899 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:11:34.648893  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.648898  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.648902  1899 net.cpp:194] Memory required for data: 470548736
I0825 11:11:34.648907  1899 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:11:34.648918  1899 net.cpp:128] Creating Layer Convolution27
I0825 11:11:34.648922  1899 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0825 11:11:34.648932  1899 net.cpp:522] Convolution27 -> Convolution27
I0825 11:11:34.661490  1899 net.cpp:172] Setting up Convolution27
I0825 11:11:34.661518  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.661522  1899 net.cpp:194] Memory required for data: 472645888
I0825 11:11:34.661533  1899 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:11:34.661545  1899 net.cpp:128] Creating Layer BatchNorm27
I0825 11:11:34.661550  1899 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:11:34.661559  1899 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:11:34.661833  1899 net.cpp:172] Setting up BatchNorm27
I0825 11:11:34.661842  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.661846  1899 net.cpp:194] Memory required for data: 474743040
I0825 11:11:34.661886  1899 layer_factory.hpp:77] Creating layer Scale27
I0825 11:11:34.661898  1899 net.cpp:128] Creating Layer Scale27
I0825 11:11:34.661903  1899 net.cpp:558] Scale27 <- Convolution27
I0825 11:11:34.661922  1899 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:11:34.662003  1899 layer_factory.hpp:77] Creating layer Scale27
I0825 11:11:34.662163  1899 net.cpp:172] Setting up Scale27
I0825 11:11:34.662173  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.662178  1899 net.cpp:194] Memory required for data: 476840192
I0825 11:11:34.662209  1899 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:11:34.662222  1899 net.cpp:128] Creating Layer ReLU26
I0825 11:11:34.662228  1899 net.cpp:558] ReLU26 <- Convolution27
I0825 11:11:34.662253  1899 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0825 11:11:34.663601  1899 net.cpp:172] Setting up ReLU26
I0825 11:11:34.663627  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.663632  1899 net.cpp:194] Memory required for data: 478937344
I0825 11:11:34.663637  1899 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:11:34.663651  1899 net.cpp:128] Creating Layer Convolution28
I0825 11:11:34.663689  1899 net.cpp:558] Convolution28 <- Convolution27
I0825 11:11:34.663705  1899 net.cpp:522] Convolution28 -> Convolution28
I0825 11:11:34.670429  1899 net.cpp:172] Setting up Convolution28
I0825 11:11:34.670451  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.670456  1899 net.cpp:194] Memory required for data: 481034496
I0825 11:11:34.670466  1899 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:11:34.670477  1899 net.cpp:128] Creating Layer BatchNorm28
I0825 11:11:34.670485  1899 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:11:34.670496  1899 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:11:34.670771  1899 net.cpp:172] Setting up BatchNorm28
I0825 11:11:34.670783  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.670789  1899 net.cpp:194] Memory required for data: 483131648
I0825 11:11:34.670799  1899 layer_factory.hpp:77] Creating layer Scale28
I0825 11:11:34.670809  1899 net.cpp:128] Creating Layer Scale28
I0825 11:11:34.670815  1899 net.cpp:558] Scale28 <- Convolution28
I0825 11:11:34.670822  1899 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:11:34.670872  1899 layer_factory.hpp:77] Creating layer Scale28
I0825 11:11:34.671027  1899 net.cpp:172] Setting up Scale28
I0825 11:11:34.671038  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.671044  1899 net.cpp:194] Memory required for data: 485228800
I0825 11:11:34.671052  1899 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:11:34.671062  1899 net.cpp:128] Creating Layer Eltwise13
I0825 11:11:34.671069  1899 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:11:34.671074  1899 net.cpp:558] Eltwise13 <- Convolution28
I0825 11:11:34.671084  1899 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:11:34.671109  1899 net.cpp:172] Setting up Eltwise13
I0825 11:11:34.671119  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.671125  1899 net.cpp:194] Memory required for data: 487325952
I0825 11:11:34.671129  1899 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:11:34.671139  1899 net.cpp:128] Creating Layer ReLU27
I0825 11:11:34.671147  1899 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:11:34.671154  1899 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:11:34.672514  1899 net.cpp:172] Setting up ReLU27
I0825 11:11:34.672533  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.672538  1899 net.cpp:194] Memory required for data: 489423104
I0825 11:11:34.672543  1899 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:11:34.672554  1899 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:11:34.672559  1899 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:11:34.672565  1899 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:11:34.672576  1899 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:11:34.672669  1899 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:11:34.672682  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.672688  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.672704  1899 net.cpp:194] Memory required for data: 493617408
I0825 11:11:34.672709  1899 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:11:34.672724  1899 net.cpp:128] Creating Layer Convolution29
I0825 11:11:34.672731  1899 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0825 11:11:34.672745  1899 net.cpp:522] Convolution29 -> Convolution29
I0825 11:11:34.681509  1899 net.cpp:172] Setting up Convolution29
I0825 11:11:34.681536  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.681540  1899 net.cpp:194] Memory required for data: 495714560
I0825 11:11:34.681551  1899 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:11:34.681561  1899 net.cpp:128] Creating Layer BatchNorm29
I0825 11:11:34.681566  1899 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:11:34.681573  1899 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:11:34.681890  1899 net.cpp:172] Setting up BatchNorm29
I0825 11:11:34.681901  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.681906  1899 net.cpp:194] Memory required for data: 497811712
I0825 11:11:34.681916  1899 layer_factory.hpp:77] Creating layer Scale29
I0825 11:11:34.681943  1899 net.cpp:128] Creating Layer Scale29
I0825 11:11:34.681958  1899 net.cpp:558] Scale29 <- Convolution29
I0825 11:11:34.681980  1899 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:11:34.682047  1899 layer_factory.hpp:77] Creating layer Scale29
I0825 11:11:34.682205  1899 net.cpp:172] Setting up Scale29
I0825 11:11:34.682216  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.682220  1899 net.cpp:194] Memory required for data: 499908864
I0825 11:11:34.682229  1899 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:11:34.682255  1899 net.cpp:128] Creating Layer ReLU28
I0825 11:11:34.682271  1899 net.cpp:558] ReLU28 <- Convolution29
I0825 11:11:34.682291  1899 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0825 11:11:34.685716  1899 net.cpp:172] Setting up ReLU28
I0825 11:11:34.685737  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.685742  1899 net.cpp:194] Memory required for data: 502006016
I0825 11:11:34.685747  1899 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:11:34.685761  1899 net.cpp:128] Creating Layer Convolution30
I0825 11:11:34.685766  1899 net.cpp:558] Convolution30 <- Convolution29
I0825 11:11:34.685777  1899 net.cpp:522] Convolution30 -> Convolution30
I0825 11:11:34.694041  1899 net.cpp:172] Setting up Convolution30
I0825 11:11:34.694068  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.694073  1899 net.cpp:194] Memory required for data: 504103168
I0825 11:11:34.694083  1899 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:11:34.694092  1899 net.cpp:128] Creating Layer BatchNorm30
I0825 11:11:34.694097  1899 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:11:34.694106  1899 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:11:34.694391  1899 net.cpp:172] Setting up BatchNorm30
I0825 11:11:34.694403  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.694408  1899 net.cpp:194] Memory required for data: 506200320
I0825 11:11:34.694418  1899 layer_factory.hpp:77] Creating layer Scale30
I0825 11:11:34.694425  1899 net.cpp:128] Creating Layer Scale30
I0825 11:11:34.694429  1899 net.cpp:558] Scale30 <- Convolution30
I0825 11:11:34.694435  1899 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:11:34.694485  1899 layer_factory.hpp:77] Creating layer Scale30
I0825 11:11:34.694681  1899 net.cpp:172] Setting up Scale30
I0825 11:11:34.694712  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.694730  1899 net.cpp:194] Memory required for data: 508297472
I0825 11:11:34.694751  1899 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:11:34.694770  1899 net.cpp:128] Creating Layer Eltwise14
I0825 11:11:34.694785  1899 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:11:34.694802  1899 net.cpp:558] Eltwise14 <- Convolution30
I0825 11:11:34.694821  1899 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:11:34.694875  1899 net.cpp:172] Setting up Eltwise14
I0825 11:11:34.694895  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.694910  1899 net.cpp:194] Memory required for data: 510394624
I0825 11:11:34.694923  1899 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:11:34.694943  1899 net.cpp:128] Creating Layer ReLU29
I0825 11:11:34.694957  1899 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:11:34.694978  1899 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:11:34.696146  1899 net.cpp:172] Setting up ReLU29
I0825 11:11:34.696161  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.696166  1899 net.cpp:194] Memory required for data: 512491776
I0825 11:11:34.696171  1899 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:11:34.696177  1899 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:11:34.696184  1899 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:11:34.696193  1899 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:11:34.696202  1899 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:11:34.696259  1899 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:11:34.696297  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.696314  1899 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:11:34.696328  1899 net.cpp:194] Memory required for data: 516686080
I0825 11:11:34.696342  1899 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:11:34.696365  1899 net.cpp:128] Creating Layer Convolution31
I0825 11:11:34.696379  1899 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0825 11:11:34.696399  1899 net.cpp:522] Convolution31 -> Convolution31
I0825 11:11:34.699911  1899 net.cpp:172] Setting up Convolution31
I0825 11:11:34.699960  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.699978  1899 net.cpp:194] Memory required for data: 517734656
I0825 11:11:34.700000  1899 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:11:34.700022  1899 net.cpp:128] Creating Layer BatchNorm31
I0825 11:11:34.700042  1899 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:11:34.700060  1899 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:11:34.700354  1899 net.cpp:172] Setting up BatchNorm31
I0825 11:11:34.700377  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.700395  1899 net.cpp:194] Memory required for data: 518783232
I0825 11:11:34.700417  1899 layer_factory.hpp:77] Creating layer Scale31
I0825 11:11:34.700444  1899 net.cpp:128] Creating Layer Scale31
I0825 11:11:34.700462  1899 net.cpp:558] Scale31 <- Convolution31
I0825 11:11:34.700479  1899 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:11:34.700546  1899 layer_factory.hpp:77] Creating layer Scale31
I0825 11:11:34.700726  1899 net.cpp:172] Setting up Scale31
I0825 11:11:34.700749  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.700767  1899 net.cpp:194] Memory required for data: 519831808
I0825 11:11:34.700786  1899 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:11:34.700815  1899 net.cpp:128] Creating Layer Convolution32
I0825 11:11:34.700835  1899 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_1
I0825 11:11:34.700856  1899 net.cpp:522] Convolution32 -> Convolution32
I0825 11:11:34.709460  1899 net.cpp:172] Setting up Convolution32
I0825 11:11:34.709496  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.709501  1899 net.cpp:194] Memory required for data: 520880384
I0825 11:11:34.709517  1899 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:11:34.709532  1899 net.cpp:128] Creating Layer BatchNorm32
I0825 11:11:34.709539  1899 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:11:34.709589  1899 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:11:34.709904  1899 net.cpp:172] Setting up BatchNorm32
I0825 11:11:34.709916  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.709920  1899 net.cpp:194] Memory required for data: 521928960
I0825 11:11:34.709969  1899 layer_factory.hpp:77] Creating layer Scale32
I0825 11:11:34.709981  1899 net.cpp:128] Creating Layer Scale32
I0825 11:11:34.709986  1899 net.cpp:558] Scale32 <- Convolution32
I0825 11:11:34.709995  1899 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:11:34.710068  1899 layer_factory.hpp:77] Creating layer Scale32
I0825 11:11:34.710240  1899 net.cpp:172] Setting up Scale32
I0825 11:11:34.710252  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.710255  1899 net.cpp:194] Memory required for data: 522977536
I0825 11:11:34.710264  1899 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:11:34.710294  1899 net.cpp:128] Creating Layer ReLU30
I0825 11:11:34.710311  1899 net.cpp:558] ReLU30 <- Convolution32
I0825 11:11:34.710333  1899 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0825 11:11:34.711314  1899 net.cpp:172] Setting up ReLU30
I0825 11:11:34.711340  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.711344  1899 net.cpp:194] Memory required for data: 524026112
I0825 11:11:34.711349  1899 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:11:34.711367  1899 net.cpp:128] Creating Layer Convolution33
I0825 11:11:34.711401  1899 net.cpp:558] Convolution33 <- Convolution32
I0825 11:11:34.711426  1899 net.cpp:522] Convolution33 -> Convolution33
I0825 11:11:34.718430  1899 net.cpp:172] Setting up Convolution33
I0825 11:11:34.718451  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.718456  1899 net.cpp:194] Memory required for data: 525074688
I0825 11:11:34.718467  1899 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:11:34.718478  1899 net.cpp:128] Creating Layer BatchNorm33
I0825 11:11:34.718484  1899 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:11:34.718490  1899 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:11:34.718782  1899 net.cpp:172] Setting up BatchNorm33
I0825 11:11:34.718794  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.718799  1899 net.cpp:194] Memory required for data: 526123264
I0825 11:11:34.718809  1899 layer_factory.hpp:77] Creating layer Scale33
I0825 11:11:34.718816  1899 net.cpp:128] Creating Layer Scale33
I0825 11:11:34.718821  1899 net.cpp:558] Scale33 <- Convolution33
I0825 11:11:34.718827  1899 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:11:34.718878  1899 layer_factory.hpp:77] Creating layer Scale33
I0825 11:11:34.719079  1899 net.cpp:172] Setting up Scale33
I0825 11:11:34.719092  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.719099  1899 net.cpp:194] Memory required for data: 527171840
I0825 11:11:34.719107  1899 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:11:34.719115  1899 net.cpp:128] Creating Layer Eltwise15
I0825 11:11:34.719120  1899 net.cpp:558] Eltwise15 <- Convolution31
I0825 11:11:34.719125  1899 net.cpp:558] Eltwise15 <- Convolution33
I0825 11:11:34.719141  1899 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:11:34.719171  1899 net.cpp:172] Setting up Eltwise15
I0825 11:11:34.719205  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.719220  1899 net.cpp:194] Memory required for data: 528220416
I0825 11:11:34.719235  1899 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:11:34.719251  1899 net.cpp:128] Creating Layer ReLU31
I0825 11:11:34.719265  1899 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:11:34.719281  1899 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:11:34.720227  1899 net.cpp:172] Setting up ReLU31
I0825 11:11:34.720271  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.720285  1899 net.cpp:194] Memory required for data: 529268992
I0825 11:11:34.720305  1899 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0825 11:11:34.720327  1899 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0825 11:11:34.720342  1899 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0825 11:11:34.720360  1899 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0825 11:11:34.720383  1899 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0825 11:11:34.720458  1899 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0825 11:11:34.720489  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.720504  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.720517  1899 net.cpp:194] Memory required for data: 531366144
I0825 11:11:34.720531  1899 layer_factory.hpp:77] Creating layer Convolution34
I0825 11:11:34.720557  1899 net.cpp:128] Creating Layer Convolution34
I0825 11:11:34.720577  1899 net.cpp:558] Convolution34 <- Eltwise15_ReLU31_0_split_0
I0825 11:11:34.720602  1899 net.cpp:522] Convolution34 -> Convolution34
I0825 11:11:34.727686  1899 net.cpp:172] Setting up Convolution34
I0825 11:11:34.727713  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.727718  1899 net.cpp:194] Memory required for data: 532414720
I0825 11:11:34.727728  1899 layer_factory.hpp:77] Creating layer BatchNorm34
I0825 11:11:34.727740  1899 net.cpp:128] Creating Layer BatchNorm34
I0825 11:11:34.727782  1899 net.cpp:558] BatchNorm34 <- Convolution34
I0825 11:11:34.727802  1899 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0825 11:11:34.728106  1899 net.cpp:172] Setting up BatchNorm34
I0825 11:11:34.728117  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.728122  1899 net.cpp:194] Memory required for data: 533463296
I0825 11:11:34.728132  1899 layer_factory.hpp:77] Creating layer Scale34
I0825 11:11:34.728161  1899 net.cpp:128] Creating Layer Scale34
I0825 11:11:34.728179  1899 net.cpp:558] Scale34 <- Convolution34
I0825 11:11:34.728197  1899 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0825 11:11:34.728266  1899 layer_factory.hpp:77] Creating layer Scale34
I0825 11:11:34.728430  1899 net.cpp:172] Setting up Scale34
I0825 11:11:34.728440  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.728444  1899 net.cpp:194] Memory required for data: 534511872
I0825 11:11:34.728453  1899 layer_factory.hpp:77] Creating layer ReLU32
I0825 11:11:34.728479  1899 net.cpp:128] Creating Layer ReLU32
I0825 11:11:34.728498  1899 net.cpp:558] ReLU32 <- Convolution34
I0825 11:11:34.728518  1899 net.cpp:509] ReLU32 -> Convolution34 (in-place)
I0825 11:11:34.731894  1899 net.cpp:172] Setting up ReLU32
I0825 11:11:34.731914  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.731918  1899 net.cpp:194] Memory required for data: 535560448
I0825 11:11:34.731923  1899 layer_factory.hpp:77] Creating layer Convolution35
I0825 11:11:34.731936  1899 net.cpp:128] Creating Layer Convolution35
I0825 11:11:34.731941  1899 net.cpp:558] Convolution35 <- Convolution34
I0825 11:11:34.731951  1899 net.cpp:522] Convolution35 -> Convolution35
I0825 11:11:34.737239  1899 net.cpp:172] Setting up Convolution35
I0825 11:11:34.737267  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.737270  1899 net.cpp:194] Memory required for data: 536609024
I0825 11:11:34.737282  1899 layer_factory.hpp:77] Creating layer BatchNorm35
I0825 11:11:34.737291  1899 net.cpp:128] Creating Layer BatchNorm35
I0825 11:11:34.737326  1899 net.cpp:558] BatchNorm35 <- Convolution35
I0825 11:11:34.737347  1899 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0825 11:11:34.737661  1899 net.cpp:172] Setting up BatchNorm35
I0825 11:11:34.737673  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.737677  1899 net.cpp:194] Memory required for data: 537657600
I0825 11:11:34.737687  1899 layer_factory.hpp:77] Creating layer Scale35
I0825 11:11:34.737720  1899 net.cpp:128] Creating Layer Scale35
I0825 11:11:34.737737  1899 net.cpp:558] Scale35 <- Convolution35
I0825 11:11:34.737756  1899 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0825 11:11:34.737825  1899 layer_factory.hpp:77] Creating layer Scale35
I0825 11:11:34.737995  1899 net.cpp:172] Setting up Scale35
I0825 11:11:34.738005  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.738009  1899 net.cpp:194] Memory required for data: 538706176
I0825 11:11:34.738018  1899 layer_factory.hpp:77] Creating layer Eltwise16
I0825 11:11:34.738045  1899 net.cpp:128] Creating Layer Eltwise16
I0825 11:11:34.738062  1899 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0825 11:11:34.738083  1899 net.cpp:558] Eltwise16 <- Convolution35
I0825 11:11:34.738091  1899 net.cpp:522] Eltwise16 -> Eltwise16
I0825 11:11:34.738131  1899 net.cpp:172] Setting up Eltwise16
I0825 11:11:34.738140  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.738144  1899 net.cpp:194] Memory required for data: 539754752
I0825 11:11:34.738148  1899 layer_factory.hpp:77] Creating layer ReLU33
I0825 11:11:34.738155  1899 net.cpp:128] Creating Layer ReLU33
I0825 11:11:34.738178  1899 net.cpp:558] ReLU33 <- Eltwise16
I0825 11:11:34.738193  1899 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0825 11:11:34.738735  1899 net.cpp:172] Setting up ReLU33
I0825 11:11:34.738760  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.738765  1899 net.cpp:194] Memory required for data: 540803328
I0825 11:11:34.738798  1899 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0825 11:11:34.738821  1899 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0825 11:11:34.738838  1899 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0825 11:11:34.738862  1899 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0825 11:11:34.738875  1899 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0825 11:11:34.738940  1899 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0825 11:11:34.738950  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.738955  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.738960  1899 net.cpp:194] Memory required for data: 542900480
I0825 11:11:34.738965  1899 layer_factory.hpp:77] Creating layer Convolution36
I0825 11:11:34.738979  1899 net.cpp:128] Creating Layer Convolution36
I0825 11:11:34.739001  1899 net.cpp:558] Convolution36 <- Eltwise16_ReLU33_0_split_0
I0825 11:11:34.739022  1899 net.cpp:522] Convolution36 -> Convolution36
I0825 11:11:34.744220  1899 net.cpp:172] Setting up Convolution36
I0825 11:11:34.744266  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.744283  1899 net.cpp:194] Memory required for data: 543949056
I0825 11:11:34.744308  1899 layer_factory.hpp:77] Creating layer BatchNorm36
I0825 11:11:34.744331  1899 net.cpp:128] Creating Layer BatchNorm36
I0825 11:11:34.744350  1899 net.cpp:558] BatchNorm36 <- Convolution36
I0825 11:11:34.744372  1899 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0825 11:11:34.744681  1899 net.cpp:172] Setting up BatchNorm36
I0825 11:11:34.744694  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.744699  1899 net.cpp:194] Memory required for data: 544997632
I0825 11:11:34.744709  1899 layer_factory.hpp:77] Creating layer Scale36
I0825 11:11:34.744717  1899 net.cpp:128] Creating Layer Scale36
I0825 11:11:34.744741  1899 net.cpp:558] Scale36 <- Convolution36
I0825 11:11:34.744758  1899 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0825 11:11:34.744829  1899 layer_factory.hpp:77] Creating layer Scale36
I0825 11:11:34.744995  1899 net.cpp:172] Setting up Scale36
I0825 11:11:34.745007  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.745010  1899 net.cpp:194] Memory required for data: 546046208
I0825 11:11:34.745018  1899 layer_factory.hpp:77] Creating layer ReLU34
I0825 11:11:34.745048  1899 net.cpp:128] Creating Layer ReLU34
I0825 11:11:34.745066  1899 net.cpp:558] ReLU34 <- Convolution36
I0825 11:11:34.745093  1899 net.cpp:509] ReLU34 -> Convolution36 (in-place)
I0825 11:11:34.746297  1899 net.cpp:172] Setting up ReLU34
I0825 11:11:34.746314  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.746317  1899 net.cpp:194] Memory required for data: 547094784
I0825 11:11:34.746322  1899 layer_factory.hpp:77] Creating layer Convolution37
I0825 11:11:34.746351  1899 net.cpp:128] Creating Layer Convolution37
I0825 11:11:34.746358  1899 net.cpp:558] Convolution37 <- Convolution36
I0825 11:11:34.746368  1899 net.cpp:522] Convolution37 -> Convolution37
I0825 11:11:34.759243  1899 net.cpp:172] Setting up Convolution37
I0825 11:11:34.759269  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.759290  1899 net.cpp:194] Memory required for data: 548143360
I0825 11:11:34.759301  1899 layer_factory.hpp:77] Creating layer BatchNorm37
I0825 11:11:34.759312  1899 net.cpp:128] Creating Layer BatchNorm37
I0825 11:11:34.759317  1899 net.cpp:558] BatchNorm37 <- Convolution37
I0825 11:11:34.759326  1899 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0825 11:11:34.759629  1899 net.cpp:172] Setting up BatchNorm37
I0825 11:11:34.759640  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.759646  1899 net.cpp:194] Memory required for data: 549191936
I0825 11:11:34.759680  1899 layer_factory.hpp:77] Creating layer Scale37
I0825 11:11:34.759721  1899 net.cpp:128] Creating Layer Scale37
I0825 11:11:34.759737  1899 net.cpp:558] Scale37 <- Convolution37
I0825 11:11:34.759754  1899 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0825 11:11:34.759826  1899 layer_factory.hpp:77] Creating layer Scale37
I0825 11:11:34.760007  1899 net.cpp:172] Setting up Scale37
I0825 11:11:34.760020  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.760025  1899 net.cpp:194] Memory required for data: 550240512
I0825 11:11:34.760033  1899 layer_factory.hpp:77] Creating layer Eltwise17
I0825 11:11:34.760043  1899 net.cpp:128] Creating Layer Eltwise17
I0825 11:11:34.760049  1899 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0825 11:11:34.760054  1899 net.cpp:558] Eltwise17 <- Convolution37
I0825 11:11:34.760061  1899 net.cpp:522] Eltwise17 -> Eltwise17
I0825 11:11:34.760092  1899 net.cpp:172] Setting up Eltwise17
I0825 11:11:34.760123  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.760138  1899 net.cpp:194] Memory required for data: 551289088
I0825 11:11:34.760151  1899 layer_factory.hpp:77] Creating layer ReLU35
I0825 11:11:34.760169  1899 net.cpp:128] Creating Layer ReLU35
I0825 11:11:34.760184  1899 net.cpp:558] ReLU35 <- Eltwise17
I0825 11:11:34.760201  1899 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0825 11:11:34.761072  1899 net.cpp:172] Setting up ReLU35
I0825 11:11:34.761092  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.761096  1899 net.cpp:194] Memory required for data: 552337664
I0825 11:11:34.761102  1899 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0825 11:11:34.761111  1899 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0825 11:11:34.761116  1899 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0825 11:11:34.761124  1899 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0825 11:11:34.761133  1899 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0825 11:11:34.761191  1899 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0825 11:11:34.761229  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.761246  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.761260  1899 net.cpp:194] Memory required for data: 554434816
I0825 11:11:34.761274  1899 layer_factory.hpp:77] Creating layer Convolution38
I0825 11:11:34.761298  1899 net.cpp:128] Creating Layer Convolution38
I0825 11:11:34.761313  1899 net.cpp:558] Convolution38 <- Eltwise17_ReLU35_0_split_0
I0825 11:11:34.761333  1899 net.cpp:522] Convolution38 -> Convolution38
I0825 11:11:34.767904  1899 net.cpp:172] Setting up Convolution38
I0825 11:11:34.767930  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.767935  1899 net.cpp:194] Memory required for data: 555483392
I0825 11:11:34.767944  1899 layer_factory.hpp:77] Creating layer BatchNorm38
I0825 11:11:34.767956  1899 net.cpp:128] Creating Layer BatchNorm38
I0825 11:11:34.767961  1899 net.cpp:558] BatchNorm38 <- Convolution38
I0825 11:11:34.767967  1899 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0825 11:11:34.768254  1899 net.cpp:172] Setting up BatchNorm38
I0825 11:11:34.768265  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.768271  1899 net.cpp:194] Memory required for data: 556531968
I0825 11:11:34.768281  1899 layer_factory.hpp:77] Creating layer Scale38
I0825 11:11:34.768291  1899 net.cpp:128] Creating Layer Scale38
I0825 11:11:34.768340  1899 net.cpp:558] Scale38 <- Convolution38
I0825 11:11:34.768357  1899 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0825 11:11:34.768427  1899 layer_factory.hpp:77] Creating layer Scale38
I0825 11:11:34.768612  1899 net.cpp:172] Setting up Scale38
I0825 11:11:34.768625  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.768630  1899 net.cpp:194] Memory required for data: 557580544
I0825 11:11:34.768640  1899 layer_factory.hpp:77] Creating layer ReLU36
I0825 11:11:34.768648  1899 net.cpp:128] Creating Layer ReLU36
I0825 11:11:34.768653  1899 net.cpp:558] ReLU36 <- Convolution38
I0825 11:11:34.768661  1899 net.cpp:509] ReLU36 -> Convolution38 (in-place)
I0825 11:11:34.770016  1899 net.cpp:172] Setting up ReLU36
I0825 11:11:34.770067  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.770087  1899 net.cpp:194] Memory required for data: 558629120
I0825 11:11:34.770102  1899 layer_factory.hpp:77] Creating layer Convolution39
I0825 11:11:34.770126  1899 net.cpp:128] Creating Layer Convolution39
I0825 11:11:34.770148  1899 net.cpp:558] Convolution39 <- Convolution38
I0825 11:11:34.770169  1899 net.cpp:522] Convolution39 -> Convolution39
I0825 11:11:34.777626  1899 net.cpp:172] Setting up Convolution39
I0825 11:11:34.777655  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.777662  1899 net.cpp:194] Memory required for data: 559677696
I0825 11:11:34.777671  1899 layer_factory.hpp:77] Creating layer BatchNorm39
I0825 11:11:34.777683  1899 net.cpp:128] Creating Layer BatchNorm39
I0825 11:11:34.777719  1899 net.cpp:558] BatchNorm39 <- Convolution39
I0825 11:11:34.777736  1899 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0825 11:11:34.778056  1899 net.cpp:172] Setting up BatchNorm39
I0825 11:11:34.778067  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.778072  1899 net.cpp:194] Memory required for data: 560726272
I0825 11:11:34.778082  1899 layer_factory.hpp:77] Creating layer Scale39
I0825 11:11:34.778108  1899 net.cpp:128] Creating Layer Scale39
I0825 11:11:34.778126  1899 net.cpp:558] Scale39 <- Convolution39
I0825 11:11:34.778144  1899 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0825 11:11:34.778215  1899 layer_factory.hpp:77] Creating layer Scale39
I0825 11:11:34.778393  1899 net.cpp:172] Setting up Scale39
I0825 11:11:34.778405  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.778410  1899 net.cpp:194] Memory required for data: 561774848
I0825 11:11:34.778419  1899 layer_factory.hpp:77] Creating layer Eltwise18
I0825 11:11:34.778445  1899 net.cpp:128] Creating Layer Eltwise18
I0825 11:11:34.778463  1899 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0825 11:11:34.778481  1899 net.cpp:558] Eltwise18 <- Convolution39
I0825 11:11:34.778499  1899 net.cpp:522] Eltwise18 -> Eltwise18
I0825 11:11:34.778540  1899 net.cpp:172] Setting up Eltwise18
I0825 11:11:34.778549  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.778554  1899 net.cpp:194] Memory required for data: 562823424
I0825 11:11:34.778558  1899 layer_factory.hpp:77] Creating layer ReLU37
I0825 11:11:34.778568  1899 net.cpp:128] Creating Layer ReLU37
I0825 11:11:34.778590  1899 net.cpp:558] ReLU37 <- Eltwise18
I0825 11:11:34.778606  1899 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0825 11:11:34.781617  1899 net.cpp:172] Setting up ReLU37
I0825 11:11:34.781636  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.781641  1899 net.cpp:194] Memory required for data: 563872000
I0825 11:11:34.781646  1899 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0825 11:11:34.781656  1899 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0825 11:11:34.781661  1899 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0825 11:11:34.781667  1899 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0825 11:11:34.781675  1899 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0825 11:11:34.781736  1899 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0825 11:11:34.781769  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.781790  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.781795  1899 net.cpp:194] Memory required for data: 565969152
I0825 11:11:34.781798  1899 layer_factory.hpp:77] Creating layer Convolution40
I0825 11:11:34.781813  1899 net.cpp:128] Creating Layer Convolution40
I0825 11:11:34.781836  1899 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_0
I0825 11:11:34.781855  1899 net.cpp:522] Convolution40 -> Convolution40
I0825 11:11:34.791410  1899 net.cpp:172] Setting up Convolution40
I0825 11:11:34.791437  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.791442  1899 net.cpp:194] Memory required for data: 567017728
I0825 11:11:34.791452  1899 layer_factory.hpp:77] Creating layer BatchNorm40
I0825 11:11:34.791463  1899 net.cpp:128] Creating Layer BatchNorm40
I0825 11:11:34.791469  1899 net.cpp:558] BatchNorm40 <- Convolution40
I0825 11:11:34.791478  1899 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0825 11:11:34.791769  1899 net.cpp:172] Setting up BatchNorm40
I0825 11:11:34.791780  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.791785  1899 net.cpp:194] Memory required for data: 568066304
I0825 11:11:34.791795  1899 layer_factory.hpp:77] Creating layer Scale40
I0825 11:11:34.791803  1899 net.cpp:128] Creating Layer Scale40
I0825 11:11:34.791810  1899 net.cpp:558] Scale40 <- Convolution40
I0825 11:11:34.791815  1899 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0825 11:11:34.791867  1899 layer_factory.hpp:77] Creating layer Scale40
I0825 11:11:34.792076  1899 net.cpp:172] Setting up Scale40
I0825 11:11:34.792090  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.792096  1899 net.cpp:194] Memory required for data: 569114880
I0825 11:11:34.792104  1899 layer_factory.hpp:77] Creating layer ReLU38
I0825 11:11:34.792111  1899 net.cpp:128] Creating Layer ReLU38
I0825 11:11:34.792116  1899 net.cpp:558] ReLU38 <- Convolution40
I0825 11:11:34.792124  1899 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0825 11:11:34.793506  1899 net.cpp:172] Setting up ReLU38
I0825 11:11:34.793524  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.793529  1899 net.cpp:194] Memory required for data: 570163456
I0825 11:11:34.793534  1899 layer_factory.hpp:77] Creating layer Convolution41
I0825 11:11:34.793547  1899 net.cpp:128] Creating Layer Convolution41
I0825 11:11:34.793553  1899 net.cpp:558] Convolution41 <- Convolution40
I0825 11:11:34.793562  1899 net.cpp:522] Convolution41 -> Convolution41
I0825 11:11:34.800645  1899 net.cpp:172] Setting up Convolution41
I0825 11:11:34.800669  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.800674  1899 net.cpp:194] Memory required for data: 571212032
I0825 11:11:34.800685  1899 layer_factory.hpp:77] Creating layer BatchNorm41
I0825 11:11:34.800695  1899 net.cpp:128] Creating Layer BatchNorm41
I0825 11:11:34.800704  1899 net.cpp:558] BatchNorm41 <- Convolution41
I0825 11:11:34.800711  1899 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0825 11:11:34.801023  1899 net.cpp:172] Setting up BatchNorm41
I0825 11:11:34.801033  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.801070  1899 net.cpp:194] Memory required for data: 572260608
I0825 11:11:34.801095  1899 layer_factory.hpp:77] Creating layer Scale41
I0825 11:11:34.801108  1899 net.cpp:128] Creating Layer Scale41
I0825 11:11:34.801113  1899 net.cpp:558] Scale41 <- Convolution41
I0825 11:11:34.801120  1899 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0825 11:11:34.801177  1899 layer_factory.hpp:77] Creating layer Scale41
I0825 11:11:34.801352  1899 net.cpp:172] Setting up Scale41
I0825 11:11:34.801363  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.801367  1899 net.cpp:194] Memory required for data: 573309184
I0825 11:11:34.801396  1899 layer_factory.hpp:77] Creating layer Eltwise19
I0825 11:11:34.801409  1899 net.cpp:128] Creating Layer Eltwise19
I0825 11:11:34.801414  1899 net.cpp:558] Eltwise19 <- Eltwise18_ReLU37_0_split_1
I0825 11:11:34.801419  1899 net.cpp:558] Eltwise19 <- Convolution41
I0825 11:11:34.801457  1899 net.cpp:522] Eltwise19 -> Eltwise19
I0825 11:11:34.801496  1899 net.cpp:172] Setting up Eltwise19
I0825 11:11:34.801506  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.801511  1899 net.cpp:194] Memory required for data: 574357760
I0825 11:11:34.801532  1899 layer_factory.hpp:77] Creating layer ReLU39
I0825 11:11:34.801553  1899 net.cpp:128] Creating Layer ReLU39
I0825 11:11:34.801561  1899 net.cpp:558] ReLU39 <- Eltwise19
I0825 11:11:34.801568  1899 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0825 11:11:34.802464  1899 net.cpp:172] Setting up ReLU39
I0825 11:11:34.802484  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.802487  1899 net.cpp:194] Memory required for data: 575406336
I0825 11:11:34.802492  1899 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0825 11:11:34.802501  1899 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0825 11:11:34.802533  1899 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0825 11:11:34.802558  1899 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0825 11:11:34.802582  1899 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0825 11:11:34.802659  1899 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0825 11:11:34.802673  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.802680  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.802683  1899 net.cpp:194] Memory required for data: 577503488
I0825 11:11:34.802707  1899 layer_factory.hpp:77] Creating layer Convolution42
I0825 11:11:34.802733  1899 net.cpp:128] Creating Layer Convolution42
I0825 11:11:34.802742  1899 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0825 11:11:34.802750  1899 net.cpp:522] Convolution42 -> Convolution42
I0825 11:11:34.811537  1899 net.cpp:172] Setting up Convolution42
I0825 11:11:34.811566  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.811571  1899 net.cpp:194] Memory required for data: 578552064
I0825 11:11:34.811583  1899 layer_factory.hpp:77] Creating layer BatchNorm42
I0825 11:11:34.811590  1899 net.cpp:128] Creating Layer BatchNorm42
I0825 11:11:34.811595  1899 net.cpp:558] BatchNorm42 <- Convolution42
I0825 11:11:34.811635  1899 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0825 11:11:34.811956  1899 net.cpp:172] Setting up BatchNorm42
I0825 11:11:34.811967  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.811972  1899 net.cpp:194] Memory required for data: 579600640
I0825 11:11:34.811985  1899 layer_factory.hpp:77] Creating layer Scale42
I0825 11:11:34.812011  1899 net.cpp:128] Creating Layer Scale42
I0825 11:11:34.812028  1899 net.cpp:558] Scale42 <- Convolution42
I0825 11:11:34.812049  1899 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0825 11:11:34.812119  1899 layer_factory.hpp:77] Creating layer Scale42
I0825 11:11:34.812294  1899 net.cpp:172] Setting up Scale42
I0825 11:11:34.812304  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.812307  1899 net.cpp:194] Memory required for data: 580649216
I0825 11:11:34.812316  1899 layer_factory.hpp:77] Creating layer ReLU40
I0825 11:11:34.812341  1899 net.cpp:128] Creating Layer ReLU40
I0825 11:11:34.812360  1899 net.cpp:558] ReLU40 <- Convolution42
I0825 11:11:34.812379  1899 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0825 11:11:34.815702  1899 net.cpp:172] Setting up ReLU40
I0825 11:11:34.815727  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.815732  1899 net.cpp:194] Memory required for data: 581697792
I0825 11:11:34.815737  1899 layer_factory.hpp:77] Creating layer Convolution43
I0825 11:11:34.815752  1899 net.cpp:128] Creating Layer Convolution43
I0825 11:11:34.815757  1899 net.cpp:558] Convolution43 <- Convolution42
I0825 11:11:34.815766  1899 net.cpp:522] Convolution43 -> Convolution43
I0825 11:11:34.824259  1899 net.cpp:172] Setting up Convolution43
I0825 11:11:34.824285  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.824290  1899 net.cpp:194] Memory required for data: 582746368
I0825 11:11:34.824301  1899 layer_factory.hpp:77] Creating layer BatchNorm43
I0825 11:11:34.824326  1899 net.cpp:128] Creating Layer BatchNorm43
I0825 11:11:34.824332  1899 net.cpp:558] BatchNorm43 <- Convolution43
I0825 11:11:34.824342  1899 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0825 11:11:34.824651  1899 net.cpp:172] Setting up BatchNorm43
I0825 11:11:34.824663  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.824669  1899 net.cpp:194] Memory required for data: 583794944
I0825 11:11:34.824679  1899 layer_factory.hpp:77] Creating layer Scale43
I0825 11:11:34.824689  1899 net.cpp:128] Creating Layer Scale43
I0825 11:11:34.824693  1899 net.cpp:558] Scale43 <- Convolution43
I0825 11:11:34.824699  1899 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0825 11:11:34.824753  1899 layer_factory.hpp:77] Creating layer Scale43
I0825 11:11:34.824968  1899 net.cpp:172] Setting up Scale43
I0825 11:11:34.824981  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.824985  1899 net.cpp:194] Memory required for data: 584843520
I0825 11:11:34.824995  1899 layer_factory.hpp:77] Creating layer Eltwise20
I0825 11:11:34.825004  1899 net.cpp:128] Creating Layer Eltwise20
I0825 11:11:34.825011  1899 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0825 11:11:34.825016  1899 net.cpp:558] Eltwise20 <- Convolution43
I0825 11:11:34.825021  1899 net.cpp:522] Eltwise20 -> Eltwise20
I0825 11:11:34.825054  1899 net.cpp:172] Setting up Eltwise20
I0825 11:11:34.825086  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.825100  1899 net.cpp:194] Memory required for data: 585892096
I0825 11:11:34.825115  1899 layer_factory.hpp:77] Creating layer ReLU41
I0825 11:11:34.825132  1899 net.cpp:128] Creating Layer ReLU41
I0825 11:11:34.825146  1899 net.cpp:558] ReLU41 <- Eltwise20
I0825 11:11:34.825162  1899 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0825 11:11:34.826071  1899 net.cpp:172] Setting up ReLU41
I0825 11:11:34.826090  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.826094  1899 net.cpp:194] Memory required for data: 586940672
I0825 11:11:34.826099  1899 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0825 11:11:34.826107  1899 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0825 11:11:34.826112  1899 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0825 11:11:34.826122  1899 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0825 11:11:34.826131  1899 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0825 11:11:34.826191  1899 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0825 11:11:34.826231  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.826247  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.826261  1899 net.cpp:194] Memory required for data: 589037824
I0825 11:11:34.826274  1899 layer_factory.hpp:77] Creating layer Convolution44
I0825 11:11:34.826297  1899 net.cpp:128] Creating Layer Convolution44
I0825 11:11:34.826311  1899 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0825 11:11:34.826331  1899 net.cpp:522] Convolution44 -> Convolution44
I0825 11:11:34.829030  1899 net.cpp:172] Setting up Convolution44
I0825 11:11:34.829078  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.829097  1899 net.cpp:194] Memory required for data: 590086400
I0825 11:11:34.829118  1899 layer_factory.hpp:77] Creating layer BatchNorm44
I0825 11:11:34.829144  1899 net.cpp:128] Creating Layer BatchNorm44
I0825 11:11:34.829162  1899 net.cpp:558] BatchNorm44 <- Convolution44
I0825 11:11:34.829181  1899 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0825 11:11:34.829506  1899 net.cpp:172] Setting up BatchNorm44
I0825 11:11:34.829531  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.829550  1899 net.cpp:194] Memory required for data: 591134976
I0825 11:11:34.829571  1899 layer_factory.hpp:77] Creating layer Scale44
I0825 11:11:34.829591  1899 net.cpp:128] Creating Layer Scale44
I0825 11:11:34.829607  1899 net.cpp:558] Scale44 <- Convolution44
I0825 11:11:34.829624  1899 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0825 11:11:34.829711  1899 layer_factory.hpp:77] Creating layer Scale44
I0825 11:11:34.829907  1899 net.cpp:172] Setting up Scale44
I0825 11:11:34.829931  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.829948  1899 net.cpp:194] Memory required for data: 592183552
I0825 11:11:34.829967  1899 layer_factory.hpp:77] Creating layer ReLU42
I0825 11:11:34.829991  1899 net.cpp:128] Creating Layer ReLU42
I0825 11:11:34.830006  1899 net.cpp:558] ReLU42 <- Convolution44
I0825 11:11:34.830021  1899 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0825 11:11:34.830291  1899 net.cpp:172] Setting up ReLU42
I0825 11:11:34.830323  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.830350  1899 net.cpp:194] Memory required for data: 593232128
I0825 11:11:34.830365  1899 layer_factory.hpp:77] Creating layer Convolution45
I0825 11:11:34.830391  1899 net.cpp:128] Creating Layer Convolution45
I0825 11:11:34.830410  1899 net.cpp:558] Convolution45 <- Convolution44
I0825 11:11:34.830428  1899 net.cpp:522] Convolution45 -> Convolution45
I0825 11:11:34.833578  1899 net.cpp:172] Setting up Convolution45
I0825 11:11:34.833624  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.833642  1899 net.cpp:194] Memory required for data: 594280704
I0825 11:11:34.833664  1899 layer_factory.hpp:77] Creating layer BatchNorm45
I0825 11:11:34.833681  1899 net.cpp:128] Creating Layer BatchNorm45
I0825 11:11:34.833700  1899 net.cpp:558] BatchNorm45 <- Convolution45
I0825 11:11:34.833719  1899 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0825 11:11:34.834046  1899 net.cpp:172] Setting up BatchNorm45
I0825 11:11:34.834070  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.834087  1899 net.cpp:194] Memory required for data: 595329280
I0825 11:11:34.834108  1899 layer_factory.hpp:77] Creating layer Scale45
I0825 11:11:34.834130  1899 net.cpp:128] Creating Layer Scale45
I0825 11:11:34.834143  1899 net.cpp:558] Scale45 <- Convolution45
I0825 11:11:34.834161  1899 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0825 11:11:34.834235  1899 layer_factory.hpp:77] Creating layer Scale45
I0825 11:11:34.834439  1899 net.cpp:172] Setting up Scale45
I0825 11:11:34.834451  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.834456  1899 net.cpp:194] Memory required for data: 596377856
I0825 11:11:34.834465  1899 layer_factory.hpp:77] Creating layer Eltwise21
I0825 11:11:34.834475  1899 net.cpp:128] Creating Layer Eltwise21
I0825 11:11:34.834502  1899 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0825 11:11:34.834511  1899 net.cpp:558] Eltwise21 <- Convolution45
I0825 11:11:34.834518  1899 net.cpp:522] Eltwise21 -> Eltwise21
I0825 11:11:34.834573  1899 net.cpp:172] Setting up Eltwise21
I0825 11:11:34.834583  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.834587  1899 net.cpp:194] Memory required for data: 597426432
I0825 11:11:34.834591  1899 layer_factory.hpp:77] Creating layer ReLU43
I0825 11:11:34.834600  1899 net.cpp:128] Creating Layer ReLU43
I0825 11:11:34.834621  1899 net.cpp:558] ReLU43 <- Eltwise21
I0825 11:11:34.834633  1899 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0825 11:11:34.835160  1899 net.cpp:172] Setting up ReLU43
I0825 11:11:34.835201  1899 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:11:34.835218  1899 net.cpp:194] Memory required for data: 598475008
I0825 11:11:34.835247  1899 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:11:34.835268  1899 net.cpp:128] Creating Layer Pooling1
I0825 11:11:34.835289  1899 net.cpp:558] Pooling1 <- Eltwise21
I0825 11:11:34.835309  1899 net.cpp:522] Pooling1 -> Pooling1
I0825 11:11:34.835654  1899 net.cpp:172] Setting up Pooling1
I0825 11:11:34.835688  1899 net.cpp:186] Top shape: 64 64 1 1 (4096)
I0825 11:11:34.835705  1899 net.cpp:194] Memory required for data: 598491392
I0825 11:11:34.835722  1899 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:11:34.835747  1899 net.cpp:128] Creating Layer InnerProduct1
I0825 11:11:34.835754  1899 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:11:34.835775  1899 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:11:34.835992  1899 net.cpp:172] Setting up InnerProduct1
I0825 11:11:34.836004  1899 net.cpp:186] Top shape: 64 10 (640)
I0825 11:11:34.836009  1899 net.cpp:194] Memory required for data: 598493952
I0825 11:11:34.836019  1899 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:11:34.836048  1899 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:11:34.836066  1899 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0825 11:11:34.836083  1899 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0825 11:11:34.836104  1899 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:11:34.836118  1899 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:11:34.836535  1899 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:11:34.836572  1899 net.cpp:186] Top shape: (1)
I0825 11:11:34.836586  1899 net.cpp:189]     with loss weight 1
I0825 11:11:34.836632  1899 net.cpp:194] Memory required for data: 598493956
I0825 11:11:34.836642  1899 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:11:34.836650  1899 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:11:34.836654  1899 net.cpp:301] Pooling1 needs backward computation.
I0825 11:11:34.836658  1899 net.cpp:301] ReLU43 needs backward computation.
I0825 11:11:34.836663  1899 net.cpp:301] Eltwise21 needs backward computation.
I0825 11:11:34.836686  1899 net.cpp:301] Scale45 needs backward computation.
I0825 11:11:34.836694  1899 net.cpp:301] BatchNorm45 needs backward computation.
I0825 11:11:34.836699  1899 net.cpp:301] Convolution45 needs backward computation.
I0825 11:11:34.836702  1899 net.cpp:301] ReLU42 needs backward computation.
I0825 11:11:34.836722  1899 net.cpp:301] Scale44 needs backward computation.
I0825 11:11:34.836730  1899 net.cpp:301] BatchNorm44 needs backward computation.
I0825 11:11:34.836733  1899 net.cpp:301] Convolution44 needs backward computation.
I0825 11:11:34.836738  1899 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0825 11:11:34.836758  1899 net.cpp:301] ReLU41 needs backward computation.
I0825 11:11:34.836766  1899 net.cpp:301] Eltwise20 needs backward computation.
I0825 11:11:34.836771  1899 net.cpp:301] Scale43 needs backward computation.
I0825 11:11:34.836776  1899 net.cpp:301] BatchNorm43 needs backward computation.
I0825 11:11:34.836794  1899 net.cpp:301] Convolution43 needs backward computation.
I0825 11:11:34.836802  1899 net.cpp:301] ReLU40 needs backward computation.
I0825 11:11:34.836807  1899 net.cpp:301] Scale42 needs backward computation.
I0825 11:11:34.836812  1899 net.cpp:301] BatchNorm42 needs backward computation.
I0825 11:11:34.836815  1899 net.cpp:301] Convolution42 needs backward computation.
I0825 11:11:34.836834  1899 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0825 11:11:34.836845  1899 net.cpp:301] ReLU39 needs backward computation.
I0825 11:11:34.836850  1899 net.cpp:301] Eltwise19 needs backward computation.
I0825 11:11:34.836854  1899 net.cpp:301] Scale41 needs backward computation.
I0825 11:11:34.836874  1899 net.cpp:301] BatchNorm41 needs backward computation.
I0825 11:11:34.836880  1899 net.cpp:301] Convolution41 needs backward computation.
I0825 11:11:34.836885  1899 net.cpp:301] ReLU38 needs backward computation.
I0825 11:11:34.836889  1899 net.cpp:301] Scale40 needs backward computation.
I0825 11:11:34.836913  1899 net.cpp:301] BatchNorm40 needs backward computation.
I0825 11:11:34.836921  1899 net.cpp:301] Convolution40 needs backward computation.
I0825 11:11:34.836926  1899 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0825 11:11:34.836932  1899 net.cpp:301] ReLU37 needs backward computation.
I0825 11:11:34.836952  1899 net.cpp:301] Eltwise18 needs backward computation.
I0825 11:11:34.836961  1899 net.cpp:301] Scale39 needs backward computation.
I0825 11:11:34.836966  1899 net.cpp:301] BatchNorm39 needs backward computation.
I0825 11:11:34.836971  1899 net.cpp:301] Convolution39 needs backward computation.
I0825 11:11:34.836989  1899 net.cpp:301] ReLU36 needs backward computation.
I0825 11:11:34.837003  1899 net.cpp:301] Scale38 needs backward computation.
I0825 11:11:34.837008  1899 net.cpp:301] BatchNorm38 needs backward computation.
I0825 11:11:34.837013  1899 net.cpp:301] Convolution38 needs backward computation.
I0825 11:11:34.837033  1899 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0825 11:11:34.837040  1899 net.cpp:301] ReLU35 needs backward computation.
I0825 11:11:34.837045  1899 net.cpp:301] Eltwise17 needs backward computation.
I0825 11:11:34.837050  1899 net.cpp:301] Scale37 needs backward computation.
I0825 11:11:34.837057  1899 net.cpp:301] BatchNorm37 needs backward computation.
I0825 11:11:34.837061  1899 net.cpp:301] Convolution37 needs backward computation.
I0825 11:11:34.837080  1899 net.cpp:301] ReLU34 needs backward computation.
I0825 11:11:34.837087  1899 net.cpp:301] Scale36 needs backward computation.
I0825 11:11:34.837091  1899 net.cpp:301] BatchNorm36 needs backward computation.
I0825 11:11:34.837096  1899 net.cpp:301] Convolution36 needs backward computation.
I0825 11:11:34.837101  1899 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0825 11:11:34.837107  1899 net.cpp:301] ReLU33 needs backward computation.
I0825 11:11:34.837112  1899 net.cpp:301] Eltwise16 needs backward computation.
I0825 11:11:34.837119  1899 net.cpp:301] Scale35 needs backward computation.
I0825 11:11:34.837124  1899 net.cpp:301] BatchNorm35 needs backward computation.
I0825 11:11:34.837129  1899 net.cpp:301] Convolution35 needs backward computation.
I0825 11:11:34.837134  1899 net.cpp:301] ReLU32 needs backward computation.
I0825 11:11:34.837152  1899 net.cpp:301] Scale34 needs backward computation.
I0825 11:11:34.837159  1899 net.cpp:301] BatchNorm34 needs backward computation.
I0825 11:11:34.837164  1899 net.cpp:301] Convolution34 needs backward computation.
I0825 11:11:34.837169  1899 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0825 11:11:34.837177  1899 net.cpp:301] ReLU31 needs backward computation.
I0825 11:11:34.837180  1899 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:11:34.837188  1899 net.cpp:301] Scale33 needs backward computation.
I0825 11:11:34.837193  1899 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:11:34.837198  1899 net.cpp:301] Convolution33 needs backward computation.
I0825 11:11:34.837205  1899 net.cpp:301] ReLU30 needs backward computation.
I0825 11:11:34.837211  1899 net.cpp:301] Scale32 needs backward computation.
I0825 11:11:34.837216  1899 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:11:34.837222  1899 net.cpp:301] Convolution32 needs backward computation.
I0825 11:11:34.837227  1899 net.cpp:301] Scale31 needs backward computation.
I0825 11:11:34.837234  1899 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:11:34.837239  1899 net.cpp:301] Convolution31 needs backward computation.
I0825 11:11:34.837245  1899 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:11:34.837265  1899 net.cpp:301] ReLU29 needs backward computation.
I0825 11:11:34.837273  1899 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:11:34.837280  1899 net.cpp:301] Scale30 needs backward computation.
I0825 11:11:34.837283  1899 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:11:34.837301  1899 net.cpp:301] Convolution30 needs backward computation.
I0825 11:11:34.837309  1899 net.cpp:301] ReLU28 needs backward computation.
I0825 11:11:34.837313  1899 net.cpp:301] Scale29 needs backward computation.
I0825 11:11:34.837318  1899 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:11:34.837322  1899 net.cpp:301] Convolution29 needs backward computation.
I0825 11:11:34.837327  1899 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:11:34.837332  1899 net.cpp:301] ReLU27 needs backward computation.
I0825 11:11:34.837357  1899 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:11:34.837365  1899 net.cpp:301] Scale28 needs backward computation.
I0825 11:11:34.837378  1899 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:11:34.837383  1899 net.cpp:301] Convolution28 needs backward computation.
I0825 11:11:34.837389  1899 net.cpp:301] ReLU26 needs backward computation.
I0825 11:11:34.837394  1899 net.cpp:301] Scale27 needs backward computation.
I0825 11:11:34.837400  1899 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:11:34.837404  1899 net.cpp:301] Convolution27 needs backward computation.
I0825 11:11:34.837424  1899 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:11:34.837431  1899 net.cpp:301] ReLU25 needs backward computation.
I0825 11:11:34.837436  1899 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:11:34.837441  1899 net.cpp:301] Scale26 needs backward computation.
I0825 11:11:34.837447  1899 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:11:34.837452  1899 net.cpp:301] Convolution26 needs backward computation.
I0825 11:11:34.837458  1899 net.cpp:301] ReLU24 needs backward computation.
I0825 11:11:34.837463  1899 net.cpp:301] Scale25 needs backward computation.
I0825 11:11:34.837468  1899 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:11:34.837473  1899 net.cpp:301] Convolution25 needs backward computation.
I0825 11:11:34.837481  1899 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:11:34.837489  1899 net.cpp:301] ReLU23 needs backward computation.
I0825 11:11:34.837493  1899 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:11:34.837501  1899 net.cpp:301] Scale24 needs backward computation.
I0825 11:11:34.837505  1899 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:11:34.837512  1899 net.cpp:301] Convolution24 needs backward computation.
I0825 11:11:34.837515  1899 net.cpp:301] ReLU22 needs backward computation.
I0825 11:11:34.837522  1899 net.cpp:301] Scale23 needs backward computation.
I0825 11:11:34.837527  1899 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:11:34.837545  1899 net.cpp:301] Convolution23 needs backward computation.
I0825 11:11:34.837553  1899 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:11:34.837558  1899 net.cpp:301] ReLU21 needs backward computation.
I0825 11:11:34.837563  1899 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:11:34.837570  1899 net.cpp:301] Scale22 needs backward computation.
I0825 11:11:34.837589  1899 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:11:34.837596  1899 net.cpp:301] Convolution22 needs backward computation.
I0825 11:11:34.837601  1899 net.cpp:301] ReLU20 needs backward computation.
I0825 11:11:34.837605  1899 net.cpp:301] Scale21 needs backward computation.
I0825 11:11:34.837623  1899 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:11:34.837631  1899 net.cpp:301] Convolution21 needs backward computation.
I0825 11:11:34.837636  1899 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:11:34.837641  1899 net.cpp:301] ReLU19 needs backward computation.
I0825 11:11:34.837646  1899 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:11:34.837651  1899 net.cpp:301] Scale20 needs backward computation.
I0825 11:11:34.837656  1899 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:11:34.837659  1899 net.cpp:301] Convolution20 needs backward computation.
I0825 11:11:34.837664  1899 net.cpp:301] ReLU18 needs backward computation.
I0825 11:11:34.837668  1899 net.cpp:301] Scale19 needs backward computation.
I0825 11:11:34.837672  1899 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:11:34.837677  1899 net.cpp:301] Convolution19 needs backward computation.
I0825 11:11:34.837682  1899 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:11:34.837687  1899 net.cpp:301] ReLU17 needs backward computation.
I0825 11:11:34.837692  1899 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:11:34.837697  1899 net.cpp:301] Scale18 needs backward computation.
I0825 11:11:34.837700  1899 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:11:34.837710  1899 net.cpp:301] Convolution18 needs backward computation.
I0825 11:11:34.837716  1899 net.cpp:301] ReLU16 needs backward computation.
I0825 11:11:34.837720  1899 net.cpp:301] Scale17 needs backward computation.
I0825 11:11:34.837724  1899 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:11:34.837729  1899 net.cpp:301] Convolution17 needs backward computation.
I0825 11:11:34.837734  1899 net.cpp:301] Scale16 needs backward computation.
I0825 11:11:34.837738  1899 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:11:34.837743  1899 net.cpp:301] Convolution16 needs backward computation.
I0825 11:11:34.837749  1899 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:11:34.837754  1899 net.cpp:301] ReLU15 needs backward computation.
I0825 11:11:34.837759  1899 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:11:34.837767  1899 net.cpp:301] Scale15 needs backward computation.
I0825 11:11:34.837772  1899 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:11:34.837776  1899 net.cpp:301] Convolution15 needs backward computation.
I0825 11:11:34.837780  1899 net.cpp:301] ReLU14 needs backward computation.
I0825 11:11:34.837785  1899 net.cpp:301] Scale14 needs backward computation.
I0825 11:11:34.837790  1899 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:11:34.837793  1899 net.cpp:301] Convolution14 needs backward computation.
I0825 11:11:34.837798  1899 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:11:34.837802  1899 net.cpp:301] ReLU13 needs backward computation.
I0825 11:11:34.837807  1899 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:11:34.837812  1899 net.cpp:301] Scale13 needs backward computation.
I0825 11:11:34.837816  1899 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:11:34.837821  1899 net.cpp:301] Convolution13 needs backward computation.
I0825 11:11:34.837824  1899 net.cpp:301] ReLU12 needs backward computation.
I0825 11:11:34.837829  1899 net.cpp:301] Scale12 needs backward computation.
I0825 11:11:34.837833  1899 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:11:34.837837  1899 net.cpp:301] Convolution12 needs backward computation.
I0825 11:11:34.837842  1899 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:11:34.837847  1899 net.cpp:301] ReLU11 needs backward computation.
I0825 11:11:34.837852  1899 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:11:34.837857  1899 net.cpp:301] Scale11 needs backward computation.
I0825 11:11:34.837860  1899 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:11:34.837865  1899 net.cpp:301] Convolution11 needs backward computation.
I0825 11:11:34.837869  1899 net.cpp:301] ReLU10 needs backward computation.
I0825 11:11:34.837873  1899 net.cpp:301] Scale10 needs backward computation.
I0825 11:11:34.837878  1899 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:11:34.837882  1899 net.cpp:301] Convolution10 needs backward computation.
I0825 11:11:34.837888  1899 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:11:34.837893  1899 net.cpp:301] ReLU9 needs backward computation.
I0825 11:11:34.837896  1899 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:11:34.837901  1899 net.cpp:301] Scale9 needs backward computation.
I0825 11:11:34.837906  1899 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:11:34.837910  1899 net.cpp:301] Convolution9 needs backward computation.
I0825 11:11:34.837915  1899 net.cpp:301] ReLU8 needs backward computation.
I0825 11:11:34.837920  1899 net.cpp:301] Scale8 needs backward computation.
I0825 11:11:34.837924  1899 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:11:34.837929  1899 net.cpp:301] Convolution8 needs backward computation.
I0825 11:11:34.837934  1899 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:11:34.837939  1899 net.cpp:301] ReLU7 needs backward computation.
I0825 11:11:34.837945  1899 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:11:34.837956  1899 net.cpp:301] Scale7 needs backward computation.
I0825 11:11:34.837961  1899 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:11:34.837966  1899 net.cpp:301] Convolution7 needs backward computation.
I0825 11:11:34.837970  1899 net.cpp:301] ReLU6 needs backward computation.
I0825 11:11:34.837975  1899 net.cpp:301] Scale6 needs backward computation.
I0825 11:11:34.837980  1899 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:11:34.837985  1899 net.cpp:301] Convolution6 needs backward computation.
I0825 11:11:34.837990  1899 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:11:34.838001  1899 net.cpp:301] ReLU5 needs backward computation.
I0825 11:11:34.838007  1899 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:11:34.838013  1899 net.cpp:301] Scale5 needs backward computation.
I0825 11:11:34.838017  1899 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:11:34.838023  1899 net.cpp:301] Convolution5 needs backward computation.
I0825 11:11:34.838027  1899 net.cpp:301] ReLU4 needs backward computation.
I0825 11:11:34.838032  1899 net.cpp:301] Scale4 needs backward computation.
I0825 11:11:34.838037  1899 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:11:34.838042  1899 net.cpp:301] Convolution4 needs backward computation.
I0825 11:11:34.838047  1899 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:11:34.838052  1899 net.cpp:301] ReLU3 needs backward computation.
I0825 11:11:34.838057  1899 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:11:34.838063  1899 net.cpp:301] Scale3 needs backward computation.
I0825 11:11:34.838068  1899 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:11:34.838090  1899 net.cpp:301] Convolution3 needs backward computation.
I0825 11:11:34.838100  1899 net.cpp:301] ReLU2 needs backward computation.
I0825 11:11:34.838105  1899 net.cpp:301] Scale2 needs backward computation.
I0825 11:11:34.838110  1899 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:11:34.838114  1899 net.cpp:301] Convolution2 needs backward computation.
I0825 11:11:34.838120  1899 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:11:34.838125  1899 net.cpp:301] ReLU1 needs backward computation.
I0825 11:11:34.838148  1899 net.cpp:301] Scale1 needs backward computation.
I0825 11:11:34.838165  1899 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:11:34.838181  1899 net.cpp:301] Convolution1 needs backward computation.
I0825 11:11:34.838201  1899 net.cpp:303] Data1 does not need backward computation.
I0825 11:11:34.838217  1899 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:11:34.838346  1899 net.cpp:363] Network initialization done.
I0825 11:11:34.840591  1899 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_44.prototxt
I0825 11:11:34.840622  1899 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:11:34.840632  1899 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_44.prototxt
I0825 11:11:34.840803  1899 net.cpp:390] layer_param.include_size():1
I0825 11:11:34.840813  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840819  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840859  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840869  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840873  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840878  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840899  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840909  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840912  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840916  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840935  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840945  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840960  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840965  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840983  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.840992  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.840996  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841001  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841019  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841028  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841032  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841037  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841055  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841063  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841068  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841071  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841074  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841094  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841101  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841107  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841110  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841115  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841121  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841126  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841130  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841150  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841157  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841162  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841166  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841187  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841194  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841199  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841202  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841207  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841213  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841217  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841222  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841240  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841248  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841253  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841255  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841276  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841284  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841287  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841291  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841295  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841315  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841322  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841326  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841331  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841349  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841357  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841361  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841365  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841369  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841373  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841377  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841398  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841404  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841409  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841413  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841426  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841430  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841434  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841440  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841445  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841449  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841467  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841475  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841480  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841482  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841487  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841492  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841497  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841501  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841508  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841512  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841516  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841522  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841526  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841531  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841536  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841540  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841544  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841550  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841554  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841558  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841564  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841568  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841588  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841595  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841600  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841603  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841608  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841614  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841617  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841621  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841640  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841647  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841652  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841656  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841660  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841667  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841671  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841675  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841694  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841702  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841706  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841711  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841714  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841718  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841722  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841740  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841748  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841753  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841756  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841775  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841784  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841787  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841800  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841804  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841809  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841814  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841820  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841823  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841830  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841833  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841837  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841843  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841847  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841851  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841857  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841861  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841866  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841886  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841893  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841897  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841902  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841920  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841928  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841933  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841936  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841954  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841962  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841966  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841970  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.841989  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.841995  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842000  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842003  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842022  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842031  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842034  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842038  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842057  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842066  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842069  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842073  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842092  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842100  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842104  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842108  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842113  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842119  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842123  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842128  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842133  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842137  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842140  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842145  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842149  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842152  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842171  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842180  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842183  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842187  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842190  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842211  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842224  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842229  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842232  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842253  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842260  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842265  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842269  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842273  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842291  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842299  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842303  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842308  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842311  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842329  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842345  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842352  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842356  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842360  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842381  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842388  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842392  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842396  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842401  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842407  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842411  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842416  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842422  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842427  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842430  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842437  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842440  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842444  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842450  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842454  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842458  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842465  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842469  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842489  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842495  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842500  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842504  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842507  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842525  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842533  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842537  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842542  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842550  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842555  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842558  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842588  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842595  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842599  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842603  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842607  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842612  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842615  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842634  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842643  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842658  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842664  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842666  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842671  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842674  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842679  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842684  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842689  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842692  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842697  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842700  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842721  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842728  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842733  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842737  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842741  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842744  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842748  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842767  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842775  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842779  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842783  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842787  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842808  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842815  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842820  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842823  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842828  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842831  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842835  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842855  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842864  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842867  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842871  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842875  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842880  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842883  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842887  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842906  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842914  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842918  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842922  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842926  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842931  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842934  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842954  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842962  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842967  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842969  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842973  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842978  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842981  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.842985  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.842989  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843009  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843017  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843021  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843025  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843029  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843042  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843046  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843051  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843070  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843080  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843082  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843087  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843091  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843096  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843113  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843134  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843142  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843147  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843149  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843154  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843158  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843179  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843186  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843191  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843194  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843199  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843202  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843206  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843214  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843219  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843222  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843226  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843245  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843253  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843257  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843261  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843266  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843269  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843287  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843295  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843299  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843303  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843307  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843312  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843329  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843338  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843341  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843345  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843349  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843353  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843371  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843379  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843384  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843387  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843391  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843395  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843400  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843403  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843423  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843430  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843435  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843439  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843442  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843456  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843461  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843464  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843485  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843493  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843497  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843502  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843504  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843509  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843529  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843538  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843541  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843545  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843549  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843554  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843572  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843581  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843585  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843588  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843592  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843597  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843603  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843607  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843611  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843616  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843619  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843641  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843647  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843652  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843655  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843659  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843663  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843683  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843690  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843695  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843698  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843703  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843706  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843726  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843734  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843739  1899 net.cpp:390] layer_param.include_size():0
I0825 11:11:34.843742  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.843746  1899 net.cpp:390] layer_param.include_size():1
I0825 11:11:34.843766  1899 net.cpp:391] layer_param.exclude_size():0
I0825 11:11:34.845024  1899 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution12"
  top: "Convolution12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Convolution12"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution13"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution14"
  top: "Convolution14"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Convolution14"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution15"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution23"
  top: "Convolution23"
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Convolution23"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Eltwise10"
  bottom: "Convolution24"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution25"
  top: "Convolution25"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Convolution25"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution26"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution27"
  top: "Convolution27"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Convolution27"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution28"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution29"
  top: "Convolution29"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Convolution29"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution30"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
 
I0825 11:11:34.845787  1899 layer_factory.hpp:77] Creating layer Data1
I0825 11:11:34.845870  1899 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0825 11:11:34.845899  1899 net.cpp:128] Creating Layer Data1
I0825 11:11:34.845906  1899 net.cpp:522] Data1 -> Data1
I0825 11:11:34.845918  1899 net.cpp:522] Data1 -> Data2
I0825 11:11:34.846101  1899 data_layer.cpp:45] output data size: 10,3,32,32
I0825 11:11:34.854216  1899 net.cpp:172] Setting up Data1
I0825 11:11:34.854260  1899 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0825 11:11:34.854267  1899 net.cpp:186] Top shape: 10 (10)
I0825 11:11:34.854271  1899 net.cpp:194] Memory required for data: 122920
I0825 11:11:34.854277  1899 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0825 11:11:34.854288  1899 net.cpp:128] Creating Layer Data2_Data1_1_split
I0825 11:11:34.854295  1899 net.cpp:558] Data2_Data1_1_split <- Data2
I0825 11:11:34.854305  1899 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0825 11:11:34.854315  1899 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0825 11:11:34.855063  1899 net.cpp:172] Setting up Data2_Data1_1_split
I0825 11:11:34.855113  1899 net.cpp:186] Top shape: 10 (10)
I0825 11:11:34.855126  1899 net.cpp:186] Top shape: 10 (10)
I0825 11:11:34.855141  1899 net.cpp:194] Memory required for data: 123000
I0825 11:11:34.855150  1899 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:11:34.855203  1899 net.cpp:128] Creating Layer Convolution1
I0825 11:11:34.855221  1899 net.cpp:558] Convolution1 <- Data1
I0825 11:11:34.855249  1899 net.cpp:522] Convolution1 -> Convolution1
I0825 11:11:34.862916  1899 net.cpp:172] Setting up Convolution1
I0825 11:11:34.862943  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.862947  1899 net.cpp:194] Memory required for data: 778360
I0825 11:11:34.862967  1899 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:11:34.862980  1899 net.cpp:128] Creating Layer BatchNorm1
I0825 11:11:34.862985  1899 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:11:34.862995  1899 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:11:34.863309  1899 net.cpp:172] Setting up BatchNorm1
I0825 11:11:34.863317  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.863322  1899 net.cpp:194] Memory required for data: 1433720
I0825 11:11:34.863333  1899 layer_factory.hpp:77] Creating layer Scale1
I0825 11:11:34.863345  1899 net.cpp:128] Creating Layer Scale1
I0825 11:11:34.863350  1899 net.cpp:558] Scale1 <- Convolution1
I0825 11:11:34.863356  1899 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:11:34.863415  1899 layer_factory.hpp:77] Creating layer Scale1
I0825 11:11:34.863590  1899 net.cpp:172] Setting up Scale1
I0825 11:11:34.863597  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.863601  1899 net.cpp:194] Memory required for data: 2089080
I0825 11:11:34.863610  1899 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:11:34.863620  1899 net.cpp:128] Creating Layer ReLU1
I0825 11:11:34.863625  1899 net.cpp:558] ReLU1 <- Convolution1
I0825 11:11:34.863629  1899 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:11:34.867110  1899 net.cpp:172] Setting up ReLU1
I0825 11:11:34.867167  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.867195  1899 net.cpp:194] Memory required for data: 2744440
I0825 11:11:34.867223  1899 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:11:34.867264  1899 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:11:34.867303  1899 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:11:34.867336  1899 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:11:34.867372  1899 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:11:34.867507  1899 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:11:34.867544  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.867574  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.867600  1899 net.cpp:194] Memory required for data: 4055160
I0825 11:11:34.867627  1899 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:11:34.867668  1899 net.cpp:128] Creating Layer Convolution2
I0825 11:11:34.867696  1899 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:11:34.867727  1899 net.cpp:522] Convolution2 -> Convolution2
I0825 11:11:34.871788  1899 net.cpp:172] Setting up Convolution2
I0825 11:11:34.871814  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.871834  1899 net.cpp:194] Memory required for data: 4710520
I0825 11:11:34.871860  1899 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:11:34.871886  1899 net.cpp:128] Creating Layer BatchNorm2
I0825 11:11:34.871896  1899 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:11:34.871918  1899 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:11:34.872313  1899 net.cpp:172] Setting up BatchNorm2
I0825 11:11:34.872326  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.872339  1899 net.cpp:194] Memory required for data: 5365880
I0825 11:11:34.872360  1899 layer_factory.hpp:77] Creating layer Scale2
I0825 11:11:34.872380  1899 net.cpp:128] Creating Layer Scale2
I0825 11:11:34.872387  1899 net.cpp:558] Scale2 <- Convolution2
I0825 11:11:34.872401  1899 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:11:34.872483  1899 layer_factory.hpp:77] Creating layer Scale2
I0825 11:11:34.872699  1899 net.cpp:172] Setting up Scale2
I0825 11:11:34.872715  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.872722  1899 net.cpp:194] Memory required for data: 6021240
I0825 11:11:34.872742  1899 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:11:34.872758  1899 net.cpp:128] Creating Layer ReLU2
I0825 11:11:34.872766  1899 net.cpp:558] ReLU2 <- Convolution2
I0825 11:11:34.872779  1899 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:11:34.873106  1899 net.cpp:172] Setting up ReLU2
I0825 11:11:34.873157  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.873180  1899 net.cpp:194] Memory required for data: 6676600
I0825 11:11:34.873204  1899 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:11:34.873240  1899 net.cpp:128] Creating Layer Convolution3
I0825 11:11:34.873263  1899 net.cpp:558] Convolution3 <- Convolution2
I0825 11:11:34.873291  1899 net.cpp:522] Convolution3 -> Convolution3
I0825 11:11:34.880908  1899 net.cpp:172] Setting up Convolution3
I0825 11:11:34.880937  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.880944  1899 net.cpp:194] Memory required for data: 7331960
I0825 11:11:34.880969  1899 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:11:34.880982  1899 net.cpp:128] Creating Layer BatchNorm3
I0825 11:11:34.881029  1899 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:11:34.881055  1899 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:11:34.881484  1899 net.cpp:172] Setting up BatchNorm3
I0825 11:11:34.881500  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.881543  1899 net.cpp:194] Memory required for data: 7987320
I0825 11:11:34.881573  1899 layer_factory.hpp:77] Creating layer Scale3
I0825 11:11:34.881620  1899 net.cpp:128] Creating Layer Scale3
I0825 11:11:34.881629  1899 net.cpp:558] Scale3 <- Convolution3
I0825 11:11:34.881639  1899 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:11:34.881705  1899 layer_factory.hpp:77] Creating layer Scale3
I0825 11:11:34.881886  1899 net.cpp:172] Setting up Scale3
I0825 11:11:34.881896  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.881899  1899 net.cpp:194] Memory required for data: 8642680
I0825 11:11:34.881908  1899 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:11:34.881917  1899 net.cpp:128] Creating Layer Eltwise1
I0825 11:11:34.881922  1899 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:11:34.881938  1899 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:11:34.881947  1899 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:11:34.881979  1899 net.cpp:172] Setting up Eltwise1
I0825 11:11:34.881989  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.881994  1899 net.cpp:194] Memory required for data: 9298040
I0825 11:11:34.881999  1899 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:11:34.882004  1899 net.cpp:128] Creating Layer ReLU3
I0825 11:11:34.882009  1899 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:11:34.882014  1899 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:11:34.884105  1899 net.cpp:172] Setting up ReLU3
I0825 11:11:34.884129  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.884133  1899 net.cpp:194] Memory required for data: 9953400
I0825 11:11:34.884137  1899 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:11:34.884148  1899 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:11:34.884153  1899 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:11:34.884160  1899 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:11:34.884168  1899 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:11:34.884229  1899 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:11:34.884238  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.884243  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.884246  1899 net.cpp:194] Memory required for data: 11264120
I0825 11:11:34.884250  1899 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:11:34.884263  1899 net.cpp:128] Creating Layer Convolution4
I0825 11:11:34.884268  1899 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:11:34.884277  1899 net.cpp:522] Convolution4 -> Convolution4
I0825 11:11:34.890923  1899 net.cpp:172] Setting up Convolution4
I0825 11:11:34.890949  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.890954  1899 net.cpp:194] Memory required for data: 11919480
I0825 11:11:34.890964  1899 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:11:34.890974  1899 net.cpp:128] Creating Layer BatchNorm4
I0825 11:11:34.890980  1899 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:11:34.890988  1899 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:11:34.891299  1899 net.cpp:172] Setting up BatchNorm4
I0825 11:11:34.891306  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.891310  1899 net.cpp:194] Memory required for data: 12574840
I0825 11:11:34.891320  1899 layer_factory.hpp:77] Creating layer Scale4
I0825 11:11:34.891333  1899 net.cpp:128] Creating Layer Scale4
I0825 11:11:34.891338  1899 net.cpp:558] Scale4 <- Convolution4
I0825 11:11:34.891343  1899 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:11:34.891397  1899 layer_factory.hpp:77] Creating layer Scale4
I0825 11:11:34.891571  1899 net.cpp:172] Setting up Scale4
I0825 11:11:34.891578  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.891582  1899 net.cpp:194] Memory required for data: 13230200
I0825 11:11:34.891590  1899 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:11:34.891598  1899 net.cpp:128] Creating Layer ReLU4
I0825 11:11:34.891602  1899 net.cpp:558] ReLU4 <- Convolution4
I0825 11:11:34.891608  1899 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:11:34.892976  1899 net.cpp:172] Setting up ReLU4
I0825 11:11:34.892993  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.892997  1899 net.cpp:194] Memory required for data: 13885560
I0825 11:11:34.893002  1899 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:11:34.893019  1899 net.cpp:128] Creating Layer Convolution5
I0825 11:11:34.893024  1899 net.cpp:558] Convolution5 <- Convolution4
I0825 11:11:34.893031  1899 net.cpp:522] Convolution5 -> Convolution5
I0825 11:11:34.899852  1899 net.cpp:172] Setting up Convolution5
I0825 11:11:34.899904  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.899921  1899 net.cpp:194] Memory required for data: 14540920
I0825 11:11:34.899957  1899 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:11:34.899981  1899 net.cpp:128] Creating Layer BatchNorm5
I0825 11:11:34.899999  1899 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:11:34.900019  1899 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:11:34.900362  1899 net.cpp:172] Setting up BatchNorm5
I0825 11:11:34.900388  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.900405  1899 net.cpp:194] Memory required for data: 15196280
I0825 11:11:34.900434  1899 layer_factory.hpp:77] Creating layer Scale5
I0825 11:11:34.900454  1899 net.cpp:128] Creating Layer Scale5
I0825 11:11:34.900470  1899 net.cpp:558] Scale5 <- Convolution5
I0825 11:11:34.900490  1899 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:11:34.900568  1899 layer_factory.hpp:77] Creating layer Scale5
I0825 11:11:34.900763  1899 net.cpp:172] Setting up Scale5
I0825 11:11:34.900785  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.900801  1899 net.cpp:194] Memory required for data: 15851640
I0825 11:11:34.900823  1899 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:11:34.900846  1899 net.cpp:128] Creating Layer Eltwise2
I0825 11:11:34.900863  1899 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:11:34.900883  1899 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:11:34.900902  1899 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:11:34.900955  1899 net.cpp:172] Setting up Eltwise2
I0825 11:11:34.900975  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.900991  1899 net.cpp:194] Memory required for data: 16507000
I0825 11:11:34.901008  1899 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:11:34.901028  1899 net.cpp:128] Creating Layer ReLU5
I0825 11:11:34.901044  1899 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:11:34.901063  1899 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:11:34.901942  1899 net.cpp:172] Setting up ReLU5
I0825 11:11:34.901985  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.901993  1899 net.cpp:194] Memory required for data: 17162360
I0825 11:11:34.902000  1899 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:11:34.902009  1899 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:11:34.902014  1899 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:11:34.902024  1899 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:11:34.902032  1899 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:11:34.902102  1899 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:11:34.902110  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.902117  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.902120  1899 net.cpp:194] Memory required for data: 18473080
I0825 11:11:34.902124  1899 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:11:34.902137  1899 net.cpp:128] Creating Layer Convolution6
I0825 11:11:34.902142  1899 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:11:34.902150  1899 net.cpp:522] Convolution6 -> Convolution6
I0825 11:11:34.914721  1899 net.cpp:172] Setting up Convolution6
I0825 11:11:34.914744  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.914749  1899 net.cpp:194] Memory required for data: 19128440
I0825 11:11:34.914759  1899 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:11:34.914770  1899 net.cpp:128] Creating Layer BatchNorm6
I0825 11:11:34.914775  1899 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:11:34.914783  1899 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:11:34.915093  1899 net.cpp:172] Setting up BatchNorm6
I0825 11:11:34.915100  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.915104  1899 net.cpp:194] Memory required for data: 19783800
I0825 11:11:34.915114  1899 layer_factory.hpp:77] Creating layer Scale6
I0825 11:11:34.915122  1899 net.cpp:128] Creating Layer Scale6
I0825 11:11:34.915125  1899 net.cpp:558] Scale6 <- Convolution6
I0825 11:11:34.915132  1899 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:11:34.915205  1899 layer_factory.hpp:77] Creating layer Scale6
I0825 11:11:34.915385  1899 net.cpp:172] Setting up Scale6
I0825 11:11:34.915392  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.915396  1899 net.cpp:194] Memory required for data: 20439160
I0825 11:11:34.915405  1899 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:11:34.915410  1899 net.cpp:128] Creating Layer ReLU6
I0825 11:11:34.915416  1899 net.cpp:558] ReLU6 <- Convolution6
I0825 11:11:34.915422  1899 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:11:34.916771  1899 net.cpp:172] Setting up ReLU6
I0825 11:11:34.916786  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.916791  1899 net.cpp:194] Memory required for data: 21094520
I0825 11:11:34.916795  1899 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:11:34.916810  1899 net.cpp:128] Creating Layer Convolution7
I0825 11:11:34.916815  1899 net.cpp:558] Convolution7 <- Convolution6
I0825 11:11:34.916824  1899 net.cpp:522] Convolution7 -> Convolution7
I0825 11:11:34.923571  1899 net.cpp:172] Setting up Convolution7
I0825 11:11:34.923597  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.923602  1899 net.cpp:194] Memory required for data: 21749880
I0825 11:11:34.923611  1899 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:11:34.923627  1899 net.cpp:128] Creating Layer BatchNorm7
I0825 11:11:34.923632  1899 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:11:34.923640  1899 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:11:34.923959  1899 net.cpp:172] Setting up BatchNorm7
I0825 11:11:34.923966  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.923970  1899 net.cpp:194] Memory required for data: 22405240
I0825 11:11:34.923980  1899 layer_factory.hpp:77] Creating layer Scale7
I0825 11:11:34.923987  1899 net.cpp:128] Creating Layer Scale7
I0825 11:11:34.923991  1899 net.cpp:558] Scale7 <- Convolution7
I0825 11:11:34.923997  1899 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:11:34.924053  1899 layer_factory.hpp:77] Creating layer Scale7
I0825 11:11:34.924226  1899 net.cpp:172] Setting up Scale7
I0825 11:11:34.924234  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.924238  1899 net.cpp:194] Memory required for data: 23060600
I0825 11:11:34.924247  1899 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:11:34.924257  1899 net.cpp:128] Creating Layer Eltwise3
I0825 11:11:34.924262  1899 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:11:34.924266  1899 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:11:34.924273  1899 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:11:34.924304  1899 net.cpp:172] Setting up Eltwise3
I0825 11:11:34.924311  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.924315  1899 net.cpp:194] Memory required for data: 23715960
I0825 11:11:34.924319  1899 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:11:34.924325  1899 net.cpp:128] Creating Layer ReLU7
I0825 11:11:34.924329  1899 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:11:34.924337  1899 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:11:34.925647  1899 net.cpp:172] Setting up ReLU7
I0825 11:11:34.925658  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.925662  1899 net.cpp:194] Memory required for data: 24371320
I0825 11:11:34.925668  1899 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:11:34.925674  1899 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:11:34.925679  1899 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:11:34.925688  1899 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:11:34.925698  1899 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:11:34.925756  1899 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:11:34.925763  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.925770  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.925773  1899 net.cpp:194] Memory required for data: 25682040
I0825 11:11:34.925792  1899 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:11:34.925806  1899 net.cpp:128] Creating Layer Convolution8
I0825 11:11:34.925810  1899 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:11:34.925819  1899 net.cpp:522] Convolution8 -> Convolution8
I0825 11:11:34.932543  1899 net.cpp:172] Setting up Convolution8
I0825 11:11:34.932562  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.932567  1899 net.cpp:194] Memory required for data: 26337400
I0825 11:11:34.932577  1899 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:11:34.932586  1899 net.cpp:128] Creating Layer BatchNorm8
I0825 11:11:34.932591  1899 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:11:34.932600  1899 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:11:34.932917  1899 net.cpp:172] Setting up BatchNorm8
I0825 11:11:34.932925  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.932929  1899 net.cpp:194] Memory required for data: 26992760
I0825 11:11:34.932938  1899 layer_factory.hpp:77] Creating layer Scale8
I0825 11:11:34.932945  1899 net.cpp:128] Creating Layer Scale8
I0825 11:11:34.932950  1899 net.cpp:558] Scale8 <- Convolution8
I0825 11:11:34.932957  1899 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:11:34.933010  1899 layer_factory.hpp:77] Creating layer Scale8
I0825 11:11:34.933179  1899 net.cpp:172] Setting up Scale8
I0825 11:11:34.933188  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.933192  1899 net.cpp:194] Memory required for data: 27648120
I0825 11:11:34.933200  1899 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:11:34.933207  1899 net.cpp:128] Creating Layer ReLU8
I0825 11:11:34.933212  1899 net.cpp:558] ReLU8 <- Convolution8
I0825 11:11:34.933217  1899 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:11:34.936749  1899 net.cpp:172] Setting up ReLU8
I0825 11:11:34.936776  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.936780  1899 net.cpp:194] Memory required for data: 28303480
I0825 11:11:34.936785  1899 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:11:34.936801  1899 net.cpp:128] Creating Layer Convolution9
I0825 11:11:34.936834  1899 net.cpp:558] Convolution9 <- Convolution8
I0825 11:11:34.936847  1899 net.cpp:522] Convolution9 -> Convolution9
I0825 11:11:34.947340  1899 net.cpp:172] Setting up Convolution9
I0825 11:11:34.947366  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.947371  1899 net.cpp:194] Memory required for data: 28958840
I0825 11:11:34.947382  1899 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:11:34.947392  1899 net.cpp:128] Creating Layer BatchNorm9
I0825 11:11:34.947432  1899 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:11:34.947444  1899 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:11:34.947767  1899 net.cpp:172] Setting up BatchNorm9
I0825 11:11:34.947778  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.947783  1899 net.cpp:194] Memory required for data: 29614200
I0825 11:11:34.947793  1899 layer_factory.hpp:77] Creating layer Scale9
I0825 11:11:34.947803  1899 net.cpp:128] Creating Layer Scale9
I0825 11:11:34.947808  1899 net.cpp:558] Scale9 <- Convolution9
I0825 11:11:34.947813  1899 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:11:34.947870  1899 layer_factory.hpp:77] Creating layer Scale9
I0825 11:11:34.948042  1899 net.cpp:172] Setting up Scale9
I0825 11:11:34.948055  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.948058  1899 net.cpp:194] Memory required for data: 30269560
I0825 11:11:34.948066  1899 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:11:34.948073  1899 net.cpp:128] Creating Layer Eltwise4
I0825 11:11:34.948077  1899 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:11:34.948083  1899 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:11:34.948092  1899 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:11:34.948124  1899 net.cpp:172] Setting up Eltwise4
I0825 11:11:34.948134  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.948153  1899 net.cpp:194] Memory required for data: 30924920
I0825 11:11:34.948158  1899 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:11:34.948164  1899 net.cpp:128] Creating Layer ReLU9
I0825 11:11:34.948169  1899 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:11:34.948174  1899 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:11:34.949404  1899 net.cpp:172] Setting up ReLU9
I0825 11:11:34.949422  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.949427  1899 net.cpp:194] Memory required for data: 31580280
I0825 11:11:34.949431  1899 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:11:34.949442  1899 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:11:34.949446  1899 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:11:34.949455  1899 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:11:34.949465  1899 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:11:34.949523  1899 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:11:34.949564  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.949575  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.949580  1899 net.cpp:194] Memory required for data: 32891000
I0825 11:11:34.949585  1899 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:11:34.949602  1899 net.cpp:128] Creating Layer Convolution10
I0825 11:11:34.949607  1899 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:11:34.949615  1899 net.cpp:522] Convolution10 -> Convolution10
I0825 11:11:34.956281  1899 net.cpp:172] Setting up Convolution10
I0825 11:11:34.956336  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.956354  1899 net.cpp:194] Memory required for data: 33546360
I0825 11:11:34.956389  1899 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:11:34.956415  1899 net.cpp:128] Creating Layer BatchNorm10
I0825 11:11:34.956430  1899 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:11:34.956446  1899 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:11:34.956784  1899 net.cpp:172] Setting up BatchNorm10
I0825 11:11:34.956796  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.956800  1899 net.cpp:194] Memory required for data: 34201720
I0825 11:11:34.956811  1899 layer_factory.hpp:77] Creating layer Scale10
I0825 11:11:34.956818  1899 net.cpp:128] Creating Layer Scale10
I0825 11:11:34.956823  1899 net.cpp:558] Scale10 <- Convolution10
I0825 11:11:34.956830  1899 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:11:34.956885  1899 layer_factory.hpp:77] Creating layer Scale10
I0825 11:11:34.957059  1899 net.cpp:172] Setting up Scale10
I0825 11:11:34.957070  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.957074  1899 net.cpp:194] Memory required for data: 34857080
I0825 11:11:34.957082  1899 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:11:34.957092  1899 net.cpp:128] Creating Layer ReLU10
I0825 11:11:34.957099  1899 net.cpp:558] ReLU10 <- Convolution10
I0825 11:11:34.957106  1899 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:11:34.958349  1899 net.cpp:172] Setting up ReLU10
I0825 11:11:34.958364  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.958369  1899 net.cpp:194] Memory required for data: 35512440
I0825 11:11:34.958374  1899 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:11:34.958389  1899 net.cpp:128] Creating Layer Convolution11
I0825 11:11:34.958395  1899 net.cpp:558] Convolution11 <- Convolution10
I0825 11:11:34.958403  1899 net.cpp:522] Convolution11 -> Convolution11
I0825 11:11:34.967108  1899 net.cpp:172] Setting up Convolution11
I0825 11:11:34.967150  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.967167  1899 net.cpp:194] Memory required for data: 36167800
I0825 11:11:34.967191  1899 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:11:34.967213  1899 net.cpp:128] Creating Layer BatchNorm11
I0825 11:11:34.967231  1899 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:11:34.967252  1899 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:11:34.967599  1899 net.cpp:172] Setting up BatchNorm11
I0825 11:11:34.967624  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.967640  1899 net.cpp:194] Memory required for data: 36823160
I0825 11:11:34.967664  1899 layer_factory.hpp:77] Creating layer Scale11
I0825 11:11:34.967687  1899 net.cpp:128] Creating Layer Scale11
I0825 11:11:34.967703  1899 net.cpp:558] Scale11 <- Convolution11
I0825 11:11:34.967722  1899 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:11:34.967797  1899 layer_factory.hpp:77] Creating layer Scale11
I0825 11:11:34.967991  1899 net.cpp:172] Setting up Scale11
I0825 11:11:34.968014  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.968030  1899 net.cpp:194] Memory required for data: 37478520
I0825 11:11:34.968051  1899 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:11:34.968072  1899 net.cpp:128] Creating Layer Eltwise5
I0825 11:11:34.968089  1899 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:11:34.968106  1899 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:11:34.968125  1899 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:11:34.968176  1899 net.cpp:172] Setting up Eltwise5
I0825 11:11:34.968196  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.968211  1899 net.cpp:194] Memory required for data: 38133880
I0825 11:11:34.968228  1899 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:11:34.968246  1899 net.cpp:128] Creating Layer ReLU11
I0825 11:11:34.968262  1899 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:11:34.968282  1899 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:11:34.969169  1899 net.cpp:172] Setting up ReLU11
I0825 11:11:34.969210  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.969225  1899 net.cpp:194] Memory required for data: 38789240
I0825 11:11:34.969240  1899 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:11:34.969259  1899 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:11:34.969272  1899 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:11:34.969292  1899 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:11:34.969311  1899 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:11:34.969391  1899 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:11:34.969409  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.969425  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.969439  1899 net.cpp:194] Memory required for data: 40099960
I0825 11:11:34.969452  1899 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:11:34.969475  1899 net.cpp:128] Creating Layer Convolution12
I0825 11:11:34.969491  1899 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:11:34.969511  1899 net.cpp:522] Convolution12 -> Convolution12
I0825 11:11:34.976830  1899 net.cpp:172] Setting up Convolution12
I0825 11:11:34.976857  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.976862  1899 net.cpp:194] Memory required for data: 40755320
I0825 11:11:34.976874  1899 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:11:34.976886  1899 net.cpp:128] Creating Layer BatchNorm12
I0825 11:11:34.976899  1899 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:11:34.976909  1899 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:11:34.977233  1899 net.cpp:172] Setting up BatchNorm12
I0825 11:11:34.977243  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.977247  1899 net.cpp:194] Memory required for data: 41410680
I0825 11:11:34.977257  1899 layer_factory.hpp:77] Creating layer Scale12
I0825 11:11:34.977272  1899 net.cpp:128] Creating Layer Scale12
I0825 11:11:34.977278  1899 net.cpp:558] Scale12 <- Convolution12
I0825 11:11:34.977284  1899 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:11:34.977345  1899 layer_factory.hpp:77] Creating layer Scale12
I0825 11:11:34.977525  1899 net.cpp:172] Setting up Scale12
I0825 11:11:34.977536  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.977558  1899 net.cpp:194] Memory required for data: 42066040
I0825 11:11:34.977568  1899 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:11:34.977578  1899 net.cpp:128] Creating Layer ReLU12
I0825 11:11:34.977617  1899 net.cpp:558] ReLU12 <- Convolution12
I0825 11:11:34.977628  1899 net.cpp:509] ReLU12 -> Convolution12 (in-place)
I0825 11:11:34.978905  1899 net.cpp:172] Setting up ReLU12
I0825 11:11:34.978924  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.978929  1899 net.cpp:194] Memory required for data: 42721400
I0825 11:11:34.978934  1899 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:11:34.978946  1899 net.cpp:128] Creating Layer Convolution13
I0825 11:11:34.978951  1899 net.cpp:558] Convolution13 <- Convolution12
I0825 11:11:34.978960  1899 net.cpp:522] Convolution13 -> Convolution13
I0825 11:11:34.985776  1899 net.cpp:172] Setting up Convolution13
I0825 11:11:34.985800  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.985805  1899 net.cpp:194] Memory required for data: 43376760
I0825 11:11:34.985816  1899 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:11:34.985826  1899 net.cpp:128] Creating Layer BatchNorm13
I0825 11:11:34.985834  1899 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:11:34.985841  1899 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:11:34.986168  1899 net.cpp:172] Setting up BatchNorm13
I0825 11:11:34.986178  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.986182  1899 net.cpp:194] Memory required for data: 44032120
I0825 11:11:34.986193  1899 layer_factory.hpp:77] Creating layer Scale13
I0825 11:11:34.986238  1899 net.cpp:128] Creating Layer Scale13
I0825 11:11:34.986248  1899 net.cpp:558] Scale13 <- Convolution13
I0825 11:11:34.986254  1899 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:11:34.986320  1899 layer_factory.hpp:77] Creating layer Scale13
I0825 11:11:34.986510  1899 net.cpp:172] Setting up Scale13
I0825 11:11:34.986521  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.986526  1899 net.cpp:194] Memory required for data: 44687480
I0825 11:11:34.986536  1899 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:11:34.986574  1899 net.cpp:128] Creating Layer Eltwise6
I0825 11:11:34.986583  1899 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:11:34.986589  1899 net.cpp:558] Eltwise6 <- Convolution13
I0825 11:11:34.986596  1899 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:11:34.986635  1899 net.cpp:172] Setting up Eltwise6
I0825 11:11:34.986645  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.986649  1899 net.cpp:194] Memory required for data: 45342840
I0825 11:11:34.986654  1899 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:11:34.986661  1899 net.cpp:128] Creating Layer ReLU13
I0825 11:11:34.986665  1899 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:11:34.986671  1899 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:11:34.987907  1899 net.cpp:172] Setting up ReLU13
I0825 11:11:34.987924  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.987929  1899 net.cpp:194] Memory required for data: 45998200
I0825 11:11:34.987933  1899 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:11:34.987942  1899 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:11:34.987947  1899 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:11:34.987954  1899 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:11:34.987962  1899 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:11:34.988023  1899 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:11:34.988059  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.988080  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.988095  1899 net.cpp:194] Memory required for data: 47308920
I0825 11:11:34.988112  1899 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:11:34.988137  1899 net.cpp:128] Creating Layer Convolution14
I0825 11:11:34.988155  1899 net.cpp:558] Convolution14 <- Eltwise6_ReLU13_0_split_0
I0825 11:11:34.988189  1899 net.cpp:522] Convolution14 -> Convolution14
I0825 11:11:34.999006  1899 net.cpp:172] Setting up Convolution14
I0825 11:11:34.999032  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.999037  1899 net.cpp:194] Memory required for data: 47964280
I0825 11:11:34.999047  1899 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:11:34.999055  1899 net.cpp:128] Creating Layer BatchNorm14
I0825 11:11:34.999060  1899 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:11:34.999069  1899 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:11:34.999390  1899 net.cpp:172] Setting up BatchNorm14
I0825 11:11:34.999402  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.999408  1899 net.cpp:194] Memory required for data: 48619640
I0825 11:11:34.999418  1899 layer_factory.hpp:77] Creating layer Scale14
I0825 11:11:34.999425  1899 net.cpp:128] Creating Layer Scale14
I0825 11:11:34.999429  1899 net.cpp:558] Scale14 <- Convolution14
I0825 11:11:34.999438  1899 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:11:34.999491  1899 layer_factory.hpp:77] Creating layer Scale14
I0825 11:11:34.999708  1899 net.cpp:172] Setting up Scale14
I0825 11:11:34.999722  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:34.999727  1899 net.cpp:194] Memory required for data: 49275000
I0825 11:11:34.999735  1899 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:11:34.999742  1899 net.cpp:128] Creating Layer ReLU14
I0825 11:11:34.999747  1899 net.cpp:558] ReLU14 <- Convolution14
I0825 11:11:34.999752  1899 net.cpp:509] ReLU14 -> Convolution14 (in-place)
I0825 11:11:35.002650  1899 net.cpp:172] Setting up ReLU14
I0825 11:11:35.002665  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.002670  1899 net.cpp:194] Memory required for data: 49930360
I0825 11:11:35.002673  1899 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:11:35.002687  1899 net.cpp:128] Creating Layer Convolution15
I0825 11:11:35.002692  1899 net.cpp:558] Convolution15 <- Convolution14
I0825 11:11:35.002703  1899 net.cpp:522] Convolution15 -> Convolution15
I0825 11:11:35.006383  1899 net.cpp:172] Setting up Convolution15
I0825 11:11:35.006430  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.006451  1899 net.cpp:194] Memory required for data: 50585720
I0825 11:11:35.006474  1899 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:11:35.006494  1899 net.cpp:128] Creating Layer BatchNorm15
I0825 11:11:35.006512  1899 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:11:35.006531  1899 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:11:35.006872  1899 net.cpp:172] Setting up BatchNorm15
I0825 11:11:35.006897  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.006916  1899 net.cpp:194] Memory required for data: 51241080
I0825 11:11:35.006937  1899 layer_factory.hpp:77] Creating layer Scale15
I0825 11:11:35.006958  1899 net.cpp:128] Creating Layer Scale15
I0825 11:11:35.006971  1899 net.cpp:558] Scale15 <- Convolution15
I0825 11:11:35.006988  1899 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:11:35.007066  1899 layer_factory.hpp:77] Creating layer Scale15
I0825 11:11:35.007263  1899 net.cpp:172] Setting up Scale15
I0825 11:11:35.007288  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.007305  1899 net.cpp:194] Memory required for data: 51896440
I0825 11:11:35.007335  1899 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:11:35.007357  1899 net.cpp:128] Creating Layer Eltwise7
I0825 11:11:35.007375  1899 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:11:35.007393  1899 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:11:35.007412  1899 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:11:35.007467  1899 net.cpp:172] Setting up Eltwise7
I0825 11:11:35.007488  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.007503  1899 net.cpp:194] Memory required for data: 52551800
I0825 11:11:35.007519  1899 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:11:35.007547  1899 net.cpp:128] Creating Layer ReLU15
I0825 11:11:35.007565  1899 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:11:35.007585  1899 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:11:35.008507  1899 net.cpp:172] Setting up ReLU15
I0825 11:11:35.008550  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.008569  1899 net.cpp:194] Memory required for data: 53207160
I0825 11:11:35.008585  1899 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:11:35.008607  1899 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:11:35.008625  1899 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:11:35.008646  1899 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:11:35.008669  1899 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:11:35.008752  1899 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:11:35.008774  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.008793  1899 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:11:35.008810  1899 net.cpp:194] Memory required for data: 54517880
I0825 11:11:35.008826  1899 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:11:35.008852  1899 net.cpp:128] Creating Layer Convolution16
I0825 11:11:35.008868  1899 net.cpp:558] Convolution16 <- Eltwise7_ReLU15_0_split_0
I0825 11:11:35.008890  1899 net.cpp:522] Convolution16 -> Convolution16
I0825 11:11:35.020936  1899 net.cpp:172] Setting up Convolution16
I0825 11:11:35.020963  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.020968  1899 net.cpp:194] Memory required for data: 54845560
I0825 11:11:35.020978  1899 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:11:35.020989  1899 net.cpp:128] Creating Layer BatchNorm16
I0825 11:11:35.020994  1899 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:11:35.021001  1899 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:11:35.021315  1899 net.cpp:172] Setting up BatchNorm16
I0825 11:11:35.021327  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.021333  1899 net.cpp:194] Memory required for data: 55173240
I0825 11:11:35.021343  1899 layer_factory.hpp:77] Creating layer Scale16
I0825 11:11:35.021350  1899 net.cpp:128] Creating Layer Scale16
I0825 11:11:35.021355  1899 net.cpp:558] Scale16 <- Convolution16
I0825 11:11:35.021360  1899 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:11:35.021419  1899 layer_factory.hpp:77] Creating layer Scale16
I0825 11:11:35.021641  1899 net.cpp:172] Setting up Scale16
I0825 11:11:35.021654  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.021661  1899 net.cpp:194] Memory required for data: 55500920
I0825 11:11:35.021669  1899 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:11:35.021682  1899 net.cpp:128] Creating Layer Convolution17
I0825 11:11:35.021687  1899 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_1
I0825 11:11:35.021697  1899 net.cpp:522] Convolution17 -> Convolution17
I0825 11:11:35.027606  1899 net.cpp:172] Setting up Convolution17
I0825 11:11:35.027631  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.027637  1899 net.cpp:194] Memory required for data: 55828600
I0825 11:11:35.027647  1899 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:11:35.027657  1899 net.cpp:128] Creating Layer BatchNorm17
I0825 11:11:35.027662  1899 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:11:35.027669  1899 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:11:35.027978  1899 net.cpp:172] Setting up BatchNorm17
I0825 11:11:35.027989  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.027995  1899 net.cpp:194] Memory required for data: 56156280
I0825 11:11:35.028005  1899 layer_factory.hpp:77] Creating layer Scale17
I0825 11:11:35.028014  1899 net.cpp:128] Creating Layer Scale17
I0825 11:11:35.028019  1899 net.cpp:558] Scale17 <- Convolution17
I0825 11:11:35.028025  1899 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:11:35.028082  1899 layer_factory.hpp:77] Creating layer Scale17
I0825 11:11:35.028318  1899 net.cpp:172] Setting up Scale17
I0825 11:11:35.028332  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.028338  1899 net.cpp:194] Memory required for data: 56483960
I0825 11:11:35.028347  1899 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:11:35.028353  1899 net.cpp:128] Creating Layer ReLU16
I0825 11:11:35.028358  1899 net.cpp:558] ReLU16 <- Convolution17
I0825 11:11:35.028367  1899 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0825 11:11:35.029667  1899 net.cpp:172] Setting up ReLU16
I0825 11:11:35.029685  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.029690  1899 net.cpp:194] Memory required for data: 56811640
I0825 11:11:35.029695  1899 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:11:35.029709  1899 net.cpp:128] Creating Layer Convolution18
I0825 11:11:35.029716  1899 net.cpp:558] Convolution18 <- Convolution17
I0825 11:11:35.029724  1899 net.cpp:522] Convolution18 -> Convolution18
I0825 11:11:35.036635  1899 net.cpp:172] Setting up Convolution18
I0825 11:11:35.036681  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.036700  1899 net.cpp:194] Memory required for data: 57139320
I0825 11:11:35.036720  1899 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:11:35.036741  1899 net.cpp:128] Creating Layer BatchNorm18
I0825 11:11:35.036762  1899 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:11:35.036782  1899 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:11:35.037114  1899 net.cpp:172] Setting up BatchNorm18
I0825 11:11:35.037139  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.037158  1899 net.cpp:194] Memory required for data: 57467000
I0825 11:11:35.037178  1899 layer_factory.hpp:77] Creating layer Scale18
I0825 11:11:35.037202  1899 net.cpp:128] Creating Layer Scale18
I0825 11:11:35.037216  1899 net.cpp:558] Scale18 <- Convolution18
I0825 11:11:35.037232  1899 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:11:35.037312  1899 layer_factory.hpp:77] Creating layer Scale18
I0825 11:11:35.037508  1899 net.cpp:172] Setting up Scale18
I0825 11:11:35.037531  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.037549  1899 net.cpp:194] Memory required for data: 57794680
I0825 11:11:35.037569  1899 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:11:35.037592  1899 net.cpp:128] Creating Layer Eltwise8
I0825 11:11:35.037607  1899 net.cpp:558] Eltwise8 <- Convolution16
I0825 11:11:35.037622  1899 net.cpp:558] Eltwise8 <- Convolution18
I0825 11:11:35.037642  1899 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:11:35.037686  1899 net.cpp:172] Setting up Eltwise8
I0825 11:11:35.037705  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.037725  1899 net.cpp:194] Memory required for data: 58122360
I0825 11:11:35.037741  1899 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:11:35.037758  1899 net.cpp:128] Creating Layer ReLU17
I0825 11:11:35.037775  1899 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:11:35.037796  1899 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:11:35.038699  1899 net.cpp:172] Setting up ReLU17
I0825 11:11:35.038715  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.038720  1899 net.cpp:194] Memory required for data: 58450040
I0825 11:11:35.038725  1899 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:11:35.038733  1899 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:11:35.038738  1899 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:11:35.038746  1899 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:11:35.038755  1899 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:11:35.038818  1899 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:11:35.038852  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.038862  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.038866  1899 net.cpp:194] Memory required for data: 59105400
I0825 11:11:35.038870  1899 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:11:35.038913  1899 net.cpp:128] Creating Layer Convolution19
I0825 11:11:35.038930  1899 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0825 11:11:35.038954  1899 net.cpp:522] Convolution19 -> Convolution19
I0825 11:11:35.051553  1899 net.cpp:172] Setting up Convolution19
I0825 11:11:35.051580  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.051584  1899 net.cpp:194] Memory required for data: 59433080
I0825 11:11:35.051595  1899 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:11:35.051607  1899 net.cpp:128] Creating Layer BatchNorm19
I0825 11:11:35.051612  1899 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:11:35.051620  1899 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:11:35.051944  1899 net.cpp:172] Setting up BatchNorm19
I0825 11:11:35.051955  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.051961  1899 net.cpp:194] Memory required for data: 59760760
I0825 11:11:35.051987  1899 layer_factory.hpp:77] Creating layer Scale19
I0825 11:11:35.052034  1899 net.cpp:128] Creating Layer Scale19
I0825 11:11:35.052050  1899 net.cpp:558] Scale19 <- Convolution19
I0825 11:11:35.052069  1899 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:11:35.052151  1899 layer_factory.hpp:77] Creating layer Scale19
I0825 11:11:35.052347  1899 net.cpp:172] Setting up Scale19
I0825 11:11:35.052361  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.052366  1899 net.cpp:194] Memory required for data: 60088440
I0825 11:11:35.052374  1899 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:11:35.052381  1899 net.cpp:128] Creating Layer ReLU18
I0825 11:11:35.052384  1899 net.cpp:558] ReLU18 <- Convolution19
I0825 11:11:35.052392  1899 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0825 11:11:35.053139  1899 net.cpp:172] Setting up ReLU18
I0825 11:11:35.053164  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.053169  1899 net.cpp:194] Memory required for data: 60416120
I0825 11:11:35.053174  1899 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:11:35.053189  1899 net.cpp:128] Creating Layer Convolution20
I0825 11:11:35.053194  1899 net.cpp:558] Convolution20 <- Convolution19
I0825 11:11:35.053205  1899 net.cpp:522] Convolution20 -> Convolution20
I0825 11:11:35.059973  1899 net.cpp:172] Setting up Convolution20
I0825 11:11:35.059995  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.060000  1899 net.cpp:194] Memory required for data: 60743800
I0825 11:11:35.060010  1899 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:11:35.060024  1899 net.cpp:128] Creating Layer BatchNorm20
I0825 11:11:35.060032  1899 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:11:35.060041  1899 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:11:35.060362  1899 net.cpp:172] Setting up BatchNorm20
I0825 11:11:35.060372  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.060376  1899 net.cpp:194] Memory required for data: 61071480
I0825 11:11:35.060387  1899 layer_factory.hpp:77] Creating layer Scale20
I0825 11:11:35.060431  1899 net.cpp:128] Creating Layer Scale20
I0825 11:11:35.060441  1899 net.cpp:558] Scale20 <- Convolution20
I0825 11:11:35.060446  1899 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:11:35.060513  1899 layer_factory.hpp:77] Creating layer Scale20
I0825 11:11:35.060695  1899 net.cpp:172] Setting up Scale20
I0825 11:11:35.060705  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.060710  1899 net.cpp:194] Memory required for data: 61399160
I0825 11:11:35.060719  1899 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:11:35.060729  1899 net.cpp:128] Creating Layer Eltwise9
I0825 11:11:35.060762  1899 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:11:35.060771  1899 net.cpp:558] Eltwise9 <- Convolution20
I0825 11:11:35.060778  1899 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:11:35.060816  1899 net.cpp:172] Setting up Eltwise9
I0825 11:11:35.060825  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.060830  1899 net.cpp:194] Memory required for data: 61726840
I0825 11:11:35.060871  1899 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:11:35.060881  1899 net.cpp:128] Creating Layer ReLU19
I0825 11:11:35.060886  1899 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:11:35.060894  1899 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:11:35.062053  1899 net.cpp:172] Setting up ReLU19
I0825 11:11:35.062073  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.062078  1899 net.cpp:194] Memory required for data: 62054520
I0825 11:11:35.062083  1899 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:11:35.062090  1899 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:11:35.062094  1899 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:11:35.062129  1899 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:11:35.062150  1899 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:11:35.062233  1899 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:11:35.062244  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.062250  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.062254  1899 net.cpp:194] Memory required for data: 62709880
I0825 11:11:35.062259  1899 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:11:35.062290  1899 net.cpp:128] Creating Layer Convolution21
I0825 11:11:35.062307  1899 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0825 11:11:35.062321  1899 net.cpp:522] Convolution21 -> Convolution21
I0825 11:11:35.071069  1899 net.cpp:172] Setting up Convolution21
I0825 11:11:35.071095  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.071100  1899 net.cpp:194] Memory required for data: 63037560
I0825 11:11:35.071110  1899 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:11:35.071121  1899 net.cpp:128] Creating Layer BatchNorm21
I0825 11:11:35.071128  1899 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:11:35.071135  1899 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:11:35.071463  1899 net.cpp:172] Setting up BatchNorm21
I0825 11:11:35.071475  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.071478  1899 net.cpp:194] Memory required for data: 63365240
I0825 11:11:35.071488  1899 layer_factory.hpp:77] Creating layer Scale21
I0825 11:11:35.071497  1899 net.cpp:128] Creating Layer Scale21
I0825 11:11:35.071502  1899 net.cpp:558] Scale21 <- Convolution21
I0825 11:11:35.071507  1899 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:11:35.071565  1899 layer_factory.hpp:77] Creating layer Scale21
I0825 11:11:35.071782  1899 net.cpp:172] Setting up Scale21
I0825 11:11:35.071794  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.071799  1899 net.cpp:194] Memory required for data: 63692920
I0825 11:11:35.071807  1899 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:11:35.071835  1899 net.cpp:128] Creating Layer ReLU20
I0825 11:11:35.071853  1899 net.cpp:558] ReLU20 <- Convolution21
I0825 11:11:35.071873  1899 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:11:35.075258  1899 net.cpp:172] Setting up ReLU20
I0825 11:11:35.075280  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.075285  1899 net.cpp:194] Memory required for data: 64020600
I0825 11:11:35.075290  1899 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:11:35.075306  1899 net.cpp:128] Creating Layer Convolution22
I0825 11:11:35.075312  1899 net.cpp:558] Convolution22 <- Convolution21
I0825 11:11:35.075320  1899 net.cpp:522] Convolution22 -> Convolution22
I0825 11:11:35.083516  1899 net.cpp:172] Setting up Convolution22
I0825 11:11:35.083542  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.083546  1899 net.cpp:194] Memory required for data: 64348280
I0825 11:11:35.083557  1899 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:11:35.083568  1899 net.cpp:128] Creating Layer BatchNorm22
I0825 11:11:35.083573  1899 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:11:35.083582  1899 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:11:35.083916  1899 net.cpp:172] Setting up BatchNorm22
I0825 11:11:35.083930  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.083935  1899 net.cpp:194] Memory required for data: 64675960
I0825 11:11:35.083945  1899 layer_factory.hpp:77] Creating layer Scale22
I0825 11:11:35.083953  1899 net.cpp:128] Creating Layer Scale22
I0825 11:11:35.083958  1899 net.cpp:558] Scale22 <- Convolution22
I0825 11:11:35.083964  1899 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:11:35.084023  1899 layer_factory.hpp:77] Creating layer Scale22
I0825 11:11:35.084209  1899 net.cpp:172] Setting up Scale22
I0825 11:11:35.084220  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.084225  1899 net.cpp:194] Memory required for data: 65003640
I0825 11:11:35.084233  1899 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:11:35.084242  1899 net.cpp:128] Creating Layer Eltwise10
I0825 11:11:35.084249  1899 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0825 11:11:35.084254  1899 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:11:35.084261  1899 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:11:35.084290  1899 net.cpp:172] Setting up Eltwise10
I0825 11:11:35.084300  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.084305  1899 net.cpp:194] Memory required for data: 65331320
I0825 11:11:35.084308  1899 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:11:35.084316  1899 net.cpp:128] Creating Layer ReLU21
I0825 11:11:35.084319  1899 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:11:35.084326  1899 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:11:35.085604  1899 net.cpp:172] Setting up ReLU21
I0825 11:11:35.085629  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.085634  1899 net.cpp:194] Memory required for data: 65659000
I0825 11:11:35.085639  1899 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:11:35.085651  1899 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:11:35.085656  1899 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:11:35.085665  1899 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:11:35.085675  1899 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:11:35.085739  1899 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:11:35.085753  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.085759  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.085763  1899 net.cpp:194] Memory required for data: 66314360
I0825 11:11:35.085767  1899 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:11:35.085779  1899 net.cpp:128] Creating Layer Convolution23
I0825 11:11:35.085784  1899 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:11:35.085794  1899 net.cpp:522] Convolution23 -> Convolution23
I0825 11:11:35.092445  1899 net.cpp:172] Setting up Convolution23
I0825 11:11:35.092476  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.092481  1899 net.cpp:194] Memory required for data: 66642040
I0825 11:11:35.092491  1899 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:11:35.092499  1899 net.cpp:128] Creating Layer BatchNorm23
I0825 11:11:35.092509  1899 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:11:35.092519  1899 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:11:35.092846  1899 net.cpp:172] Setting up BatchNorm23
I0825 11:11:35.092856  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.092860  1899 net.cpp:194] Memory required for data: 66969720
I0825 11:11:35.092870  1899 layer_factory.hpp:77] Creating layer Scale23
I0825 11:11:35.092878  1899 net.cpp:128] Creating Layer Scale23
I0825 11:11:35.092882  1899 net.cpp:558] Scale23 <- Convolution23
I0825 11:11:35.092888  1899 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:11:35.092947  1899 layer_factory.hpp:77] Creating layer Scale23
I0825 11:11:35.093129  1899 net.cpp:172] Setting up Scale23
I0825 11:11:35.093142  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.093149  1899 net.cpp:194] Memory required for data: 67297400
I0825 11:11:35.093178  1899 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:11:35.093186  1899 net.cpp:128] Creating Layer ReLU22
I0825 11:11:35.093191  1899 net.cpp:558] ReLU22 <- Convolution23
I0825 11:11:35.093196  1899 net.cpp:509] ReLU22 -> Convolution23 (in-place)
I0825 11:11:35.094714  1899 net.cpp:172] Setting up ReLU22
I0825 11:11:35.094732  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.094735  1899 net.cpp:194] Memory required for data: 67625080
I0825 11:11:35.094740  1899 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:11:35.094753  1899 net.cpp:128] Creating Layer Convolution24
I0825 11:11:35.094760  1899 net.cpp:558] Convolution24 <- Convolution23
I0825 11:11:35.094769  1899 net.cpp:522] Convolution24 -> Convolution24
I0825 11:11:35.099973  1899 net.cpp:172] Setting up Convolution24
I0825 11:11:35.100000  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.100005  1899 net.cpp:194] Memory required for data: 67952760
I0825 11:11:35.100015  1899 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:11:35.100028  1899 net.cpp:128] Creating Layer BatchNorm24
I0825 11:11:35.100033  1899 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:11:35.100039  1899 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:11:35.100374  1899 net.cpp:172] Setting up BatchNorm24
I0825 11:11:35.100383  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.100387  1899 net.cpp:194] Memory required for data: 68280440
I0825 11:11:35.100397  1899 layer_factory.hpp:77] Creating layer Scale24
I0825 11:11:35.100407  1899 net.cpp:128] Creating Layer Scale24
I0825 11:11:35.100411  1899 net.cpp:558] Scale24 <- Convolution24
I0825 11:11:35.100416  1899 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:11:35.100476  1899 layer_factory.hpp:77] Creating layer Scale24
I0825 11:11:35.100662  1899 net.cpp:172] Setting up Scale24
I0825 11:11:35.100672  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.100677  1899 net.cpp:194] Memory required for data: 68608120
I0825 11:11:35.100684  1899 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:11:35.100694  1899 net.cpp:128] Creating Layer Eltwise11
I0825 11:11:35.100699  1899 net.cpp:558] Eltwise11 <- Eltwise10_ReLU21_0_split_1
I0825 11:11:35.100704  1899 net.cpp:558] Eltwise11 <- Convolution24
I0825 11:11:35.100713  1899 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:11:35.100739  1899 net.cpp:172] Setting up Eltwise11
I0825 11:11:35.100749  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.100752  1899 net.cpp:194] Memory required for data: 68935800
I0825 11:11:35.100756  1899 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:11:35.100765  1899 net.cpp:128] Creating Layer ReLU23
I0825 11:11:35.100770  1899 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:11:35.100775  1899 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:11:35.101034  1899 net.cpp:172] Setting up ReLU23
I0825 11:11:35.101049  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.101054  1899 net.cpp:194] Memory required for data: 69263480
I0825 11:11:35.101058  1899 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:11:35.101068  1899 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:11:35.101073  1899 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:11:35.101081  1899 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:11:35.101090  1899 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:11:35.101152  1899 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:11:35.101164  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.101171  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.101174  1899 net.cpp:194] Memory required for data: 69918840
I0825 11:11:35.101178  1899 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:11:35.101191  1899 net.cpp:128] Creating Layer Convolution25
I0825 11:11:35.101197  1899 net.cpp:558] Convolution25 <- Eltwise11_ReLU23_0_split_0
I0825 11:11:35.101223  1899 net.cpp:522] Convolution25 -> Convolution25
I0825 11:11:35.102846  1899 net.cpp:172] Setting up Convolution25
I0825 11:11:35.102869  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.102874  1899 net.cpp:194] Memory required for data: 70246520
I0825 11:11:35.102885  1899 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:11:35.102895  1899 net.cpp:128] Creating Layer BatchNorm25
I0825 11:11:35.102900  1899 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:11:35.102908  1899 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:11:35.103230  1899 net.cpp:172] Setting up BatchNorm25
I0825 11:11:35.103240  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.103245  1899 net.cpp:194] Memory required for data: 70574200
I0825 11:11:35.103255  1899 layer_factory.hpp:77] Creating layer Scale25
I0825 11:11:35.103261  1899 net.cpp:128] Creating Layer Scale25
I0825 11:11:35.103266  1899 net.cpp:558] Scale25 <- Convolution25
I0825 11:11:35.103271  1899 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:11:35.103329  1899 layer_factory.hpp:77] Creating layer Scale25
I0825 11:11:35.103514  1899 net.cpp:172] Setting up Scale25
I0825 11:11:35.103524  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.103528  1899 net.cpp:194] Memory required for data: 70901880
I0825 11:11:35.103536  1899 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:11:35.103544  1899 net.cpp:128] Creating Layer ReLU24
I0825 11:11:35.103549  1899 net.cpp:558] ReLU24 <- Convolution25
I0825 11:11:35.103554  1899 net.cpp:509] ReLU24 -> Convolution25 (in-place)
I0825 11:11:35.103816  1899 net.cpp:172] Setting up ReLU24
I0825 11:11:35.103830  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.103835  1899 net.cpp:194] Memory required for data: 71229560
I0825 11:11:35.103839  1899 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:11:35.103852  1899 net.cpp:128] Creating Layer Convolution26
I0825 11:11:35.103858  1899 net.cpp:558] Convolution26 <- Convolution25
I0825 11:11:35.103868  1899 net.cpp:522] Convolution26 -> Convolution26
I0825 11:11:35.105496  1899 net.cpp:172] Setting up Convolution26
I0825 11:11:35.105521  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.105526  1899 net.cpp:194] Memory required for data: 71557240
I0825 11:11:35.105536  1899 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:11:35.105547  1899 net.cpp:128] Creating Layer BatchNorm26
I0825 11:11:35.105552  1899 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:11:35.105561  1899 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:11:35.105895  1899 net.cpp:172] Setting up BatchNorm26
I0825 11:11:35.105906  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.105911  1899 net.cpp:194] Memory required for data: 71884920
I0825 11:11:35.105921  1899 layer_factory.hpp:77] Creating layer Scale26
I0825 11:11:35.105932  1899 net.cpp:128] Creating Layer Scale26
I0825 11:11:35.105937  1899 net.cpp:558] Scale26 <- Convolution26
I0825 11:11:35.105943  1899 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:11:35.106003  1899 layer_factory.hpp:77] Creating layer Scale26
I0825 11:11:35.106189  1899 net.cpp:172] Setting up Scale26
I0825 11:11:35.106197  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.106201  1899 net.cpp:194] Memory required for data: 72212600
I0825 11:11:35.106210  1899 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:11:35.106220  1899 net.cpp:128] Creating Layer Eltwise12
I0825 11:11:35.106225  1899 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:11:35.106235  1899 net.cpp:558] Eltwise12 <- Convolution26
I0825 11:11:35.106241  1899 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:11:35.106271  1899 net.cpp:172] Setting up Eltwise12
I0825 11:11:35.106277  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.106281  1899 net.cpp:194] Memory required for data: 72540280
I0825 11:11:35.106286  1899 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:11:35.106297  1899 net.cpp:128] Creating Layer ReLU25
I0825 11:11:35.106321  1899 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:11:35.106328  1899 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:11:35.106879  1899 net.cpp:172] Setting up ReLU25
I0825 11:11:35.106904  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.106907  1899 net.cpp:194] Memory required for data: 72867960
I0825 11:11:35.106914  1899 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:11:35.106930  1899 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:11:35.106935  1899 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:11:35.106945  1899 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:11:35.106956  1899 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:11:35.107029  1899 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:11:35.107040  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.107046  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.107050  1899 net.cpp:194] Memory required for data: 73523320
I0825 11:11:35.107059  1899 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:11:35.107071  1899 net.cpp:128] Creating Layer Convolution27
I0825 11:11:35.107079  1899 net.cpp:558] Convolution27 <- Eltwise12_ReLU25_0_split_0
I0825 11:11:35.107089  1899 net.cpp:522] Convolution27 -> Convolution27
I0825 11:11:35.117835  1899 net.cpp:172] Setting up Convolution27
I0825 11:11:35.117863  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.117868  1899 net.cpp:194] Memory required for data: 73851000
I0825 11:11:35.117880  1899 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:11:35.117893  1899 net.cpp:128] Creating Layer BatchNorm27
I0825 11:11:35.117902  1899 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:11:35.117909  1899 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:11:35.118238  1899 net.cpp:172] Setting up BatchNorm27
I0825 11:11:35.118247  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.118252  1899 net.cpp:194] Memory required for data: 74178680
I0825 11:11:35.118263  1899 layer_factory.hpp:77] Creating layer Scale27
I0825 11:11:35.118274  1899 net.cpp:128] Creating Layer Scale27
I0825 11:11:35.118279  1899 net.cpp:558] Scale27 <- Convolution27
I0825 11:11:35.118285  1899 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:11:35.118355  1899 layer_factory.hpp:77] Creating layer Scale27
I0825 11:11:35.118544  1899 net.cpp:172] Setting up Scale27
I0825 11:11:35.118556  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.118561  1899 net.cpp:194] Memory required for data: 74506360
I0825 11:11:35.118568  1899 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:11:35.118582  1899 net.cpp:128] Creating Layer ReLU26
I0825 11:11:35.118587  1899 net.cpp:558] ReLU26 <- Convolution27
I0825 11:11:35.118594  1899 net.cpp:509] ReLU26 -> Convolution27 (in-place)
I0825 11:11:35.120975  1899 net.cpp:172] Setting up ReLU26
I0825 11:11:35.120992  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.120997  1899 net.cpp:194] Memory required for data: 74834040
I0825 11:11:35.121001  1899 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:11:35.121016  1899 net.cpp:128] Creating Layer Convolution28
I0825 11:11:35.121021  1899 net.cpp:558] Convolution28 <- Convolution27
I0825 11:11:35.121034  1899 net.cpp:522] Convolution28 -> Convolution28
I0825 11:11:35.127893  1899 net.cpp:172] Setting up Convolution28
I0825 11:11:35.127919  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.127924  1899 net.cpp:194] Memory required for data: 75161720
I0825 11:11:35.127933  1899 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:11:35.127945  1899 net.cpp:128] Creating Layer BatchNorm28
I0825 11:11:35.127950  1899 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:11:35.127960  1899 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:11:35.128290  1899 net.cpp:172] Setting up BatchNorm28
I0825 11:11:35.128301  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.128322  1899 net.cpp:194] Memory required for data: 75489400
I0825 11:11:35.128334  1899 layer_factory.hpp:77] Creating layer Scale28
I0825 11:11:35.128343  1899 net.cpp:128] Creating Layer Scale28
I0825 11:11:35.128348  1899 net.cpp:558] Scale28 <- Convolution28
I0825 11:11:35.128353  1899 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:11:35.128415  1899 layer_factory.hpp:77] Creating layer Scale28
I0825 11:11:35.128605  1899 net.cpp:172] Setting up Scale28
I0825 11:11:35.128617  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.128621  1899 net.cpp:194] Memory required for data: 75817080
I0825 11:11:35.128629  1899 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:11:35.128639  1899 net.cpp:128] Creating Layer Eltwise13
I0825 11:11:35.128645  1899 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:11:35.128650  1899 net.cpp:558] Eltwise13 <- Convolution28
I0825 11:11:35.128657  1899 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:11:35.128688  1899 net.cpp:172] Setting up Eltwise13
I0825 11:11:35.128698  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.128703  1899 net.cpp:194] Memory required for data: 76144760
I0825 11:11:35.128707  1899 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:11:35.128713  1899 net.cpp:128] Creating Layer ReLU27
I0825 11:11:35.128717  1899 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:11:35.128722  1899 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:11:35.129946  1899 net.cpp:172] Setting up ReLU27
I0825 11:11:35.129959  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.129963  1899 net.cpp:194] Memory required for data: 76472440
I0825 11:11:35.129968  1899 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:11:35.129976  1899 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:11:35.129981  1899 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:11:35.129989  1899 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:11:35.129998  1899 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:11:35.130059  1899 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:11:35.130071  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.130077  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.130081  1899 net.cpp:194] Memory required for data: 77127800
I0825 11:11:35.130085  1899 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:11:35.130100  1899 net.cpp:128] Creating Layer Convolution29
I0825 11:11:35.130105  1899 net.cpp:558] Convolution29 <- Eltwise13_ReLU27_0_split_0
I0825 11:11:35.130115  1899 net.cpp:522] Convolution29 -> Convolution29
I0825 11:11:35.137948  1899 net.cpp:172] Setting up Convolution29
I0825 11:11:35.137974  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.137979  1899 net.cpp:194] Memory required for data: 77455480
I0825 11:11:35.137990  1899 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:11:35.137998  1899 net.cpp:128] Creating Layer BatchNorm29
I0825 11:11:35.138010  1899 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:11:35.138017  1899 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:11:35.138372  1899 net.cpp:172] Setting up BatchNorm29
I0825 11:11:35.138384  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.138388  1899 net.cpp:194] Memory required for data: 77783160
I0825 11:11:35.138398  1899 layer_factory.hpp:77] Creating layer Scale29
I0825 11:11:35.138406  1899 net.cpp:128] Creating Layer Scale29
I0825 11:11:35.138411  1899 net.cpp:558] Scale29 <- Convolution29
I0825 11:11:35.138418  1899 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:11:35.138480  1899 layer_factory.hpp:77] Creating layer Scale29
I0825 11:11:35.138669  1899 net.cpp:172] Setting up Scale29
I0825 11:11:35.138680  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.138684  1899 net.cpp:194] Memory required for data: 78110840
I0825 11:11:35.138692  1899 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:11:35.138716  1899 net.cpp:128] Creating Layer ReLU28
I0825 11:11:35.138721  1899 net.cpp:558] ReLU28 <- Convolution29
I0825 11:11:35.138727  1899 net.cpp:509] ReLU28 -> Convolution29 (in-place)
I0825 11:11:35.141799  1899 net.cpp:172] Setting up ReLU28
I0825 11:11:35.141825  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.141829  1899 net.cpp:194] Memory required for data: 78438520
I0825 11:11:35.141834  1899 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:11:35.141850  1899 net.cpp:128] Creating Layer Convolution30
I0825 11:11:35.141861  1899 net.cpp:558] Convolution30 <- Convolution29
I0825 11:11:35.141872  1899 net.cpp:522] Convolution30 -> Convolution30
I0825 11:11:35.144970  1899 net.cpp:172] Setting up Convolution30
I0825 11:11:35.144996  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.145001  1899 net.cpp:194] Memory required for data: 78766200
I0825 11:11:35.145011  1899 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:11:35.145022  1899 net.cpp:128] Creating Layer BatchNorm30
I0825 11:11:35.145027  1899 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:11:35.145037  1899 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:11:35.145366  1899 net.cpp:172] Setting up BatchNorm30
I0825 11:11:35.145377  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.145381  1899 net.cpp:194] Memory required for data: 79093880
I0825 11:11:35.145391  1899 layer_factory.hpp:77] Creating layer Scale30
I0825 11:11:35.145401  1899 net.cpp:128] Creating Layer Scale30
I0825 11:11:35.145406  1899 net.cpp:558] Scale30 <- Convolution30
I0825 11:11:35.145411  1899 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:11:35.145470  1899 layer_factory.hpp:77] Creating layer Scale30
I0825 11:11:35.145658  1899 net.cpp:172] Setting up Scale30
I0825 11:11:35.145669  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.145674  1899 net.cpp:194] Memory required for data: 79421560
I0825 11:11:35.145682  1899 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:11:35.145694  1899 net.cpp:128] Creating Layer Eltwise14
I0825 11:11:35.145699  1899 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:11:35.145705  1899 net.cpp:558] Eltwise14 <- Convolution30
I0825 11:11:35.145711  1899 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:11:35.145740  1899 net.cpp:172] Setting up Eltwise14
I0825 11:11:35.145751  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.145756  1899 net.cpp:194] Memory required for data: 79749240
I0825 11:11:35.145759  1899 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:11:35.145766  1899 net.cpp:128] Creating Layer ReLU29
I0825 11:11:35.145771  1899 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:11:35.145776  1899 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:11:35.147073  1899 net.cpp:172] Setting up ReLU29
I0825 11:11:35.147092  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.147096  1899 net.cpp:194] Memory required for data: 80076920
I0825 11:11:35.147101  1899 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:11:35.147109  1899 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:11:35.147114  1899 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:11:35.147122  1899 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:11:35.147131  1899 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:11:35.147195  1899 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:11:35.147202  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.147208  1899 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:11:35.147212  1899 net.cpp:194] Memory required for data: 80732280
I0825 11:11:35.147217  1899 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:11:35.147231  1899 net.cpp:128] Creating Layer Convolution31
I0825 11:11:35.147236  1899 net.cpp:558] Convolution31 <- Eltwise14_ReLU29_0_split_0
I0825 11:11:35.147248  1899 net.cpp:522] Convolution31 -> Convolution31
I0825 11:11:35.153966  1899 net.cpp:172] Setting up Convolution31
I0825 11:11:35.153991  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.153996  1899 net.cpp:194] Memory required for data: 80896120
I0825 11:11:35.154006  1899 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:11:35.154016  1899 net.cpp:128] Creating Layer BatchNorm31
I0825 11:11:35.154022  1899 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:11:35.154029  1899 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:11:35.154381  1899 net.cpp:172] Setting up BatchNorm31
I0825 11:11:35.154392  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.154397  1899 net.cpp:194] Memory required for data: 81059960
I0825 11:11:35.154407  1899 layer_factory.hpp:77] Creating layer Scale31
I0825 11:11:35.154417  1899 net.cpp:128] Creating Layer Scale31
I0825 11:11:35.154420  1899 net.cpp:558] Scale31 <- Convolution31
I0825 11:11:35.154426  1899 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:11:35.154489  1899 layer_factory.hpp:77] Creating layer Scale31
I0825 11:11:35.154680  1899 net.cpp:172] Setting up Scale31
I0825 11:11:35.154693  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.154697  1899 net.cpp:194] Memory required for data: 81223800
I0825 11:11:35.154706  1899 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:11:35.154722  1899 net.cpp:128] Creating Layer Convolution32
I0825 11:11:35.154727  1899 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_1
I0825 11:11:35.154736  1899 net.cpp:522] Convolution32 -> Convolution32
I0825 11:11:35.164826  1899 net.cpp:172] Setting up Convolution32
I0825 11:11:35.164851  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.164856  1899 net.cpp:194] Memory required for data: 81387640
I0825 11:11:35.164866  1899 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:11:35.164875  1899 net.cpp:128] Creating Layer BatchNorm32
I0825 11:11:35.164880  1899 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:11:35.164891  1899 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:11:35.165225  1899 net.cpp:172] Setting up BatchNorm32
I0825 11:11:35.165236  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.165241  1899 net.cpp:194] Memory required for data: 81551480
I0825 11:11:35.165251  1899 layer_factory.hpp:77] Creating layer Scale32
I0825 11:11:35.165259  1899 net.cpp:128] Creating Layer Scale32
I0825 11:11:35.165264  1899 net.cpp:558] Scale32 <- Convolution32
I0825 11:11:35.165271  1899 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:11:35.165329  1899 layer_factory.hpp:77] Creating layer Scale32
I0825 11:11:35.165522  1899 net.cpp:172] Setting up Scale32
I0825 11:11:35.165534  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.165539  1899 net.cpp:194] Memory required for data: 81715320
I0825 11:11:35.165545  1899 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:11:35.165555  1899 net.cpp:128] Creating Layer ReLU30
I0825 11:11:35.165560  1899 net.cpp:558] ReLU30 <- Convolution32
I0825 11:11:35.165565  1899 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0825 11:11:35.168334  1899 net.cpp:172] Setting up ReLU30
I0825 11:11:35.168354  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.168359  1899 net.cpp:194] Memory required for data: 81879160
I0825 11:11:35.168362  1899 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:11:35.168380  1899 net.cpp:128] Creating Layer Convolution33
I0825 11:11:35.168386  1899 net.cpp:558] Convolution33 <- Convolution32
I0825 11:11:35.168395  1899 net.cpp:522] Convolution33 -> Convolution33
I0825 11:11:35.175225  1899 net.cpp:172] Setting up Convolution33
I0825 11:11:35.175251  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.175256  1899 net.cpp:194] Memory required for data: 82043000
I0825 11:11:35.175266  1899 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:11:35.175277  1899 net.cpp:128] Creating Layer BatchNorm33
I0825 11:11:35.175283  1899 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:11:35.175292  1899 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:11:35.175653  1899 net.cpp:172] Setting up BatchNorm33
I0825 11:11:35.175665  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.175669  1899 net.cpp:194] Memory required for data: 82206840
I0825 11:11:35.175679  1899 layer_factory.hpp:77] Creating layer Scale33
I0825 11:11:35.175686  1899 net.cpp:128] Creating Layer Scale33
I0825 11:11:35.175691  1899 net.cpp:558] Scale33 <- Convolution33
I0825 11:11:35.175698  1899 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:11:35.175757  1899 layer_factory.hpp:77] Creating layer Scale33
I0825 11:11:35.175953  1899 net.cpp:172] Setting up Scale33
I0825 11:11:35.175964  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.175969  1899 net.cpp:194] Memory required for data: 82370680
I0825 11:11:35.175977  1899 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:11:35.175987  1899 net.cpp:128] Creating Layer Eltwise15
I0825 11:11:35.175992  1899 net.cpp:558] Eltwise15 <- Convolution31
I0825 11:11:35.175997  1899 net.cpp:558] Eltwise15 <- Convolution33
I0825 11:11:35.176003  1899 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:11:35.176038  1899 net.cpp:172] Setting up Eltwise15
I0825 11:11:35.176048  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.176053  1899 net.cpp:194] Memory required for data: 82534520
I0825 11:11:35.176057  1899 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:11:35.176064  1899 net.cpp:128] Creating Layer ReLU31
I0825 11:11:35.176067  1899 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:11:35.176074  1899 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:11:35.177266  1899 net.cpp:172] Setting up ReLU31
I0825 11:11:35.177290  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.177296  1899 net.cpp:194] Memory required for data: 82698360
I0825 11:11:35.177301  1899 layer_factory.hpp:77] Creating layer Eltwise15_ReLU31_0_split
I0825 11:11:35.177310  1899 net.cpp:128] Creating Layer Eltwise15_ReLU31_0_split
I0825 11:11:35.177315  1899 net.cpp:558] Eltwise15_ReLU31_0_split <- Eltwise15
I0825 11:11:35.177328  1899 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_0
I0825 11:11:35.177338  1899 net.cpp:522] Eltwise15_ReLU31_0_split -> Eltwise15_ReLU31_0_split_1
I0825 11:11:35.177410  1899 net.cpp:172] Setting up Eltwise15_ReLU31_0_split
I0825 11:11:35.177423  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.177428  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.177433  1899 net.cpp:194] Memory required for data: 83026040
I0825 11:11:35.177436  1899 layer_factory.hpp:77] Creating layer Convolution34
I0825 11:11:35.177449  1899 net.cpp:128] Creating Layer Convolution34
I0825 11:11:35.177454  1899 net.cpp:558] Convolution34 <- Eltwise15_ReLU31_0_split_0
I0825 11:11:35.177464  1899 net.cpp:522] Convolution34 -> Convolution34
I0825 11:11:35.185354  1899 net.cpp:172] Setting up Convolution34
I0825 11:11:35.185381  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.185385  1899 net.cpp:194] Memory required for data: 83189880
I0825 11:11:35.185396  1899 layer_factory.hpp:77] Creating layer BatchNorm34
I0825 11:11:35.185406  1899 net.cpp:128] Creating Layer BatchNorm34
I0825 11:11:35.185416  1899 net.cpp:558] BatchNorm34 <- Convolution34
I0825 11:11:35.185423  1899 net.cpp:509] BatchNorm34 -> Convolution34 (in-place)
I0825 11:11:35.185775  1899 net.cpp:172] Setting up BatchNorm34
I0825 11:11:35.185786  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.185791  1899 net.cpp:194] Memory required for data: 83353720
I0825 11:11:35.185801  1899 layer_factory.hpp:77] Creating layer Scale34
I0825 11:11:35.185811  1899 net.cpp:128] Creating Layer Scale34
I0825 11:11:35.185818  1899 net.cpp:558] Scale34 <- Convolution34
I0825 11:11:35.185823  1899 net.cpp:509] Scale34 -> Convolution34 (in-place)
I0825 11:11:35.185883  1899 layer_factory.hpp:77] Creating layer Scale34
I0825 11:11:35.186079  1899 net.cpp:172] Setting up Scale34
I0825 11:11:35.186089  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.186094  1899 net.cpp:194] Memory required for data: 83517560
I0825 11:11:35.186120  1899 layer_factory.hpp:77] Creating layer ReLU32
I0825 11:11:35.186127  1899 net.cpp:128] Creating Layer ReLU32
I0825 11:11:35.186132  1899 net.cpp:558] ReLU32 <- Convolution34
I0825 11:11:35.186141  1899 net.cpp:509] ReLU32 -> Convolution34 (in-place)
I0825 11:11:35.189257  1899 net.cpp:172] Setting up ReLU32
I0825 11:11:35.189277  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.189281  1899 net.cpp:194] Memory required for data: 83681400
I0825 11:11:35.189286  1899 layer_factory.hpp:77] Creating layer Convolution35
I0825 11:11:35.189301  1899 net.cpp:128] Creating Layer Convolution35
I0825 11:11:35.189307  1899 net.cpp:558] Convolution35 <- Convolution34
I0825 11:11:35.189324  1899 net.cpp:522] Convolution35 -> Convolution35
I0825 11:11:35.198369  1899 net.cpp:172] Setting up Convolution35
I0825 11:11:35.198397  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.198402  1899 net.cpp:194] Memory required for data: 83845240
I0825 11:11:35.198413  1899 layer_factory.hpp:77] Creating layer BatchNorm35
I0825 11:11:35.198424  1899 net.cpp:128] Creating Layer BatchNorm35
I0825 11:11:35.198429  1899 net.cpp:558] BatchNorm35 <- Convolution35
I0825 11:11:35.198443  1899 net.cpp:509] BatchNorm35 -> Convolution35 (in-place)
I0825 11:11:35.198794  1899 net.cpp:172] Setting up BatchNorm35
I0825 11:11:35.198806  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.198809  1899 net.cpp:194] Memory required for data: 84009080
I0825 11:11:35.198819  1899 layer_factory.hpp:77] Creating layer Scale35
I0825 11:11:35.198833  1899 net.cpp:128] Creating Layer Scale35
I0825 11:11:35.198838  1899 net.cpp:558] Scale35 <- Convolution35
I0825 11:11:35.198843  1899 net.cpp:509] Scale35 -> Convolution35 (in-place)
I0825 11:11:35.198909  1899 layer_factory.hpp:77] Creating layer Scale35
I0825 11:11:35.199105  1899 net.cpp:172] Setting up Scale35
I0825 11:11:35.199115  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.199118  1899 net.cpp:194] Memory required for data: 84172920
I0825 11:11:35.199126  1899 layer_factory.hpp:77] Creating layer Eltwise16
I0825 11:11:35.199141  1899 net.cpp:128] Creating Layer Eltwise16
I0825 11:11:35.199147  1899 net.cpp:558] Eltwise16 <- Eltwise15_ReLU31_0_split_1
I0825 11:11:35.199152  1899 net.cpp:558] Eltwise16 <- Convolution35
I0825 11:11:35.199158  1899 net.cpp:522] Eltwise16 -> Eltwise16
I0825 11:11:35.199193  1899 net.cpp:172] Setting up Eltwise16
I0825 11:11:35.199203  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.199208  1899 net.cpp:194] Memory required for data: 84336760
I0825 11:11:35.199211  1899 layer_factory.hpp:77] Creating layer ReLU33
I0825 11:11:35.199218  1899 net.cpp:128] Creating Layer ReLU33
I0825 11:11:35.199231  1899 net.cpp:558] ReLU33 <- Eltwise16
I0825 11:11:35.199237  1899 net.cpp:509] ReLU33 -> Eltwise16 (in-place)
I0825 11:11:35.200667  1899 net.cpp:172] Setting up ReLU33
I0825 11:11:35.200692  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.200696  1899 net.cpp:194] Memory required for data: 84500600
I0825 11:11:35.200701  1899 layer_factory.hpp:77] Creating layer Eltwise16_ReLU33_0_split
I0825 11:11:35.200713  1899 net.cpp:128] Creating Layer Eltwise16_ReLU33_0_split
I0825 11:11:35.200718  1899 net.cpp:558] Eltwise16_ReLU33_0_split <- Eltwise16
I0825 11:11:35.200726  1899 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_0
I0825 11:11:35.200737  1899 net.cpp:522] Eltwise16_ReLU33_0_split -> Eltwise16_ReLU33_0_split_1
I0825 11:11:35.200811  1899 net.cpp:172] Setting up Eltwise16_ReLU33_0_split
I0825 11:11:35.200822  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.200829  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.200832  1899 net.cpp:194] Memory required for data: 84828280
I0825 11:11:35.200836  1899 layer_factory.hpp:77] Creating layer Convolution36
I0825 11:11:35.200850  1899 net.cpp:128] Creating Layer Convolution36
I0825 11:11:35.200857  1899 net.cpp:558] Convolution36 <- Eltwise16_ReLU33_0_split_0
I0825 11:11:35.200882  1899 net.cpp:522] Convolution36 -> Convolution36
I0825 11:11:35.207500  1899 net.cpp:172] Setting up Convolution36
I0825 11:11:35.207522  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.207527  1899 net.cpp:194] Memory required for data: 84992120
I0825 11:11:35.207537  1899 layer_factory.hpp:77] Creating layer BatchNorm36
I0825 11:11:35.207547  1899 net.cpp:128] Creating Layer BatchNorm36
I0825 11:11:35.207553  1899 net.cpp:558] BatchNorm36 <- Convolution36
I0825 11:11:35.207562  1899 net.cpp:509] BatchNorm36 -> Convolution36 (in-place)
I0825 11:11:35.207922  1899 net.cpp:172] Setting up BatchNorm36
I0825 11:11:35.207929  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.207933  1899 net.cpp:194] Memory required for data: 85155960
I0825 11:11:35.207943  1899 layer_factory.hpp:77] Creating layer Scale36
I0825 11:11:35.207952  1899 net.cpp:128] Creating Layer Scale36
I0825 11:11:35.207957  1899 net.cpp:558] Scale36 <- Convolution36
I0825 11:11:35.207962  1899 net.cpp:509] Scale36 -> Convolution36 (in-place)
I0825 11:11:35.208024  1899 layer_factory.hpp:77] Creating layer Scale36
I0825 11:11:35.208220  1899 net.cpp:172] Setting up Scale36
I0825 11:11:35.208226  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.208230  1899 net.cpp:194] Memory required for data: 85319800
I0825 11:11:35.208238  1899 layer_factory.hpp:77] Creating layer ReLU34
I0825 11:11:35.208246  1899 net.cpp:128] Creating Layer ReLU34
I0825 11:11:35.208251  1899 net.cpp:558] ReLU34 <- Convolution36
I0825 11:11:35.208256  1899 net.cpp:509] ReLU34 -> Convolution36 (in-place)
I0825 11:11:35.209323  1899 net.cpp:172] Setting up ReLU34
I0825 11:11:35.209334  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.209338  1899 net.cpp:194] Memory required for data: 85483640
I0825 11:11:35.209343  1899 layer_factory.hpp:77] Creating layer Convolution37
I0825 11:11:35.209355  1899 net.cpp:128] Creating Layer Convolution37
I0825 11:11:35.209360  1899 net.cpp:558] Convolution37 <- Convolution36
I0825 11:11:35.209369  1899 net.cpp:522] Convolution37 -> Convolution37
I0825 11:11:35.220369  1899 net.cpp:172] Setting up Convolution37
I0825 11:11:35.220396  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.220401  1899 net.cpp:194] Memory required for data: 85647480
I0825 11:11:35.220410  1899 layer_factory.hpp:77] Creating layer BatchNorm37
I0825 11:11:35.220420  1899 net.cpp:128] Creating Layer BatchNorm37
I0825 11:11:35.220427  1899 net.cpp:558] BatchNorm37 <- Convolution37
I0825 11:11:35.220432  1899 net.cpp:509] BatchNorm37 -> Convolution37 (in-place)
I0825 11:11:35.220789  1899 net.cpp:172] Setting up BatchNorm37
I0825 11:11:35.220800  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.220806  1899 net.cpp:194] Memory required for data: 85811320
I0825 11:11:35.220840  1899 layer_factory.hpp:77] Creating layer Scale37
I0825 11:11:35.220849  1899 net.cpp:128] Creating Layer Scale37
I0825 11:11:35.220854  1899 net.cpp:558] Scale37 <- Convolution37
I0825 11:11:35.220860  1899 net.cpp:509] Scale37 -> Convolution37 (in-place)
I0825 11:11:35.220922  1899 layer_factory.hpp:77] Creating layer Scale37
I0825 11:11:35.221122  1899 net.cpp:172] Setting up Scale37
I0825 11:11:35.221133  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.221138  1899 net.cpp:194] Memory required for data: 85975160
I0825 11:11:35.221146  1899 layer_factory.hpp:77] Creating layer Eltwise17
I0825 11:11:35.221153  1899 net.cpp:128] Creating Layer Eltwise17
I0825 11:11:35.221158  1899 net.cpp:558] Eltwise17 <- Eltwise16_ReLU33_0_split_1
I0825 11:11:35.221163  1899 net.cpp:558] Eltwise17 <- Convolution37
I0825 11:11:35.221173  1899 net.cpp:522] Eltwise17 -> Eltwise17
I0825 11:11:35.221209  1899 net.cpp:172] Setting up Eltwise17
I0825 11:11:35.221215  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.221220  1899 net.cpp:194] Memory required for data: 86139000
I0825 11:11:35.221223  1899 layer_factory.hpp:77] Creating layer ReLU35
I0825 11:11:35.221230  1899 net.cpp:128] Creating Layer ReLU35
I0825 11:11:35.221251  1899 net.cpp:558] ReLU35 <- Eltwise17
I0825 11:11:35.221257  1899 net.cpp:509] ReLU35 -> Eltwise17 (in-place)
I0825 11:11:35.224041  1899 net.cpp:172] Setting up ReLU35
I0825 11:11:35.224066  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.224071  1899 net.cpp:194] Memory required for data: 86302840
I0825 11:11:35.224076  1899 layer_factory.hpp:77] Creating layer Eltwise17_ReLU35_0_split
I0825 11:11:35.224089  1899 net.cpp:128] Creating Layer Eltwise17_ReLU35_0_split
I0825 11:11:35.224094  1899 net.cpp:558] Eltwise17_ReLU35_0_split <- Eltwise17
I0825 11:11:35.224103  1899 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_0
I0825 11:11:35.224113  1899 net.cpp:522] Eltwise17_ReLU35_0_split -> Eltwise17_ReLU35_0_split_1
I0825 11:11:35.224181  1899 net.cpp:172] Setting up Eltwise17_ReLU35_0_split
I0825 11:11:35.224190  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.224196  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.224200  1899 net.cpp:194] Memory required for data: 86630520
I0825 11:11:35.224205  1899 layer_factory.hpp:77] Creating layer Convolution38
I0825 11:11:35.224218  1899 net.cpp:128] Creating Layer Convolution38
I0825 11:11:35.224223  1899 net.cpp:558] Convolution38 <- Eltwise17_ReLU35_0_split_0
I0825 11:11:35.224234  1899 net.cpp:522] Convolution38 -> Convolution38
I0825 11:11:35.231194  1899 net.cpp:172] Setting up Convolution38
I0825 11:11:35.231222  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.231227  1899 net.cpp:194] Memory required for data: 86794360
I0825 11:11:35.231237  1899 layer_factory.hpp:77] Creating layer BatchNorm38
I0825 11:11:35.231246  1899 net.cpp:128] Creating Layer BatchNorm38
I0825 11:11:35.231251  1899 net.cpp:558] BatchNorm38 <- Convolution38
I0825 11:11:35.231261  1899 net.cpp:509] BatchNorm38 -> Convolution38 (in-place)
I0825 11:11:35.231626  1899 net.cpp:172] Setting up BatchNorm38
I0825 11:11:35.231637  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.231642  1899 net.cpp:194] Memory required for data: 86958200
I0825 11:11:35.231652  1899 layer_factory.hpp:77] Creating layer Scale38
I0825 11:11:35.231659  1899 net.cpp:128] Creating Layer Scale38
I0825 11:11:35.231664  1899 net.cpp:558] Scale38 <- Convolution38
I0825 11:11:35.231670  1899 net.cpp:509] Scale38 -> Convolution38 (in-place)
I0825 11:11:35.231734  1899 layer_factory.hpp:77] Creating layer Scale38
I0825 11:11:35.231935  1899 net.cpp:172] Setting up Scale38
I0825 11:11:35.231950  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.231953  1899 net.cpp:194] Memory required for data: 87122040
I0825 11:11:35.231961  1899 layer_factory.hpp:77] Creating layer ReLU36
I0825 11:11:35.231968  1899 net.cpp:128] Creating Layer ReLU36
I0825 11:11:35.231973  1899 net.cpp:558] ReLU36 <- Convolution38
I0825 11:11:35.231978  1899 net.cpp:509] ReLU36 -> Convolution38 (in-place)
I0825 11:11:35.232987  1899 net.cpp:172] Setting up ReLU36
I0825 11:11:35.233008  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.233012  1899 net.cpp:194] Memory required for data: 87285880
I0825 11:11:35.233017  1899 layer_factory.hpp:77] Creating layer Convolution39
I0825 11:11:35.233031  1899 net.cpp:128] Creating Layer Convolution39
I0825 11:11:35.233036  1899 net.cpp:558] Convolution39 <- Convolution38
I0825 11:11:35.233043  1899 net.cpp:522] Convolution39 -> Convolution39
I0825 11:11:35.239784  1899 net.cpp:172] Setting up Convolution39
I0825 11:11:35.239811  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.239816  1899 net.cpp:194] Memory required for data: 87449720
I0825 11:11:35.239827  1899 layer_factory.hpp:77] Creating layer BatchNorm39
I0825 11:11:35.239840  1899 net.cpp:128] Creating Layer BatchNorm39
I0825 11:11:35.239848  1899 net.cpp:558] BatchNorm39 <- Convolution39
I0825 11:11:35.239857  1899 net.cpp:509] BatchNorm39 -> Convolution39 (in-place)
I0825 11:11:35.240216  1899 net.cpp:172] Setting up BatchNorm39
I0825 11:11:35.240227  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.240231  1899 net.cpp:194] Memory required for data: 87613560
I0825 11:11:35.240260  1899 layer_factory.hpp:77] Creating layer Scale39
I0825 11:11:35.240272  1899 net.cpp:128] Creating Layer Scale39
I0825 11:11:35.240276  1899 net.cpp:558] Scale39 <- Convolution39
I0825 11:11:35.240284  1899 net.cpp:509] Scale39 -> Convolution39 (in-place)
I0825 11:11:35.240350  1899 layer_factory.hpp:77] Creating layer Scale39
I0825 11:11:35.240553  1899 net.cpp:172] Setting up Scale39
I0825 11:11:35.240563  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.240567  1899 net.cpp:194] Memory required for data: 87777400
I0825 11:11:35.240576  1899 layer_factory.hpp:77] Creating layer Eltwise18
I0825 11:11:35.240586  1899 net.cpp:128] Creating Layer Eltwise18
I0825 11:11:35.240592  1899 net.cpp:558] Eltwise18 <- Eltwise17_ReLU35_0_split_1
I0825 11:11:35.240597  1899 net.cpp:558] Eltwise18 <- Convolution39
I0825 11:11:35.240607  1899 net.cpp:522] Eltwise18 -> Eltwise18
I0825 11:11:35.240645  1899 net.cpp:172] Setting up Eltwise18
I0825 11:11:35.240658  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.240661  1899 net.cpp:194] Memory required for data: 87941240
I0825 11:11:35.240665  1899 layer_factory.hpp:77] Creating layer ReLU37
I0825 11:11:35.240671  1899 net.cpp:128] Creating Layer ReLU37
I0825 11:11:35.240679  1899 net.cpp:558] ReLU37 <- Eltwise18
I0825 11:11:35.240689  1899 net.cpp:509] ReLU37 -> Eltwise18 (in-place)
I0825 11:11:35.243960  1899 net.cpp:172] Setting up ReLU37
I0825 11:11:35.243979  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.243983  1899 net.cpp:194] Memory required for data: 88105080
I0825 11:11:35.243988  1899 layer_factory.hpp:77] Creating layer Eltwise18_ReLU37_0_split
I0825 11:11:35.243995  1899 net.cpp:128] Creating Layer Eltwise18_ReLU37_0_split
I0825 11:11:35.244000  1899 net.cpp:558] Eltwise18_ReLU37_0_split <- Eltwise18
I0825 11:11:35.244009  1899 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_0
I0825 11:11:35.244021  1899 net.cpp:522] Eltwise18_ReLU37_0_split -> Eltwise18_ReLU37_0_split_1
I0825 11:11:35.244091  1899 net.cpp:172] Setting up Eltwise18_ReLU37_0_split
I0825 11:11:35.244101  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.244107  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.244112  1899 net.cpp:194] Memory required for data: 88432760
I0825 11:11:35.244117  1899 layer_factory.hpp:77] Creating layer Convolution40
I0825 11:11:35.244129  1899 net.cpp:128] Creating Layer Convolution40
I0825 11:11:35.244134  1899 net.cpp:558] Convolution40 <- Eltwise18_ReLU37_0_split_0
I0825 11:11:35.244144  1899 net.cpp:522] Convolution40 -> Convolution40
I0825 11:11:35.251899  1899 net.cpp:172] Setting up Convolution40
I0825 11:11:35.251922  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.251927  1899 net.cpp:194] Memory required for data: 88596600
I0825 11:11:35.251938  1899 layer_factory.hpp:77] Creating layer BatchNorm40
I0825 11:11:35.251950  1899 net.cpp:128] Creating Layer BatchNorm40
I0825 11:11:35.251960  1899 net.cpp:558] BatchNorm40 <- Convolution40
I0825 11:11:35.251967  1899 net.cpp:509] BatchNorm40 -> Convolution40 (in-place)
I0825 11:11:35.252336  1899 net.cpp:172] Setting up BatchNorm40
I0825 11:11:35.252347  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.252351  1899 net.cpp:194] Memory required for data: 88760440
I0825 11:11:35.252362  1899 layer_factory.hpp:77] Creating layer Scale40
I0825 11:11:35.252372  1899 net.cpp:128] Creating Layer Scale40
I0825 11:11:35.252375  1899 net.cpp:558] Scale40 <- Convolution40
I0825 11:11:35.252382  1899 net.cpp:509] Scale40 -> Convolution40 (in-place)
I0825 11:11:35.252449  1899 layer_factory.hpp:77] Creating layer Scale40
I0825 11:11:35.252653  1899 net.cpp:172] Setting up Scale40
I0825 11:11:35.252663  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.252667  1899 net.cpp:194] Memory required for data: 88924280
I0825 11:11:35.252676  1899 layer_factory.hpp:77] Creating layer ReLU38
I0825 11:11:35.252687  1899 net.cpp:128] Creating Layer ReLU38
I0825 11:11:35.252707  1899 net.cpp:558] ReLU38 <- Convolution40
I0825 11:11:35.252717  1899 net.cpp:509] ReLU38 -> Convolution40 (in-place)
I0825 11:11:35.253679  1899 net.cpp:172] Setting up ReLU38
I0825 11:11:35.253706  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.253711  1899 net.cpp:194] Memory required for data: 89088120
I0825 11:11:35.253717  1899 layer_factory.hpp:77] Creating layer Convolution41
I0825 11:11:35.253731  1899 net.cpp:128] Creating Layer Convolution41
I0825 11:11:35.253741  1899 net.cpp:558] Convolution41 <- Convolution40
I0825 11:11:35.253751  1899 net.cpp:522] Convolution41 -> Convolution41
I0825 11:11:35.260534  1899 net.cpp:172] Setting up Convolution41
I0825 11:11:35.260561  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.260566  1899 net.cpp:194] Memory required for data: 89251960
I0825 11:11:35.260577  1899 layer_factory.hpp:77] Creating layer BatchNorm41
I0825 11:11:35.260586  1899 net.cpp:128] Creating Layer BatchNorm41
I0825 11:11:35.260591  1899 net.cpp:558] BatchNorm41 <- Convolution41
I0825 11:11:35.260599  1899 net.cpp:509] BatchNorm41 -> Convolution41 (in-place)
I0825 11:11:35.260965  1899 net.cpp:172] Setting up BatchNorm41
I0825 11:11:35.260975  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.260979  1899 net.cpp:194] Memory required for data: 89415800
I0825 11:11:35.260989  1899 layer_factory.hpp:77] Creating layer Scale41
I0825 11:11:35.260999  1899 net.cpp:128] Creating Layer Scale41
I0825 11:11:35.261003  1899 net.cpp:558] Scale41 <- Convolution41
I0825 11:11:35.261011  1899 net.cpp:509] Scale41 -> Convolution41 (in-place)
I0825 11:11:35.261077  1899 layer_factory.hpp:77] Creating layer Scale41
I0825 11:11:35.261279  1899 net.cpp:172] Setting up Scale41
I0825 11:11:35.261289  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.261294  1899 net.cpp:194] Memory required for data: 89579640
I0825 11:11:35.261302  1899 layer_factory.hpp:77] Creating layer Eltwise19
I0825 11:11:35.261317  1899 net.cpp:128] Creating Layer Eltwise19
I0825 11:11:35.261324  1899 net.cpp:558] Eltwise19 <- Eltwise18_ReLU37_0_split_1
I0825 11:11:35.261330  1899 net.cpp:558] Eltwise19 <- Convolution41
I0825 11:11:35.261337  1899 net.cpp:522] Eltwise19 -> Eltwise19
I0825 11:11:35.261374  1899 net.cpp:172] Setting up Eltwise19
I0825 11:11:35.261384  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.261389  1899 net.cpp:194] Memory required for data: 89743480
I0825 11:11:35.261392  1899 layer_factory.hpp:77] Creating layer ReLU39
I0825 11:11:35.261399  1899 net.cpp:128] Creating Layer ReLU39
I0825 11:11:35.261406  1899 net.cpp:558] ReLU39 <- Eltwise19
I0825 11:11:35.261415  1899 net.cpp:509] ReLU39 -> Eltwise19 (in-place)
I0825 11:11:35.262600  1899 net.cpp:172] Setting up ReLU39
I0825 11:11:35.262616  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.262621  1899 net.cpp:194] Memory required for data: 89907320
I0825 11:11:35.262626  1899 layer_factory.hpp:77] Creating layer Eltwise19_ReLU39_0_split
I0825 11:11:35.262634  1899 net.cpp:128] Creating Layer Eltwise19_ReLU39_0_split
I0825 11:11:35.262639  1899 net.cpp:558] Eltwise19_ReLU39_0_split <- Eltwise19
I0825 11:11:35.262647  1899 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_0
I0825 11:11:35.262660  1899 net.cpp:522] Eltwise19_ReLU39_0_split -> Eltwise19_ReLU39_0_split_1
I0825 11:11:35.262728  1899 net.cpp:172] Setting up Eltwise19_ReLU39_0_split
I0825 11:11:35.262739  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.262745  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.262749  1899 net.cpp:194] Memory required for data: 90235000
I0825 11:11:35.262753  1899 layer_factory.hpp:77] Creating layer Convolution42
I0825 11:11:35.262765  1899 net.cpp:128] Creating Layer Convolution42
I0825 11:11:35.262774  1899 net.cpp:558] Convolution42 <- Eltwise19_ReLU39_0_split_0
I0825 11:11:35.262785  1899 net.cpp:522] Convolution42 -> Convolution42
I0825 11:11:35.267582  1899 net.cpp:172] Setting up Convolution42
I0825 11:11:35.267606  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.267627  1899 net.cpp:194] Memory required for data: 90398840
I0825 11:11:35.267638  1899 layer_factory.hpp:77] Creating layer BatchNorm42
I0825 11:11:35.267652  1899 net.cpp:128] Creating Layer BatchNorm42
I0825 11:11:35.267659  1899 net.cpp:558] BatchNorm42 <- Convolution42
I0825 11:11:35.267668  1899 net.cpp:509] BatchNorm42 -> Convolution42 (in-place)
I0825 11:11:35.268038  1899 net.cpp:172] Setting up BatchNorm42
I0825 11:11:35.268049  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.268052  1899 net.cpp:194] Memory required for data: 90562680
I0825 11:11:35.268062  1899 layer_factory.hpp:77] Creating layer Scale42
I0825 11:11:35.268074  1899 net.cpp:128] Creating Layer Scale42
I0825 11:11:35.268081  1899 net.cpp:558] Scale42 <- Convolution42
I0825 11:11:35.268086  1899 net.cpp:509] Scale42 -> Convolution42 (in-place)
I0825 11:11:35.268149  1899 layer_factory.hpp:77] Creating layer Scale42
I0825 11:11:35.268352  1899 net.cpp:172] Setting up Scale42
I0825 11:11:35.268362  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.268368  1899 net.cpp:194] Memory required for data: 90726520
I0825 11:11:35.268375  1899 layer_factory.hpp:77] Creating layer ReLU40
I0825 11:11:35.268384  1899 net.cpp:128] Creating Layer ReLU40
I0825 11:11:35.268389  1899 net.cpp:558] ReLU40 <- Convolution42
I0825 11:11:35.268398  1899 net.cpp:509] ReLU40 -> Convolution42 (in-place)
I0825 11:11:35.268671  1899 net.cpp:172] Setting up ReLU40
I0825 11:11:35.268687  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.268692  1899 net.cpp:194] Memory required for data: 90890360
I0825 11:11:35.268699  1899 layer_factory.hpp:77] Creating layer Convolution43
I0825 11:11:35.268713  1899 net.cpp:128] Creating Layer Convolution43
I0825 11:11:35.268721  1899 net.cpp:558] Convolution43 <- Convolution42
I0825 11:11:35.268733  1899 net.cpp:522] Convolution43 -> Convolution43
I0825 11:11:35.278481  1899 net.cpp:172] Setting up Convolution43
I0825 11:11:35.278512  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.278517  1899 net.cpp:194] Memory required for data: 91054200
I0825 11:11:35.278529  1899 layer_factory.hpp:77] Creating layer BatchNorm43
I0825 11:11:35.278543  1899 net.cpp:128] Creating Layer BatchNorm43
I0825 11:11:35.278553  1899 net.cpp:558] BatchNorm43 <- Convolution43
I0825 11:11:35.278564  1899 net.cpp:509] BatchNorm43 -> Convolution43 (in-place)
I0825 11:11:35.278926  1899 net.cpp:172] Setting up BatchNorm43
I0825 11:11:35.278936  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.278941  1899 net.cpp:194] Memory required for data: 91218040
I0825 11:11:35.278954  1899 layer_factory.hpp:77] Creating layer Scale43
I0825 11:11:35.278964  1899 net.cpp:128] Creating Layer Scale43
I0825 11:11:35.278972  1899 net.cpp:558] Scale43 <- Convolution43
I0825 11:11:35.278978  1899 net.cpp:509] Scale43 -> Convolution43 (in-place)
I0825 11:11:35.279047  1899 layer_factory.hpp:77] Creating layer Scale43
I0825 11:11:35.279255  1899 net.cpp:172] Setting up Scale43
I0825 11:11:35.279264  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.279269  1899 net.cpp:194] Memory required for data: 91381880
I0825 11:11:35.279278  1899 layer_factory.hpp:77] Creating layer Eltwise20
I0825 11:11:35.279287  1899 net.cpp:128] Creating Layer Eltwise20
I0825 11:11:35.279294  1899 net.cpp:558] Eltwise20 <- Eltwise19_ReLU39_0_split_1
I0825 11:11:35.279299  1899 net.cpp:558] Eltwise20 <- Convolution43
I0825 11:11:35.279307  1899 net.cpp:522] Eltwise20 -> Eltwise20
I0825 11:11:35.279342  1899 net.cpp:172] Setting up Eltwise20
I0825 11:11:35.279352  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.279356  1899 net.cpp:194] Memory required for data: 91545720
I0825 11:11:35.279361  1899 layer_factory.hpp:77] Creating layer ReLU41
I0825 11:11:35.279369  1899 net.cpp:128] Creating Layer ReLU41
I0825 11:11:35.279377  1899 net.cpp:558] ReLU41 <- Eltwise20
I0825 11:11:35.279383  1899 net.cpp:509] ReLU41 -> Eltwise20 (in-place)
I0825 11:11:35.280519  1899 net.cpp:172] Setting up ReLU41
I0825 11:11:35.280565  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.280570  1899 net.cpp:194] Memory required for data: 91709560
I0825 11:11:35.280575  1899 layer_factory.hpp:77] Creating layer Eltwise20_ReLU41_0_split
I0825 11:11:35.280583  1899 net.cpp:128] Creating Layer Eltwise20_ReLU41_0_split
I0825 11:11:35.280591  1899 net.cpp:558] Eltwise20_ReLU41_0_split <- Eltwise20
I0825 11:11:35.280601  1899 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_0
I0825 11:11:35.280611  1899 net.cpp:522] Eltwise20_ReLU41_0_split -> Eltwise20_ReLU41_0_split_1
I0825 11:11:35.280683  1899 net.cpp:172] Setting up Eltwise20_ReLU41_0_split
I0825 11:11:35.280694  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.280699  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.280706  1899 net.cpp:194] Memory required for data: 92037240
I0825 11:11:35.280711  1899 layer_factory.hpp:77] Creating layer Convolution44
I0825 11:11:35.280727  1899 net.cpp:128] Creating Layer Convolution44
I0825 11:11:35.280732  1899 net.cpp:558] Convolution44 <- Eltwise20_ReLU41_0_split_0
I0825 11:11:35.280743  1899 net.cpp:522] Convolution44 -> Convolution44
I0825 11:11:35.287602  1899 net.cpp:172] Setting up Convolution44
I0825 11:11:35.287631  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.287636  1899 net.cpp:194] Memory required for data: 92201080
I0825 11:11:35.287647  1899 layer_factory.hpp:77] Creating layer BatchNorm44
I0825 11:11:35.287654  1899 net.cpp:128] Creating Layer BatchNorm44
I0825 11:11:35.287660  1899 net.cpp:558] BatchNorm44 <- Convolution44
I0825 11:11:35.287670  1899 net.cpp:509] BatchNorm44 -> Convolution44 (in-place)
I0825 11:11:35.288043  1899 net.cpp:172] Setting up BatchNorm44
I0825 11:11:35.288053  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.288059  1899 net.cpp:194] Memory required for data: 92364920
I0825 11:11:35.288069  1899 layer_factory.hpp:77] Creating layer Scale44
I0825 11:11:35.288076  1899 net.cpp:128] Creating Layer Scale44
I0825 11:11:35.288081  1899 net.cpp:558] Scale44 <- Convolution44
I0825 11:11:35.288089  1899 net.cpp:509] Scale44 -> Convolution44 (in-place)
I0825 11:11:35.288152  1899 layer_factory.hpp:77] Creating layer Scale44
I0825 11:11:35.288358  1899 net.cpp:172] Setting up Scale44
I0825 11:11:35.288368  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.288374  1899 net.cpp:194] Memory required for data: 92528760
I0825 11:11:35.288383  1899 layer_factory.hpp:77] Creating layer ReLU42
I0825 11:11:35.288391  1899 net.cpp:128] Creating Layer ReLU42
I0825 11:11:35.288396  1899 net.cpp:558] ReLU42 <- Convolution44
I0825 11:11:35.288401  1899 net.cpp:509] ReLU42 -> Convolution44 (in-place)
I0825 11:11:35.289376  1899 net.cpp:172] Setting up ReLU42
I0825 11:11:35.289393  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.289398  1899 net.cpp:194] Memory required for data: 92692600
I0825 11:11:35.289402  1899 layer_factory.hpp:77] Creating layer Convolution45
I0825 11:11:35.289420  1899 net.cpp:128] Creating Layer Convolution45
I0825 11:11:35.289427  1899 net.cpp:558] Convolution45 <- Convolution44
I0825 11:11:35.289433  1899 net.cpp:522] Convolution45 -> Convolution45
I0825 11:11:35.296635  1899 net.cpp:172] Setting up Convolution45
I0825 11:11:35.296663  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.296667  1899 net.cpp:194] Memory required for data: 92856440
I0825 11:11:35.296677  1899 layer_factory.hpp:77] Creating layer BatchNorm45
I0825 11:11:35.296687  1899 net.cpp:128] Creating Layer BatchNorm45
I0825 11:11:35.296694  1899 net.cpp:558] BatchNorm45 <- Convolution45
I0825 11:11:35.296701  1899 net.cpp:509] BatchNorm45 -> Convolution45 (in-place)
I0825 11:11:35.297068  1899 net.cpp:172] Setting up BatchNorm45
I0825 11:11:35.297078  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.297083  1899 net.cpp:194] Memory required for data: 93020280
I0825 11:11:35.297093  1899 layer_factory.hpp:77] Creating layer Scale45
I0825 11:11:35.297106  1899 net.cpp:128] Creating Layer Scale45
I0825 11:11:35.297128  1899 net.cpp:558] Scale45 <- Convolution45
I0825 11:11:35.297135  1899 net.cpp:509] Scale45 -> Convolution45 (in-place)
I0825 11:11:35.297204  1899 layer_factory.hpp:77] Creating layer Scale45
I0825 11:11:35.297410  1899 net.cpp:172] Setting up Scale45
I0825 11:11:35.297420  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.297423  1899 net.cpp:194] Memory required for data: 93184120
I0825 11:11:35.297431  1899 layer_factory.hpp:77] Creating layer Eltwise21
I0825 11:11:35.297444  1899 net.cpp:128] Creating Layer Eltwise21
I0825 11:11:35.297449  1899 net.cpp:558] Eltwise21 <- Eltwise20_ReLU41_0_split_1
I0825 11:11:35.297456  1899 net.cpp:558] Eltwise21 <- Convolution45
I0825 11:11:35.297461  1899 net.cpp:522] Eltwise21 -> Eltwise21
I0825 11:11:35.297497  1899 net.cpp:172] Setting up Eltwise21
I0825 11:11:35.297508  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.297511  1899 net.cpp:194] Memory required for data: 93347960
I0825 11:11:35.297515  1899 layer_factory.hpp:77] Creating layer ReLU43
I0825 11:11:35.297521  1899 net.cpp:128] Creating Layer ReLU43
I0825 11:11:35.297526  1899 net.cpp:558] ReLU43 <- Eltwise21
I0825 11:11:35.297531  1899 net.cpp:509] ReLU43 -> Eltwise21 (in-place)
I0825 11:11:35.300807  1899 net.cpp:172] Setting up ReLU43
I0825 11:11:35.300827  1899 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:11:35.300832  1899 net.cpp:194] Memory required for data: 93511800
I0825 11:11:35.300837  1899 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:11:35.300845  1899 net.cpp:128] Creating Layer Pooling1
I0825 11:11:35.300849  1899 net.cpp:558] Pooling1 <- Eltwise21
I0825 11:11:35.300860  1899 net.cpp:522] Pooling1 -> Pooling1
I0825 11:11:35.305176  1899 net.cpp:172] Setting up Pooling1
I0825 11:11:35.305196  1899 net.cpp:186] Top shape: 10 64 1 1 (640)
I0825 11:11:35.305199  1899 net.cpp:194] Memory required for data: 93514360
I0825 11:11:35.305204  1899 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:11:35.305217  1899 net.cpp:128] Creating Layer InnerProduct1
I0825 11:11:35.305222  1899 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:11:35.305229  1899 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:11:35.305444  1899 net.cpp:172] Setting up InnerProduct1
I0825 11:11:35.305456  1899 net.cpp:186] Top shape: 10 10 (100)
I0825 11:11:35.305460  1899 net.cpp:194] Memory required for data: 93514760
I0825 11:11:35.305470  1899 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0825 11:11:35.305477  1899 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0825 11:11:35.305482  1899 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0825 11:11:35.305490  1899 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0825 11:11:35.305500  1899 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0825 11:11:35.305560  1899 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0825 11:11:35.305570  1899 net.cpp:186] Top shape: 10 10 (100)
I0825 11:11:35.305577  1899 net.cpp:186] Top shape: 10 10 (100)
I0825 11:11:35.305579  1899 net.cpp:194] Memory required for data: 93515560
I0825 11:11:35.305584  1899 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:11:35.305591  1899 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:11:35.305596  1899 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0825 11:11:35.305603  1899 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0825 11:11:35.305610  1899 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:11:35.305620  1899 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:11:35.308356  1899 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:11:35.308380  1899 net.cpp:186] Top shape: (1)
I0825 11:11:35.308384  1899 net.cpp:189]     with loss weight 1
I0825 11:11:35.308401  1899 net.cpp:194] Memory required for data: 93515564
I0825 11:11:35.308406  1899 layer_factory.hpp:77] Creating layer Accuracy1
I0825 11:11:35.308434  1899 net.cpp:128] Creating Layer Accuracy1
I0825 11:11:35.308439  1899 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0825 11:11:35.308445  1899 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0825 11:11:35.308454  1899 net.cpp:522] Accuracy1 -> Accuracy1
I0825 11:11:35.308462  1899 net.cpp:172] Setting up Accuracy1
I0825 11:11:35.308468  1899 net.cpp:186] Top shape: (1)
I0825 11:11:35.308472  1899 net.cpp:194] Memory required for data: 93515568
I0825 11:11:35.308477  1899 net.cpp:303] Accuracy1 does not need backward computation.
I0825 11:11:35.308482  1899 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:11:35.308488  1899 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0825 11:11:35.308492  1899 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:11:35.308497  1899 net.cpp:301] Pooling1 needs backward computation.
I0825 11:11:35.308501  1899 net.cpp:301] ReLU43 needs backward computation.
I0825 11:11:35.308506  1899 net.cpp:301] Eltwise21 needs backward computation.
I0825 11:11:35.308511  1899 net.cpp:301] Scale45 needs backward computation.
I0825 11:11:35.308514  1899 net.cpp:301] BatchNorm45 needs backward computation.
I0825 11:11:35.308518  1899 net.cpp:301] Convolution45 needs backward computation.
I0825 11:11:35.308523  1899 net.cpp:301] ReLU42 needs backward computation.
I0825 11:11:35.308527  1899 net.cpp:301] Scale44 needs backward computation.
I0825 11:11:35.308531  1899 net.cpp:301] BatchNorm44 needs backward computation.
I0825 11:11:35.308535  1899 net.cpp:301] Convolution44 needs backward computation.
I0825 11:11:35.308539  1899 net.cpp:301] Eltwise20_ReLU41_0_split needs backward computation.
I0825 11:11:35.308544  1899 net.cpp:301] ReLU41 needs backward computation.
I0825 11:11:35.308548  1899 net.cpp:301] Eltwise20 needs backward computation.
I0825 11:11:35.308557  1899 net.cpp:301] Scale43 needs backward computation.
I0825 11:11:35.308560  1899 net.cpp:301] BatchNorm43 needs backward computation.
I0825 11:11:35.308564  1899 net.cpp:301] Convolution43 needs backward computation.
I0825 11:11:35.308569  1899 net.cpp:301] ReLU40 needs backward computation.
I0825 11:11:35.308573  1899 net.cpp:301] Scale42 needs backward computation.
I0825 11:11:35.308578  1899 net.cpp:301] BatchNorm42 needs backward computation.
I0825 11:11:35.308583  1899 net.cpp:301] Convolution42 needs backward computation.
I0825 11:11:35.308588  1899 net.cpp:301] Eltwise19_ReLU39_0_split needs backward computation.
I0825 11:11:35.308593  1899 net.cpp:301] ReLU39 needs backward computation.
I0825 11:11:35.308598  1899 net.cpp:301] Eltwise19 needs backward computation.
I0825 11:11:35.308603  1899 net.cpp:301] Scale41 needs backward computation.
I0825 11:11:35.308607  1899 net.cpp:301] BatchNorm41 needs backward computation.
I0825 11:11:35.308611  1899 net.cpp:301] Convolution41 needs backward computation.
I0825 11:11:35.308616  1899 net.cpp:301] ReLU38 needs backward computation.
I0825 11:11:35.308621  1899 net.cpp:301] Scale40 needs backward computation.
I0825 11:11:35.308625  1899 net.cpp:301] BatchNorm40 needs backward computation.
I0825 11:11:35.308630  1899 net.cpp:301] Convolution40 needs backward computation.
I0825 11:11:35.308635  1899 net.cpp:301] Eltwise18_ReLU37_0_split needs backward computation.
I0825 11:11:35.308640  1899 net.cpp:301] ReLU37 needs backward computation.
I0825 11:11:35.308645  1899 net.cpp:301] Eltwise18 needs backward computation.
I0825 11:11:35.308650  1899 net.cpp:301] Scale39 needs backward computation.
I0825 11:11:35.308655  1899 net.cpp:301] BatchNorm39 needs backward computation.
I0825 11:11:35.308658  1899 net.cpp:301] Convolution39 needs backward computation.
I0825 11:11:35.308663  1899 net.cpp:301] ReLU36 needs backward computation.
I0825 11:11:35.308667  1899 net.cpp:301] Scale38 needs backward computation.
I0825 11:11:35.308672  1899 net.cpp:301] BatchNorm38 needs backward computation.
I0825 11:11:35.308676  1899 net.cpp:301] Convolution38 needs backward computation.
I0825 11:11:35.308681  1899 net.cpp:301] Eltwise17_ReLU35_0_split needs backward computation.
I0825 11:11:35.308694  1899 net.cpp:301] ReLU35 needs backward computation.
I0825 11:11:35.308699  1899 net.cpp:301] Eltwise17 needs backward computation.
I0825 11:11:35.308704  1899 net.cpp:301] Scale37 needs backward computation.
I0825 11:11:35.308709  1899 net.cpp:301] BatchNorm37 needs backward computation.
I0825 11:11:35.308713  1899 net.cpp:301] Convolution37 needs backward computation.
I0825 11:11:35.308717  1899 net.cpp:301] ReLU34 needs backward computation.
I0825 11:11:35.308722  1899 net.cpp:301] Scale36 needs backward computation.
I0825 11:11:35.308727  1899 net.cpp:301] BatchNorm36 needs backward computation.
I0825 11:11:35.308732  1899 net.cpp:301] Convolution36 needs backward computation.
I0825 11:11:35.308739  1899 net.cpp:301] Eltwise16_ReLU33_0_split needs backward computation.
I0825 11:11:35.308745  1899 net.cpp:301] ReLU33 needs backward computation.
I0825 11:11:35.308749  1899 net.cpp:301] Eltwise16 needs backward computation.
I0825 11:11:35.308754  1899 net.cpp:301] Scale35 needs backward computation.
I0825 11:11:35.308759  1899 net.cpp:301] BatchNorm35 needs backward computation.
I0825 11:11:35.308763  1899 net.cpp:301] Convolution35 needs backward computation.
I0825 11:11:35.308768  1899 net.cpp:301] ReLU32 needs backward computation.
I0825 11:11:35.308773  1899 net.cpp:301] Scale34 needs backward computation.
I0825 11:11:35.308778  1899 net.cpp:301] BatchNorm34 needs backward computation.
I0825 11:11:35.308781  1899 net.cpp:301] Convolution34 needs backward computation.
I0825 11:11:35.308786  1899 net.cpp:301] Eltwise15_ReLU31_0_split needs backward computation.
I0825 11:11:35.308792  1899 net.cpp:301] ReLU31 needs backward computation.
I0825 11:11:35.308796  1899 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:11:35.308801  1899 net.cpp:301] Scale33 needs backward computation.
I0825 11:11:35.308806  1899 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:11:35.308810  1899 net.cpp:301] Convolution33 needs backward computation.
I0825 11:11:35.308815  1899 net.cpp:301] ReLU30 needs backward computation.
I0825 11:11:35.308820  1899 net.cpp:301] Scale32 needs backward computation.
I0825 11:11:35.308825  1899 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:11:35.308830  1899 net.cpp:301] Convolution32 needs backward computation.
I0825 11:11:35.308835  1899 net.cpp:301] Scale31 needs backward computation.
I0825 11:11:35.308840  1899 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:11:35.308845  1899 net.cpp:301] Convolution31 needs backward computation.
I0825 11:11:35.308849  1899 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:11:35.308856  1899 net.cpp:301] ReLU29 needs backward computation.
I0825 11:11:35.308859  1899 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:11:35.308866  1899 net.cpp:301] Scale30 needs backward computation.
I0825 11:11:35.308871  1899 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:11:35.308874  1899 net.cpp:301] Convolution30 needs backward computation.
I0825 11:11:35.308879  1899 net.cpp:301] ReLU28 needs backward computation.
I0825 11:11:35.308884  1899 net.cpp:301] Scale29 needs backward computation.
I0825 11:11:35.308888  1899 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:11:35.308893  1899 net.cpp:301] Convolution29 needs backward computation.
I0825 11:11:35.308898  1899 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:11:35.308903  1899 net.cpp:301] ReLU27 needs backward computation.
I0825 11:11:35.308908  1899 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:11:35.308913  1899 net.cpp:301] Scale28 needs backward computation.
I0825 11:11:35.308918  1899 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:11:35.308923  1899 net.cpp:301] Convolution28 needs backward computation.
I0825 11:11:35.308928  1899 net.cpp:301] ReLU26 needs backward computation.
I0825 11:11:35.308933  1899 net.cpp:301] Scale27 needs backward computation.
I0825 11:11:35.308946  1899 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:11:35.308951  1899 net.cpp:301] Convolution27 needs backward computation.
I0825 11:11:35.308956  1899 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:11:35.308962  1899 net.cpp:301] ReLU25 needs backward computation.
I0825 11:11:35.308967  1899 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:11:35.308974  1899 net.cpp:301] Scale26 needs backward computation.
I0825 11:11:35.308980  1899 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:11:35.308985  1899 net.cpp:301] Convolution26 needs backward computation.
I0825 11:11:35.308991  1899 net.cpp:301] ReLU24 needs backward computation.
I0825 11:11:35.308996  1899 net.cpp:301] Scale25 needs backward computation.
I0825 11:11:35.309000  1899 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:11:35.309005  1899 net.cpp:301] Convolution25 needs backward computation.
I0825 11:11:35.309010  1899 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:11:35.309015  1899 net.cpp:301] ReLU23 needs backward computation.
I0825 11:11:35.309020  1899 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:11:35.309026  1899 net.cpp:301] Scale24 needs backward computation.
I0825 11:11:35.309029  1899 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:11:35.309034  1899 net.cpp:301] Convolution24 needs backward computation.
I0825 11:11:35.309039  1899 net.cpp:301] ReLU22 needs backward computation.
I0825 11:11:35.309043  1899 net.cpp:301] Scale23 needs backward computation.
I0825 11:11:35.309048  1899 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:11:35.309053  1899 net.cpp:301] Convolution23 needs backward computation.
I0825 11:11:35.309057  1899 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:11:35.309063  1899 net.cpp:301] ReLU21 needs backward computation.
I0825 11:11:35.309068  1899 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:11:35.309074  1899 net.cpp:301] Scale22 needs backward computation.
I0825 11:11:35.309079  1899 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:11:35.309083  1899 net.cpp:301] Convolution22 needs backward computation.
I0825 11:11:35.309088  1899 net.cpp:301] ReLU20 needs backward computation.
I0825 11:11:35.309092  1899 net.cpp:301] Scale21 needs backward computation.
I0825 11:11:35.309098  1899 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:11:35.309103  1899 net.cpp:301] Convolution21 needs backward computation.
I0825 11:11:35.309106  1899 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:11:35.309113  1899 net.cpp:301] ReLU19 needs backward computation.
I0825 11:11:35.309116  1899 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:11:35.309121  1899 net.cpp:301] Scale20 needs backward computation.
I0825 11:11:35.309126  1899 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:11:35.309131  1899 net.cpp:301] Convolution20 needs backward computation.
I0825 11:11:35.309135  1899 net.cpp:301] ReLU18 needs backward computation.
I0825 11:11:35.309140  1899 net.cpp:301] Scale19 needs backward computation.
I0825 11:11:35.309144  1899 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:11:35.309149  1899 net.cpp:301] Convolution19 needs backward computation.
I0825 11:11:35.309154  1899 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:11:35.309164  1899 net.cpp:301] ReLU17 needs backward computation.
I0825 11:11:35.309170  1899 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:11:35.309175  1899 net.cpp:301] Scale18 needs backward computation.
I0825 11:11:35.309180  1899 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:11:35.309183  1899 net.cpp:301] Convolution18 needs backward computation.
I0825 11:11:35.309188  1899 net.cpp:301] ReLU16 needs backward computation.
I0825 11:11:35.309192  1899 net.cpp:301] Scale17 needs backward computation.
I0825 11:11:35.309197  1899 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:11:35.309211  1899 net.cpp:301] Convolution17 needs backward computation.
I0825 11:11:35.309216  1899 net.cpp:301] Scale16 needs backward computation.
I0825 11:11:35.309221  1899 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:11:35.309226  1899 net.cpp:301] Convolution16 needs backward computation.
I0825 11:11:35.309231  1899 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:11:35.309236  1899 net.cpp:301] ReLU15 needs backward computation.
I0825 11:11:35.309240  1899 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:11:35.309245  1899 net.cpp:301] Scale15 needs backward computation.
I0825 11:11:35.309249  1899 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:11:35.309254  1899 net.cpp:301] Convolution15 needs backward computation.
I0825 11:11:35.309258  1899 net.cpp:301] ReLU14 needs backward computation.
I0825 11:11:35.309263  1899 net.cpp:301] Scale14 needs backward computation.
I0825 11:11:35.309268  1899 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:11:35.309273  1899 net.cpp:301] Convolution14 needs backward computation.
I0825 11:11:35.309278  1899 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:11:35.309283  1899 net.cpp:301] ReLU13 needs backward computation.
I0825 11:11:35.309288  1899 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:11:35.309293  1899 net.cpp:301] Scale13 needs backward computation.
I0825 11:11:35.309298  1899 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:11:35.309303  1899 net.cpp:301] Convolution13 needs backward computation.
I0825 11:11:35.309309  1899 net.cpp:301] ReLU12 needs backward computation.
I0825 11:11:35.309312  1899 net.cpp:301] Scale12 needs backward computation.
I0825 11:11:35.309317  1899 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:11:35.309321  1899 net.cpp:301] Convolution12 needs backward computation.
I0825 11:11:35.309326  1899 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:11:35.309331  1899 net.cpp:301] ReLU11 needs backward computation.
I0825 11:11:35.309336  1899 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:11:35.309341  1899 net.cpp:301] Scale11 needs backward computation.
I0825 11:11:35.309346  1899 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:11:35.309350  1899 net.cpp:301] Convolution11 needs backward computation.
I0825 11:11:35.309355  1899 net.cpp:301] ReLU10 needs backward computation.
I0825 11:11:35.309360  1899 net.cpp:301] Scale10 needs backward computation.
I0825 11:11:35.309365  1899 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:11:35.309370  1899 net.cpp:301] Convolution10 needs backward computation.
I0825 11:11:35.309375  1899 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:11:35.309378  1899 net.cpp:301] ReLU9 needs backward computation.
I0825 11:11:35.309383  1899 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:11:35.309391  1899 net.cpp:301] Scale9 needs backward computation.
I0825 11:11:35.309396  1899 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:11:35.309401  1899 net.cpp:301] Convolution9 needs backward computation.
I0825 11:11:35.309406  1899 net.cpp:301] ReLU8 needs backward computation.
I0825 11:11:35.309411  1899 net.cpp:301] Scale8 needs backward computation.
I0825 11:11:35.309414  1899 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:11:35.309418  1899 net.cpp:301] Convolution8 needs backward computation.
I0825 11:11:35.309424  1899 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:11:35.309429  1899 net.cpp:301] ReLU7 needs backward computation.
I0825 11:11:35.309433  1899 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:11:35.309439  1899 net.cpp:301] Scale7 needs backward computation.
I0825 11:11:35.309443  1899 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:11:35.309448  1899 net.cpp:301] Convolution7 needs backward computation.
I0825 11:11:35.309453  1899 net.cpp:301] ReLU6 needs backward computation.
I0825 11:11:35.309464  1899 net.cpp:301] Scale6 needs backward computation.
I0825 11:11:35.309469  1899 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:11:35.309473  1899 net.cpp:301] Convolution6 needs backward computation.
I0825 11:11:35.309478  1899 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:11:35.309484  1899 net.cpp:301] ReLU5 needs backward computation.
I0825 11:11:35.309487  1899 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:11:35.309494  1899 net.cpp:301] Scale5 needs backward computation.
I0825 11:11:35.309497  1899 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:11:35.309501  1899 net.cpp:301] Convolution5 needs backward computation.
I0825 11:11:35.309506  1899 net.cpp:301] ReLU4 needs backward computation.
I0825 11:11:35.309511  1899 net.cpp:301] Scale4 needs backward computation.
I0825 11:11:35.309515  1899 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:11:35.309520  1899 net.cpp:301] Convolution4 needs backward computation.
I0825 11:11:35.309525  1899 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:11:35.309530  1899 net.cpp:301] ReLU3 needs backward computation.
I0825 11:11:35.309533  1899 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:11:35.309538  1899 net.cpp:301] Scale3 needs backward computation.
I0825 11:11:35.309543  1899 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:11:35.309547  1899 net.cpp:301] Convolution3 needs backward computation.
I0825 11:11:35.309552  1899 net.cpp:301] ReLU2 needs backward computation.
I0825 11:11:35.309556  1899 net.cpp:301] Scale2 needs backward computation.
I0825 11:11:35.309561  1899 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:11:35.309566  1899 net.cpp:301] Convolution2 needs backward computation.
I0825 11:11:35.309571  1899 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:11:35.309577  1899 net.cpp:301] ReLU1 needs backward computation.
I0825 11:11:35.309581  1899 net.cpp:301] Scale1 needs backward computation.
I0825 11:11:35.309587  1899 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:11:35.309590  1899 net.cpp:301] Convolution1 needs backward computation.
I0825 11:11:35.309597  1899 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0825 11:11:35.309602  1899 net.cpp:303] Data1 does not need backward computation.
I0825 11:11:35.309605  1899 net.cpp:348] This network produces output Accuracy1
I0825 11:11:35.309609  1899 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:11:35.309721  1899 net.cpp:363] Network initialization done.
I0825 11:11:35.310596  1899 solver.cpp:110] Solver scaffolding done.
I0825 11:11:35.326761  1899 caffe.cpp:313] Starting Optimization
I0825 11:11:35.326791  1899 solver.cpp:425] Solving resnet_cifar10
I0825 11:11:35.326797  1899 solver.cpp:427] Learning Rate Policy: multistep
I0825 11:11:35.334669  1899 solver.cpp:514] Iteration 0, Testing net (#0)
I0825 11:12:29.099375  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:12:29.418380  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0
I0825 11:12:29.418493  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.30259 (* 1 = 2.30259 loss)
I0825 11:12:30.735330  1899 solver.cpp:357] Iteration 0 (9.094e+17 iter/s, 55.5755s/100 iters), loss = 3.43619
I0825 11:12:30.735463  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 3.63482 (* 1 = 3.63482 loss)
I0825 11:12:30.735514  1899 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0825 11:14:21.060652  1899 solver.cpp:357] Iteration 100 (0.906903 iter/s, 110.265s/100 iters), loss = 1.9034
I0825 11:14:21.060899  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.9135 (* 1 = 1.9135 loss)
I0825 11:14:21.060946  1899 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0825 11:16:08.041883  1899 solver.cpp:357] Iteration 200 (0.9348 iter/s, 106.975s/100 iters), loss = 1.72584
I0825 11:16:08.042235  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.74616 (* 1 = 1.74616 loss)
I0825 11:16:08.042282  1899 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0825 11:17:56.336993  1899 solver.cpp:357] Iteration 300 (0.923434 iter/s, 108.291s/100 iters), loss = 1.69452
I0825 11:17:56.337186  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.86784 (* 1 = 1.86784 loss)
I0825 11:17:56.337214  1899 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0825 11:19:34.950702  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:19:47.648593  1899 solver.cpp:357] Iteration 400 (0.8985 iter/s, 111.297s/100 iters), loss = 1.50812
I0825 11:19:47.648772  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.28949 (* 1 = 1.28949 loss)
I0825 11:19:47.648816  1899 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0825 11:21:34.530903  1899 solver.cpp:514] Iteration 500, Testing net (#0)
I0825 11:22:45.183611  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:22:45.446952  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1922
I0825 11:22:45.447118  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.31321 (* 1 = 2.31321 loss)
I0825 11:22:46.272151  1899 solver.cpp:357] Iteration 500 (0.559854 iter/s, 178.618s/100 iters), loss = 1.4043
I0825 11:22:46.272326  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.5105 (* 1 = 1.5105 loss)
I0825 11:22:46.272370  1899 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0825 11:24:31.084071  1899 solver.cpp:357] Iteration 600 (0.954079 iter/s, 104.813s/100 iters), loss = 1.33835
I0825 11:24:31.085649  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.47635 (* 1 = 1.47635 loss)
I0825 11:24:31.085696  1899 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0825 11:26:17.910408  1899 solver.cpp:357] Iteration 700 (0.936109 iter/s, 106.825s/100 iters), loss = 1.20267
I0825 11:26:17.910589  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.21875 (* 1 = 1.21875 loss)
I0825 11:26:17.910615  1899 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0825 11:27:45.107044  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:28:09.009436  1899 solver.cpp:357] Iteration 800 (0.900133 iter/s, 111.095s/100 iters), loss = 1.06652
I0825 11:28:09.009582  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.06141 (* 1 = 1.06141 loss)
I0825 11:28:09.009608  1899 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0825 11:29:55.005890  1899 solver.cpp:357] Iteration 900 (0.943471 iter/s, 105.992s/100 iters), loss = 1.152
I0825 11:29:55.006134  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.1417 (* 1 = 1.1417 loss)
I0825 11:29:55.006180  1899 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0825 11:31:43.488768  1899 solver.cpp:514] Iteration 1000, Testing net (#0)
I0825 11:32:51.124832  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:32:51.338492  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2593
I0825 11:32:51.338625  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.13036 (* 1 = 2.13036 loss)
I0825 11:32:52.203887  1899 solver.cpp:357] Iteration 1000 (0.564398 iter/s, 177.18s/100 iters), loss = 1.04442
I0825 11:32:52.204068  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.960481 (* 1 = 0.960481 loss)
I0825 11:32:52.204114  1899 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0825 11:34:37.202837  1899 solver.cpp:357] Iteration 1100 (0.952505 iter/s, 104.986s/100 iters), loss = 0.86556
I0825 11:34:37.203059  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.745628 (* 1 = 0.745628 loss)
I0825 11:34:37.203104  1899 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0825 11:35:53.561434  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:36:28.018470  1899 solver.cpp:357] Iteration 1200 (0.902471 iter/s, 110.807s/100 iters), loss = 0.884258
I0825 11:36:28.018769  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.857586 (* 1 = 0.857586 loss)
I0825 11:36:28.018817  1899 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0825 11:38:16.559418  1899 solver.cpp:357] Iteration 1300 (0.921409 iter/s, 108.529s/100 iters), loss = 0.803005
I0825 11:38:16.559657  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.864064 (* 1 = 0.864064 loss)
I0825 11:38:16.559705  1899 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0825 11:39:57.627355  1899 solver.cpp:357] Iteration 1400 (0.989493 iter/s, 101.062s/100 iters), loss = 0.741347
I0825 11:39:57.627593  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.675733 (* 1 = 0.675733 loss)
I0825 11:39:57.627621  1899 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0825 11:41:48.379101  1899 solver.cpp:514] Iteration 1500, Testing net (#0)
I0825 11:42:55.996232  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:42:56.275820  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1607
I0825 11:42:56.276175  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.86852 (* 1 = 2.86852 loss)
I0825 11:42:56.996178  1899 solver.cpp:357] Iteration 1500 (0.557542 iter/s, 179.359s/100 iters), loss = 0.653375
I0825 11:42:56.996577  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.710717 (* 1 = 0.710717 loss)
I0825 11:42:56.996757  1899 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0825 11:44:01.044678  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:44:46.356451  1899 solver.cpp:357] Iteration 1600 (0.914452 iter/s, 109.355s/100 iters), loss = 0.675515
I0825 11:44:46.356660  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.707351 (* 1 = 0.707351 loss)
I0825 11:44:46.356706  1899 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0825 11:46:36.728539  1899 solver.cpp:357] Iteration 1700 (0.906103 iter/s, 110.363s/100 iters), loss = 0.794913
I0825 11:46:36.728787  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.744425 (* 1 = 0.744425 loss)
I0825 11:46:36.728834  1899 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0825 11:48:18.792475  1899 solver.cpp:357] Iteration 1800 (0.979716 iter/s, 102.07s/100 iters), loss = 0.849919
I0825 11:48:18.792707  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.854061 (* 1 = 0.854061 loss)
I0825 11:48:18.792753  1899 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0825 11:50:09.624809  1899 solver.cpp:357] Iteration 1900 (0.902087 iter/s, 110.854s/100 iters), loss = 0.751576
I0825 11:50:09.625048  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.705937 (* 1 = 0.705937 loss)
I0825 11:50:09.625094  1899 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0825 11:51:03.594691  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:51:55.016412  1899 solver.cpp:514] Iteration 2000, Testing net (#0)
I0825 11:53:01.342003  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:53:01.672356  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.124699
I0825 11:53:01.672513  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.82696 (* 1 = 2.82696 loss)
I0825 11:53:02.477855  1899 solver.cpp:357] Iteration 2000 (0.578457 iter/s, 172.874s/100 iters), loss = 0.628018
I0825 11:53:02.478031  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.514634 (* 1 = 0.514634 loss)
I0825 11:53:02.478076  1899 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0825 11:54:49.849190  1899 solver.cpp:357] Iteration 2100 (0.931302 iter/s, 107.377s/100 iters), loss = 0.757056
I0825 11:54:49.849398  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.801641 (* 1 = 0.801641 loss)
I0825 11:54:49.849443  1899 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0825 11:56:37.159507  1899 solver.cpp:357] Iteration 2200 (0.931855 iter/s, 107.313s/100 iters), loss = 0.752519
I0825 11:56:37.159889  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.639603 (* 1 = 0.639603 loss)
I0825 11:56:37.159983  1899 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0825 11:58:23.712203  1899 solver.cpp:357] Iteration 2300 (0.938476 iter/s, 106.556s/100 iters), loss = 0.614638
I0825 11:58:23.712503  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.576102 (* 1 = 0.576102 loss)
I0825 11:58:23.712551  1899 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0825 11:59:10.291019  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:00:14.832896  1899 solver.cpp:357] Iteration 2400 (0.899924 iter/s, 111.121s/100 iters), loss = 0.54371
I0825 12:00:14.833127  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.565299 (* 1 = 0.565299 loss)
I0825 12:00:14.833173  1899 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0825 12:01:56.562685  1899 solver.cpp:514] Iteration 2500, Testing net (#0)
I0825 12:03:05.799304  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:03:06.021499  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2138
I0825 12:03:06.021764  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.41246 (* 1 = 2.41246 loss)
I0825 12:03:06.822134  1899 solver.cpp:357] Iteration 2500 (0.581437 iter/s, 171.988s/100 iters), loss = 0.671293
I0825 12:03:06.822546  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.583199 (* 1 = 0.583199 loss)
I0825 12:03:06.822724  1899 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0825 12:04:55.495227  1899 solver.cpp:357] Iteration 2600 (0.920181 iter/s, 108.674s/100 iters), loss = 0.786152
I0825 12:04:55.495764  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.749009 (* 1 = 0.749009 loss)
I0825 12:04:55.495939  1899 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0825 12:06:42.593874  1899 solver.cpp:357] Iteration 2700 (0.933759 iter/s, 107.094s/100 iters), loss = 0.834543
I0825 12:06:42.594426  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.764869 (* 1 = 0.764869 loss)
I0825 12:06:42.594612  1899 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0825 12:07:17.315456  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:08:33.315325  1899 solver.cpp:357] Iteration 2800 (0.903182 iter/s, 110.72s/100 iters), loss = 0.503314
I0825 12:08:33.315641  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497531 (* 1 = 0.497531 loss)
I0825 12:08:33.315732  1899 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0825 12:10:17.131249  1899 solver.cpp:357] Iteration 2900 (0.963241 iter/s, 103.816s/100 iters), loss = 0.667734
I0825 12:10:17.131486  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.626384 (* 1 = 0.626384 loss)
I0825 12:10:17.131533  1899 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0825 12:12:04.397552  1899 solver.cpp:514] Iteration 3000, Testing net (#0)
I0825 12:13:15.395881  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:13:15.672341  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3075
I0825 12:13:15.672689  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.2364 (* 1 = 2.2364 loss)
I0825 12:13:16.407289  1899 solver.cpp:357] Iteration 3000 (0.557808 iter/s, 179.273s/100 iters), loss = 0.593136
I0825 12:13:16.407690  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.65423 (* 1 = 0.65423 loss)
I0825 12:13:16.407865  1899 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0825 12:15:02.974035  1899 solver.cpp:357] Iteration 3100 (0.938408 iter/s, 106.563s/100 iters), loss = 0.519581
I0825 12:15:02.974232  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.567968 (* 1 = 0.567968 loss)
I0825 12:15:02.974283  1899 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0825 12:15:26.207743  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:16:46.710722  1899 solver.cpp:357] Iteration 3200 (0.964028 iter/s, 103.731s/100 iters), loss = 0.675808
I0825 12:16:46.710921  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483327 (* 1 = 0.483327 loss)
I0825 12:16:46.710966  1899 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0825 12:18:35.076629  1899 solver.cpp:357] Iteration 3300 (0.922841 iter/s, 108.361s/100 iters), loss = 0.641454
I0825 12:18:35.076895  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.630452 (* 1 = 0.630452 loss)
I0825 12:18:35.076927  1899 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0825 12:20:21.228498  1899 solver.cpp:357] Iteration 3400 (0.942092 iter/s, 106.147s/100 iters), loss = 0.365772
I0825 12:20:21.228754  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.337313 (* 1 = 0.337313 loss)
I0825 12:20:21.228799  1899 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0825 12:22:11.621395  1899 solver.cpp:514] Iteration 3500, Testing net (#0)
I0825 12:23:22.752368  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:23:23.039834  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5061
I0825 12:23:23.039971  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.48804 (* 1 = 1.48804 loss)
I0825 12:23:23.873909  1899 solver.cpp:357] Iteration 3500 (0.547521 iter/s, 182.641s/100 iters), loss = 0.504989
I0825 12:23:23.874135  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.546854 (* 1 = 0.546854 loss)
I0825 12:23:23.874181  1899 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0825 12:23:37.963587  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:25:06.623275  1899 solver.cpp:357] Iteration 3600 (0.973251 iter/s, 102.748s/100 iters), loss = 0.684428
I0825 12:25:06.623483  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.557946 (* 1 = 0.557946 loss)
I0825 12:25:06.623528  1899 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0825 12:26:56.854234  1899 solver.cpp:357] Iteration 3700 (0.907213 iter/s, 110.228s/100 iters), loss = 0.464473
I0825 12:26:56.854759  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440299 (* 1 = 0.440299 loss)
I0825 12:26:56.854938  1899 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0825 12:28:40.282445  1899 solver.cpp:357] Iteration 3800 (0.966861 iter/s, 103.427s/100 iters), loss = 0.404371
I0825 12:28:40.282645  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.485263 (* 1 = 0.485263 loss)
I0825 12:28:40.282691  1899 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0825 12:30:28.720249  1899 solver.cpp:357] Iteration 3900 (0.922193 iter/s, 108.437s/100 iters), loss = 0.543226
I0825 12:30:28.720409  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.693583 (* 1 = 0.693583 loss)
I0825 12:30:28.720436  1899 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0825 12:30:32.986990  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:32:16.074650  1899 solver.cpp:514] Iteration 4000, Testing net (#0)
I0825 12:33:21.910284  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:33:22.137668  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6495
I0825 12:33:22.137810  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00814 (* 1 = 1.00814 loss)
I0825 12:33:22.945883  1899 solver.cpp:357] Iteration 4000 (0.573967 iter/s, 174.226s/100 iters), loss = 0.551939
I0825 12:33:22.946010  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.562444 (* 1 = 0.562444 loss)
I0825 12:33:22.946038  1899 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0825 12:35:12.414330  1899 solver.cpp:357] Iteration 4100 (0.913545 iter/s, 109.464s/100 iters), loss = 0.553732
I0825 12:35:12.414536  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.516235 (* 1 = 0.516235 loss)
I0825 12:35:12.414593  1899 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0825 12:37:03.538048  1899 solver.cpp:357] Iteration 4200 (0.899919 iter/s, 111.121s/100 iters), loss = 0.478372
I0825 12:37:03.538631  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.370498 (* 1 = 0.370498 loss)
I0825 12:37:03.538678  1899 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0825 12:38:42.646106  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:38:48.955142  1899 solver.cpp:357] Iteration 4300 (0.948657 iter/s, 105.412s/100 iters), loss = 0.451742
I0825 12:38:48.955276  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.476002 (* 1 = 0.476002 loss)
I0825 12:38:48.955302  1899 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0825 12:40:35.455854  1899 solver.cpp:357] Iteration 4400 (0.938922 iter/s, 106.505s/100 iters), loss = 0.490807
I0825 12:40:35.456075  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467924 (* 1 = 0.467924 loss)
I0825 12:40:35.456121  1899 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0825 12:42:20.400324  1899 solver.cpp:514] Iteration 4500, Testing net (#0)
I0825 12:43:27.732553  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:43:28.016746  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6376
I0825 12:43:28.017093  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.15201 (* 1 = 1.15201 loss)
I0825 12:43:28.941783  1899 solver.cpp:357] Iteration 4500 (0.57638 iter/s, 173.497s/100 iters), loss = 0.628865
I0825 12:43:28.942184  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.406805 (* 1 = 0.406805 loss)
I0825 12:43:28.942379  1899 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0825 12:45:19.775061  1899 solver.cpp:357] Iteration 4600 (0.902235 iter/s, 110.836s/100 iters), loss = 0.434473
I0825 12:45:19.775530  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442195 (* 1 = 0.442195 loss)
I0825 12:45:19.775705  1899 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0825 12:46:51.503089  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:47:05.009831  1899 solver.cpp:357] Iteration 4700 (0.950206 iter/s, 105.24s/100 iters), loss = 0.493958
I0825 12:47:05.009986  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.411999 (* 1 = 0.411999 loss)
I0825 12:47:05.010030  1899 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0825 12:48:52.971410  1899 solver.cpp:357] Iteration 4800 (0.926214 iter/s, 107.966s/100 iters), loss = 0.550462
I0825 12:48:52.971572  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.700245 (* 1 = 0.700245 loss)
I0825 12:48:52.971601  1899 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0825 12:50:44.458534  1899 solver.cpp:357] Iteration 4900 (0.896964 iter/s, 111.487s/100 iters), loss = 0.45316
I0825 12:50:44.458744  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325594 (* 1 = 0.325594 loss)
I0825 12:50:44.458787  1899 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0825 12:52:28.796598  1899 solver.cpp:514] Iteration 5000, Testing net (#0)
I0825 12:53:39.399673  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:53:39.610388  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5891
I0825 12:53:39.610544  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.26936 (* 1 = 1.26936 loss)
I0825 12:53:40.502262  1899 solver.cpp:357] Iteration 5000 (0.568012 iter/s, 176.053s/100 iters), loss = 0.385212
I0825 12:53:40.502444  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.482005 (* 1 = 0.482005 loss)
I0825 12:53:40.502491  1899 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0825 12:54:56.146687  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:55:23.243240  1899 solver.cpp:357] Iteration 5100 (0.973314 iter/s, 102.742s/100 iters), loss = 0.501502
I0825 12:55:23.243374  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357408 (* 1 = 0.357408 loss)
I0825 12:55:23.243402  1899 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0825 12:57:08.312723  1899 solver.cpp:357] Iteration 5200 (0.951748 iter/s, 105.07s/100 iters), loss = 0.745143
I0825 12:57:08.312911  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.815512 (* 1 = 0.815512 loss)
I0825 12:57:08.312938  1899 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0825 12:58:59.640817  1899 solver.cpp:357] Iteration 5300 (0.898243 iter/s, 111.328s/100 iters), loss = 0.548125
I0825 12:58:59.641072  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.689968 (* 1 = 0.689968 loss)
I0825 12:58:59.641098  1899 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0825 13:00:47.862551  1899 solver.cpp:357] Iteration 5400 (0.924011 iter/s, 108.224s/100 iters), loss = 0.291524
I0825 13:00:47.862951  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31855 (* 1 = 0.31855 loss)
I0825 13:00:47.863044  1899 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0825 13:01:57.219007  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:02:31.823297  1899 solver.cpp:514] Iteration 5500, Testing net (#0)
I0825 13:03:41.994019  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:03:42.274839  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.744
I0825 13:03:42.274976  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.763778 (* 1 = 0.763778 loss)
I0825 13:03:43.221371  1899 solver.cpp:357] Iteration 5500 (0.570251 iter/s, 175.361s/100 iters), loss = 0.371268
I0825 13:03:43.221547  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392713 (* 1 = 0.392713 loss)
I0825 13:03:43.221591  1899 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0825 13:05:31.426726  1899 solver.cpp:357] Iteration 5600 (0.924184 iter/s, 108.204s/100 iters), loss = 0.478986
I0825 13:05:31.426951  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.311268 (* 1 = 0.311268 loss)
I0825 13:05:31.426997  1899 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0825 13:07:18.819861  1899 solver.cpp:357] Iteration 5700 (0.931145 iter/s, 107.395s/100 iters), loss = 0.493889
I0825 13:07:18.820086  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371219 (* 1 = 0.371219 loss)
I0825 13:07:18.820132  1899 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0825 13:09:09.738312  1899 solver.cpp:357] Iteration 5800 (0.901567 iter/s, 110.918s/100 iters), loss = 0.348469
I0825 13:09:09.738515  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430371 (* 1 = 0.430371 loss)
I0825 13:09:09.738543  1899 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0825 13:10:04.191243  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:10:51.370553  1899 solver.cpp:357] Iteration 5900 (0.983928 iter/s, 101.633s/100 iters), loss = 0.557704
I0825 13:10:51.370756  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437368 (* 1 = 0.437368 loss)
I0825 13:10:51.370803  1899 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0825 13:12:41.147236  1899 solver.cpp:514] Iteration 6000, Testing net (#0)
I0825 13:13:51.949091  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:13:52.225955  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6619
I0825 13:13:52.226095  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02814 (* 1 = 1.02814 loss)
I0825 13:13:53.106086  1899 solver.cpp:357] Iteration 6000 (0.550242 iter/s, 181.738s/100 iters), loss = 0.472889
I0825 13:13:53.106264  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384944 (* 1 = 0.384944 loss)
I0825 13:13:53.106309  1899 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0825 13:15:38.190348  1899 solver.cpp:357] Iteration 6100 (0.95178 iter/s, 105.066s/100 iters), loss = 0.467382
I0825 13:15:38.191857  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.489981 (* 1 = 0.489981 loss)
I0825 13:15:38.191893  1899 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0825 13:17:25.815243  1899 solver.cpp:357] Iteration 6200 (0.929271 iter/s, 107.611s/100 iters), loss = 0.435703
I0825 13:17:25.815768  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442388 (* 1 = 0.442388 loss)
I0825 13:17:25.815946  1899 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0825 13:18:16.757855  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:19:06.231762  1899 solver.cpp:357] Iteration 6300 (0.995926 iter/s, 100.409s/100 iters), loss = 0.464618
I0825 13:19:06.231940  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.47868 (* 1 = 0.47868 loss)
I0825 13:19:06.231969  1899 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0825 13:20:54.820066  1899 solver.cpp:357] Iteration 6400 (0.92101 iter/s, 108.576s/100 iters), loss = 0.553717
I0825 13:20:54.820395  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.68175 (* 1 = 0.68175 loss)
I0825 13:20:54.820441  1899 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0825 13:22:44.532851  1899 solver.cpp:514] Iteration 6500, Testing net (#0)
I0825 13:23:51.095368  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:23:51.332383  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.710101
I0825 13:23:51.332547  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.897254 (* 1 = 0.897254 loss)
I0825 13:23:52.226940  1899 solver.cpp:357] Iteration 6500 (0.56371 iter/s, 177.396s/100 iters), loss = 0.407876
I0825 13:23:52.227095  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45187 (* 1 = 0.45187 loss)
I0825 13:23:52.227139  1899 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0825 13:25:37.594244  1899 solver.cpp:357] Iteration 6600 (0.949115 iter/s, 105.361s/100 iters), loss = 0.549422
I0825 13:25:37.594480  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.600736 (* 1 = 0.600736 loss)
I0825 13:25:37.594525  1899 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0825 13:26:19.238679  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:27:27.873700  1899 solver.cpp:357] Iteration 6700 (0.906856 iter/s, 110.271s/100 iters), loss = 0.485331
I0825 13:27:27.874222  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415079 (* 1 = 0.415079 loss)
I0825 13:27:27.874415  1899 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0825 13:29:13.796016  1899 solver.cpp:357] Iteration 6800 (0.944137 iter/s, 105.917s/100 iters), loss = 0.521508
I0825 13:29:13.796213  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.56239 (* 1 = 0.56239 loss)
I0825 13:29:13.796259  1899 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0825 13:31:04.422729  1899 solver.cpp:357] Iteration 6900 (0.903965 iter/s, 110.624s/100 iters), loss = 0.465313
I0825 13:31:04.422900  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.484119 (* 1 = 0.484119 loss)
I0825 13:31:04.422929  1899 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0825 13:32:44.819039  1899 solver.cpp:514] Iteration 7000, Testing net (#0)
I0825 13:33:52.263095  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:33:52.588557  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7112
I0825 13:33:52.588711  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.885889 (* 1 = 0.885889 loss)
I0825 13:33:53.404417  1899 solver.cpp:357] Iteration 7000 (0.591796 iter/s, 168.977s/100 iters), loss = 0.49531
I0825 13:33:53.404595  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.652711 (* 1 = 0.652711 loss)
I0825 13:33:53.404640  1899 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0825 13:34:23.960175  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:35:42.775244  1899 solver.cpp:357] Iteration 7100 (0.914355 iter/s, 109.367s/100 iters), loss = 0.513091
I0825 13:35:42.775483  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.619219 (* 1 = 0.619219 loss)
I0825 13:35:42.775528  1899 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0825 13:37:30.774631  1899 solver.cpp:357] Iteration 7200 (0.925946 iter/s, 107.998s/100 iters), loss = 0.591493
I0825 13:37:30.774860  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530518 (* 1 = 0.530518 loss)
I0825 13:37:30.774919  1899 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0825 13:39:19.785027  1899 solver.cpp:357] Iteration 7300 (0.917391 iter/s, 109.005s/100 iters), loss = 0.410019
I0825 13:39:19.785559  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39453 (* 1 = 0.39453 loss)
I0825 13:39:19.785737  1899 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0825 13:41:04.792429  1899 solver.cpp:357] Iteration 7400 (0.952326 iter/s, 105.006s/100 iters), loss = 0.49592
I0825 13:41:04.792672  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495791 (* 1 = 0.495791 loss)
I0825 13:41:04.792701  1899 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0825 13:41:26.567344  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:42:48.932821  1899 solver.cpp:514] Iteration 7500, Testing net (#0)
I0825 13:43:58.081152  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:43:58.345584  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6378
I0825 13:43:58.345746  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.13392 (* 1 = 1.13392 loss)
I0825 13:43:59.208647  1899 solver.cpp:357] Iteration 7500 (0.573343 iter/s, 174.416s/100 iters), loss = 0.381856
I0825 13:43:59.208817  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372225 (* 1 = 0.372225 loss)
I0825 13:43:59.208863  1899 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0825 13:45:49.853132  1899 solver.cpp:357] Iteration 7600 (0.903839 iter/s, 110.639s/100 iters), loss = 0.404978
I0825 13:45:49.853346  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.301389 (* 1 = 0.301389 loss)
I0825 13:45:49.853390  1899 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0825 13:47:30.320413  1899 solver.cpp:357] Iteration 7700 (0.995343 iter/s, 100.468s/100 iters), loss = 0.409988
I0825 13:47:30.320788  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.476758 (* 1 = 0.476758 loss)
I0825 13:47:30.320881  1899 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0825 13:49:20.683159  1899 solver.cpp:357] Iteration 7800 (0.906053 iter/s, 110.369s/100 iters), loss = 0.393599
I0825 13:49:20.683331  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421184 (* 1 = 0.421184 loss)
I0825 13:49:20.683357  1899 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0825 13:49:31.631692  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:51:08.849812  1899 solver.cpp:357] Iteration 7900 (0.924463 iter/s, 108.171s/100 iters), loss = 0.426059
I0825 13:51:08.850044  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491529 (* 1 = 0.491529 loss)
I0825 13:51:08.850091  1899 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0825 13:52:56.113884  1899 solver.cpp:514] Iteration 8000, Testing net (#0)
I0825 13:54:06.531268  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:54:06.706881  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.745
I0825 13:54:06.707239  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.790727 (* 1 = 0.790727 loss)
I0825 13:54:07.577121  1899 solver.cpp:357] Iteration 8000 (0.559497 iter/s, 178.732s/100 iters), loss = 0.390942
I0825 13:54:07.577530  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432502 (* 1 = 0.432502 loss)
I0825 13:54:07.577706  1899 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0825 13:55:48.232205  1899 solver.cpp:357] Iteration 8100 (0.99347 iter/s, 100.657s/100 iters), loss = 0.419694
I0825 13:55:48.232390  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.434227 (* 1 = 0.434227 loss)
I0825 13:55:48.232436  1899 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0825 13:57:35.095659  1899 solver.cpp:357] Iteration 8200 (0.935755 iter/s, 106.866s/100 iters), loss = 0.399092
I0825 13:57:35.095875  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431225 (* 1 = 0.431225 loss)
I0825 13:57:35.095919  1899 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0825 13:57:35.837539  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:59:25.572227  1899 solver.cpp:357] Iteration 8300 (0.905154 iter/s, 110.478s/100 iters), loss = 0.526288
I0825 13:59:25.572742  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463717 (* 1 = 0.463717 loss)
I0825 13:59:25.572918  1899 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0825 14:01:10.732003  1899 solver.cpp:357] Iteration 8400 (0.950921 iter/s, 105.161s/100 iters), loss = 0.48798
I0825 14:01:10.732311  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456369 (* 1 = 0.456369 loss)
I0825 14:01:10.732357  1899 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0825 14:02:55.776788  1899 solver.cpp:514] Iteration 8500, Testing net (#0)
I0825 14:04:06.469884  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:04:06.764144  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6997
I0825 14:04:06.764303  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.924356 (* 1 = 0.924356 loss)
I0825 14:04:07.602423  1899 solver.cpp:357] Iteration 8500 (0.565375 iter/s, 176.874s/100 iters), loss = 0.414764
I0825 14:04:07.602593  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495458 (* 1 = 0.495458 loss)
I0825 14:04:07.602638  1899 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0825 14:05:43.267045  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:05:52.967628  1899 solver.cpp:357] Iteration 8600 (0.949091 iter/s, 105.364s/100 iters), loss = 0.4531
I0825 14:05:52.967813  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496676 (* 1 = 0.496676 loss)
I0825 14:05:52.967859  1899 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0825 14:07:42.853847  1899 solver.cpp:357] Iteration 8700 (0.910042 iter/s, 109.885s/100 iters), loss = 0.470423
I0825 14:07:42.854080  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503834 (* 1 = 0.503834 loss)
I0825 14:07:42.854125  1899 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0825 14:09:26.939009  1899 solver.cpp:357] Iteration 8800 (0.960765 iter/s, 104.084s/100 iters), loss = 0.423504
I0825 14:09:26.939236  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.411916 (* 1 = 0.411916 loss)
I0825 14:09:26.939285  1899 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0825 14:11:12.069022  1899 solver.cpp:357] Iteration 8900 (0.951217 iter/s, 105.129s/100 iters), loss = 0.456174
I0825 14:11:12.069265  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414675 (* 1 = 0.414675 loss)
I0825 14:11:12.069310  1899 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0825 14:12:42.787375  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:13:01.378479  1899 solver.cpp:514] Iteration 9000, Testing net (#0)
I0825 14:14:07.948372  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:14:08.146394  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.672
I0825 14:14:08.146636  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.0768 (* 1 = 1.0768 loss)
I0825 14:14:08.962285  1899 solver.cpp:357] Iteration 9000 (0.565305 iter/s, 176.896s/100 iters), loss = 0.276299
I0825 14:14:08.962607  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.322352 (* 1 = 0.322352 loss)
I0825 14:14:08.962719  1899 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0825 14:15:57.730706  1899 solver.cpp:357] Iteration 9100 (0.919385 iter/s, 108.768s/100 iters), loss = 0.464934
I0825 14:15:57.730911  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.586346 (* 1 = 0.586346 loss)
I0825 14:15:57.730937  1899 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0825 14:17:44.714977  1899 solver.cpp:357] Iteration 9200 (0.934727 iter/s, 106.983s/100 iters), loss = 0.378031
I0825 14:17:44.715126  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399803 (* 1 = 0.399803 loss)
I0825 14:17:44.715153  1899 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0825 14:19:30.402488  1899 solver.cpp:357] Iteration 9300 (0.94622 iter/s, 105.684s/100 iters), loss = 0.426269
I0825 14:19:30.403594  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.555572 (* 1 = 0.555572 loss)
I0825 14:19:30.403692  1899 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0825 14:20:50.471246  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:21:21.064189  1899 solver.cpp:357] Iteration 9400 (0.903651 iter/s, 110.662s/100 iters), loss = 0.512092
I0825 14:21:21.064535  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.322377 (* 1 = 0.322377 loss)
I0825 14:21:21.064584  1899 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0825 14:23:06.003502  1899 solver.cpp:514] Iteration 9500, Testing net (#0)
I0825 14:24:13.480198  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:24:13.815819  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.774901
I0825 14:24:13.815975  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.674141 (* 1 = 0.674141 loss)
I0825 14:24:14.752684  1899 solver.cpp:357] Iteration 9500 (0.575739 iter/s, 173.69s/100 iters), loss = 0.516421
I0825 14:24:14.752852  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.582047 (* 1 = 0.582047 loss)
I0825 14:24:14.752895  1899 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0825 14:26:00.059320  1899 solver.cpp:357] Iteration 9600 (0.949627 iter/s, 105.304s/100 iters), loss = 0.454799
I0825 14:26:00.059808  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530407 (* 1 = 0.530407 loss)
I0825 14:26:00.059984  1899 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0825 14:27:46.189498  1899 solver.cpp:357] Iteration 9700 (0.942276 iter/s, 106.126s/100 iters), loss = 0.492702
I0825 14:27:46.189744  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502638 (* 1 = 0.502638 loss)
I0825 14:27:46.189790  1899 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0825 14:28:55.093806  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:29:36.149495  1899 solver.cpp:357] Iteration 9800 (0.909438 iter/s, 109.958s/100 iters), loss = 0.386711
I0825 14:29:36.149677  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422701 (* 1 = 0.422701 loss)
I0825 14:29:36.149703  1899 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0825 14:31:25.619186  1899 solver.cpp:357] Iteration 9900 (0.913529 iter/s, 109.466s/100 iters), loss = 0.356821
I0825 14:31:25.619840  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373063 (* 1 = 0.373063 loss)
I0825 14:31:25.620015  1899 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0825 14:33:06.023715  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_10000.caffemodel
I0825 14:33:06.066232  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_10000.solverstate
I0825 14:33:06.098934  1899 solver.cpp:514] Iteration 10000, Testing net (#0)
I0825 14:34:14.681156  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:34:15.011132  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7478
I0825 14:34:15.011288  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.75919 (* 1 = 0.75919 loss)
I0825 14:34:15.890628  1899 solver.cpp:357] Iteration 10000 (0.5873 iter/s, 170.271s/100 iters), loss = 0.383352
I0825 14:34:15.890790  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365243 (* 1 = 0.365243 loss)
I0825 14:34:15.890836  1899 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0825 14:36:06.928432  1899 solver.cpp:357] Iteration 10100 (0.900609 iter/s, 111.036s/100 iters), loss = 0.428314
I0825 14:36:06.928618  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432124 (* 1 = 0.432124 loss)
I0825 14:36:06.928650  1899 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0825 14:37:00.673234  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:37:51.201261  1899 solver.cpp:357] Iteration 10200 (0.959022 iter/s, 104.273s/100 iters), loss = 0.414385
I0825 14:37:51.201486  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415482 (* 1 = 0.415482 loss)
I0825 14:37:51.201530  1899 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0825 14:39:40.783720  1899 solver.cpp:357] Iteration 10300 (0.912542 iter/s, 109.584s/100 iters), loss = 0.395183
I0825 14:39:40.783932  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431066 (* 1 = 0.431066 loss)
I0825 14:39:40.783978  1899 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0825 14:41:23.343142  1899 solver.cpp:357] Iteration 10400 (0.975044 iter/s, 102.559s/100 iters), loss = 0.432254
I0825 14:41:23.343704  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.553206 (* 1 = 0.553206 loss)
I0825 14:41:23.343799  1899 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0825 14:43:10.439499  1899 solver.cpp:514] Iteration 10500, Testing net (#0)
I0825 14:44:21.805565  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:44:22.156311  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5091
I0825 14:44:22.156458  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.80547 (* 1 = 1.80547 loss)
I0825 14:44:23.083443  1899 solver.cpp:357] Iteration 10500 (0.556361 iter/s, 179.739s/100 iters), loss = 0.448001
I0825 14:44:23.083608  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36983 (* 1 = 0.36983 loss)
I0825 14:44:23.083652  1899 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0825 14:45:11.351357  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:46:07.179476  1899 solver.cpp:357] Iteration 10600 (0.96067 iter/s, 104.094s/100 iters), loss = 0.412997
I0825 14:46:07.179988  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472954 (* 1 = 0.472954 loss)
I0825 14:46:07.180166  1899 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0825 14:47:46.287770  1899 solver.cpp:357] Iteration 10700 (1.009 iter/s, 99.1081s/100 iters), loss = 0.280514
I0825 14:47:46.287976  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305645 (* 1 = 0.305645 loss)
I0825 14:47:46.288022  1899 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0825 14:49:37.710477  1899 solver.cpp:357] Iteration 10800 (0.897498 iter/s, 111.421s/100 iters), loss = 0.354841
I0825 14:49:37.710713  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402782 (* 1 = 0.402782 loss)
I0825 14:49:37.710762  1899 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0825 14:51:22.627178  1899 solver.cpp:357] Iteration 10900 (0.953136 iter/s, 104.917s/100 iters), loss = 0.468358
I0825 14:51:22.627360  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53325 (* 1 = 0.53325 loss)
I0825 14:51:22.627388  1899 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0825 14:52:01.559739  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:53:12.445808  1899 solver.cpp:514] Iteration 11000, Testing net (#0)
I0825 14:54:23.956909  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:54:24.156507  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6597
I0825 14:54:24.156657  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07917 (* 1 = 1.07917 loss)
I0825 14:54:25.011941  1899 solver.cpp:357] Iteration 11000 (0.548298 iter/s, 182.382s/100 iters), loss = 0.311313
I0825 14:54:25.012115  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.283695 (* 1 = 0.283695 loss)
I0825 14:54:25.012161  1899 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0825 14:56:05.819197  1899 solver.cpp:357] Iteration 11100 (0.991981 iter/s, 100.808s/100 iters), loss = 0.431675
I0825 14:56:05.819509  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.592242 (* 1 = 0.592242 loss)
I0825 14:56:05.819602  1899 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0825 14:57:57.038465  1899 solver.cpp:357] Iteration 11200 (0.898997 iter/s, 111.235s/100 iters), loss = 0.311562
I0825 14:57:57.038596  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.269698 (* 1 = 0.269698 loss)
I0825 14:57:57.038645  1899 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0825 14:59:39.855849  1899 solver.cpp:357] Iteration 11300 (0.972493 iter/s, 102.828s/100 iters), loss = 0.426098
I0825 14:59:39.856391  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407614 (* 1 = 0.407614 loss)
I0825 14:59:39.856568  1899 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0825 15:00:04.106689  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:01:26.048477  1899 solver.cpp:357] Iteration 11400 (0.941593 iter/s, 106.203s/100 iters), loss = 0.41161
I0825 15:01:26.048714  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360979 (* 1 = 0.360979 loss)
I0825 15:01:26.048739  1899 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0825 15:03:10.832103  1899 solver.cpp:514] Iteration 11500, Testing net (#0)
I0825 15:04:17.752024  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:04:18.070515  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7541
I0825 15:04:18.070667  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.763962 (* 1 = 0.763962 loss)
I0825 15:04:18.896709  1899 solver.cpp:357] Iteration 11500 (0.578492 iter/s, 172.863s/100 iters), loss = 0.519647
I0825 15:04:18.896880  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.520711 (* 1 = 0.520711 loss)
I0825 15:04:18.896925  1899 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0825 15:06:07.354241  1899 solver.cpp:357] Iteration 11600 (0.921967 iter/s, 108.464s/100 iters), loss = 0.325412
I0825 15:06:07.355957  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.310373 (* 1 = 0.310373 loss)
I0825 15:06:07.356016  1899 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0825 15:07:58.125303  1899 solver.cpp:357] Iteration 11700 (0.902736 iter/s, 110.774s/100 iters), loss = 0.435721
I0825 15:07:58.125548  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.258183 (* 1 = 0.258183 loss)
I0825 15:07:58.125593  1899 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0825 15:08:15.988777  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:09:41.369560  1899 solver.cpp:357] Iteration 11800 (0.968529 iter/s, 103.249s/100 iters), loss = 0.561054
I0825 15:09:41.369802  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487371 (* 1 = 0.487371 loss)
I0825 15:09:41.369849  1899 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0825 15:11:29.612650  1899 solver.cpp:357] Iteration 11900 (0.923833 iter/s, 108.245s/100 iters), loss = 0.374721
I0825 15:11:29.612880  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359239 (* 1 = 0.359239 loss)
I0825 15:11:29.612926  1899 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0825 15:13:13.008491  1899 solver.cpp:514] Iteration 12000, Testing net (#0)
I0825 15:14:20.083293  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:14:20.350852  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.644699
I0825 15:14:20.350942  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.03227 (* 1 = 1.03227 loss)
I0825 15:14:21.253157  1899 solver.cpp:357] Iteration 12000 (0.582581 iter/s, 171.65s/100 iters), loss = 0.39758
I0825 15:14:21.253331  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448627 (* 1 = 0.448627 loss)
I0825 15:14:21.253377  1899 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0825 15:16:12.821077  1899 solver.cpp:357] Iteration 12100 (0.89631 iter/s, 111.569s/100 iters), loss = 0.36115
I0825 15:16:12.821308  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343285 (* 1 = 0.343285 loss)
I0825 15:16:12.821353  1899 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0825 15:16:20.407397  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:17:55.669267  1899 solver.cpp:357] Iteration 12200 (0.972269 iter/s, 102.852s/100 iters), loss = 0.33551
I0825 15:17:55.669777  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296989 (* 1 = 0.296989 loss)
I0825 15:17:55.669987  1899 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0825 15:19:43.737500  1899 solver.cpp:357] Iteration 12300 (0.925324 iter/s, 108.07s/100 iters), loss = 0.332646
I0825 15:19:43.737738  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351908 (* 1 = 0.351908 loss)
I0825 15:19:43.737784  1899 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0825 15:21:33.474099  1899 solver.cpp:357] Iteration 12400 (0.911257 iter/s, 109.739s/100 iters), loss = 0.636761
I0825 15:21:33.478448  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.712749 (* 1 = 0.712749 loss)
I0825 15:21:33.478509  1899 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0825 15:23:15.519701  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:23:17.589937  1899 solver.cpp:514] Iteration 12500, Testing net (#0)
I0825 15:24:25.557847  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:24:25.790132  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6638
I0825 15:24:25.790289  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08528 (* 1 = 1.08528 loss)
I0825 15:24:26.625385  1899 solver.cpp:357] Iteration 12500 (0.577512 iter/s, 173.156s/100 iters), loss = 0.445471
I0825 15:24:26.625545  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.505404 (* 1 = 0.505404 loss)
I0825 15:24:26.625591  1899 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0825 15:26:12.421245  1899 solver.cpp:357] Iteration 12600 (0.945186 iter/s, 105.799s/100 iters), loss = 0.412183
I0825 15:26:12.421759  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.291608 (* 1 = 0.291608 loss)
I0825 15:26:12.421938  1899 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0825 15:27:58.219578  1899 solver.cpp:357] Iteration 12700 (0.945202 iter/s, 105.797s/100 iters), loss = 0.53539
I0825 15:27:58.220016  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509029 (* 1 = 0.509029 loss)
I0825 15:27:58.220062  1899 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0825 15:29:48.807404  1899 solver.cpp:357] Iteration 12800 (0.904265 iter/s, 110.587s/100 iters), loss = 0.430735
I0825 15:29:48.807888  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.553177 (* 1 = 0.553177 loss)
I0825 15:29:48.808060  1899 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0825 15:31:24.826691  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:31:36.645035  1899 solver.cpp:357] Iteration 12900 (0.927305 iter/s, 107.839s/100 iters), loss = 0.427633
I0825 15:31:36.645212  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.314029 (* 1 = 0.314029 loss)
I0825 15:31:36.645258  1899 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0825 15:33:19.261185  1899 solver.cpp:514] Iteration 13000, Testing net (#0)
I0825 15:34:29.695107  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:34:30.032992  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.568201
I0825 15:34:30.033344  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.73131 (* 1 = 1.73131 loss)
I0825 15:34:30.764690  1899 solver.cpp:357] Iteration 13000 (0.5743 iter/s, 174.125s/100 iters), loss = 0.354834
I0825 15:34:30.765086  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384616 (* 1 = 0.384616 loss)
I0825 15:34:30.765265  1899 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0825 15:36:13.648772  1899 solver.cpp:357] Iteration 13100 (0.971959 iter/s, 102.885s/100 iters), loss = 0.395668
I0825 15:36:13.649041  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385433 (* 1 = 0.385433 loss)
I0825 15:36:13.649088  1899 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0825 15:38:00.151216  1899 solver.cpp:357] Iteration 13200 (0.938955 iter/s, 106.501s/100 iters), loss = 0.319236
I0825 15:38:00.151419  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.271921 (* 1 = 0.271921 loss)
I0825 15:38:00.151463  1899 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0825 15:39:26.660780  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:39:48.055878  1899 solver.cpp:357] Iteration 13300 (0.9267 iter/s, 107.91s/100 iters), loss = 0.452741
I0825 15:39:48.056043  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400462 (* 1 = 0.400462 loss)
I0825 15:39:48.056088  1899 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0825 15:41:31.359346  1899 solver.cpp:357] Iteration 13400 (0.967994 iter/s, 103.306s/100 iters), loss = 0.592829
I0825 15:41:31.359625  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486798 (* 1 = 0.486798 loss)
I0825 15:41:31.359670  1899 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0825 15:43:20.280375  1899 solver.cpp:514] Iteration 13500, Testing net (#0)
I0825 15:44:31.061939  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:44:31.406488  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7184
I0825 15:44:31.406637  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.949767 (* 1 = 0.949767 loss)
I0825 15:44:32.300637  1899 solver.cpp:357] Iteration 13500 (0.552658 iter/s, 180.944s/100 iters), loss = 0.417091
I0825 15:44:32.300797  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.427386 (* 1 = 0.427386 loss)
I0825 15:44:32.300843  1899 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0825 15:46:17.496165  1899 solver.cpp:357] Iteration 13600 (0.950584 iter/s, 105.198s/100 iters), loss = 0.377051
I0825 15:46:17.496382  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.33571 (* 1 = 0.33571 loss)
I0825 15:46:17.496429  1899 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0825 15:47:31.655356  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:48:03.268337  1899 solver.cpp:357] Iteration 13700 (0.945711 iter/s, 105.741s/100 iters), loss = 0.442701
I0825 15:48:03.270030  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393312 (* 1 = 0.393312 loss)
I0825 15:48:03.270081  1899 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0825 15:49:48.286707  1899 solver.cpp:357] Iteration 13800 (0.952664 iter/s, 104.969s/100 iters), loss = 0.285306
I0825 15:49:48.286976  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352023 (* 1 = 0.352023 loss)
I0825 15:49:48.287029  1899 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0825 15:51:33.892562  1899 solver.cpp:357] Iteration 13900 (0.947224 iter/s, 105.572s/100 iters), loss = 0.328052
I0825 15:51:33.892787  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32247 (* 1 = 0.32247 loss)
I0825 15:51:33.892832  1899 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0825 15:53:23.860414  1899 solver.cpp:514] Iteration 14000, Testing net (#0)
I0825 15:54:30.672570  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:54:30.867486  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7057
I0825 15:54:30.867637  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.887728 (* 1 = 0.887728 loss)
I0825 15:54:31.694591  1899 solver.cpp:357] Iteration 14000 (0.562537 iter/s, 177.766s/100 iters), loss = 0.319869
I0825 15:54:31.694763  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.306063 (* 1 = 0.306063 loss)
I0825 15:54:31.694808  1899 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0825 15:55:30.987749  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:56:15.433362  1899 solver.cpp:357] Iteration 14100 (0.964092 iter/s, 103.725s/100 iters), loss = 0.410235
I0825 15:56:15.433543  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440483 (* 1 = 0.440483 loss)
I0825 15:56:15.433573  1899 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0825 15:58:06.451958  1899 solver.cpp:357] Iteration 14200 (0.900881 iter/s, 111.002s/100 iters), loss = 0.425065
I0825 15:58:06.453637  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.316524 (* 1 = 0.316524 loss)
I0825 15:58:06.453685  1899 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0825 15:59:51.861517  1899 solver.cpp:357] Iteration 14300 (0.948785 iter/s, 105.398s/100 iters), loss = 0.439919
I0825 15:59:51.861757  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429714 (* 1 = 0.429714 loss)
I0825 15:59:51.861802  1899 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0825 16:01:41.304103  1899 solver.cpp:357] Iteration 14400 (0.913809 iter/s, 109.432s/100 iters), loss = 0.33065
I0825 16:01:41.304332  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.257182 (* 1 = 0.257182 loss)
I0825 16:01:41.304378  1899 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0825 16:02:30.426695  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:03:21.122185  1899 solver.cpp:514] Iteration 14500, Testing net (#0)
I0825 16:04:27.611218  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:04:27.905280  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7069
I0825 16:04:27.905638  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.864527 (* 1 = 0.864527 loss)
I0825 16:04:28.677912  1899 solver.cpp:357] Iteration 14500 (0.597488 iter/s, 167.367s/100 iters), loss = 0.433785
I0825 16:04:28.678311  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363951 (* 1 = 0.363951 loss)
I0825 16:04:28.678496  1899 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0825 16:06:19.073076  1899 solver.cpp:357] Iteration 14600 (0.905714 iter/s, 110.41s/100 iters), loss = 0.468723
I0825 16:06:19.073343  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4319 (* 1 = 0.4319 loss)
I0825 16:06:19.073390  1899 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0825 16:08:05.784756  1899 solver.cpp:357] Iteration 14700 (0.937043 iter/s, 106.719s/100 iters), loss = 0.405408
I0825 16:08:05.785174  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394324 (* 1 = 0.394324 loss)
I0825 16:08:05.785289  1899 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0825 16:09:51.182509  1899 solver.cpp:357] Iteration 14800 (0.948728 iter/s, 105.404s/100 iters), loss = 0.328731
I0825 16:09:51.182693  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371758 (* 1 = 0.371758 loss)
I0825 16:09:51.182721  1899 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0825 16:10:34.848779  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:11:40.153014  1899 solver.cpp:357] Iteration 14900 (0.917649 iter/s, 108.974s/100 iters), loss = 0.336043
I0825 16:11:40.153554  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32669 (* 1 = 0.32669 loss)
I0825 16:11:40.153734  1899 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0825 16:13:23.282929  1899 solver.cpp:514] Iteration 15000, Testing net (#0)
I0825 16:14:32.804841  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:14:33.077188  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7
I0825 16:14:33.077544  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.954499 (* 1 = 0.954499 loss)
I0825 16:14:33.880859  1899 solver.cpp:357] Iteration 15000 (0.57561 iter/s, 173.729s/100 iters), loss = 0.499486
I0825 16:14:33.881258  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.551913 (* 1 = 0.551913 loss)
I0825 16:14:33.881433  1899 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0825 16:16:22.727969  1899 solver.cpp:357] Iteration 15100 (0.918737 iter/s, 108.845s/100 iters), loss = 0.496988
I0825 16:16:22.728202  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486595 (* 1 = 0.486595 loss)
I0825 16:16:22.728248  1899 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0825 16:18:03.905282  1899 solver.cpp:357] Iteration 15200 (0.988396 iter/s, 101.174s/100 iters), loss = 0.569973
I0825 16:18:03.905506  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437016 (* 1 = 0.437016 loss)
I0825 16:18:03.905552  1899 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0825 16:18:39.166005  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:19:54.584213  1899 solver.cpp:357] Iteration 15300 (0.903561 iter/s, 110.673s/100 iters), loss = 0.404315
I0825 16:19:54.584478  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.470281 (* 1 = 0.470281 loss)
I0825 16:19:54.584525  1899 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0825 16:21:42.226981  1899 solver.cpp:357] Iteration 15400 (0.929032 iter/s, 107.639s/100 iters), loss = 0.400538
I0825 16:21:42.227412  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.33681 (* 1 = 0.33681 loss)
I0825 16:21:42.227527  1899 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0825 16:23:27.979218  1899 solver.cpp:514] Iteration 15500, Testing net (#0)
I0825 16:24:37.381562  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:24:37.614763  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6635
I0825 16:24:37.615110  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02606 (* 1 = 1.02606 loss)
I0825 16:24:38.424669  1899 solver.cpp:357] Iteration 15500 (0.567554 iter/s, 176.195s/100 iters), loss = 0.332794
I0825 16:24:38.425062  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383486 (* 1 = 0.383486 loss)
I0825 16:24:38.425231  1899 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0825 16:26:19.299280  1899 solver.cpp:357] Iteration 15600 (0.991328 iter/s, 100.875s/100 iters), loss = 0.446308
I0825 16:26:19.299475  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444809 (* 1 = 0.444809 loss)
I0825 16:26:19.299521  1899 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0825 16:26:41.491710  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:28:06.274037  1899 solver.cpp:357] Iteration 15700 (0.934816 iter/s, 106.973s/100 iters), loss = 0.384631
I0825 16:28:06.274266  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.264328 (* 1 = 0.264328 loss)
I0825 16:28:06.274312  1899 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0825 16:29:57.176441  1899 solver.cpp:357] Iteration 15800 (0.901726 iter/s, 110.898s/100 iters), loss = 0.434873
I0825 16:29:57.176661  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.511987 (* 1 = 0.511987 loss)
I0825 16:29:57.176709  1899 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0825 16:31:43.237077  1899 solver.cpp:357] Iteration 15900 (0.942893 iter/s, 106.057s/100 iters), loss = 0.234798
I0825 16:31:43.237282  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.148649 (* 1 = 0.148649 loss)
I0825 16:31:43.237329  1899 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0825 16:33:28.947731  1899 solver.cpp:514] Iteration 16000, Testing net (#0)
I0825 16:34:40.050772  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:34:40.352094  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7938
I0825 16:34:40.352218  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.617315 (* 1 = 0.617315 loss)
I0825 16:34:41.170217  1899 solver.cpp:357] Iteration 16000 (0.562021 iter/s, 177.929s/100 iters), loss = 0.374189
I0825 16:34:41.170363  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441363 (* 1 = 0.441363 loss)
I0825 16:34:41.170393  1899 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0825 16:34:56.166689  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:36:26.923307  1899 solver.cpp:357] Iteration 16100 (0.9456 iter/s, 105.753s/100 iters), loss = 0.54682
I0825 16:36:26.923851  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450604 (* 1 = 0.450604 loss)
I0825 16:36:26.924033  1899 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0825 16:38:17.082748  1899 solver.cpp:357] Iteration 16200 (0.907792 iter/s, 110.157s/100 iters), loss = 0.351326
I0825 16:38:17.082939  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.269358 (* 1 = 0.269358 loss)
I0825 16:38:17.082983  1899 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0825 16:40:00.327576  1899 solver.cpp:357] Iteration 16300 (0.968612 iter/s, 103.241s/100 iters), loss = 0.296078
I0825 16:40:00.327812  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.362401 (* 1 = 0.362401 loss)
I0825 16:40:00.327875  1899 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0825 16:41:47.244477  1899 solver.cpp:357] Iteration 16400 (0.935342 iter/s, 106.913s/100 iters), loss = 0.40395
I0825 16:41:47.244650  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502974 (* 1 = 0.502974 loss)
I0825 16:41:47.244678  1899 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0825 16:41:51.487035  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:43:37.206506  1899 solver.cpp:514] Iteration 16500, Testing net (#0)
I0825 16:44:45.089468  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:44:45.374178  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6153
I0825 16:44:45.374333  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32831 (* 1 = 1.32831 loss)
I0825 16:44:46.217895  1899 solver.cpp:357] Iteration 16500 (0.558755 iter/s, 178.969s/100 iters), loss = 0.404916
I0825 16:44:46.218055  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421075 (* 1 = 0.421075 loss)
I0825 16:44:46.218099  1899 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0825 16:46:35.820066  1899 solver.cpp:357] Iteration 16600 (0.912409 iter/s, 109.6s/100 iters), loss = 0.402925
I0825 16:46:35.820281  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48089 (* 1 = 0.48089 loss)
I0825 16:46:35.820327  1899 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0825 16:48:22.510073  1899 solver.cpp:357] Iteration 16700 (0.937315 iter/s, 106.688s/100 iters), loss = 0.385194
I0825 16:48:22.510607  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3047 (* 1 = 0.3047 loss)
I0825 16:48:22.510785  1899 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0825 16:50:01.322024  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:50:07.477779  1899 solver.cpp:357] Iteration 16800 (0.952694 iter/s, 104.965s/100 iters), loss = 0.461723
I0825 16:50:07.478045  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449192 (* 1 = 0.449192 loss)
I0825 16:50:07.478138  1899 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0825 16:51:55.290313  1899 solver.cpp:357] Iteration 16900 (0.927555 iter/s, 107.81s/100 iters), loss = 0.356434
I0825 16:51:55.291887  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334155 (* 1 = 0.334155 loss)
I0825 16:51:55.291935  1899 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0825 16:53:42.993024  1899 solver.cpp:514] Iteration 17000, Testing net (#0)
I0825 16:54:47.257033  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:54:47.491271  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.706
I0825 16:54:47.491391  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.901067 (* 1 = 0.901067 loss)
I0825 16:54:48.384850  1899 solver.cpp:357] Iteration 17000 (0.577733 iter/s, 173.09s/100 iters), loss = 0.343719
I0825 16:54:48.385021  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.212912 (* 1 = 0.212912 loss)
I0825 16:54:48.385066  1899 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0825 16:56:37.499548  1899 solver.cpp:357] Iteration 17100 (0.916387 iter/s, 109.124s/100 iters), loss = 0.420271
I0825 16:56:37.499776  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394229 (* 1 = 0.394229 loss)
I0825 16:56:37.499825  1899 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0825 16:58:09.747702  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:58:25.202505  1899 solver.cpp:357] Iteration 17200 (0.928356 iter/s, 107.717s/100 iters), loss = 0.303296
I0825 16:58:25.202687  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.333783 (* 1 = 0.333783 loss)
I0825 16:58:25.202733  1899 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0825 17:00:14.644839  1899 solver.cpp:357] Iteration 17300 (0.91361 iter/s, 109.456s/100 iters), loss = 0.360002
I0825 17:00:14.645047  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389158 (* 1 = 0.389158 loss)
I0825 17:00:14.645105  1899 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0825 17:02:04.732503  1899 solver.cpp:357] Iteration 17400 (0.908275 iter/s, 110.099s/100 iters), loss = 0.365991
I0825 17:02:04.732717  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386967 (* 1 = 0.386967 loss)
I0825 17:02:04.732769  1899 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0825 17:03:43.924131  1899 solver.cpp:514] Iteration 17500, Testing net (#0)
I0825 17:04:52.515892  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:04:52.899032  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7855
I0825 17:04:52.899188  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.65344 (* 1 = 0.65344 loss)
I0825 17:04:53.687194  1899 solver.cpp:357] Iteration 17500 (0.591825 iter/s, 168.969s/100 iters), loss = 0.274839
I0825 17:04:53.687367  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352787 (* 1 = 0.352787 loss)
I0825 17:04:53.687413  1899 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0825 17:06:17.074710  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:06:44.212347  1899 solver.cpp:357] Iteration 17600 (0.904751 iter/s, 110.528s/100 iters), loss = 0.330192
I0825 17:06:44.212523  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.189321 (* 1 = 0.189321 loss)
I0825 17:06:44.212569  1899 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0825 17:08:28.926203  1899 solver.cpp:357] Iteration 17700 (0.954956 iter/s, 104.717s/100 iters), loss = 0.558122
I0825 17:08:28.926445  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.592601 (* 1 = 0.592601 loss)
I0825 17:08:28.926491  1899 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0825 17:10:16.907778  1899 solver.cpp:357] Iteration 17800 (0.926046 iter/s, 107.986s/100 iters), loss = 0.406357
I0825 17:10:16.907987  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413875 (* 1 = 0.413875 loss)
I0825 17:10:16.908033  1899 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0825 17:12:04.206511  1899 solver.cpp:357] Iteration 17900 (0.931946 iter/s, 107.302s/100 iters), loss = 0.31392
I0825 17:12:04.206764  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.292977 (* 1 = 0.292977 loss)
I0825 17:12:04.206812  1899 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0825 17:13:15.979038  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:13:52.620829  1899 solver.cpp:514] Iteration 18000, Testing net (#0)
I0825 17:15:03.224375  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:15:03.564520  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7449
I0825 17:15:03.564690  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.77591 (* 1 = 0.77591 loss)
I0825 17:15:04.358753  1899 solver.cpp:357] Iteration 18000 (0.555074 iter/s, 180.156s/100 iters), loss = 0.288263
I0825 17:15:04.358947  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260598 (* 1 = 0.260598 loss)
I0825 17:15:04.358994  1899 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0825 17:16:45.478152  1899 solver.cpp:357] Iteration 18100 (0.988952 iter/s, 101.117s/100 iters), loss = 0.356563
I0825 17:16:45.478387  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.247123 (* 1 = 0.247123 loss)
I0825 17:16:45.478433  1899 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0825 17:18:31.943030  1899 solver.cpp:357] Iteration 18200 (0.939279 iter/s, 106.465s/100 iters), loss = 0.458297
I0825 17:18:31.943248  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339098 (* 1 = 0.339098 loss)
I0825 17:18:31.943294  1899 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0825 17:20:22.512169  1899 solver.cpp:357] Iteration 18300 (0.904432 iter/s, 110.567s/100 iters), loss = 0.314297
I0825 17:20:22.512408  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.287077 (* 1 = 0.287077 loss)
I0825 17:20:22.512452  1899 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0825 17:21:20.821996  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:22:07.644799  1899 solver.cpp:357] Iteration 18400 (0.951207 iter/s, 105.13s/100 iters), loss = 0.620672
I0825 17:22:07.645045  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499897 (* 1 = 0.499897 loss)
I0825 17:22:07.645092  1899 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0825 17:23:57.679775  1899 solver.cpp:514] Iteration 18500, Testing net (#0)
I0825 17:25:04.680845  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:25:04.907198  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7574
I0825 17:25:04.907349  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.727249 (* 1 = 0.727249 loss)
I0825 17:25:05.694269  1899 solver.cpp:357] Iteration 18500 (0.561638 iter/s, 178.051s/100 iters), loss = 0.417522
I0825 17:25:05.694450  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331413 (* 1 = 0.331413 loss)
I0825 17:25:05.694496  1899 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0825 17:26:50.379285  1899 solver.cpp:357] Iteration 18600 (0.955233 iter/s, 104.686s/100 iters), loss = 0.355506
I0825 17:26:50.379518  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398453 (* 1 = 0.398453 loss)
I0825 17:26:50.379562  1899 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0825 17:28:41.321923  1899 solver.cpp:357] Iteration 18700 (0.901359 iter/s, 110.944s/100 iters), loss = 0.401727
I0825 17:28:41.322152  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39941 (* 1 = 0.39941 loss)
I0825 17:28:41.322197  1899 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0825 17:29:33.651700  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:30:25.359169  1899 solver.cpp:357] Iteration 18800 (0.961227 iter/s, 104.034s/100 iters), loss = 0.453656
I0825 17:30:25.359390  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.33003 (* 1 = 0.33003 loss)
I0825 17:30:25.359439  1899 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0825 17:32:14.515733  1899 solver.cpp:357] Iteration 18900 (0.916181 iter/s, 109.149s/100 iters), loss = 0.443837
I0825 17:32:14.515909  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410828 (* 1 = 0.410828 loss)
I0825 17:32:14.515936  1899 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0825 17:34:01.962931  1899 solver.cpp:514] Iteration 19000, Testing net (#0)
I0825 17:35:06.721469  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:35:07.016851  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7409
I0825 17:35:07.017153  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.772306 (* 1 = 0.772306 loss)
I0825 17:35:07.849917  1899 solver.cpp:357] Iteration 19000 (0.57697 iter/s, 173.319s/100 iters), loss = 0.325358
I0825 17:35:07.850096  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356647 (* 1 = 0.356647 loss)
I0825 17:35:07.850142  1899 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0825 17:36:59.249275  1899 solver.cpp:357] Iteration 19100 (0.897747 iter/s, 111.39s/100 iters), loss = 0.467329
I0825 17:36:59.249519  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.362747 (* 1 = 0.362747 loss)
I0825 17:36:59.249567  1899 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0825 17:37:42.282693  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:38:50.298652  1899 solver.cpp:357] Iteration 19200 (0.900584 iter/s, 111.039s/100 iters), loss = 0.373982
I0825 17:38:50.298877  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330484 (* 1 = 0.330484 loss)
I0825 17:38:50.298921  1899 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0825 17:40:31.694926  1899 solver.cpp:357] Iteration 19300 (0.986319 iter/s, 101.387s/100 iters), loss = 0.423511
I0825 17:40:31.695147  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385871 (* 1 = 0.385871 loss)
I0825 17:40:31.695194  1899 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0825 17:42:19.211117  1899 solver.cpp:357] Iteration 19400 (0.930143 iter/s, 107.51s/100 iters), loss = 0.424837
I0825 17:42:19.211297  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342893 (* 1 = 0.342893 loss)
I0825 17:42:19.211328  1899 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0825 17:44:04.983727  1899 solver.cpp:514] Iteration 19500, Testing net (#0)
I0825 17:45:13.613144  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:45:13.895272  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6641
I0825 17:45:13.895416  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30869 (* 1 = 1.30869 loss)
I0825 17:45:14.726850  1899 solver.cpp:357] Iteration 19500 (0.569769 iter/s, 175.51s/100 iters), loss = 0.408079
I0825 17:45:14.727035  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513218 (* 1 = 0.513218 loss)
I0825 17:45:14.727080  1899 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0825 17:45:46.552387  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:47:05.148891  1899 solver.cpp:357] Iteration 19600 (0.905643 iter/s, 110.419s/100 iters), loss = 0.356422
I0825 17:47:05.149113  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.451607 (* 1 = 0.451607 loss)
I0825 17:47:05.149158  1899 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0825 17:48:46.570118  1899 solver.cpp:357] Iteration 19700 (0.986025 iter/s, 101.417s/100 iters), loss = 0.549099
I0825 17:48:46.570350  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502461 (* 1 = 0.502461 loss)
I0825 17:48:46.570397  1899 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0825 17:50:37.049722  1899 solver.cpp:357] Iteration 19800 (0.90518 iter/s, 110.475s/100 iters), loss = 0.464924
I0825 17:50:37.050196  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36296 (* 1 = 0.36296 loss)
I0825 17:50:37.050391  1899 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0825 17:52:26.473264  1899 solver.cpp:357] Iteration 19900 (0.913932 iter/s, 109.417s/100 iters), loss = 0.434803
I0825 17:52:26.473843  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382334 (* 1 = 0.382334 loss)
I0825 17:52:26.474020  1899 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0825 17:52:47.516888  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:54:09.546844  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_20000.caffemodel
I0825 17:54:09.611412  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_20000.solverstate
I0825 17:54:09.639909  1899 solver.cpp:514] Iteration 20000, Testing net (#0)
I0825 17:55:15.991080  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:55:16.265384  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6052
I0825 17:55:16.265561  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.29278 (* 1 = 1.29278 loss)
I0825 17:55:17.139160  1899 solver.cpp:357] Iteration 20000 (0.585947 iter/s, 170.664s/100 iters), loss = 0.348176
I0825 17:55:17.139289  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412723 (* 1 = 0.412723 loss)
I0825 17:55:17.139315  1899 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0825 17:57:07.889433  1899 solver.cpp:357] Iteration 20100 (0.902949 iter/s, 110.748s/100 iters), loss = 0.37327
I0825 17:57:07.889650  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344058 (* 1 = 0.344058 loss)
I0825 17:57:07.889678  1899 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0825 17:58:54.407862  1899 solver.cpp:357] Iteration 20200 (0.938822 iter/s, 106.517s/100 iters), loss = 0.414245
I0825 17:58:54.408078  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.540245 (* 1 = 0.540245 loss)
I0825 17:58:54.408124  1899 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0825 18:00:45.036875  1899 solver.cpp:357] Iteration 20300 (0.903937 iter/s, 110.627s/100 iters), loss = 0.309053
I0825 18:00:45.037119  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.322036 (* 1 = 0.322036 loss)
I0825 18:00:45.037184  1899 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0825 18:00:55.815763  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:02:28.921756  1899 solver.cpp:357] Iteration 20400 (0.96266 iter/s, 103.879s/100 iters), loss = 0.271574
I0825 18:02:28.921983  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.292237 (* 1 = 0.292237 loss)
I0825 18:02:28.922029  1899 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0825 18:04:17.165612  1899 solver.cpp:514] Iteration 20500, Testing net (#0)
I0825 18:05:28.501610  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:05:28.678972  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.762901
I0825 18:05:28.679116  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.762561 (* 1 = 0.762561 loss)
I0825 18:05:29.579310  1899 solver.cpp:357] Iteration 20500 (0.553544 iter/s, 180.654s/100 iters), loss = 0.409058
I0825 18:05:29.579484  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38079 (* 1 = 0.38079 loss)
I0825 18:05:29.579530  1899 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0825 18:07:09.323407  1899 solver.cpp:357] Iteration 20600 (1.00262 iter/s, 99.7392s/100 iters), loss = 0.411496
I0825 18:07:09.323572  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394236 (* 1 = 0.394236 loss)
I0825 18:07:09.323601  1899 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0825 18:08:59.875752  1899 solver.cpp:357] Iteration 20700 (0.904555 iter/s, 110.552s/100 iters), loss = 0.380173
I0825 18:08:59.875929  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353955 (* 1 = 0.353955 loss)
I0825 18:08:59.875957  1899 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0825 18:09:00.734707  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:10:46.941802  1899 solver.cpp:357] Iteration 20800 (0.934048 iter/s, 107.061s/100 iters), loss = 0.383732
I0825 18:10:46.942020  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385763 (* 1 = 0.385763 loss)
I0825 18:10:46.942065  1899 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0825 18:12:33.006392  1899 solver.cpp:357] Iteration 20900 (0.942855 iter/s, 106.061s/100 iters), loss = 0.365679
I0825 18:12:33.009289  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.465369 (* 1 = 0.465369 loss)
I0825 18:12:33.009392  1899 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0825 18:14:22.808310  1899 solver.cpp:514] Iteration 21000, Testing net (#0)
I0825 18:15:32.787134  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:15:33.071502  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7482
I0825 18:15:33.071655  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.82038 (* 1 = 0.82038 loss)
I0825 18:15:33.899968  1899 solver.cpp:357] Iteration 21000 (0.552818 iter/s, 180.891s/100 iters), loss = 0.423343
I0825 18:15:33.900131  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.58836 (* 1 = 0.58836 loss)
I0825 18:15:33.900177  1899 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0825 18:17:09.123059  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:17:18.199676  1899 solver.cpp:357] Iteration 21100 (0.958786 iter/s, 104.299s/100 iters), loss = 0.356603
I0825 18:17:18.199862  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3195 (* 1 = 0.3195 loss)
I0825 18:17:18.199909  1899 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0825 18:19:03.342597  1899 solver.cpp:357] Iteration 21200 (0.951116 iter/s, 105.14s/100 iters), loss = 0.377932
I0825 18:19:03.343247  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378345 (* 1 = 0.378345 loss)
I0825 18:19:03.343425  1899 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0825 18:20:45.730455  1899 solver.cpp:357] Iteration 21300 (0.976676 iter/s, 102.388s/100 iters), loss = 0.409396
I0825 18:20:45.730927  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38927 (* 1 = 0.38927 loss)
I0825 18:20:45.731134  1899 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0825 18:22:36.043823  1899 solver.cpp:357] Iteration 21400 (0.906552 iter/s, 110.308s/100 iters), loss = 0.465551
I0825 18:22:36.044041  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.578531 (* 1 = 0.578531 loss)
I0825 18:22:36.044085  1899 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0825 18:24:06.089993  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:24:25.653679  1899 solver.cpp:514] Iteration 21500, Testing net (#0)
I0825 18:25:27.749917  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:25:28.039664  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7367
I0825 18:25:28.040024  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.833348 (* 1 = 0.833348 loss)
I0825 18:25:28.904629  1899 solver.cpp:357] Iteration 21500 (0.578502 iter/s, 172.86s/100 iters), loss = 0.196743
I0825 18:25:28.905022  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.211122 (* 1 = 0.211122 loss)
I0825 18:25:28.905197  1899 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0825 18:27:18.588454  1899 solver.cpp:357] Iteration 21600 (0.911739 iter/s, 109.68s/100 iters), loss = 0.374966
I0825 18:27:18.588718  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368948 (* 1 = 0.368948 loss)
I0825 18:27:18.588752  1899 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0825 18:29:09.539711  1899 solver.cpp:357] Iteration 21700 (0.901307 iter/s, 110.95s/100 iters), loss = 0.392134
I0825 18:29:09.539875  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421513 (* 1 = 0.421513 loss)
I0825 18:29:09.539901  1899 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0825 18:30:56.234639  1899 solver.cpp:357] Iteration 21800 (0.937276 iter/s, 106.692s/100 iters), loss = 0.271386
I0825 18:30:56.234864  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.337257 (* 1 = 0.337257 loss)
I0825 18:30:56.234910  1899 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0825 18:32:12.219691  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:32:40.806641  1899 solver.cpp:357] Iteration 21900 (0.95631 iter/s, 104.569s/100 iters), loss = 0.567076
I0825 18:32:40.806813  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347608 (* 1 = 0.347608 loss)
I0825 18:32:40.806857  1899 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0825 18:34:25.154999  1899 solver.cpp:514] Iteration 22000, Testing net (#0)
I0825 18:35:34.737356  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:35:35.073009  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.421299
I0825 18:35:35.073150  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.27168 (* 1 = 2.27168 loss)
I0825 18:35:35.891356  1899 solver.cpp:357] Iteration 22000 (0.571159 iter/s, 175.082s/100 iters), loss = 0.360071
I0825 18:35:35.891535  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456691 (* 1 = 0.456691 loss)
I0825 18:35:35.891580  1899 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0825 18:37:26.568401  1899 solver.cpp:357] Iteration 22100 (0.903575 iter/s, 110.672s/100 iters), loss = 0.329599
I0825 18:37:26.568931  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435209 (* 1 = 0.435209 loss)
I0825 18:37:26.569108  1899 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0825 18:39:09.890182  1899 solver.cpp:357] Iteration 22200 (0.967883 iter/s, 103.318s/100 iters), loss = 0.344223
I0825 18:39:09.890369  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400425 (* 1 = 0.400425 loss)
I0825 18:39:09.890398  1899 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0825 18:40:17.107573  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:40:55.828044  1899 solver.cpp:357] Iteration 22300 (0.943903 iter/s, 105.943s/100 iters), loss = 0.371827
I0825 18:40:55.828217  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416407 (* 1 = 0.416407 loss)
I0825 18:40:55.828261  1899 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0825 18:42:46.733781  1899 solver.cpp:357] Iteration 22400 (0.901597 iter/s, 110.914s/100 iters), loss = 0.352302
I0825 18:42:46.734004  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386924 (* 1 = 0.386924 loss)
I0825 18:42:46.734048  1899 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0825 18:44:28.096582  1899 solver.cpp:514] Iteration 22500, Testing net (#0)
I0825 18:45:37.054481  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:45:37.354122  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7508
I0825 18:45:37.354285  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.766829 (* 1 = 0.766829 loss)
I0825 18:45:38.181850  1899 solver.cpp:357] Iteration 22500 (0.58322 iter/s, 171.462s/100 iters), loss = 0.381572
I0825 18:45:38.182021  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342872 (* 1 = 0.342872 loss)
I0825 18:45:38.182066  1899 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0825 18:47:25.473475  1899 solver.cpp:357] Iteration 22600 (0.931987 iter/s, 107.298s/100 iters), loss = 0.424119
I0825 18:47:25.473959  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487361 (* 1 = 0.487361 loss)
I0825 18:47:25.474136  1899 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0825 18:48:17.700017  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:49:08.495348  1899 solver.cpp:357] Iteration 22700 (0.970638 iter/s, 103.025s/100 iters), loss = 0.320246
I0825 18:49:08.495543  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.34702 (* 1 = 0.34702 loss)
I0825 18:49:08.495569  1899 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0825 18:50:59.276432  1899 solver.cpp:357] Iteration 22800 (0.902646 iter/s, 110.785s/100 iters), loss = 0.299906
I0825 18:50:59.276676  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374398 (* 1 = 0.374398 loss)
I0825 18:50:59.276723  1899 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0825 18:52:45.516161  1899 solver.cpp:357] Iteration 22900 (0.941271 iter/s, 106.239s/100 iters), loss = 0.389229
I0825 18:52:45.516654  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.47639 (* 1 = 0.47639 loss)
I0825 18:52:45.516830  1899 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0825 18:54:35.313314  1899 solver.cpp:514] Iteration 23000, Testing net (#0)
I0825 18:55:41.988487  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:55:42.327937  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7492
I0825 18:55:42.328089  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.744707 (* 1 = 0.744707 loss)
I0825 18:55:43.128389  1899 solver.cpp:357] Iteration 23000 (0.563006 iter/s, 177.618s/100 iters), loss = 0.517529
I0825 18:55:43.128528  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496504 (* 1 = 0.496504 loss)
I0825 18:55:43.128556  1899 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0825 18:56:32.383332  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:57:22.722517  1899 solver.cpp:357] Iteration 23100 (1.00406 iter/s, 99.5958s/100 iters), loss = 0.42093
I0825 18:57:22.722743  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390501 (* 1 = 0.390501 loss)
I0825 18:57:22.722790  1899 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0825 18:59:13.210446  1899 solver.cpp:357] Iteration 23200 (0.905078 iter/s, 110.488s/100 iters), loss = 0.304235
I0825 18:59:13.210651  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369899 (* 1 = 0.369899 loss)
I0825 18:59:13.210696  1899 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0825 19:01:02.312443  1899 solver.cpp:357] Iteration 23300 (0.916544 iter/s, 109.105s/100 iters), loss = 0.432875
I0825 19:01:02.312678  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430638 (* 1 = 0.430638 loss)
I0825 19:01:02.312724  1899 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0825 19:02:44.591563  1899 solver.cpp:357] Iteration 23400 (0.977747 iter/s, 102.276s/100 iters), loss = 0.365632
I0825 19:02:44.591789  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419919 (* 1 = 0.419919 loss)
I0825 19:02:44.591835  1899 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0825 19:03:22.507220  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:04:33.439488  1899 solver.cpp:514] Iteration 23500, Testing net (#0)
I0825 19:05:44.154739  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:05:44.423686  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4512
I0825 19:05:44.423776  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.15482 (* 1 = 2.15482 loss)
I0825 19:05:45.196563  1899 solver.cpp:357] Iteration 23500 (0.553685 iter/s, 180.608s/100 iters), loss = 0.322279
I0825 19:05:45.196696  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.270023 (* 1 = 0.270023 loss)
I0825 19:05:45.196724  1899 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0825 19:07:30.140794  1899 solver.cpp:357] Iteration 23600 (0.952899 iter/s, 104.943s/100 iters), loss = 0.386096
I0825 19:07:30.141300  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424576 (* 1 = 0.424576 loss)
I0825 19:07:30.141475  1899 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0825 19:09:20.979084  1899 solver.cpp:357] Iteration 23700 (0.902226 iter/s, 110.837s/100 iters), loss = 0.334801
I0825 19:09:20.979312  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305523 (* 1 = 0.305523 loss)
I0825 19:09:20.979359  1899 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0825 19:10:57.546838  1899 solver.cpp:357] Iteration 23800 (1.03555 iter/s, 96.5668s/100 iters), loss = 0.334774
I0825 19:10:57.547067  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.332748 (* 1 = 0.332748 loss)
I0825 19:10:57.547113  1899 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0825 19:11:25.230689  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:12:47.177812  1899 solver.cpp:357] Iteration 23900 (0.912163 iter/s, 109.63s/100 iters), loss = 0.326879
I0825 19:12:47.178051  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.253085 (* 1 = 0.253085 loss)
I0825 19:12:47.178097  1899 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0825 19:14:36.878046  1899 solver.cpp:514] Iteration 24000, Testing net (#0)
I0825 19:15:41.732924  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:15:41.930131  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7652
I0825 19:15:41.930272  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.735515 (* 1 = 0.735515 loss)
I0825 19:15:42.893339  1899 solver.cpp:357] Iteration 24000 (0.569132 iter/s, 175.706s/100 iters), loss = 0.386408
I0825 19:15:42.893512  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281097 (* 1 = 0.281097 loss)
I0825 19:15:42.893558  1899 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0825 19:17:29.796460  1899 solver.cpp:357] Iteration 24100 (0.935515 iter/s, 106.893s/100 iters), loss = 0.379619
I0825 19:17:29.796869  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.370149 (* 1 = 0.370149 loss)
I0825 19:17:29.796981  1899 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0825 19:19:17.652789  1899 solver.cpp:357] Iteration 24200 (0.927233 iter/s, 107.848s/100 iters), loss = 0.357016
I0825 19:19:17.654513  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3077 (* 1 = 0.3077 loss)
I0825 19:19:17.654547  1899 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0825 19:19:34.278992  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:21:02.988688  1899 solver.cpp:357] Iteration 24300 (0.949415 iter/s, 105.328s/100 iters), loss = 0.409656
I0825 19:21:02.989166  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353736 (* 1 = 0.353736 loss)
I0825 19:21:02.989339  1899 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0825 19:22:48.929550  1899 solver.cpp:357] Iteration 24400 (0.943955 iter/s, 105.937s/100 iters), loss = 0.402515
I0825 19:22:48.929739  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425628 (* 1 = 0.425628 loss)
I0825 19:22:48.929764  1899 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0825 19:24:34.285429  1899 solver.cpp:514] Iteration 24500, Testing net (#0)
I0825 19:25:39.425817  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:25:39.783598  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5155
I0825 19:25:39.783742  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.14251 (* 1 = 2.14251 loss)
I0825 19:25:40.572206  1899 solver.cpp:357] Iteration 24500 (0.582631 iter/s, 171.635s/100 iters), loss = 0.492218
I0825 19:25:40.572388  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396692 (* 1 = 0.396692 loss)
I0825 19:25:40.572433  1899 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0825 19:27:30.358631  1899 solver.cpp:357] Iteration 24600 (0.910924 iter/s, 109.779s/100 iters), loss = 0.301363
I0825 19:27:30.359030  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380084 (* 1 = 0.380084 loss)
I0825 19:27:30.359117  1899 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0825 19:27:37.743348  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:29:14.405467  1899 solver.cpp:357] Iteration 24700 (0.961122 iter/s, 104.045s/100 iters), loss = 0.367536
I0825 19:29:14.405619  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348796 (* 1 = 0.348796 loss)
I0825 19:29:14.405647  1899 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0825 19:31:05.032171  1899 solver.cpp:357] Iteration 24800 (0.903981 iter/s, 110.622s/100 iters), loss = 0.311023
I0825 19:31:05.032398  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.252406 (* 1 = 0.252406 loss)
I0825 19:31:05.032444  1899 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0825 19:32:51.458369  1899 solver.cpp:357] Iteration 24900 (0.939639 iter/s, 106.424s/100 iters), loss = 0.482611
I0825 19:32:51.458755  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468862 (* 1 = 0.468862 loss)
I0825 19:32:51.458848  1899 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0825 19:34:30.004163  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:34:32.001772  1899 solver.cpp:514] Iteration 25000, Testing net (#0)
I0825 19:35:41.694679  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:35:41.947566  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.698
I0825 19:35:41.947921  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00819 (* 1 = 1.00819 loss)
I0825 19:35:42.728595  1899 solver.cpp:357] Iteration 25000 (0.583885 iter/s, 171.267s/100 iters), loss = 0.395923
I0825 19:35:42.728988  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441989 (* 1 = 0.441989 loss)
I0825 19:35:42.729166  1899 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0825 19:37:32.928241  1899 solver.cpp:357] Iteration 25100 (0.907461 iter/s, 110.198s/100 iters), loss = 0.28915
I0825 19:37:32.928460  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.230677 (* 1 = 0.230677 loss)
I0825 19:37:32.928508  1899 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0825 19:39:18.945143  1899 solver.cpp:357] Iteration 25200 (0.943282 iter/s, 106.013s/100 iters), loss = 0.557093
I0825 19:39:18.945333  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543877 (* 1 = 0.543877 loss)
I0825 19:39:18.945379  1899 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0825 19:41:04.645393  1899 solver.cpp:357] Iteration 25300 (0.946088 iter/s, 105.698s/100 iters), loss = 0.363394
I0825 19:41:04.645632  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377947 (* 1 = 0.377947 loss)
I0825 19:41:04.645679  1899 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0825 19:42:38.718691  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:42:50.847527  1899 solver.cpp:357] Iteration 25400 (0.941654 iter/s, 106.196s/100 iters), loss = 0.399718
I0825 19:42:50.847726  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.276716 (* 1 = 0.276716 loss)
I0825 19:42:50.847772  1899 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0825 19:44:39.942472  1899 solver.cpp:514] Iteration 25500, Testing net (#0)
I0825 19:45:50.900246  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:45:51.110396  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6558
I0825 19:45:51.110540  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.1172 (* 1 = 1.1172 loss)
I0825 19:45:52.095065  1899 solver.cpp:357] Iteration 25500 (0.551742 iter/s, 181.244s/100 iters), loss = 0.285713
I0825 19:45:52.095242  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.277436 (* 1 = 0.277436 loss)
I0825 19:45:52.095286  1899 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0825 19:47:33.763237  1899 solver.cpp:357] Iteration 25600 (0.983608 iter/s, 101.667s/100 iters), loss = 0.452485
I0825 19:47:33.763557  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372805 (* 1 = 0.372805 loss)
I0825 19:47:33.763607  1899 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0825 19:49:20.768355  1899 solver.cpp:357] Iteration 25700 (0.934509 iter/s, 107.008s/100 iters), loss = 0.297109
I0825 19:49:20.768712  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328166 (* 1 = 0.328166 loss)
I0825 19:49:20.768805  1899 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0825 19:50:47.824787  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:51:11.151695  1899 solver.cpp:357] Iteration 25800 (0.905927 iter/s, 110.384s/100 iters), loss = 0.402123
I0825 19:51:11.151881  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.23639 (* 1 = 0.23639 loss)
I0825 19:51:11.151926  1899 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0825 19:52:56.308926  1899 solver.cpp:357] Iteration 25900 (0.950943 iter/s, 105.159s/100 iters), loss = 0.490504
I0825 19:52:56.309113  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480326 (* 1 = 0.480326 loss)
I0825 19:52:56.309139  1899 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0825 19:54:45.908941  1899 solver.cpp:514] Iteration 26000, Testing net (#0)
I0825 19:55:51.160519  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:55:51.382552  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.624901
I0825 19:55:51.382923  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.24276 (* 1 = 1.24276 loss)
I0825 19:55:52.208340  1899 solver.cpp:357] Iteration 26000 (0.568487 iter/s, 175.905s/100 iters), loss = 0.379958
I0825 19:55:52.208745  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.321256 (* 1 = 0.321256 loss)
I0825 19:55:52.208921  1899 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0825 19:57:38.896244  1899 solver.cpp:357] Iteration 26100 (0.9373 iter/s, 106.689s/100 iters), loss = 0.312186
I0825 19:57:38.896440  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.274322 (* 1 = 0.274322 loss)
I0825 19:57:38.896472  1899 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0825 19:58:54.139549  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:59:27.182698  1899 solver.cpp:357] Iteration 26200 (0.923487 iter/s, 108.285s/100 iters), loss = 0.371906
I0825 19:59:27.182917  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412371 (* 1 = 0.412371 loss)
I0825 19:59:27.182962  1899 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0825 20:01:12.024081  1899 solver.cpp:357] Iteration 26300 (0.953812 iter/s, 104.842s/100 iters), loss = 0.308367
I0825 20:01:12.024312  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383711 (* 1 = 0.383711 loss)
I0825 20:01:12.024358  1899 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0825 20:02:59.611482  1899 solver.cpp:357] Iteration 26400 (0.929488 iter/s, 107.586s/100 iters), loss = 0.282655
I0825 20:02:59.611639  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.213402 (* 1 = 0.213402 loss)
I0825 20:02:59.611680  1899 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0825 20:04:48.972004  1899 solver.cpp:514] Iteration 26500, Testing net (#0)
I0825 20:05:53.670883  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:05:53.988845  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6524
I0825 20:05:53.989004  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20066 (* 1 = 1.20066 loss)
I0825 20:05:54.910770  1899 solver.cpp:357] Iteration 26500 (0.570453 iter/s, 175.299s/100 iters), loss = 0.38066
I0825 20:05:54.910934  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343383 (* 1 = 0.343383 loss)
I0825 20:05:54.910979  1899 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0825 20:07:00.967257  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:07:45.134279  1899 solver.cpp:357] Iteration 26600 (0.907245 iter/s, 110.224s/100 iters), loss = 0.340577
I0825 20:07:45.134516  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.258651 (* 1 = 0.258651 loss)
I0825 20:07:45.134560  1899 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0825 20:09:34.911036  1899 solver.cpp:357] Iteration 26700 (0.910939 iter/s, 109.777s/100 iters), loss = 0.579261
I0825 20:09:34.911247  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.500188 (* 1 = 0.500188 loss)
I0825 20:09:34.911290  1899 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0825 20:11:16.320358  1899 solver.cpp:357] Iteration 26800 (0.986125 iter/s, 101.407s/100 iters), loss = 0.424303
I0825 20:11:16.320555  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398056 (* 1 = 0.398056 loss)
I0825 20:11:16.320588  1899 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0825 20:13:03.774636  1899 solver.cpp:357] Iteration 26900 (0.930648 iter/s, 107.452s/100 iters), loss = 0.356392
I0825 20:13:03.774824  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.249512 (* 1 = 0.249512 loss)
I0825 20:13:03.774853  1899 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0825 20:13:59.538704  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:14:49.085103  1899 solver.cpp:514] Iteration 27000, Testing net (#0)
I0825 20:15:58.989084  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:15:59.358263  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.694601
I0825 20:15:59.358433  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.97329 (* 1 = 0.97329 loss)
I0825 20:16:00.151993  1899 solver.cpp:357] Iteration 27000 (0.566969 iter/s, 176.376s/100 iters), loss = 0.414119
I0825 20:16:00.152175  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296337 (* 1 = 0.296337 loss)
I0825 20:16:00.152221  1899 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0825 20:17:48.368204  1899 solver.cpp:357] Iteration 27100 (0.924114 iter/s, 108.212s/100 iters), loss = 0.50146
I0825 20:17:48.369750  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.583843 (* 1 = 0.583843 loss)
I0825 20:17:48.369798  1899 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0825 20:19:33.428006  1899 solver.cpp:357] Iteration 27200 (0.951852 iter/s, 105.058s/100 iters), loss = 0.406852
I0825 20:19:33.428238  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356771 (* 1 = 0.356771 loss)
I0825 20:19:33.428283  1899 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0825 20:21:24.302207  1899 solver.cpp:357] Iteration 27300 (0.901943 iter/s, 110.872s/100 iters), loss = 0.363392
I0825 20:21:24.302418  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.301905 (* 1 = 0.301905 loss)
I0825 20:21:24.302465  1899 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0825 20:22:09.185261  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:23:14.192751  1899 solver.cpp:357] Iteration 27400 (0.90999 iter/s, 109.891s/100 iters), loss = 0.289023
I0825 20:23:14.192973  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331965 (* 1 = 0.331965 loss)
I0825 20:23:14.193037  1899 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0825 20:24:56.154530  1899 solver.cpp:514] Iteration 27500, Testing net (#0)
I0825 20:26:01.897126  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:26:02.244395  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6628
I0825 20:26:02.244501  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.09879 (* 1 = 1.09879 loss)
I0825 20:26:03.127454  1899 solver.cpp:357] Iteration 27500 (0.59193 iter/s, 168.939s/100 iters), loss = 0.383701
I0825 20:26:03.127629  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365224 (* 1 = 0.365224 loss)
I0825 20:26:03.127674  1899 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0825 20:27:52.022197  1899 solver.cpp:357] Iteration 27600 (0.918286 iter/s, 108.899s/100 iters), loss = 0.447963
I0825 20:27:52.022490  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.35652 (* 1 = 0.35652 loss)
I0825 20:27:52.022537  1899 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0825 20:29:39.626255  1899 solver.cpp:357] Iteration 27700 (0.929315 iter/s, 107.606s/100 iters), loss = 0.603866
I0825 20:29:39.626487  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491595 (* 1 = 0.491595 loss)
I0825 20:29:39.626533  1899 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0825 20:30:14.101991  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:31:30.245921  1899 solver.cpp:357] Iteration 27800 (0.904001 iter/s, 110.619s/100 iters), loss = 0.325964
I0825 20:31:30.246078  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399519 (* 1 = 0.399519 loss)
I0825 20:31:30.246124  1899 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0825 20:33:12.476912  1899 solver.cpp:357] Iteration 27900 (0.978205 iter/s, 102.228s/100 iters), loss = 0.437203
I0825 20:33:12.477144  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366289 (* 1 = 0.366289 loss)
I0825 20:33:12.477190  1899 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0825 20:35:02.214692  1899 solver.cpp:514] Iteration 28000, Testing net (#0)
I0825 20:36:12.998975  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:36:13.161098  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5166
I0825 20:36:13.161232  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.13192 (* 1 = 2.13192 loss)
I0825 20:36:14.116804  1899 solver.cpp:357] Iteration 28000 (0.550529 iter/s, 181.643s/100 iters), loss = 0.304591
I0825 20:36:14.116928  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412918 (* 1 = 0.412918 loss)
I0825 20:36:14.116953  1899 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0825 20:37:53.860456  1899 solver.cpp:357] Iteration 28100 (1.00258 iter/s, 99.7428s/100 iters), loss = 0.372487
I0825 20:37:53.860687  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.43911 (* 1 = 0.43911 loss)
I0825 20:37:53.860731  1899 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0825 20:38:18.928103  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:39:44.043700  1899 solver.cpp:357] Iteration 28200 (0.907589 iter/s, 110.182s/100 iters), loss = 0.439108
I0825 20:39:44.043925  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.278424 (* 1 = 0.278424 loss)
I0825 20:39:44.043970  1899 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0825 20:41:29.391692  1899 solver.cpp:357] Iteration 28300 (0.949248 iter/s, 105.347s/100 iters), loss = 0.377111
I0825 20:41:29.391862  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449027 (* 1 = 0.449027 loss)
I0825 20:41:29.391888  1899 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0825 20:43:14.847811  1899 solver.cpp:357] Iteration 28400 (0.948257 iter/s, 105.457s/100 iters), loss = 0.336238
I0825 20:43:14.848035  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.324462 (* 1 = 0.324462 loss)
I0825 20:43:14.848083  1899 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0825 20:45:04.742419  1899 solver.cpp:514] Iteration 28500, Testing net (#0)
I0825 20:46:13.176874  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:46:13.388453  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7655
I0825 20:46:13.388571  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.706041 (* 1 = 0.706041 loss)
I0825 20:46:14.229568  1899 solver.cpp:357] Iteration 28500 (0.557469 iter/s, 179.382s/100 iters), loss = 0.412406
I0825 20:46:14.229751  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467985 (* 1 = 0.467985 loss)
I0825 20:46:14.229799  1899 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0825 20:46:27.590680  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:47:37.091598  1899 solver.cpp:357] Iteration 28600 (1.20689 iter/s, 82.8577s/100 iters), loss = 0.419183
I0825 20:47:37.091809  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364955 (* 1 = 0.364955 loss)
I0825 20:47:37.091822  1899 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0825 20:48:57.353530  1899 solver.cpp:357] Iteration 28700 (1.24589 iter/s, 80.2637s/100 iters), loss = 0.315852
I0825 20:48:57.353840  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.243461 (* 1 = 0.243461 loss)
I0825 20:48:57.353904  1899 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0825 20:50:20.002311  1899 solver.cpp:357] Iteration 28800 (1.20994 iter/s, 82.6486s/100 iters), loss = 0.330193
I0825 20:50:20.002441  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326792 (* 1 = 0.326792 loss)
I0825 20:50:20.002455  1899 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0825 20:51:44.639153  1899 solver.cpp:357] Iteration 28900 (1.18155 iter/s, 84.6346s/100 iters), loss = 0.440751
I0825 20:51:44.639468  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503103 (* 1 = 0.503103 loss)
I0825 20:51:44.639535  1899 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0825 20:51:47.712532  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:53:06.675561  1899 solver.cpp:514] Iteration 29000, Testing net (#0)
I0825 20:53:56.238939  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:53:56.517046  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6651
I0825 20:53:56.517110  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.19202 (* 1 = 1.19202 loss)
I0825 20:53:57.188364  1899 solver.cpp:357] Iteration 29000 (0.754443 iter/s, 132.548s/100 iters), loss = 0.4493
I0825 20:53:57.188431  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357414 (* 1 = 0.357414 loss)
I0825 20:53:57.188441  1899 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0825 20:55:22.135180  1899 solver.cpp:357] Iteration 29100 (1.17721 iter/s, 84.9466s/100 iters), loss = 0.347383
I0825 20:55:22.135347  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378173 (* 1 = 0.378173 loss)
I0825 20:55:22.135360  1899 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0825 20:56:47.159344  1899 solver.cpp:357] Iteration 29200 (1.17612 iter/s, 85.0254s/100 iters), loss = 0.483524
I0825 20:56:47.159471  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32064 (* 1 = 0.32064 loss)
I0825 20:56:47.159482  1899 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0825 20:58:06.795958  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:58:11.629462  1899 solver.cpp:357] Iteration 29300 (1.18378 iter/s, 84.4752s/100 iters), loss = 0.383546
I0825 20:58:11.629598  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31852 (* 1 = 0.31852 loss)
I0825 20:58:11.629627  1899 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0825 20:59:24.771989  1899 solver.cpp:357] Iteration 29400 (1.36711 iter/s, 73.1468s/100 iters), loss = 0.33666
I0825 20:59:24.772091  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.303474 (* 1 = 0.303474 loss)
I0825 20:59:24.772101  1899 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0825 21:00:48.732982  1899 solver.cpp:514] Iteration 29500, Testing net (#0)
I0825 21:01:44.063665  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:01:44.304823  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5589
I0825 21:01:44.304927  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.51965 (* 1 = 1.51965 loss)
I0825 21:01:44.940407  1899 solver.cpp:357] Iteration 29500 (0.713373 iter/s, 140.179s/100 iters), loss = 0.346591
I0825 21:01:44.940484  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355819 (* 1 = 0.355819 loss)
I0825 21:01:44.940495  1899 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0825 21:03:09.770447  1899 solver.cpp:357] Iteration 29600 (1.17881 iter/s, 84.8316s/100 iters), loss = 0.313643
I0825 21:03:09.770651  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334774 (* 1 = 0.334774 loss)
I0825 21:03:09.770664  1899 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0825 21:04:21.712079  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:04:33.780853  1899 solver.cpp:357] Iteration 29700 (1.19028 iter/s, 84.0136s/100 iters), loss = 0.353061
I0825 21:04:33.780926  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359876 (* 1 = 0.359876 loss)
I0825 21:04:33.780939  1899 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0825 21:05:52.072899  1899 solver.cpp:357] Iteration 29800 (1.27726 iter/s, 78.2924s/100 iters), loss = 0.380907
I0825 21:05:52.073065  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.482483 (* 1 = 0.482483 loss)
I0825 21:05:52.073077  1899 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0825 21:07:16.994208  1899 solver.cpp:357] Iteration 29900 (1.17753 iter/s, 84.9238s/100 iters), loss = 0.353544
I0825 21:07:16.994418  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417418 (* 1 = 0.417418 loss)
I0825 21:07:16.994432  1899 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0825 21:08:38.050707  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_30000.caffemodel
I0825 21:08:38.073900  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_30000.solverstate
I0825 21:08:38.081778  1899 solver.cpp:514] Iteration 30000, Testing net (#0)
I0825 21:09:31.619873  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:09:31.829813  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5379
I0825 21:09:31.829866  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.69474 (* 1 = 1.69474 loss)
I0825 21:09:32.463639  1899 solver.cpp:357] Iteration 30000 (0.73816 iter/s, 135.472s/100 iters), loss = 0.283224
I0825 21:09:32.463711  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.315475 (* 1 = 0.315475 loss)
I0825 21:09:32.463722  1899 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0825 21:10:33.416553  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:10:51.332157  1899 solver.cpp:357] Iteration 30100 (1.26794 iter/s, 78.8679s/100 iters), loss = 0.374122
I0825 21:10:51.332221  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.235242 (* 1 = 0.235242 loss)
I0825 21:10:51.332232  1899 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0825 21:12:15.369000  1899 solver.cpp:357] Iteration 30200 (1.1899 iter/s, 84.0406s/100 iters), loss = 0.42439
I0825 21:12:15.369139  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502002 (* 1 = 0.502002 loss)
I0825 21:12:15.369151  1899 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0825 21:13:40.039502  1899 solver.cpp:357] Iteration 30300 (1.18103 iter/s, 84.6719s/100 iters), loss = 0.399868
I0825 21:13:40.039634  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385112 (* 1 = 0.385112 loss)
I0825 21:13:40.039647  1899 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0825 21:15:04.846493  1899 solver.cpp:357] Iteration 30400 (1.17913 iter/s, 84.8083s/100 iters), loss = 0.246285
I0825 21:15:04.846612  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.19658 (* 1 = 0.19658 loss)
I0825 21:15:04.846623  1899 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0825 21:16:01.107830  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:16:26.140015  1899 solver.cpp:514] Iteration 30500, Testing net (#0)
I0825 21:17:16.750879  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:17:16.979120  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.714
I0825 21:17:16.979182  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.963785 (* 1 = 0.963785 loss)
I0825 21:17:17.583477  1899 solver.cpp:357] Iteration 30500 (0.753352 iter/s, 132.74s/100 iters), loss = 0.291288
I0825 21:17:17.583547  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383814 (* 1 = 0.383814 loss)
I0825 21:17:17.583559  1899 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0825 21:18:46.093164  1899 solver.cpp:357] Iteration 30600 (1.12983 iter/s, 88.5088s/100 iters), loss = 0.33525
I0825 21:18:46.093355  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.224655 (* 1 = 0.224655 loss)
I0825 21:18:46.093367  1899 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0825 21:20:11.119833  1899 solver.cpp:357] Iteration 30700 (1.17609 iter/s, 85.0276s/100 iters), loss = 0.329923
I0825 21:20:11.120007  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.183632 (* 1 = 0.183632 loss)
I0825 21:20:11.120018  1899 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0825 21:21:35.836143  1899 solver.cpp:357] Iteration 30800 (1.1804 iter/s, 84.7172s/100 iters), loss = 0.318274
I0825 21:21:35.836315  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388756 (* 1 = 0.388756 loss)
I0825 21:21:35.836329  1899 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0825 21:22:27.603394  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:23:02.218905  1899 solver.cpp:357] Iteration 30900 (1.15765 iter/s, 86.3815s/100 iters), loss = 0.475231
I0825 21:23:02.219058  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.290041 (* 1 = 0.290041 loss)
I0825 21:23:02.219070  1899 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0825 21:24:26.444242  1899 solver.cpp:514] Iteration 31000, Testing net (#0)
I0825 21:25:21.716367  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:25:21.849602  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6318
I0825 21:25:21.849659  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.21046 (* 1 = 1.21046 loss)
I0825 21:25:22.547379  1899 solver.cpp:357] Iteration 31000 (0.7126 iter/s, 140.331s/100 iters), loss = 0.350731
I0825 21:25:22.547456  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.301643 (* 1 = 0.301643 loss)
I0825 21:25:22.547467  1899 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0825 21:26:47.543810  1899 solver.cpp:357] Iteration 31100 (1.17654 iter/s, 84.9951s/100 iters), loss = 0.37992
I0825 21:26:47.543970  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409378 (* 1 = 0.409378 loss)
I0825 21:26:47.543982  1899 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0825 21:28:07.178921  1899 solver.cpp:357] Iteration 31200 (1.25572 iter/s, 79.6357s/100 iters), loss = 0.382275
I0825 21:28:07.179100  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448435 (* 1 = 0.448435 loss)
I0825 21:28:07.179126  1899 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0825 21:28:40.490247  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:29:25.264436  1899 solver.cpp:357] Iteration 31300 (1.28064 iter/s, 78.086s/100 iters), loss = 0.244446
I0825 21:29:25.264590  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.191391 (* 1 = 0.191391 loss)
I0825 21:29:25.264603  1899 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0825 21:30:50.511803  1899 solver.cpp:357] Iteration 31400 (1.17318 iter/s, 85.2383s/100 iters), loss = 0.462752
I0825 21:30:50.512023  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480264 (* 1 = 0.480264 loss)
I0825 21:30:50.512048  1899 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0825 21:32:15.002511  1899 solver.cpp:514] Iteration 31500, Testing net (#0)
I0825 21:33:10.306133  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:33:10.590801  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6633
I0825 21:33:10.590865  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20259 (* 1 = 1.20259 loss)
I0825 21:33:11.204272  1899 solver.cpp:357] Iteration 31500 (0.710885 iter/s, 140.67s/100 iters), loss = 0.353331
I0825 21:33:11.204344  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327059 (* 1 = 0.327059 loss)
I0825 21:33:11.204356  1899 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0825 21:34:28.692684  1899 solver.cpp:357] Iteration 31600 (1.29071 iter/s, 77.477s/100 iters), loss = 0.258445
I0825 21:34:28.692862  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.217651 (* 1 = 0.217651 loss)
I0825 21:34:28.692875  1899 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0825 21:35:01.081816  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:35:53.447906  1899 solver.cpp:357] Iteration 31700 (1.18002 iter/s, 84.7444s/100 iters), loss = 0.385279
I0825 21:35:53.448236  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409853 (* 1 = 0.409853 loss)
I0825 21:35:53.448415  1899 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0825 21:37:18.238482  1899 solver.cpp:357] Iteration 31800 (1.17954 iter/s, 84.779s/100 iters), loss = 0.424246
I0825 21:37:18.238597  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415418 (* 1 = 0.415418 loss)
I0825 21:37:18.238626  1899 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0825 21:38:38.434991  1899 solver.cpp:357] Iteration 31900 (1.24706 iter/s, 80.1885s/100 iters), loss = 0.367058
I0825 21:38:38.435134  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.308102 (* 1 = 0.308102 loss)
I0825 21:38:38.435148  1899 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0825 21:39:57.870697  1899 solver.cpp:514] Iteration 32000, Testing net (#0)
I0825 21:40:50.460620  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:40:50.746889  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.623801
I0825 21:40:50.746949  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30309 (* 1 = 1.30309 loss)
I0825 21:40:51.351562  1899 solver.cpp:357] Iteration 32000 (0.752409 iter/s, 132.906s/100 iters), loss = 0.288919
I0825 21:40:51.351630  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351023 (* 1 = 0.351023 loss)
I0825 21:40:51.351639  1899 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0825 21:40:51.351646  1899 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0825 21:41:15.672233  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:42:16.359760  1899 solver.cpp:357] Iteration 32100 (1.17642 iter/s, 85.0035s/100 iters), loss = 0.278352
I0825 21:42:16.359879  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.310755 (* 1 = 0.310755 loss)
I0825 21:42:16.359889  1899 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0825 21:43:41.233981  1899 solver.cpp:357] Iteration 32200 (1.17829 iter/s, 84.8685s/100 iters), loss = 0.295518
I0825 21:43:41.234166  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.28538 (* 1 = 0.28538 loss)
I0825 21:43:41.234179  1899 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0825 21:45:06.292989  1899 solver.cpp:357] Iteration 32300 (1.17576 iter/s, 85.0517s/100 iters), loss = 0.187194
I0825 21:45:06.293145  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.132762 (* 1 = 0.132762 loss)
I0825 21:45:06.293159  1899 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0825 21:46:23.892115  1899 solver.cpp:357] Iteration 32400 (1.28878 iter/s, 77.5925s/100 iters), loss = 0.182424
I0825 21:46:23.892264  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.180271 (* 1 = 0.180271 loss)
I0825 21:46:23.892277  1899 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0825 21:46:40.075850  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:47:44.711529  1899 solver.cpp:514] Iteration 32500, Testing net (#0)
I0825 21:48:38.195608  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:48:38.415453  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.886702
I0825 21:48:38.415508  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.336268 (* 1 = 0.336268 loss)
I0825 21:48:39.093377  1899 solver.cpp:357] Iteration 32500 (0.73968 iter/s, 135.194s/100 iters), loss = 0.168175
I0825 21:48:39.093453  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160255 (* 1 = 0.160255 loss)
I0825 21:48:39.093467  1899 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0825 21:50:04.169734  1899 solver.cpp:357] Iteration 32600 (1.1755 iter/s, 85.0704s/100 iters), loss = 0.166283
I0825 21:50:04.169903  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0739588 (* 1 = 0.0739588 loss)
I0825 21:50:04.169914  1899 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0825 21:51:25.687503  1899 solver.cpp:357] Iteration 32700 (1.22676 iter/s, 81.5155s/100 iters), loss = 0.210423
I0825 21:51:25.687639  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.294486 (* 1 = 0.294486 loss)
I0825 21:51:25.687654  1899 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0825 21:52:46.791808  1899 solver.cpp:357] Iteration 32800 (1.23306 iter/s, 81.0988s/100 iters), loss = 0.139531
I0825 21:52:46.791975  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.148547 (* 1 = 0.148547 loss)
I0825 21:52:46.791987  1899 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0825 21:52:54.900435  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:54:11.475812  1899 solver.cpp:357] Iteration 32900 (1.18094 iter/s, 84.6786s/100 iters), loss = 0.106105
I0825 21:54:11.476091  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108576 (* 1 = 0.108576 loss)
I0825 21:54:11.476148  1899 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0825 21:55:35.621875  1899 solver.cpp:514] Iteration 33000, Testing net (#0)
I0825 21:56:30.597909  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:56:30.857782  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.883002
I0825 21:56:30.857849  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.341679 (* 1 = 0.341679 loss)
I0825 21:56:31.588546  1899 solver.cpp:357] Iteration 33000 (0.71372 iter/s, 140.111s/100 iters), loss = 0.117462
I0825 21:56:31.588611  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108266 (* 1 = 0.108266 loss)
I0825 21:56:31.588624  1899 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0825 21:57:44.523643  1899 solver.cpp:357] Iteration 33100 (1.37113 iter/s, 72.9323s/100 iters), loss = 0.102627
I0825 21:57:44.523777  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0881883 (* 1 = 0.0881883 loss)
I0825 21:57:44.523788  1899 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0825 21:59:09.560607  1899 solver.cpp:357] Iteration 33200 (1.176 iter/s, 85.0341s/100 iters), loss = 0.145695
I0825 21:59:09.560745  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122061 (* 1 = 0.122061 loss)
I0825 21:59:09.560757  1899 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0825 21:59:10.044015  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:00:34.355373  1899 solver.cpp:357] Iteration 33300 (1.17936 iter/s, 84.792s/100 iters), loss = 0.249475
I0825 22:00:34.355679  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.220053 (* 1 = 0.220053 loss)
I0825 22:00:34.355741  1899 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0825 22:01:59.438891  1899 solver.cpp:357] Iteration 33400 (1.17538 iter/s, 85.0787s/100 iters), loss = 0.170451
I0825 22:01:59.439030  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.200328 (* 1 = 0.200328 loss)
I0825 22:01:59.439043  1899 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0825 22:03:18.015350  1899 solver.cpp:514] Iteration 33500, Testing net (#0)
I0825 22:04:11.749940  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:04:11.963618  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.902202
I0825 22:04:11.963680  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.286435 (* 1 = 0.286435 loss)
I0825 22:04:12.711194  1899 solver.cpp:357] Iteration 33500 (0.750371 iter/s, 133.267s/100 iters), loss = 0.159151
I0825 22:04:12.711268  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.21172 (* 1 = 0.21172 loss)
I0825 22:04:12.711280  1899 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0825 22:05:30.089679  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:05:37.617566  1899 solver.cpp:357] Iteration 33600 (1.17762 iter/s, 84.9172s/100 iters), loss = 0.193687
I0825 22:05:37.617637  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.236153 (* 1 = 0.236153 loss)
I0825 22:05:37.617648  1899 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0825 22:06:59.810720  1899 solver.cpp:357] Iteration 33700 (1.2165 iter/s, 82.2033s/100 iters), loss = 0.16438
I0825 22:06:59.810883  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.250552 (* 1 = 0.250552 loss)
I0825 22:06:59.810894  1899 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0825 22:08:22.263116  1899 solver.cpp:357] Iteration 33800 (1.21267 iter/s, 82.4628s/100 iters), loss = 0.0764786
I0825 22:08:22.263268  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0548698 (* 1 = 0.0548698 loss)
I0825 22:08:22.263280  1899 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0825 22:09:40.162178  1899 solver.cpp:357] Iteration 33900 (1.28361 iter/s, 77.9052s/100 iters), loss = 0.125374
I0825 22:09:40.162343  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119781 (* 1 = 0.119781 loss)
I0825 22:09:40.162354  1899 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0825 22:10:49.390497  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:11:04.342324  1899 solver.cpp:514] Iteration 34000, Testing net (#0)
I0825 22:11:59.649338  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:11:59.865828  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880102
I0825 22:11:59.865890  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.358954 (* 1 = 0.358954 loss)
I0825 22:12:00.469734  1899 solver.cpp:357] Iteration 34000 (0.712661 iter/s, 140.319s/100 iters), loss = 0.0711368
I0825 22:12:00.469810  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0719924 (* 1 = 0.0719924 loss)
I0825 22:12:00.469822  1899 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0825 22:13:25.376788  1899 solver.cpp:357] Iteration 34100 (1.17771 iter/s, 84.9109s/100 iters), loss = 0.130999
I0825 22:13:25.376921  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145827 (* 1 = 0.145827 loss)
I0825 22:13:25.376935  1899 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0825 22:14:45.713724  1899 solver.cpp:357] Iteration 34200 (1.24472 iter/s, 80.3394s/100 iters), loss = 0.0594808
I0825 22:14:45.713858  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0704363 (* 1 = 0.0704363 loss)
I0825 22:14:45.713871  1899 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0825 22:16:08.046068  1899 solver.cpp:357] Iteration 34300 (1.21453 iter/s, 82.3364s/100 iters), loss = 0.104473
I0825 22:16:08.046203  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.105529 (* 1 = 0.105529 loss)
I0825 22:16:08.046213  1899 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0825 22:17:05.689821  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:17:28.155032  1899 solver.cpp:357] Iteration 34400 (1.24822 iter/s, 80.1142s/100 iters), loss = 0.178941
I0825 22:17:28.155105  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.164586 (* 1 = 0.164586 loss)
I0825 22:17:28.155117  1899 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0825 22:18:52.326270  1899 solver.cpp:514] Iteration 34500, Testing net (#0)
I0825 22:19:47.709769  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:19:47.883675  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.886602
I0825 22:19:47.883740  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.354709 (* 1 = 0.354709 loss)
I0825 22:19:48.635674  1899 solver.cpp:357] Iteration 34500 (0.711811 iter/s, 140.487s/100 iters), loss = 0.0819227
I0825 22:19:48.635747  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0678933 (* 1 = 0.0678933 loss)
I0825 22:19:48.635761  1899 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0825 22:21:06.479926  1899 solver.cpp:357] Iteration 34600 (1.28462 iter/s, 77.844s/100 iters), loss = 0.0972003
I0825 22:21:06.480208  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117698 (* 1 = 0.117698 loss)
I0825 22:21:06.480237  1899 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0825 22:22:31.415495  1899 solver.cpp:357] Iteration 34700 (1.17734 iter/s, 84.9375s/100 iters), loss = 0.1171
I0825 22:22:31.415616  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117162 (* 1 = 0.117162 loss)
I0825 22:22:31.415628  1899 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0825 22:23:25.202852  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:23:56.379483  1899 solver.cpp:357] Iteration 34800 (1.17695 iter/s, 84.9655s/100 iters), loss = 0.0461422
I0825 22:23:56.379597  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.053459 (* 1 = 0.053459 loss)
I0825 22:23:56.379609  1899 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0825 22:25:21.392047  1899 solver.cpp:357] Iteration 34900 (1.17628 iter/s, 85.0138s/100 iters), loss = 0.0594573
I0825 22:25:21.392207  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0574084 (* 1 = 0.0574084 loss)
I0825 22:25:21.392220  1899 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0825 22:26:35.830188  1899 solver.cpp:514] Iteration 35000, Testing net (#0)
I0825 22:27:28.423903  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:27:28.641322  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.898202
I0825 22:27:28.641381  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.306745 (* 1 = 0.306745 loss)
I0825 22:27:29.247498  1899 solver.cpp:357] Iteration 35000 (0.782117 iter/s, 127.858s/100 iters), loss = 0.102007
I0825 22:27:29.247570  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119587 (* 1 = 0.119587 loss)
I0825 22:27:29.247581  1899 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0825 22:28:54.456760  1899 solver.cpp:357] Iteration 35100 (1.17357 iter/s, 85.21s/100 iters), loss = 0.142159
I0825 22:28:54.456900  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129725 (* 1 = 0.129725 loss)
I0825 22:28:54.456912  1899 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0825 22:29:40.172106  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:30:19.560360  1899 solver.cpp:357] Iteration 35200 (1.17503 iter/s, 85.1042s/100 iters), loss = 0.133316
I0825 22:30:19.560684  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0590643 (* 1 = 0.0590643 loss)
I0825 22:30:19.560748  1899 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0825 22:31:44.226130  1899 solver.cpp:357] Iteration 35300 (1.18111 iter/s, 84.6663s/100 iters), loss = 0.0972351
I0825 22:31:44.226251  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.159655 (* 1 = 0.159655 loss)
I0825 22:31:44.226265  1899 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0825 22:33:02.658895  1899 solver.cpp:357] Iteration 35400 (1.27497 iter/s, 78.4329s/100 iters), loss = 0.104239
I0825 22:33:02.659092  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101088 (* 1 = 0.101088 loss)
I0825 22:33:02.659121  1899 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0825 22:34:26.712695  1899 solver.cpp:514] Iteration 35500, Testing net (#0)
I0825 22:35:22.040223  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:35:22.313474  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.893202
I0825 22:35:22.313535  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.349444 (* 1 = 0.349444 loss)
I0825 22:35:22.916615  1899 solver.cpp:357] Iteration 35500 (0.712964 iter/s, 140.26s/100 iters), loss = 0.137769
I0825 22:35:22.916693  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0974752 (* 1 = 0.0974752 loss)
I0825 22:35:22.916705  1899 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0825 22:35:59.146543  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:36:43.030774  1899 solver.cpp:357] Iteration 35600 (1.24825 iter/s, 80.1121s/100 iters), loss = 0.106087
I0825 22:36:43.030933  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0697373 (* 1 = 0.0697373 loss)
I0825 22:36:43.030946  1899 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0825 22:38:01.421800  1899 solver.cpp:357] Iteration 35700 (1.27566 iter/s, 78.391s/100 iters), loss = 0.106538
I0825 22:38:01.421918  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113363 (* 1 = 0.113363 loss)
I0825 22:38:01.421932  1899 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0825 22:39:25.083060  1899 solver.cpp:357] Iteration 35800 (1.19532 iter/s, 83.6598s/100 iters), loss = 0.0751248
I0825 22:39:25.083196  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0549119 (* 1 = 0.0549119 loss)
I0825 22:39:25.083209  1899 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0825 22:40:50.265987  1899 solver.cpp:357] Iteration 35900 (1.17398 iter/s, 85.1804s/100 iters), loss = 0.0926521
I0825 22:40:50.266134  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104328 (* 1 = 0.104328 loss)
I0825 22:40:50.266146  1899 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0825 22:41:19.545933  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:42:14.489395  1899 solver.cpp:514] Iteration 36000, Testing net (#0)
I0825 22:43:09.263449  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:43:09.558732  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.895602
I0825 22:43:09.558794  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.337001 (* 1 = 0.337001 loss)
I0825 22:43:10.211629  1899 solver.cpp:357] Iteration 36000 (0.714559 iter/s, 139.946s/100 iters), loss = 0.115978
I0825 22:43:10.211705  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158905 (* 1 = 0.158905 loss)
I0825 22:43:10.211716  1899 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0825 22:44:27.761409  1899 solver.cpp:357] Iteration 36100 (1.28954 iter/s, 77.5471s/100 iters), loss = 0.132179
I0825 22:44:27.761549  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10689 (* 1 = 0.10689 loss)
I0825 22:44:27.761561  1899 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0825 22:45:50.968070  1899 solver.cpp:357] Iteration 36200 (1.20183 iter/s, 83.2062s/100 iters), loss = 0.0452418
I0825 22:45:50.968190  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.053824 (* 1 = 0.053824 loss)
I0825 22:45:50.968201  1899 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0825 22:47:12.809152  1899 solver.cpp:357] Iteration 36300 (1.22186 iter/s, 81.8427s/100 iters), loss = 0.145545
I0825 22:47:12.809330  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.109683 (* 1 = 0.109683 loss)
I0825 22:47:12.809345  1899 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0825 22:47:34.423255  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:48:37.514358  1899 solver.cpp:357] Iteration 36400 (1.1806 iter/s, 84.7027s/100 iters), loss = 0.0651793
I0825 22:48:37.514518  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.041889 (* 1 = 0.041889 loss)
I0825 22:48:37.514530  1899 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0825 22:49:54.231305  1899 solver.cpp:514] Iteration 36500, Testing net (#0)
I0825 22:50:49.623340  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:50:49.905308  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894202
I0825 22:50:49.905372  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.340373 (* 1 = 0.340373 loss)
I0825 22:50:50.514611  1899 solver.cpp:357] Iteration 36500 (0.751886 iter/s, 132.999s/100 iters), loss = 0.163371
I0825 22:50:50.514691  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.105272 (* 1 = 0.105272 loss)
I0825 22:50:50.514704  1899 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0825 22:52:15.472347  1899 solver.cpp:357] Iteration 36600 (1.17709 iter/s, 84.9553s/100 iters), loss = 0.0872805
I0825 22:52:15.472630  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0501193 (* 1 = 0.0501193 loss)
I0825 22:52:15.472643  1899 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0825 22:53:40.559936  1899 solver.cpp:357] Iteration 36700 (1.17526 iter/s, 85.0873s/100 iters), loss = 0.0424416
I0825 22:53:40.560082  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0178239 (* 1 = 0.0178239 loss)
I0825 22:53:40.560094  1899 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0825 22:53:54.448222  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:55:04.012262  1899 solver.cpp:357] Iteration 36800 (1.19829 iter/s, 83.452s/100 iters), loss = 0.0990903
I0825 22:55:04.012400  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0593723 (* 1 = 0.0593723 loss)
I0825 22:55:04.012411  1899 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0825 22:56:18.288987  1899 solver.cpp:357] Iteration 36900 (1.34633 iter/s, 74.2762s/100 iters), loss = 0.144511
I0825 22:56:18.289096  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.182844 (* 1 = 0.182844 loss)
I0825 22:56:18.289109  1899 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0825 22:57:42.506093  1899 solver.cpp:514] Iteration 37000, Testing net (#0)
I0825 22:58:37.831918  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:58:38.127990  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.902202
I0825 22:58:38.128052  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.31847 (* 1 = 0.31847 loss)
I0825 22:58:38.748687  1899 solver.cpp:357] Iteration 37000 (0.711953 iter/s, 140.459s/100 iters), loss = 0.111954
I0825 22:58:38.748754  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119046 (* 1 = 0.119046 loss)
I0825 22:58:38.748767  1899 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0825 23:00:03.421245  1899 solver.cpp:357] Iteration 37100 (1.18102 iter/s, 84.6722s/100 iters), loss = 0.104654
I0825 23:00:03.421361  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.073303 (* 1 = 0.073303 loss)
I0825 23:00:03.421372  1899 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0825 23:00:09.021236  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:01:21.585646  1899 solver.cpp:357] Iteration 37200 (1.27933 iter/s, 78.166s/100 iters), loss = 0.105882
I0825 23:01:21.585819  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.159253 (* 1 = 0.159253 loss)
I0825 23:01:21.585832  1899 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0825 23:02:45.807727  1899 solver.cpp:357] Iteration 37300 (1.18734 iter/s, 84.2218s/100 iters), loss = 0.0620423
I0825 23:02:45.807860  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0491649 (* 1 = 0.0491649 loss)
I0825 23:02:45.807874  1899 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0825 23:04:10.412022  1899 solver.cpp:357] Iteration 37400 (1.18201 iter/s, 84.6019s/100 iters), loss = 0.118926
I0825 23:04:10.412158  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108846 (* 1 = 0.108846 loss)
I0825 23:04:10.412168  1899 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0825 23:05:30.483302  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:05:31.990509  1899 solver.cpp:514] Iteration 37500, Testing net (#0)
I0825 23:06:24.643029  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:06:24.829298  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.900102
I0825 23:06:24.829346  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.330639 (* 1 = 0.330639 loss)
I0825 23:06:25.531730  1899 solver.cpp:357] Iteration 37500 (0.74008 iter/s, 135.121s/100 iters), loss = 0.0990425
I0825 23:06:25.531810  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0594189 (* 1 = 0.0594189 loss)
I0825 23:06:25.531821  1899 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0825 23:07:43.329536  1899 solver.cpp:357] Iteration 37600 (1.28539 iter/s, 77.7973s/100 iters), loss = 0.0507141
I0825 23:07:43.329726  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.027345 (* 1 = 0.027345 loss)
I0825 23:07:43.329737  1899 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0825 23:09:08.488596  1899 solver.cpp:357] Iteration 37700 (1.17428 iter/s, 85.1588s/100 iters), loss = 0.0788971
I0825 23:09:08.488765  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0372886 (* 1 = 0.0372886 loss)
I0825 23:09:08.488776  1899 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0825 23:10:33.562438  1899 solver.cpp:357] Iteration 37800 (1.17545 iter/s, 85.0736s/100 iters), loss = 0.0623595
I0825 23:10:33.562597  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0718765 (* 1 = 0.0718765 loss)
I0825 23:10:33.562609  1899 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0825 23:11:48.499640  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:11:58.535954  1899 solver.cpp:357] Iteration 37900 (1.17687 iter/s, 84.9711s/100 iters), loss = 0.0782937
I0825 23:11:58.536032  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0449749 (* 1 = 0.0449749 loss)
I0825 23:11:58.536043  1899 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0825 23:13:15.456017  1899 solver.cpp:514] Iteration 38000, Testing net (#0)
I0825 23:14:10.906313  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:14:11.095252  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.891302
I0825 23:14:11.095305  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.382243 (* 1 = 0.382243 loss)
I0825 23:14:11.764058  1899 solver.cpp:357] Iteration 38000 (0.750603 iter/s, 133.226s/100 iters), loss = 0.0623926
I0825 23:14:11.764132  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0606436 (* 1 = 0.0606436 loss)
I0825 23:14:11.764144  1899 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0825 23:15:32.518833  1899 solver.cpp:357] Iteration 38100 (1.23841 iter/s, 80.7489s/100 iters), loss = 0.102168
I0825 23:15:32.518980  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.141765 (* 1 = 0.141765 loss)
I0825 23:15:32.518993  1899 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0825 23:16:56.690979  1899 solver.cpp:357] Iteration 38200 (1.18809 iter/s, 84.1688s/100 iters), loss = 0.0337429
I0825 23:16:56.691153  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.047329 (* 1 = 0.047329 loss)
I0825 23:16:56.691165  1899 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0825 23:18:03.436420  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:18:19.211591  1899 solver.cpp:357] Iteration 38300 (1.21189 iter/s, 82.5155s/100 iters), loss = 0.0880375
I0825 23:18:19.211665  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0489174 (* 1 = 0.0489174 loss)
I0825 23:18:19.211678  1899 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0825 23:19:39.246812  1899 solver.cpp:357] Iteration 38400 (1.24949 iter/s, 80.0323s/100 iters), loss = 0.0905728
I0825 23:19:39.246965  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0655577 (* 1 = 0.0655577 loss)
I0825 23:19:39.246978  1899 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0825 23:21:03.672907  1899 solver.cpp:514] Iteration 38500, Testing net (#0)
I0825 23:21:58.904068  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:21:59.107158  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.886202
I0825 23:21:59.107209  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.399134 (* 1 = 0.399134 loss)
I0825 23:21:59.739357  1899 solver.cpp:357] Iteration 38500 (0.711805 iter/s, 140.488s/100 iters), loss = 0.054001
I0825 23:21:59.739434  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0869443 (* 1 = 0.0869443 loss)
I0825 23:21:59.739444  1899 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0825 23:23:24.989116  1899 solver.cpp:357] Iteration 38600 (1.17305 iter/s, 85.2476s/100 iters), loss = 0.0908332
I0825 23:23:24.989291  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.09045 (* 1 = 0.09045 loss)
I0825 23:23:24.989305  1899 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0825 23:24:19.262073  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:24:41.118116  1899 solver.cpp:357] Iteration 38700 (1.31359 iter/s, 76.1271s/100 iters), loss = 0.102479
I0825 23:24:41.118191  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.183237 (* 1 = 0.183237 loss)
I0825 23:24:41.118201  1899 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0825 23:26:02.292348  1899 solver.cpp:357] Iteration 38800 (1.23195 iter/s, 81.1723s/100 iters), loss = 0.0432555
I0825 23:26:02.292593  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0488994 (* 1 = 0.0488994 loss)
I0825 23:26:02.292618  1899 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0825 23:27:27.177659  1899 solver.cpp:357] Iteration 38900 (1.17808 iter/s, 84.8837s/100 iters), loss = 0.0366023
I0825 23:27:27.177800  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0322803 (* 1 = 0.0322803 loss)
I0825 23:27:27.177812  1899 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0825 23:28:51.461045  1899 solver.cpp:514] Iteration 39000, Testing net (#0)
I0825 23:29:45.484668  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:29:45.632815  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.901402
I0825 23:29:45.632901  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.341558 (* 1 = 0.341558 loss)
I0825 23:29:46.234547  1899 solver.cpp:357] Iteration 39000 (0.719146 iter/s, 139.054s/100 iters), loss = 0.0369326
I0825 23:29:46.234627  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0168328 (* 1 = 0.0168328 loss)
I0825 23:29:46.234639  1899 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0825 23:30:30.896124  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:31:05.334264  1899 solver.cpp:357] Iteration 39100 (1.26425 iter/s, 79.0982s/100 iters), loss = 0.10088
I0825 23:31:05.334457  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12389 (* 1 = 0.12389 loss)
I0825 23:31:05.334487  1899 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0825 23:32:30.345818  1899 solver.cpp:357] Iteration 39200 (1.17633 iter/s, 85.0102s/100 iters), loss = 0.169366
I0825 23:32:30.350445  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0797705 (* 1 = 0.0797705 loss)
I0825 23:32:30.350461  1899 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0825 23:33:55.304091  1899 solver.cpp:357] Iteration 39300 (1.17712 iter/s, 84.9534s/100 iters), loss = 0.110244
I0825 23:33:55.304229  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0644068 (* 1 = 0.0644068 loss)
I0825 23:33:55.304241  1899 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0825 23:35:16.069195  1899 solver.cpp:357] Iteration 39400 (1.23821 iter/s, 80.7617s/100 iters), loss = 0.093818
I0825 23:35:16.069339  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0439197 (* 1 = 0.0439197 loss)
I0825 23:35:16.069350  1899 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0825 23:35:54.454468  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:36:32.364509  1899 solver.cpp:514] Iteration 39500, Testing net (#0)
I0825 23:37:27.476516  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:37:27.565174  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.864802
I0825 23:37:27.565225  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.499931 (* 1 = 0.499931 loss)
I0825 23:37:28.280293  1899 solver.cpp:357] Iteration 39500 (0.756358 iter/s, 132.213s/100 iters), loss = 0.0784264
I0825 23:37:28.280371  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0319348 (* 1 = 0.0319348 loss)
I0825 23:37:28.280385  1899 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0825 23:38:53.294934  1899 solver.cpp:357] Iteration 39600 (1.17631 iter/s, 85.0114s/100 iters), loss = 0.0925832
I0825 23:38:53.295331  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0966055 (* 1 = 0.0966055 loss)
I0825 23:38:53.295395  1899 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0825 23:40:18.557770  1899 solver.cpp:357] Iteration 39700 (1.17289 iter/s, 85.2596s/100 iters), loss = 0.080629
I0825 23:40:18.557895  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0981923 (* 1 = 0.0981923 loss)
I0825 23:40:18.557907  1899 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0825 23:41:39.445734  1899 solver.cpp:357] Iteration 39800 (1.23633 iter/s, 80.8847s/100 iters), loss = 0.0448751
I0825 23:41:39.445866  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0408879 (* 1 = 0.0408879 loss)
I0825 23:41:39.445880  1899 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0825 23:42:10.691082  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:43:00.718034  1899 solver.cpp:357] Iteration 39900 (1.23045 iter/s, 81.2712s/100 iters), loss = 0.0839735
I0825 23:43:00.718226  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10912 (* 1 = 0.10912 loss)
I0825 23:43:00.718255  1899 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0825 23:44:22.846379  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_40000.caffemodel
I0825 23:44:22.871079  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_40000.solverstate
I0825 23:44:22.879112  1899 solver.cpp:514] Iteration 40000, Testing net (#0)
I0825 23:45:14.914549  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:45:15.145906  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.887602
I0825 23:45:15.145969  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.42803 (* 1 = 0.42803 loss)
I0825 23:45:15.767990  1899 solver.cpp:357] Iteration 40000 (0.740469 iter/s, 135.05s/100 iters), loss = 0.14344
I0825 23:45:15.768072  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0372602 (* 1 = 0.0372602 loss)
I0825 23:45:15.768084  1899 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0825 23:46:40.675426  1899 solver.cpp:357] Iteration 40100 (1.17777 iter/s, 84.9064s/100 iters), loss = 0.0981847
I0825 23:46:40.675552  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12739 (* 1 = 0.12739 loss)
I0825 23:46:40.675562  1899 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0825 23:47:58.365226  1899 solver.cpp:357] Iteration 40200 (1.28715 iter/s, 77.6909s/100 iters), loss = 0.0671795
I0825 23:47:58.365365  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0537988 (* 1 = 0.0537988 loss)
I0825 23:47:58.365376  1899 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0825 23:48:25.292570  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:49:23.350589  1899 solver.cpp:357] Iteration 40300 (1.17668 iter/s, 84.9848s/100 iters), loss = 0.11708
I0825 23:49:23.350725  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.134488 (* 1 = 0.134488 loss)
I0825 23:49:23.350738  1899 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0825 23:50:48.698875  1899 solver.cpp:357] Iteration 40400 (1.17168 iter/s, 85.3477s/100 iters), loss = 0.0960676
I0825 23:50:48.699020  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121902 (* 1 = 0.121902 loss)
I0825 23:50:48.699031  1899 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0825 23:52:12.934128  1899 solver.cpp:514] Iteration 40500, Testing net (#0)
I0825 23:53:05.709434  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:53:05.919111  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.899402
I0825 23:53:05.919203  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.337619 (* 1 = 0.337619 loss)
I0825 23:53:06.491255  1899 solver.cpp:357] Iteration 40500 (0.725728 iter/s, 137.793s/100 iters), loss = 0.0824227
I0825 23:53:06.491319  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0465878 (* 1 = 0.0465878 loss)
I0825 23:53:06.491331  1899 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0825 23:54:23.202652  1899 solver.cpp:357] Iteration 40600 (1.3036 iter/s, 76.7105s/100 iters), loss = 0.0689548
I0825 23:54:23.202925  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.083453 (* 1 = 0.083453 loss)
I0825 23:54:23.202955  1899 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0825 23:54:40.869833  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:55:46.894038  1899 solver.cpp:357] Iteration 40700 (1.19488 iter/s, 83.6906s/100 iters), loss = 0.0919479
I0825 23:55:46.894217  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.078236 (* 1 = 0.078236 loss)
I0825 23:55:46.894229  1899 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0825 23:57:11.711167  1899 solver.cpp:357] Iteration 40800 (1.17905 iter/s, 84.8142s/100 iters), loss = 0.0564033
I0825 23:57:11.711307  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0357122 (* 1 = 0.0357122 loss)
I0825 23:57:11.711318  1899 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0825 23:58:36.730635  1899 solver.cpp:357] Iteration 40900 (1.17621 iter/s, 85.0186s/100 iters), loss = 0.103207
I0825 23:58:36.730751  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0474297 (* 1 = 0.0474297 loss)
I0825 23:58:36.730762  1899 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0825 23:59:53.930969  1899 solver.cpp:514] Iteration 41000, Testing net (#0)
I0826 00:00:49.142494  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:00:49.282945  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.870701
I0826 00:00:49.283041  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.425953 (* 1 = 0.425953 loss)
I0826 00:00:50.013176  1899 solver.cpp:357] Iteration 41000 (0.750275 iter/s, 133.285s/100 iters), loss = 0.13526
I0826 00:00:50.013252  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11363 (* 1 = 0.11363 loss)
I0826 00:00:50.013263  1899 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0826 00:01:01.184942  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:02:14.859529  1899 solver.cpp:357] Iteration 41100 (1.17861 iter/s, 84.8455s/100 iters), loss = 0.0293545
I0826 00:02:14.859710  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0196841 (* 1 = 0.0196841 loss)
I0826 00:02:14.859724  1899 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0826 00:03:38.848361  1899 solver.cpp:357] Iteration 41200 (1.19065 iter/s, 83.988s/100 iters), loss = 0.107409
I0826 00:03:38.848484  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0821516 (* 1 = 0.0821516 loss)
I0826 00:03:38.848498  1899 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0826 00:04:54.925823  1899 solver.cpp:357] Iteration 41300 (1.3145 iter/s, 76.0744s/100 iters), loss = 0.0881771
I0826 00:04:54.925932  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102322 (* 1 = 0.102322 loss)
I0826 00:04:54.925943  1899 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0826 00:06:17.165529  1899 solver.cpp:357] Iteration 41400 (1.21594 iter/s, 82.2408s/100 iters), loss = 0.0562917
I0826 00:06:17.165683  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0784711 (* 1 = 0.0784711 loss)
I0826 00:06:17.165694  1899 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0826 00:06:20.261279  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:07:41.370904  1899 solver.cpp:514] Iteration 41500, Testing net (#0)
I0826 00:08:36.634172  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:08:36.856559  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.901602
I0826 00:08:36.856606  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.330352 (* 1 = 0.330352 loss)
I0826 00:08:37.544956  1899 solver.cpp:357] Iteration 41500 (0.712355 iter/s, 140.379s/100 iters), loss = 0.0991689
I0826 00:08:37.545033  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0263534 (* 1 = 0.0263534 loss)
I0826 00:08:37.545045  1899 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0826 00:10:02.762867  1899 solver.cpp:357] Iteration 41600 (1.17347 iter/s, 85.217s/100 iters), loss = 0.0991908
I0826 00:10:02.763106  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117566 (* 1 = 0.117566 loss)
I0826 00:10:02.763120  1899 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0826 00:11:20.452350  1899 solver.cpp:357] Iteration 41700 (1.28723 iter/s, 77.6863s/100 iters), loss = 0.0446436
I0826 00:11:20.452499  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0332952 (* 1 = 0.0332952 loss)
I0826 00:11:20.452512  1899 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0826 00:12:40.555076  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:12:45.515241  1899 solver.cpp:357] Iteration 41800 (1.17564 iter/s, 85.0598s/100 iters), loss = 0.138217
I0826 00:12:45.515318  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.184072 (* 1 = 0.184072 loss)
I0826 00:12:45.515331  1899 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0826 00:14:06.498083  1899 solver.cpp:357] Iteration 41900 (1.23488 iter/s, 80.9798s/100 iters), loss = 0.0987422
I0826 00:14:06.498261  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0682308 (* 1 = 0.0682308 loss)
I0826 00:14:06.498291  1899 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0826 00:15:29.669183  1899 solver.cpp:514] Iteration 42000, Testing net (#0)
I0826 00:16:21.688431  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:16:21.786711  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.897202
I0826 00:16:21.786764  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.389025 (* 1 = 0.389025 loss)
I0826 00:16:22.422287  1899 solver.cpp:357] Iteration 42000 (0.735705 iter/s, 135.924s/100 iters), loss = 0.06704
I0826 00:16:22.422374  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0931825 (* 1 = 0.0931825 loss)
I0826 00:16:22.422386  1899 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0826 00:17:43.464704  1899 solver.cpp:357] Iteration 42100 (1.23397 iter/s, 81.0393s/100 iters), loss = 0.0478525
I0826 00:17:43.464831  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0352654 (* 1 = 0.0352654 loss)
I0826 00:17:43.464844  1899 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0826 00:18:55.197417  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:19:08.305055  1899 solver.cpp:357] Iteration 42200 (1.1787 iter/s, 84.8395s/100 iters), loss = 0.0625156
I0826 00:19:08.305132  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0605998 (* 1 = 0.0605998 loss)
I0826 00:19:08.305145  1899 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0826 00:20:33.121294  1899 solver.cpp:357] Iteration 42300 (1.17903 iter/s, 84.8153s/100 iters), loss = 0.0577397
I0826 00:20:33.121441  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.054001 (* 1 = 0.054001 loss)
I0826 00:20:33.121453  1899 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0826 00:21:58.002933  1899 solver.cpp:357] Iteration 42400 (1.17815 iter/s, 84.8792s/100 iters), loss = 0.129001
I0826 00:21:58.003049  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.069447 (* 1 = 0.069447 loss)
I0826 00:21:58.003062  1899 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0826 00:23:13.784204  1899 solver.cpp:514] Iteration 42500, Testing net (#0)
I0826 00:24:05.244097  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:24:05.496048  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888502
I0826 00:24:05.496111  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.394475 (* 1 = 0.394475 loss)
I0826 00:24:06.239522  1899 solver.cpp:357] Iteration 42500 (0.779847 iter/s, 128.23s/100 iters), loss = 0.0338634
I0826 00:24:06.239590  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0357868 (* 1 = 0.0357868 loss)
I0826 00:24:06.239603  1899 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0826 00:25:10.479004  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:25:31.104648  1899 solver.cpp:357] Iteration 42600 (1.1784 iter/s, 84.8608s/100 iters), loss = 0.0578089
I0826 00:25:31.104718  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00738044 (* 1 = 0.00738044 loss)
I0826 00:25:31.104732  1899 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0826 00:26:56.296594  1899 solver.cpp:357] Iteration 42700 (1.17387 iter/s, 85.1881s/100 iters), loss = 0.118224
I0826 00:26:56.296802  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.171959 (* 1 = 0.171959 loss)
I0826 00:26:56.296830  1899 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0826 00:28:15.855533  1899 solver.cpp:357] Iteration 42800 (1.25702 iter/s, 79.5533s/100 iters), loss = 0.0942107
I0826 00:28:15.855841  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0922353 (* 1 = 0.0922353 loss)
I0826 00:28:15.855901  1899 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0826 00:29:39.198529  1899 solver.cpp:357] Iteration 42900 (1.1999 iter/s, 83.3402s/100 iters), loss = 0.0658588
I0826 00:29:39.198679  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0269007 (* 1 = 0.0269007 loss)
I0826 00:29:39.198691  1899 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0826 00:30:35.606842  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:31:03.609243  1899 solver.cpp:514] Iteration 43000, Testing net (#0)
I0826 00:31:58.713779  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:31:58.861203  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888202
I0826 00:31:58.861272  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.398939 (* 1 = 0.398939 loss)
I0826 00:31:59.641072  1899 solver.cpp:357] Iteration 43000 (0.712063 iter/s, 140.437s/100 iters), loss = 0.0836549
I0826 00:31:59.641145  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0709428 (* 1 = 0.0709428 loss)
I0826 00:31:59.641157  1899 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0826 00:33:21.435008  1899 solver.cpp:357] Iteration 43100 (1.22266 iter/s, 81.7891s/100 iters), loss = 0.0879645
I0826 00:33:21.435148  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0936174 (* 1 = 0.0936174 loss)
I0826 00:33:21.435159  1899 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0826 00:34:37.100713  1899 solver.cpp:357] Iteration 43200 (1.32168 iter/s, 75.6611s/100 iters), loss = 0.0818749
I0826 00:34:37.100847  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100613 (* 1 = 0.100613 loss)
I0826 00:34:37.100859  1899 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0826 00:36:02.160390  1899 solver.cpp:357] Iteration 43300 (1.17568 iter/s, 85.0573s/100 iters), loss = 0.030863
I0826 00:36:02.160666  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0518375 (* 1 = 0.0518375 loss)
I0826 00:36:02.160724  1899 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0826 00:36:50.434270  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:37:27.158396  1899 solver.cpp:357] Iteration 43400 (1.17651 iter/s, 84.9974s/100 iters), loss = 0.107193
I0826 00:37:27.158555  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108669 (* 1 = 0.108669 loss)
I0826 00:37:27.158571  1899 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0826 00:38:51.470381  1899 solver.cpp:514] Iteration 43500, Testing net (#0)
I0826 00:39:41.533195  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:39:41.628964  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.890401
I0826 00:39:41.629026  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.386958 (* 1 = 0.386958 loss)
I0826 00:39:42.306834  1899 solver.cpp:357] Iteration 43500 (0.739928 iter/s, 135.148s/100 iters), loss = 0.0733288
I0826 00:39:42.306913  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0438829 (* 1 = 0.0438829 loss)
I0826 00:39:42.306924  1899 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0826 00:41:04.875639  1899 solver.cpp:357] Iteration 43600 (1.21114 iter/s, 82.5667s/100 iters), loss = 0.0989316
I0826 00:41:04.875844  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0874268 (* 1 = 0.0874268 loss)
I0826 00:41:04.875855  1899 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0826 00:42:29.566959  1899 solver.cpp:357] Iteration 43700 (1.18076 iter/s, 84.6913s/100 iters), loss = 0.0887346
I0826 00:42:29.567075  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103747 (* 1 = 0.103747 loss)
I0826 00:42:29.567087  1899 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0826 00:43:07.118240  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:43:50.255323  1899 solver.cpp:357] Iteration 43800 (1.23934 iter/s, 80.6879s/100 iters), loss = 0.052061
I0826 00:43:50.255455  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0235701 (* 1 = 0.0235701 loss)
I0826 00:43:50.255465  1899 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0826 00:45:13.445233  1899 solver.cpp:357] Iteration 43900 (1.2021 iter/s, 83.188s/100 iters), loss = 0.0810011
I0826 00:45:13.445343  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0909299 (* 1 = 0.0909299 loss)
I0826 00:45:13.445354  1899 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0826 00:46:32.119329  1899 solver.cpp:514] Iteration 44000, Testing net (#0)
I0826 00:47:27.258096  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:47:27.527961  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.899702
I0826 00:47:27.528030  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.330084 (* 1 = 0.330084 loss)
I0826 00:47:28.264281  1899 solver.cpp:357] Iteration 44000 (0.741733 iter/s, 134.819s/100 iters), loss = 0.0589921
I0826 00:47:28.264348  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0395849 (* 1 = 0.0395849 loss)
I0826 00:47:28.264360  1899 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0826 00:48:53.302062  1899 solver.cpp:357] Iteration 44100 (1.17597 iter/s, 85.0359s/100 iters), loss = 0.0433456
I0826 00:48:53.302172  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0401255 (* 1 = 0.0401255 loss)
I0826 00:48:53.302183  1899 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0826 00:49:25.451448  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:50:18.064888  1899 solver.cpp:357] Iteration 44200 (1.17978 iter/s, 84.7616s/100 iters), loss = 0.0753845
I0826 00:50:18.065028  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0727086 (* 1 = 0.0727086 loss)
I0826 00:50:18.065040  1899 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0826 00:51:35.851296  1899 solver.cpp:357] Iteration 44300 (1.28564 iter/s, 77.7825s/100 iters), loss = 0.110963
I0826 00:51:35.851426  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14904 (* 1 = 0.14904 loss)
I0826 00:51:35.851438  1899 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0826 00:52:56.892953  1899 solver.cpp:357] Iteration 44400 (1.23393 iter/s, 81.0419s/100 iters), loss = 0.126272
I0826 00:52:56.893157  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0531682 (* 1 = 0.0531682 loss)
I0826 00:52:56.893185  1899 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0826 00:54:19.509444  1899 solver.cpp:514] Iteration 44500, Testing net (#0)
I0826 00:55:14.917882  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:55:15.086762  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.892502
I0826 00:55:15.086815  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.390217 (* 1 = 0.390217 loss)
I0826 00:55:15.839753  1899 solver.cpp:357] Iteration 44500 (0.719699 iter/s, 138.947s/100 iters), loss = 0.0710585
I0826 00:55:15.839833  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0703812 (* 1 = 0.0703812 loss)
I0826 00:55:15.839845  1899 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0826 00:55:39.954778  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:56:40.306279  1899 solver.cpp:357] Iteration 44600 (1.18392 iter/s, 84.4655s/100 iters), loss = 0.101498
I0826 00:56:40.306481  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156667 (* 1 = 0.156667 loss)
I0826 00:56:40.306505  1899 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0826 00:57:58.169600  1899 solver.cpp:357] Iteration 44700 (1.28425 iter/s, 77.8666s/100 iters), loss = 0.114737
I0826 00:57:58.169741  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113528 (* 1 = 0.113528 loss)
I0826 00:57:58.169751  1899 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0826 00:59:23.171207  1899 solver.cpp:357] Iteration 44800 (1.17641 iter/s, 85.0046s/100 iters), loss = 0.0689748
I0826 00:59:23.171370  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0890153 (* 1 = 0.0890153 loss)
I0826 00:59:23.171383  1899 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0826 01:00:48.873950  1899 solver.cpp:357] Iteration 44900 (1.16681 iter/s, 85.7036s/100 iters), loss = 0.0350104
I0826 01:00:48.874110  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0437365 (* 1 = 0.0437365 loss)
I0826 01:00:48.874122  1899 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0826 01:01:05.096956  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:02:12.063624  1899 solver.cpp:514] Iteration 45000, Testing net (#0)
I0826 01:02:56.974057  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:02:57.154160  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.883701
I0826 01:02:57.154247  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.437974 (* 1 = 0.437974 loss)
I0826 01:02:57.724745  1899 solver.cpp:357] Iteration 45000 (0.776093 iter/s, 128.851s/100 iters), loss = 0.0528263
I0826 01:02:57.724817  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0522458 (* 1 = 0.0522458 loss)
I0826 01:02:57.724831  1899 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0826 01:04:21.001893  1899 solver.cpp:357] Iteration 45100 (1.20083 iter/s, 83.2754s/100 iters), loss = 0.0718173
I0826 01:04:21.002041  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0485522 (* 1 = 0.0485522 loss)
I0826 01:04:21.002054  1899 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0826 01:05:45.889116  1899 solver.cpp:357] Iteration 45200 (1.17804 iter/s, 84.8871s/100 iters), loss = 0.158173
I0826 01:05:45.889256  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127578 (* 1 = 0.127578 loss)
I0826 01:05:45.889269  1899 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0826 01:07:10.959806  1899 solver.cpp:357] Iteration 45300 (1.17552 iter/s, 85.0684s/100 iters), loss = 0.0921059
I0826 01:07:10.959957  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113623 (* 1 = 0.113623 loss)
I0826 01:07:10.959971  1899 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0826 01:07:19.184777  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:08:33.124204  1899 solver.cpp:357] Iteration 45400 (1.21711 iter/s, 82.1619s/100 iters), loss = 0.0761142
I0826 01:08:33.124306  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0733413 (* 1 = 0.0733413 loss)
I0826 01:08:33.124317  1899 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0826 01:09:53.053333  1899 solver.cpp:514] Iteration 45500, Testing net (#0)
I0826 01:10:48.081058  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:10:48.262177  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.881801
I0826 01:10:48.262235  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.444708 (* 1 = 0.444708 loss)
I0826 01:10:48.932201  1899 solver.cpp:357] Iteration 45500 (0.73632 iter/s, 135.81s/100 iters), loss = 0.0282169
I0826 01:10:48.932277  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.032574 (* 1 = 0.032574 loss)
I0826 01:10:48.932291  1899 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0826 01:12:11.553201  1899 solver.cpp:357] Iteration 45600 (1.21039 iter/s, 82.6182s/100 iters), loss = 0.180001
I0826 01:12:11.553442  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.124976 (* 1 = 0.124976 loss)
I0826 01:12:11.553454  1899 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0826 01:13:34.392380  1899 solver.cpp:357] Iteration 45700 (1.2072 iter/s, 82.8363s/100 iters), loss = 0.0764179
I0826 01:13:34.392554  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0730064 (* 1 = 0.0730064 loss)
I0826 01:13:34.392566  1899 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0826 01:13:34.807061  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:14:51.984248  1899 solver.cpp:357] Iteration 45800 (1.28881 iter/s, 77.5909s/100 iters), loss = 0.0751029
I0826 01:14:51.984382  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10898 (* 1 = 0.10898 loss)
I0826 01:14:51.984395  1899 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0826 01:16:16.894282  1899 solver.cpp:357] Iteration 45900 (1.17773 iter/s, 84.9092s/100 iters), loss = 0.0258532
I0826 01:16:16.894397  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0391638 (* 1 = 0.0391638 loss)
I0826 01:16:16.894407  1899 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0826 01:17:41.361568  1899 solver.cpp:514] Iteration 46000, Testing net (#0)
I0826 01:18:36.918159  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:18:37.127784  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.892402
I0826 01:18:37.127835  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.378192 (* 1 = 0.378192 loss)
I0826 01:18:37.772704  1899 solver.cpp:357] Iteration 46000 (0.709823 iter/s, 140.88s/100 iters), loss = 0.0621912
I0826 01:18:37.772783  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0842885 (* 1 = 0.0842885 loss)
I0826 01:18:37.772794  1899 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0826 01:19:54.552968  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:20:01.037497  1899 solver.cpp:357] Iteration 46100 (1.20103 iter/s, 83.2616s/100 iters), loss = 0.120254
I0826 01:20:01.037569  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114829 (* 1 = 0.114829 loss)
I0826 01:20:01.037581  1899 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0826 01:21:20.486251  1899 solver.cpp:357] Iteration 46200 (1.25869 iter/s, 79.4476s/100 iters), loss = 0.108753
I0826 01:21:20.486425  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.134292 (* 1 = 0.134292 loss)
I0826 01:21:20.486438  1899 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0826 01:22:40.075239  1899 solver.cpp:357] Iteration 46300 (1.25647 iter/s, 79.5879s/100 iters), loss = 0.0920804
I0826 01:22:40.075489  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0483853 (* 1 = 0.0483853 loss)
I0826 01:22:40.075536  1899 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0826 01:24:05.061556  1899 solver.cpp:357] Iteration 46400 (1.1767 iter/s, 84.9831s/100 iters), loss = 0.0532647
I0826 01:24:05.061709  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0637297 (* 1 = 0.0637297 loss)
I0826 01:24:05.061720  1899 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0826 01:25:14.449478  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:25:29.285665  1899 solver.cpp:514] Iteration 46500, Testing net (#0)
I0826 01:26:17.058441  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:26:17.275990  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.881902
I0826 01:26:17.276157  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.440728 (* 1 = 0.440728 loss)
I0826 01:26:17.820291  1899 solver.cpp:357] Iteration 46500 (0.753249 iter/s, 132.758s/100 iters), loss = 0.101655
I0826 01:26:17.820358  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143409 (* 1 = 0.143409 loss)
I0826 01:26:17.820369  1899 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0826 01:27:42.577837  1899 solver.cpp:357] Iteration 46600 (1.17982 iter/s, 84.7585s/100 iters), loss = 0.138863
I0826 01:27:42.578022  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.196723 (* 1 = 0.196723 loss)
I0826 01:27:42.578034  1899 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0826 01:29:07.635998  1899 solver.cpp:357] Iteration 46700 (1.17568 iter/s, 85.0571s/100 iters), loss = 0.0888618
I0826 01:29:07.636211  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0614831 (* 1 = 0.0614831 loss)
I0826 01:29:07.636240  1899 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0826 01:30:32.727577  1899 solver.cpp:357] Iteration 46800 (1.1752 iter/s, 85.092s/100 iters), loss = 0.0602849
I0826 01:30:32.727802  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0650621 (* 1 = 0.0650621 loss)
I0826 01:30:32.727849  1899 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0826 01:31:31.551144  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:31:50.328024  1899 solver.cpp:357] Iteration 46900 (1.28859 iter/s, 77.6045s/100 iters), loss = 0.131672
I0826 01:31:50.328092  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.060742 (* 1 = 0.060742 loss)
I0826 01:31:50.328106  1899 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0826 01:33:09.579228  1899 solver.cpp:514] Iteration 47000, Testing net (#0)
I0826 01:34:04.895809  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:34:05.171363  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.866202
I0826 01:34:05.171423  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.526483 (* 1 = 0.526483 loss)
I0826 01:34:05.764643  1899 solver.cpp:357] Iteration 47000 (0.738313 iter/s, 135.444s/100 iters), loss = 0.125562
I0826 01:34:05.764720  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.123886 (* 1 = 0.123886 loss)
I0826 01:34:05.764731  1899 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0826 01:35:30.651870  1899 solver.cpp:357] Iteration 47100 (1.17802 iter/s, 84.8881s/100 iters), loss = 0.0448869
I0826 01:35:30.652034  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.03013 (* 1 = 0.03013 loss)
I0826 01:35:30.652047  1899 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0826 01:36:55.526748  1899 solver.cpp:357] Iteration 47200 (1.1782 iter/s, 84.8753s/100 iters), loss = 0.156566
I0826 01:36:55.527107  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.225747 (* 1 = 0.225747 loss)
I0826 01:36:55.527120  1899 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0826 01:37:44.080157  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:38:13.119577  1899 solver.cpp:357] Iteration 47300 (1.28878 iter/s, 77.5925s/100 iters), loss = 0.0470614
I0826 01:38:13.119653  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0543989 (* 1 = 0.0543989 loss)
I0826 01:38:13.119666  1899 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0826 01:39:37.991466  1899 solver.cpp:357] Iteration 47400 (1.17825 iter/s, 84.8716s/100 iters), loss = 0.080881
I0826 01:39:37.991679  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14287 (* 1 = 0.14287 loss)
I0826 01:39:37.991708  1899 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0826 01:41:01.786204  1899 solver.cpp:514] Iteration 47500, Testing net (#0)
I0826 01:41:52.917021  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:41:53.142688  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.863601
I0826 01:41:53.142743  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.534981 (* 1 = 0.534981 loss)
I0826 01:41:53.778142  1899 solver.cpp:357] Iteration 47500 (0.736441 iter/s, 135.788s/100 iters), loss = 0.0894089
I0826 01:41:53.778216  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136399 (* 1 = 0.136399 loss)
I0826 01:41:53.778228  1899 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0826 01:43:15.449805  1899 solver.cpp:357] Iteration 47600 (1.22443 iter/s, 81.6706s/100 iters), loss = 0.0920143
I0826 01:43:15.450073  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0660244 (* 1 = 0.0660244 loss)
I0826 01:43:15.450086  1899 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0826 01:43:51.709017  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:44:17.761600  1899 solver.cpp:357] Iteration 47700 (1.60483 iter/s, 62.3119s/100 iters), loss = 0.0840377
I0826 01:44:17.761662  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0863395 (* 1 = 0.0863395 loss)
I0826 01:44:17.761672  1899 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0826 01:45:14.194213  1899 solver.cpp:357] Iteration 47800 (1.77196 iter/s, 56.4345s/100 iters), loss = 0.04272
I0826 01:45:14.194365  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0450324 (* 1 = 0.0450324 loss)
I0826 01:45:14.194375  1899 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0826 01:46:10.800783  1899 solver.cpp:357] Iteration 47900 (1.76652 iter/s, 56.6083s/100 iters), loss = 0.0570236
I0826 01:46:10.800926  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0424118 (* 1 = 0.0424118 loss)
I0826 01:46:10.800938  1899 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0826 01:47:06.774560  1899 solver.cpp:514] Iteration 48000, Testing net (#0)
I0826 01:47:44.021119  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:47:44.185623  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.893302
I0826 01:47:44.185683  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.403905 (* 1 = 0.403905 loss)
I0826 01:47:44.604890  1899 solver.cpp:357] Iteration 48000 (1.06602 iter/s, 93.807s/100 iters), loss = 0.0576664
I0826 01:47:44.604964  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.045921 (* 1 = 0.045921 loss)
I0826 01:47:44.604974  1899 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0826 01:47:44.604982  1899 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0826 01:48:09.505252  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:48:37.775447  1899 solver.cpp:357] Iteration 48100 (1.88076 iter/s, 53.1701s/100 iters), loss = 0.0378328
I0826 01:48:37.775566  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0340682 (* 1 = 0.0340682 loss)
I0826 01:48:37.775579  1899 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0826 01:49:31.154518  1899 solver.cpp:357] Iteration 48200 (1.87341 iter/s, 53.3786s/100 iters), loss = 0.0434538
I0826 01:49:31.154708  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0256375 (* 1 = 0.0256375 loss)
I0826 01:49:31.154721  1899 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0826 01:50:27.884975  1899 solver.cpp:357] Iteration 48300 (1.76274 iter/s, 56.73s/100 iters), loss = 0.0365168
I0826 01:50:27.885135  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0556753 (* 1 = 0.0556753 loss)
I0826 01:50:27.885148  1899 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0826 01:51:24.087610  1899 solver.cpp:357] Iteration 48400 (1.77929 iter/s, 56.2022s/100 iters), loss = 0.0323639
I0826 01:51:24.087786  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.058402 (* 1 = 0.058402 loss)
I0826 01:51:24.087797  1899 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0826 01:51:43.614781  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:52:20.267184  1899 solver.cpp:514] Iteration 48500, Testing net (#0)
I0826 01:52:57.077446  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:52:57.246067  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922702
I0826 01:52:57.246125  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.277212 (* 1 = 0.277212 loss)
I0826 01:52:57.769898  1899 solver.cpp:357] Iteration 48500 (1.06743 iter/s, 93.6829s/100 iters), loss = 0.0811176
I0826 01:52:57.769959  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117925 (* 1 = 0.117925 loss)
I0826 01:52:57.769970  1899 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0826 01:53:54.349100  1899 solver.cpp:357] Iteration 48600 (1.76739 iter/s, 56.5807s/100 iters), loss = 0.0286833
I0826 01:53:54.349491  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0345244 (* 1 = 0.0345244 loss)
I0826 01:53:54.349555  1899 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0826 01:54:50.616691  1899 solver.cpp:357] Iteration 48700 (1.77724 iter/s, 56.267s/100 iters), loss = 0.043941
I0826 01:54:50.616798  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0518971 (* 1 = 0.0518971 loss)
I0826 01:54:50.616811  1899 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0826 01:55:40.985069  1899 solver.cpp:357] Iteration 48800 (1.98532 iter/s, 50.3696s/100 iters), loss = 0.0211038
I0826 01:55:40.985194  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0162886 (* 1 = 0.0162886 loss)
I0826 01:55:40.985206  1899 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0826 01:55:55.520992  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:56:37.631367  1899 solver.cpp:357] Iteration 48900 (1.76536 iter/s, 56.6457s/100 iters), loss = 0.0354923
I0826 01:56:37.631485  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0437824 (* 1 = 0.0437824 loss)
I0826 01:56:37.631497  1899 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0826 01:57:33.826975  1899 solver.cpp:514] Iteration 49000, Testing net (#0)
I0826 01:58:10.957810  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:58:11.074124  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.924303
I0826 01:58:11.074183  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.277695 (* 1 = 0.277695 loss)
I0826 01:58:11.641294  1899 solver.cpp:357] Iteration 49000 (1.06371 iter/s, 94.0103s/100 iters), loss = 0.0398078
I0826 01:58:11.641355  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0465463 (* 1 = 0.0465463 loss)
I0826 01:58:11.641366  1899 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0826 01:59:08.115694  1899 solver.cpp:357] Iteration 49100 (1.77067 iter/s, 56.4758s/100 iters), loss = 0.0130029
I0826 01:59:08.115798  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0162931 (* 1 = 0.0162931 loss)
I0826 01:59:08.115809  1899 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0826 02:00:04.821666  1899 solver.cpp:357] Iteration 49200 (1.76344 iter/s, 56.7073s/100 iters), loss = 0.0452477
I0826 02:00:04.821832  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0548358 (* 1 = 0.0548358 loss)
I0826 02:00:04.821844  1899 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0826 02:00:13.968880  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:01:01.403071  1899 solver.cpp:357] Iteration 49300 (1.76739 iter/s, 56.5807s/100 iters), loss = 0.0361683
I0826 02:01:01.403187  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.031392 (* 1 = 0.031392 loss)
I0826 02:01:01.403198  1899 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0826 02:01:52.878648  1899 solver.cpp:357] Iteration 49400 (1.94262 iter/s, 51.4768s/100 iters), loss = 0.0220516
I0826 02:01:52.878783  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222988 (* 1 = 0.0222988 loss)
I0826 02:01:52.878795  1899 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0826 02:02:47.134191  1899 solver.cpp:514] Iteration 49500, Testing net (#0)
I0826 02:03:23.592336  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:03:23.636799  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927402
I0826 02:03:23.636847  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.279156 (* 1 = 0.279156 loss)
I0826 02:03:24.192762  1899 solver.cpp:357] Iteration 49500 (1.09509 iter/s, 91.3163s/100 iters), loss = 0.0348675
I0826 02:03:24.192837  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0458799 (* 1 = 0.0458799 loss)
I0826 02:03:24.192850  1899 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0826 02:04:20.804471  1899 solver.cpp:357] Iteration 49600 (1.76647 iter/s, 56.6102s/100 iters), loss = 0.0330859
I0826 02:04:20.804677  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187598 (* 1 = 0.0187598 loss)
I0826 02:04:20.804688  1899 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0826 02:04:24.548121  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:05:17.559557  1899 solver.cpp:357] Iteration 49700 (1.76203 iter/s, 56.7527s/100 iters), loss = 0.01412
I0826 02:05:17.559736  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0181895 (* 1 = 0.0181895 loss)
I0826 02:05:17.559749  1899 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0826 02:06:14.168668  1899 solver.cpp:357] Iteration 49800 (1.76657 iter/s, 56.6069s/100 iters), loss = 0.0161513
I0826 02:06:14.168772  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0105131 (* 1 = 0.0105131 loss)
I0826 02:06:14.168784  1899 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0826 02:07:10.718963  1899 solver.cpp:357] Iteration 49900 (1.76834 iter/s, 56.5502s/100 iters), loss = 0.0310464
I0826 02:07:10.719132  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0292163 (* 1 = 0.0292163 loss)
I0826 02:07:10.719146  1899 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0826 02:08:03.814563  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:08:04.773243  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_50000.caffemodel
I0826 02:08:04.792697  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_50000.solverstate
I0826 02:08:04.800366  1899 solver.cpp:514] Iteration 50000, Testing net (#0)
I0826 02:08:35.585850  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:08:35.648108  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928403
I0826 02:08:35.648171  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273115 (* 1 = 0.273115 loss)
I0826 02:08:36.207315  1899 solver.cpp:357] Iteration 50000 (1.16977 iter/s, 85.4865s/100 iters), loss = 0.0270849
I0826 02:08:36.207386  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0178156 (* 1 = 0.0178156 loss)
I0826 02:08:36.207397  1899 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0826 02:09:32.695785  1899 solver.cpp:357] Iteration 50100 (1.77033 iter/s, 56.4866s/100 iters), loss = 0.0244626
I0826 02:09:32.695930  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0228841 (* 1 = 0.0228841 loss)
I0826 02:09:32.695942  1899 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0826 02:10:29.191833  1899 solver.cpp:357] Iteration 50200 (1.77009 iter/s, 56.4943s/100 iters), loss = 0.0355508
I0826 02:10:29.191985  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.037482 (* 1 = 0.037482 loss)
I0826 02:10:29.191998  1899 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0826 02:11:25.665477  1899 solver.cpp:357] Iteration 50300 (1.77079 iter/s, 56.472s/100 iters), loss = 0.00623919
I0826 02:11:25.665640  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00864503 (* 1 = 0.00864503 loss)
I0826 02:11:25.665652  1899 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0826 02:12:15.367179  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:12:22.098126  1899 solver.cpp:357] Iteration 50400 (1.77207 iter/s, 56.4311s/100 iters), loss = 0.0185017
I0826 02:12:22.098201  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0184104 (* 1 = 0.0184104 loss)
I0826 02:12:22.098212  1899 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0826 02:13:18.274698  1899 solver.cpp:514] Iteration 50500, Testing net (#0)
I0826 02:13:54.758177  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:13:54.871351  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927102
I0826 02:13:54.871408  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.28152 (* 1 = 0.28152 loss)
I0826 02:13:55.442930  1899 solver.cpp:357] Iteration 50500 (1.07131 iter/s, 93.3438s/100 iters), loss = 0.0176249
I0826 02:13:55.443007  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0156871 (* 1 = 0.0156871 loss)
I0826 02:13:55.443023  1899 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0826 02:14:47.090539  1899 solver.cpp:357] Iteration 50600 (1.93625 iter/s, 51.6461s/100 iters), loss = 0.00674693
I0826 02:14:47.090745  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00506779 (* 1 = 0.00506779 loss)
I0826 02:14:47.090757  1899 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0826 02:15:41.683333  1899 solver.cpp:357] Iteration 50700 (1.83173 iter/s, 54.5933s/100 iters), loss = 0.0307473
I0826 02:15:41.683492  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0478093 (* 1 = 0.0478093 loss)
I0826 02:15:41.683504  1899 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0826 02:16:26.193631  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:16:38.335224  1899 solver.cpp:357] Iteration 50800 (1.76515 iter/s, 56.6525s/100 iters), loss = 0.0275783
I0826 02:16:38.335289  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0227479 (* 1 = 0.0227479 loss)
I0826 02:16:38.335299  1899 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0826 02:17:34.836820  1899 solver.cpp:357] Iteration 50900 (1.76984 iter/s, 56.5024s/100 iters), loss = 0.0167611
I0826 02:17:34.836937  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00665568 (* 1 = 0.00665568 loss)
I0826 02:17:34.836948  1899 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0826 02:18:30.940814  1899 solver.cpp:514] Iteration 51000, Testing net (#0)
I0826 02:19:08.072635  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:19:08.149948  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928802
I0826 02:19:08.149991  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.284114 (* 1 = 0.284114 loss)
I0826 02:19:08.709614  1899 solver.cpp:357] Iteration 51000 (1.06526 iter/s, 93.8742s/100 iters), loss = 0.0212362
I0826 02:19:08.709684  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0214775 (* 1 = 0.0214775 loss)
I0826 02:19:08.709697  1899 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0826 02:20:05.292945  1899 solver.cpp:357] Iteration 51100 (1.76734 iter/s, 56.5821s/100 iters), loss = 0.00438112
I0826 02:20:05.293087  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00428461 (* 1 = 0.00428461 loss)
I0826 02:20:05.293099  1899 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0826 02:20:44.423877  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:20:59.919498  1899 solver.cpp:357] Iteration 51200 (1.83065 iter/s, 54.6253s/100 iters), loss = 0.0262217
I0826 02:20:59.919566  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0434415 (* 1 = 0.0434415 loss)
I0826 02:20:59.919579  1899 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0826 02:21:51.740921  1899 solver.cpp:357] Iteration 51300 (1.92975 iter/s, 51.8202s/100 iters), loss = 0.0163828
I0826 02:21:51.741070  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0274764 (* 1 = 0.0274764 loss)
I0826 02:21:51.741080  1899 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0826 02:22:48.452714  1899 solver.cpp:357] Iteration 51400 (1.76328 iter/s, 56.7126s/100 iters), loss = 0.0142828
I0826 02:22:48.452832  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00891611 (* 1 = 0.00891611 loss)
I0826 02:22:48.452845  1899 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0826 02:23:44.492506  1899 solver.cpp:514] Iteration 51500, Testing net (#0)
I0826 02:24:21.546085  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:24:21.709925  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927002
I0826 02:24:21.709981  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290636 (* 1 = 0.290636 loss)
I0826 02:24:22.122448  1899 solver.cpp:357] Iteration 51500 (1.06759 iter/s, 93.6693s/100 iters), loss = 0.00741795
I0826 02:24:22.122508  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00728137 (* 1 = 0.00728137 loss)
I0826 02:24:22.122519  1899 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0826 02:24:55.932360  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:25:18.782559  1899 solver.cpp:357] Iteration 51600 (1.76488 iter/s, 56.6611s/100 iters), loss = 0.00543174
I0826 02:25:18.782632  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00409952 (* 1 = 0.00409952 loss)
I0826 02:25:18.782644  1899 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0826 02:26:15.285166  1899 solver.cpp:357] Iteration 51700 (1.76986 iter/s, 56.5015s/100 iters), loss = 0.0277865
I0826 02:26:15.285337  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0172019 (* 1 = 0.0172019 loss)
I0826 02:26:15.285348  1899 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0826 02:27:12.019534  1899 solver.cpp:357] Iteration 51800 (1.76257 iter/s, 56.7352s/100 iters), loss = 0.0558384
I0826 02:27:12.019707  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0612291 (* 1 = 0.0612291 loss)
I0826 02:27:12.019721  1899 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0826 02:28:01.928736  1899 solver.cpp:357] Iteration 51900 (2.00369 iter/s, 49.908s/100 iters), loss = 0.00691638
I0826 02:28:01.928848  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00879537 (* 1 = 0.00879537 loss)
I0826 02:28:01.928860  1899 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0826 02:28:30.303627  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:28:57.866194  1899 solver.cpp:514] Iteration 52000, Testing net (#0)
I0826 02:29:34.405514  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:29:34.487561  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927502
I0826 02:29:34.487603  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.287159 (* 1 = 0.287159 loss)
I0826 02:29:34.995616  1899 solver.cpp:357] Iteration 52000 (1.0745 iter/s, 93.0666s/100 iters), loss = 0.0151505
I0826 02:29:34.995687  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0136508 (* 1 = 0.0136508 loss)
I0826 02:29:34.995699  1899 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0826 02:30:31.559625  1899 solver.cpp:357] Iteration 52100 (1.76794 iter/s, 56.563s/100 iters), loss = 0.0132398
I0826 02:30:31.559728  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187708 (* 1 = 0.0187708 loss)
I0826 02:30:31.559738  1899 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0826 02:31:28.306550  1899 solver.cpp:357] Iteration 52200 (1.76218 iter/s, 56.7479s/100 iters), loss = 0.020539
I0826 02:31:28.306663  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0148434 (* 1 = 0.0148434 loss)
I0826 02:31:28.306675  1899 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0826 02:32:24.965181  1899 solver.cpp:357] Iteration 52300 (1.76499 iter/s, 56.6576s/100 iters), loss = 0.00584188
I0826 02:32:24.965344  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0033123 (* 1 = 0.0033123 loss)
I0826 02:32:24.965358  1899 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0826 02:32:48.242043  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:33:21.583876  1899 solver.cpp:357] Iteration 52400 (1.76623 iter/s, 56.6176s/100 iters), loss = 0.0126202
I0826 02:33:21.584023  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0133348 (* 1 = 0.0133348 loss)
I0826 02:33:21.584040  1899 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0826 02:34:14.308441  1899 solver.cpp:514] Iteration 52500, Testing net (#0)
I0826 02:34:46.901322  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:34:47.069834  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927702
I0826 02:34:47.069886  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290724 (* 1 = 0.290724 loss)
I0826 02:34:47.582564  1899 solver.cpp:357] Iteration 52500 (1.16281 iter/s, 85.9983s/100 iters), loss = 0.0261605
I0826 02:34:47.582624  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112979 (* 1 = 0.0112979 loss)
I0826 02:34:47.582634  1899 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0826 02:35:44.130815  1899 solver.cpp:357] Iteration 52600 (1.76837 iter/s, 56.5493s/100 iters), loss = 0.0111823
I0826 02:35:44.131018  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122841 (* 1 = 0.0122841 loss)
I0826 02:35:44.131031  1899 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0826 02:36:40.636608  1899 solver.cpp:357] Iteration 52700 (1.76976 iter/s, 56.5047s/100 iters), loss = 0.0204671
I0826 02:36:40.636772  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0146828 (* 1 = 0.0146828 loss)
I0826 02:36:40.636786  1899 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0826 02:36:58.444015  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:37:37.200624  1899 solver.cpp:357] Iteration 52800 (1.76794 iter/s, 56.563s/100 iters), loss = 0.0156598
I0826 02:37:37.200891  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0291898 (* 1 = 0.0291898 loss)
I0826 02:37:37.200901  1899 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0826 02:38:33.873011  1899 solver.cpp:357] Iteration 52900 (1.76446 iter/s, 56.6745s/100 iters), loss = 0.014683
I0826 02:38:33.873134  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.013603 (* 1 = 0.013603 loss)
I0826 02:38:33.873147  1899 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0826 02:39:29.867746  1899 solver.cpp:514] Iteration 53000, Testing net (#0)
I0826 02:40:06.938580  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:40:07.013484  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928402
I0826 02:40:07.013528  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290997 (* 1 = 0.290997 loss)
I0826 02:40:07.525359  1899 solver.cpp:357] Iteration 53000 (1.06774 iter/s, 93.6556s/100 iters), loss = 0.0256911
I0826 02:40:07.525431  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.047784 (* 1 = 0.047784 loss)
I0826 02:40:07.525442  1899 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0826 02:40:57.561806  1899 solver.cpp:357] Iteration 53100 (1.99852 iter/s, 50.037s/100 iters), loss = 0.0140046
I0826 02:40:57.561952  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157983 (* 1 = 0.0157983 loss)
I0826 02:40:57.561964  1899 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0826 02:41:09.974000  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:41:54.009404  1899 solver.cpp:357] Iteration 53200 (1.77147 iter/s, 56.4503s/100 iters), loss = 0.0130088
I0826 02:41:54.009716  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0216659 (* 1 = 0.0216659 loss)
I0826 02:41:54.009781  1899 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0826 02:42:50.440176  1899 solver.cpp:357] Iteration 53300 (1.77206 iter/s, 56.4314s/100 iters), loss = 0.00371228
I0826 02:42:50.440353  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00466887 (* 1 = 0.00466887 loss)
I0826 02:42:50.440367  1899 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0826 02:43:47.082845  1899 solver.cpp:357] Iteration 53400 (1.76544 iter/s, 56.6431s/100 iters), loss = 0.0311316
I0826 02:43:47.083016  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0422705 (* 1 = 0.0422705 loss)
I0826 02:43:47.083029  1899 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0826 02:44:43.006937  1899 solver.cpp:514] Iteration 53500, Testing net (#0)
I0826 02:45:19.830441  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:45:19.999914  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930402
I0826 02:45:19.999967  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.289774 (* 1 = 0.289774 loss)
I0826 02:45:20.481866  1899 solver.cpp:357] Iteration 53500 (1.07065 iter/s, 93.401s/100 iters), loss = 0.0336634
I0826 02:45:20.481927  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0520998 (* 1 = 0.0520998 loss)
I0826 02:45:20.481937  1899 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0826 02:45:28.035838  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:46:17.024268  1899 solver.cpp:357] Iteration 53600 (1.76851 iter/s, 56.5447s/100 iters), loss = 0.011399
I0826 02:46:17.024510  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0126962 (* 1 = 0.0126962 loss)
I0826 02:46:17.024524  1899 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0826 02:47:10.048543  1899 solver.cpp:357] Iteration 53700 (1.88593 iter/s, 53.0242s/100 iters), loss = 0.0234692
I0826 02:47:10.048728  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0439353 (* 1 = 0.0439353 loss)
I0826 02:47:10.048741  1899 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0826 02:48:03.310258  1899 solver.cpp:357] Iteration 53800 (1.87752 iter/s, 53.2616s/100 iters), loss = 0.00506179
I0826 02:48:03.310386  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00769513 (* 1 = 0.00769513 loss)
I0826 02:48:03.310400  1899 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0826 02:48:59.955025  1899 solver.cpp:357] Iteration 53900 (1.76539 iter/s, 56.6448s/100 iters), loss = 0.0118868
I0826 02:48:59.955196  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00933645 (* 1 = 0.00933645 loss)
I0826 02:48:59.955209  1899 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0826 02:49:01.957901  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:49:55.998103  1899 solver.cpp:514] Iteration 54000, Testing net (#0)
I0826 02:50:32.477411  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:50:32.658552  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928902
I0826 02:50:32.658601  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290564 (* 1 = 0.290564 loss)
I0826 02:50:33.120137  1899 solver.cpp:357] Iteration 54000 (1.07335 iter/s, 93.1664s/100 iters), loss = 0.0102264
I0826 02:50:33.120200  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00565663 (* 1 = 0.00565663 loss)
I0826 02:50:33.120210  1899 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0826 02:51:29.670939  1899 solver.cpp:357] Iteration 54100 (1.76826 iter/s, 56.5527s/100 iters), loss = 0.0112197
I0826 02:51:29.671113  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00887665 (* 1 = 0.00887665 loss)
I0826 02:51:29.671126  1899 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0826 02:52:26.264986  1899 solver.cpp:357] Iteration 54200 (1.76698 iter/s, 56.5938s/100 iters), loss = 0.00589066
I0826 02:52:26.265107  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00485445 (* 1 = 0.00485445 loss)
I0826 02:52:26.265120  1899 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0826 02:53:19.557638  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:53:22.489176  1899 solver.cpp:357] Iteration 54300 (1.7786 iter/s, 56.2239s/100 iters), loss = 0.0155427
I0826 02:53:22.489244  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00819136 (* 1 = 0.00819136 loss)
I0826 02:53:22.489256  1899 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0826 02:54:12.861079  1899 solver.cpp:357] Iteration 54400 (1.98522 iter/s, 50.3724s/100 iters), loss = 0.00435702
I0826 02:54:12.861238  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00507065 (* 1 = 0.00507065 loss)
I0826 02:54:12.861249  1899 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0826 02:55:08.945549  1899 solver.cpp:514] Iteration 54500, Testing net (#0)
I0826 02:55:45.774591  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:55:45.943562  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928803
I0826 02:55:45.943619  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.29781 (* 1 = 0.29781 loss)
I0826 02:55:46.437511  1899 solver.cpp:357] Iteration 54500 (1.06863 iter/s, 93.5773s/100 iters), loss = 0.0101845
I0826 02:55:46.437572  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0151251 (* 1 = 0.0151251 loss)
I0826 02:55:46.437582  1899 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0826 02:56:43.111069  1899 solver.cpp:357] Iteration 54600 (1.76444 iter/s, 56.6753s/100 iters), loss = 0.00578385
I0826 02:56:43.111358  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00570172 (* 1 = 0.00570172 loss)
I0826 02:56:43.111373  1899 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0826 02:57:30.960079  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 02:57:39.700450  1899 solver.cpp:357] Iteration 54700 (1.76713 iter/s, 56.589s/100 iters), loss = 0.0276265
I0826 02:57:39.700531  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00396459 (* 1 = 0.00396459 loss)
I0826 02:57:39.700541  1899 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0826 02:58:36.293686  1899 solver.cpp:357] Iteration 54800 (1.76701 iter/s, 56.5928s/100 iters), loss = 0.0112998
I0826 02:58:36.293849  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0172995 (* 1 = 0.0172995 loss)
I0826 02:58:36.293861  1899 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0826 02:59:32.846679  1899 solver.cpp:357] Iteration 54900 (1.76827 iter/s, 56.5526s/100 iters), loss = 0.0196708
I0826 02:59:32.846833  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00385005 (* 1 = 0.00385005 loss)
I0826 02:59:32.846844  1899 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0826 03:00:23.852381  1899 solver.cpp:514] Iteration 55000, Testing net (#0)
I0826 03:00:58.391961  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:00:58.569614  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929403
I0826 03:00:58.569684  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294372 (* 1 = 0.294372 loss)
I0826 03:00:59.007005  1899 solver.cpp:357] Iteration 55000 (1.16059 iter/s, 86.1628s/100 iters), loss = 0.00583678
I0826 03:00:59.007074  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00797593 (* 1 = 0.00797593 loss)
I0826 03:00:59.007084  1899 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0826 03:01:41.873643  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:01:55.711534  1899 solver.cpp:357] Iteration 55100 (1.76348 iter/s, 56.7061s/100 iters), loss = 0.0313913
I0826 03:01:55.711616  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0152081 (* 1 = 0.0152081 loss)
I0826 03:01:55.711628  1899 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0826 03:02:52.323215  1899 solver.cpp:357] Iteration 55200 (1.76644 iter/s, 56.6112s/100 iters), loss = 0.0101264
I0826 03:02:52.323313  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0150076 (* 1 = 0.0150076 loss)
I0826 03:02:52.323324  1899 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0826 03:03:49.030496  1899 solver.cpp:357] Iteration 55300 (1.76339 iter/s, 56.7088s/100 iters), loss = 0.0269523
I0826 03:03:49.030678  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0184156 (* 1 = 0.0184156 loss)
I0826 03:03:49.030689  1899 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0826 03:04:45.718030  1899 solver.cpp:357] Iteration 55400 (1.76407 iter/s, 56.687s/100 iters), loss = 0.0082089
I0826 03:04:45.718175  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00132224 (* 1 = 0.00132224 loss)
I0826 03:04:45.718186  1899 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0826 03:05:23.120635  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:05:41.716343  1899 solver.cpp:514] Iteration 55500, Testing net (#0)
I0826 03:06:17.972934  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:06:18.081996  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929002
I0826 03:06:18.082048  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.299679 (* 1 = 0.299679 loss)
I0826 03:06:18.523452  1899 solver.cpp:357] Iteration 55500 (1.07749 iter/s, 92.808s/100 iters), loss = 0.00303167
I0826 03:06:18.523519  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00429099 (* 1 = 0.00429099 loss)
I0826 03:06:18.523531  1899 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0826 03:07:09.240051  1899 solver.cpp:357] Iteration 55600 (1.97177 iter/s, 50.7159s/100 iters), loss = 0.0110689
I0826 03:07:09.240293  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00203344 (* 1 = 0.00203344 loss)
I0826 03:07:09.240308  1899 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0826 03:08:05.638923  1899 solver.cpp:357] Iteration 55700 (1.7731 iter/s, 56.3983s/100 iters), loss = 0.0206205
I0826 03:08:05.639076  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00837331 (* 1 = 0.00837331 loss)
I0826 03:08:05.639088  1899 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0826 03:09:02.153829  1899 solver.cpp:357] Iteration 55800 (1.7694 iter/s, 56.5163s/100 iters), loss = 0.0135621
I0826 03:09:02.153960  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00361963 (* 1 = 0.00361963 loss)
I0826 03:09:02.153973  1899 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0826 03:09:34.237382  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:09:58.857256  1899 solver.cpp:357] Iteration 55900 (1.76358 iter/s, 56.7029s/100 iters), loss = 0.0244897
I0826 03:09:58.857323  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0104874 (* 1 = 0.0104874 loss)
I0826 03:09:58.857334  1899 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0826 03:10:54.792728  1899 solver.cpp:514] Iteration 56000, Testing net (#0)
I0826 03:11:31.591651  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:11:31.753463  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931902
I0826 03:11:31.753520  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.293061 (* 1 = 0.293061 loss)
I0826 03:11:32.171339  1899 solver.cpp:357] Iteration 56000 (1.07162 iter/s, 93.3167s/100 iters), loss = 0.0154436
I0826 03:11:32.171407  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00368369 (* 1 = 0.00368369 loss)
I0826 03:11:32.171419  1899 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0826 03:12:28.642822  1899 solver.cpp:357] Iteration 56100 (1.7708 iter/s, 56.4716s/100 iters), loss = 0.0126908
I0826 03:12:28.643007  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00332369 (* 1 = 0.00332369 loss)
I0826 03:12:28.643019  1899 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0826 03:13:19.781762  1899 solver.cpp:357] Iteration 56200 (1.95542 iter/s, 51.14s/100 iters), loss = 0.00526911
I0826 03:13:19.781877  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00469723 (* 1 = 0.00469723 loss)
I0826 03:13:19.781888  1899 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0826 03:13:44.918141  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:14:14.887248  1899 solver.cpp:357] Iteration 56300 (1.81466 iter/s, 55.1067s/100 iters), loss = 0.00393464
I0826 03:14:14.887395  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00392165 (* 1 = 0.00392165 loss)
I0826 03:14:14.887408  1899 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0826 03:15:11.263094  1899 solver.cpp:357] Iteration 56400 (1.77371 iter/s, 56.3789s/100 iters), loss = 0.00615085
I0826 03:15:11.263280  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00305938 (* 1 = 0.00305938 loss)
I0826 03:15:11.263293  1899 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0826 03:16:07.343076  1899 solver.cpp:514] Iteration 56500, Testing net (#0)
I0826 03:16:44.073247  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:16:44.120884  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929202
I0826 03:16:44.120934  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.299995 (* 1 = 0.299995 loss)
I0826 03:16:44.673210  1899 solver.cpp:357] Iteration 56500 (1.07051 iter/s, 93.413s/100 iters), loss = 0.00792062
I0826 03:16:44.673279  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0129401 (* 1 = 0.0129401 loss)
I0826 03:16:44.673291  1899 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0826 03:17:41.152977  1899 solver.cpp:357] Iteration 56600 (1.77052 iter/s, 56.4805s/100 iters), loss = 0.00256559
I0826 03:17:41.153252  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00184986 (* 1 = 0.00184986 loss)
I0826 03:17:41.153266  1899 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0826 03:18:02.691742  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:18:37.730819  1899 solver.cpp:357] Iteration 56700 (1.76745 iter/s, 56.5786s/100 iters), loss = 0.00448503
I0826 03:18:37.730978  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00626179 (* 1 = 0.00626179 loss)
I0826 03:18:37.730989  1899 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0826 03:19:32.010047  1899 solver.cpp:357] Iteration 56800 (1.84231 iter/s, 54.2796s/100 iters), loss = 0.00819195
I0826 03:19:32.010210  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00803149 (* 1 = 0.00803149 loss)
I0826 03:19:32.010223  1899 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0826 03:20:23.957418  1899 solver.cpp:357] Iteration 56900 (1.92502 iter/s, 51.9476s/100 iters), loss = 0.016743
I0826 03:20:23.957558  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00936583 (* 1 = 0.00936583 loss)
I0826 03:20:23.957568  1899 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0826 03:21:20.122581  1899 solver.cpp:514] Iteration 57000, Testing net (#0)
I0826 03:21:57.273910  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:21:57.364126  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0826 03:21:57.364187  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294246 (* 1 = 0.294246 loss)
I0826 03:21:57.923692  1899 solver.cpp:357] Iteration 57000 (1.06417 iter/s, 93.9703s/100 iters), loss = 0.00606922
I0826 03:21:57.923766  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00747273 (* 1 = 0.00747273 loss)
I0826 03:21:57.923779  1899 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0826 03:22:14.180898  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:22:54.621538  1899 solver.cpp:357] Iteration 57100 (1.76373 iter/s, 56.698s/100 iters), loss = 0.00941423
I0826 03:22:54.621836  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0132698 (* 1 = 0.0132698 loss)
I0826 03:22:54.621850  1899 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0826 03:23:50.921689  1899 solver.cpp:357] Iteration 57200 (1.77619 iter/s, 56.3003s/100 iters), loss = 0.0104761
I0826 03:23:50.921867  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0143575 (* 1 = 0.0143575 loss)
I0826 03:23:50.921880  1899 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0826 03:24:47.725476  1899 solver.cpp:357] Iteration 57300 (1.76044 iter/s, 56.8039s/100 iters), loss = 0.00433942
I0826 03:24:47.725648  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00282968 (* 1 = 0.00282968 loss)
I0826 03:24:47.725661  1899 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0826 03:25:44.335147  1899 solver.cpp:357] Iteration 57400 (1.76648 iter/s, 56.6097s/100 iters), loss = 0.00779976
I0826 03:25:44.335273  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0117743 (* 1 = 0.0117743 loss)
I0826 03:25:44.335286  1899 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0826 03:25:54.377497  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:26:33.623314  1899 solver.cpp:514] Iteration 57500, Testing net (#0)
I0826 03:27:10.223861  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:27:10.400475  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931302
I0826 03:27:10.400540  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.293619 (* 1 = 0.293619 loss)
I0826 03:27:10.830698  1899 solver.cpp:357] Iteration 57500 (1.15611 iter/s, 86.4967s/100 iters), loss = 0.00880209
I0826 03:27:10.830766  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00411524 (* 1 = 0.00411524 loss)
I0826 03:27:10.830776  1899 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0826 03:28:07.486359  1899 solver.cpp:357] Iteration 57600 (1.76499 iter/s, 56.6577s/100 iters), loss = 0.00940733
I0826 03:28:07.486584  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0145814 (* 1 = 0.0145814 loss)
I0826 03:28:07.486598  1899 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0826 03:29:04.043676  1899 solver.cpp:357] Iteration 57700 (1.76812 iter/s, 56.5573s/100 iters), loss = 0.0308007
I0826 03:29:04.043799  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0264772 (* 1 = 0.0264772 loss)
I0826 03:29:04.043809  1899 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0826 03:30:00.773891  1899 solver.cpp:357] Iteration 57800 (1.76267 iter/s, 56.7321s/100 iters), loss = 0.00252778
I0826 03:30:00.774003  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00274062 (* 1 = 0.00274062 loss)
I0826 03:30:00.774015  1899 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0826 03:30:06.141155  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:30:57.215800  1899 solver.cpp:357] Iteration 57900 (1.77174 iter/s, 56.4418s/100 iters), loss = 0.00561517
I0826 03:30:57.216125  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0047319 (* 1 = 0.0047319 loss)
I0826 03:30:57.216189  1899 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0826 03:31:53.199362  1899 solver.cpp:514] Iteration 58000, Testing net (#0)
I0826 03:32:27.010885  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:32:27.138490  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929702
I0826 03:32:27.138533  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.300629 (* 1 = 0.300629 loss)
I0826 03:32:27.576797  1899 solver.cpp:357] Iteration 58000 (1.10666 iter/s, 90.362s/100 iters), loss = 0.00441499
I0826 03:32:27.576866  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00770915 (* 1 = 0.00770915 loss)
I0826 03:32:27.576879  1899 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0826 03:33:19.455510  1899 solver.cpp:357] Iteration 58100 (1.92759 iter/s, 51.8784s/100 iters), loss = 0.010094
I0826 03:33:19.455616  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0128352 (* 1 = 0.0128352 loss)
I0826 03:33:19.455628  1899 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0826 03:34:15.628509  1899 solver.cpp:357] Iteration 58200 (1.78016 iter/s, 56.1747s/100 iters), loss = 0.00409748
I0826 03:34:15.628654  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00454182 (* 1 = 0.00454182 loss)
I0826 03:34:15.628666  1899 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0826 03:34:16.040460  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:35:12.255445  1899 solver.cpp:357] Iteration 58300 (1.76589 iter/s, 56.6287s/100 iters), loss = 0.00589522
I0826 03:35:12.255619  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00688365 (* 1 = 0.00688365 loss)
I0826 03:35:12.255631  1899 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0826 03:36:08.881366  1899 solver.cpp:357] Iteration 58400 (1.76598 iter/s, 56.6257s/100 iters), loss = 0.00691624
I0826 03:36:08.881536  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00343685 (* 1 = 0.00343685 loss)
I0826 03:36:08.881547  1899 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0826 03:37:04.944756  1899 solver.cpp:514] Iteration 58500, Testing net (#0)
I0826 03:37:41.667170  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:37:41.778787  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931402
I0826 03:37:41.778832  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.305514 (* 1 = 0.305514 loss)
I0826 03:37:42.287994  1899 solver.cpp:357] Iteration 58500 (1.07058 iter/s, 93.4077s/100 iters), loss = 0.010456
I0826 03:37:42.288059  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0103564 (* 1 = 0.0103564 loss)
I0826 03:37:42.288070  1899 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0826 03:38:33.868625  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:38:38.891525  1899 solver.cpp:357] Iteration 58600 (1.76662 iter/s, 56.6053s/100 iters), loss = 0.0178805
I0826 03:38:38.891597  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0236873 (* 1 = 0.0236873 loss)
I0826 03:38:38.891607  1899 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0826 03:39:28.910161  1899 solver.cpp:357] Iteration 58700 (1.99919 iter/s, 50.0202s/100 iters), loss = 0.00542801
I0826 03:39:28.910490  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00718257 (* 1 = 0.00718257 loss)
I0826 03:39:28.910552  1899 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0826 03:40:25.430452  1899 solver.cpp:357] Iteration 58800 (1.76924 iter/s, 56.5215s/100 iters), loss = 0.00645978
I0826 03:40:25.430600  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00427667 (* 1 = 0.00427667 loss)
I0826 03:40:25.430613  1899 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0826 03:41:21.988627  1899 solver.cpp:357] Iteration 58900 (1.76804 iter/s, 56.5599s/100 iters), loss = 0.012539
I0826 03:41:21.988780  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0159117 (* 1 = 0.0159117 loss)
I0826 03:41:21.988796  1899 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0826 03:42:08.041617  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:42:17.939250  1899 solver.cpp:514] Iteration 59000, Testing net (#0)
I0826 03:42:54.771167  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:42:54.941217  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931803
I0826 03:42:54.941269  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.300883 (* 1 = 0.300883 loss)
I0826 03:42:55.352322  1899 solver.cpp:357] Iteration 59000 (1.07107 iter/s, 93.3646s/100 iters), loss = 0.00456159
I0826 03:42:55.352381  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0057555 (* 1 = 0.0057555 loss)
I0826 03:42:55.352394  1899 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0826 03:43:52.104308  1899 solver.cpp:357] Iteration 59100 (1.76206 iter/s, 56.7517s/100 iters), loss = 0.0105126
I0826 03:43:52.104481  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0116468 (* 1 = 0.0116468 loss)
I0826 03:43:52.104496  1899 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0826 03:44:48.469780  1899 solver.cpp:357] Iteration 59200 (1.77415 iter/s, 56.3652s/100 iters), loss = 0.00756634
I0826 03:44:48.469944  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00459284 (* 1 = 0.00459284 loss)
I0826 03:44:48.469956  1899 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0826 03:45:41.318049  1899 solver.cpp:357] Iteration 59300 (1.89222 iter/s, 52.8478s/100 iters), loss = 0.017995
I0826 03:45:41.318223  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00391853 (* 1 = 0.00391853 loss)
I0826 03:45:41.318236  1899 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0826 03:46:19.042562  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:46:34.879719  1899 solver.cpp:357] Iteration 59400 (1.86711 iter/s, 53.5588s/100 iters), loss = 0.0121543
I0826 03:46:34.879801  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0161666 (* 1 = 0.0161666 loss)
I0826 03:46:34.879812  1899 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0826 03:47:30.922727  1899 solver.cpp:514] Iteration 59500, Testing net (#0)
I0826 03:48:07.416525  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:48:07.583282  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931003
I0826 03:48:07.583333  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.299289 (* 1 = 0.299289 loss)
I0826 03:48:08.079341  1899 solver.cpp:357] Iteration 59500 (1.07311 iter/s, 93.1867s/100 iters), loss = 0.00419516
I0826 03:48:08.079403  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00406053 (* 1 = 0.00406053 loss)
I0826 03:48:08.079413  1899 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0826 03:49:04.541337  1899 solver.cpp:357] Iteration 59600 (1.77128 iter/s, 56.4563s/100 iters), loss = 0.0132289
I0826 03:49:04.541538  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222259 (* 1 = 0.0222259 loss)
I0826 03:49:04.541548  1899 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0826 03:50:00.984403  1899 solver.cpp:357] Iteration 59700 (1.77186 iter/s, 56.4379s/100 iters), loss = 0.00992298
I0826 03:50:00.984560  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0134737 (* 1 = 0.0134737 loss)
I0826 03:50:00.984571  1899 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0826 03:50:36.680625  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:50:57.464135  1899 solver.cpp:357] Iteration 59800 (1.77069 iter/s, 56.4752s/100 iters), loss = 0.00574484
I0826 03:50:57.464212  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00659244 (* 1 = 0.00659244 loss)
I0826 03:50:57.464224  1899 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0826 03:51:53.473695  1899 solver.cpp:357] Iteration 59900 (1.7856 iter/s, 56.0035s/100 iters), loss = 0.00367087
I0826 03:51:53.473811  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0029369 (* 1 = 0.0029369 loss)
I0826 03:51:53.473824  1899 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0826 03:52:43.407490  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_60000.caffemodel
I0826 03:52:43.436255  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_60000.solverstate
I0826 03:52:43.444160  1899 solver.cpp:514] Iteration 60000, Testing net (#0)
I0826 03:53:20.187307  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:53:20.267626  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929903
I0826 03:53:20.267679  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.310308 (* 1 = 0.310308 loss)
I0826 03:53:20.832059  1899 solver.cpp:357] Iteration 60000 (1.14481 iter/s, 87.351s/100 iters), loss = 0.0107342
I0826 03:53:20.832139  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00359735 (* 1 = 0.00359735 loss)
I0826 03:53:20.832156  1899 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0826 03:54:17.279178  1899 solver.cpp:357] Iteration 60100 (1.77173 iter/s, 56.4421s/100 iters), loss = 0.00781135
I0826 03:54:17.279310  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00254619 (* 1 = 0.00254619 loss)
I0826 03:54:17.279322  1899 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0826 03:54:47.604689  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:55:13.880831  1899 solver.cpp:357] Iteration 60200 (1.76683 iter/s, 56.5985s/100 iters), loss = 0.0143645
I0826 03:55:13.880897  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157861 (* 1 = 0.0157861 loss)
I0826 03:55:13.880909  1899 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0826 03:56:10.403633  1899 solver.cpp:357] Iteration 60300 (1.76927 iter/s, 56.5205s/100 iters), loss = 0.00334609
I0826 03:56:10.403900  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00454386 (* 1 = 0.00454386 loss)
I0826 03:56:10.403960  1899 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0826 03:57:07.132299  1899 solver.cpp:357] Iteration 60400 (1.76285 iter/s, 56.7265s/100 iters), loss = 0.00266766
I0826 03:57:07.132460  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00419284 (* 1 = 0.00419284 loss)
I0826 03:57:07.132474  1899 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0826 03:58:03.318254  1899 solver.cpp:514] Iteration 60500, Testing net (#0)
I0826 03:58:35.697340  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:58:35.799074  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929502
I0826 03:58:35.799124  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.309723 (* 1 = 0.309723 loss)
I0826 03:58:36.244561  1899 solver.cpp:357] Iteration 60500 (1.12224 iter/s, 89.1075s/100 iters), loss = 0.0106096
I0826 03:58:36.244633  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0119752 (* 1 = 0.0119752 loss)
I0826 03:58:36.244645  1899 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0826 03:58:58.140693  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 03:59:29.735211  1899 solver.cpp:357] Iteration 60600 (1.86961 iter/s, 53.4872s/100 iters), loss = 0.00559542
I0826 03:59:29.735354  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00386219 (* 1 = 0.00386219 loss)
I0826 03:59:29.735365  1899 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0826 04:00:26.341570  1899 solver.cpp:357] Iteration 60700 (1.76663 iter/s, 56.605s/100 iters), loss = 0.012122
I0826 04:00:26.341738  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0164843 (* 1 = 0.0164843 loss)
I0826 04:00:26.341750  1899 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0826 04:01:22.979423  1899 solver.cpp:357] Iteration 60800 (1.7657 iter/s, 56.6347s/100 iters), loss = 0.014857
I0826 04:01:22.979542  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0210781 (* 1 = 0.0210781 loss)
I0826 04:01:22.979553  1899 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0826 04:02:19.618072  1899 solver.cpp:357] Iteration 60900 (1.76568 iter/s, 56.6356s/100 iters), loss = 0.00560351
I0826 04:02:19.618400  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00946638 (* 1 = 0.00946638 loss)
I0826 04:02:19.618464  1899 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0826 04:02:39.102975  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:03:15.700079  1899 solver.cpp:514] Iteration 61000, Testing net (#0)
I0826 04:03:52.645017  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:03:52.732033  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931702
I0826 04:03:52.732077  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.305862 (* 1 = 0.305862 loss)
I0826 04:03:53.253145  1899 solver.cpp:357] Iteration 61000 (1.06801 iter/s, 93.6317s/100 iters), loss = 0.00748715
I0826 04:03:53.253218  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00824578 (* 1 = 0.00824578 loss)
I0826 04:03:53.253232  1899 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0826 04:04:49.257408  1899 solver.cpp:357] Iteration 61100 (1.78567 iter/s, 56.0015s/100 iters), loss = 0.0175197
I0826 04:04:49.257516  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.017468 (* 1 = 0.017468 loss)
I0826 04:04:49.257529  1899 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0826 04:05:39.804600  1899 solver.cpp:357] Iteration 61200 (1.97845 iter/s, 50.5446s/100 iters), loss = 0.00323184
I0826 04:05:39.804764  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0018827 (* 1 = 0.0018827 loss)
I0826 04:05:39.804777  1899 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0826 04:06:36.334312  1899 solver.cpp:357] Iteration 61300 (1.76906 iter/s, 56.5271s/100 iters), loss = 0.0111038
I0826 04:06:36.334466  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.012863 (* 1 = 0.012863 loss)
I0826 04:06:36.334477  1899 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0826 04:06:50.737962  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:07:32.954273  1899 solver.cpp:357] Iteration 61400 (1.76618 iter/s, 56.6195s/100 iters), loss = 0.00366998
I0826 04:07:32.954414  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00423502 (* 1 = 0.00423502 loss)
I0826 04:07:32.954427  1899 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0826 04:08:29.051415  1899 solver.cpp:514] Iteration 61500, Testing net (#0)
I0826 04:09:05.947412  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:09:06.060549  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929603
I0826 04:09:06.060603  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.306645 (* 1 = 0.306645 loss)
I0826 04:09:06.601994  1899 solver.cpp:357] Iteration 61500 (1.06786 iter/s, 93.6452s/100 iters), loss = 0.0211426
I0826 04:09:06.602064  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0339033 (* 1 = 0.0339033 loss)
I0826 04:09:06.602077  1899 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0826 04:10:03.140528  1899 solver.cpp:357] Iteration 61600 (1.76878 iter/s, 56.5362s/100 iters), loss = 0.00495794
I0826 04:10:03.140647  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00301581 (* 1 = 0.00301581 loss)
I0826 04:10:03.140658  1899 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0826 04:10:59.792721  1899 solver.cpp:357] Iteration 61700 (1.76516 iter/s, 56.6519s/100 iters), loss = 0.00353603
I0826 04:10:59.792899  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00384237 (* 1 = 0.00384237 loss)
I0826 04:10:59.792912  1899 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0826 04:11:09.023901  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:11:51.085289  1899 solver.cpp:357] Iteration 61800 (1.94969 iter/s, 51.2903s/100 iters), loss = 0.00643539
I0826 04:11:51.085410  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00774069 (* 1 = 0.00774069 loss)
I0826 04:11:51.085424  1899 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0826 04:12:46.202502  1899 solver.cpp:357] Iteration 61900 (1.81439 iter/s, 55.115s/100 iters), loss = 0.0054224
I0826 04:12:46.202736  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00180361 (* 1 = 0.00180361 loss)
I0826 04:12:46.202764  1899 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0826 04:13:42.278651  1899 solver.cpp:514] Iteration 62000, Testing net (#0)
I0826 04:14:18.708374  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:14:18.861094  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929802
I0826 04:14:18.861140  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.317061 (* 1 = 0.317061 loss)
I0826 04:14:19.352015  1899 solver.cpp:357] Iteration 62000 (1.07357 iter/s, 93.1474s/100 iters), loss = 0.0113787
I0826 04:14:19.352087  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122088 (* 1 = 0.0122088 loss)
I0826 04:14:19.352100  1899 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0826 04:15:15.832592  1899 solver.cpp:357] Iteration 62100 (1.77059 iter/s, 56.4785s/100 iters), loss = 0.00846657
I0826 04:15:15.832734  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00372719 (* 1 = 0.00372719 loss)
I0826 04:15:15.832746  1899 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0826 04:15:19.521558  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:16:12.246951  1899 solver.cpp:357] Iteration 62200 (1.7726 iter/s, 56.4143s/100 iters), loss = 0.00391496
I0826 04:16:12.247102  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00353272 (* 1 = 0.00353272 loss)
I0826 04:16:12.247112  1899 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0826 04:17:08.940024  1899 solver.cpp:357] Iteration 62300 (1.76389 iter/s, 56.693s/100 iters), loss = 0.0110134
I0826 04:17:08.940192  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00506013 (* 1 = 0.00506013 loss)
I0826 04:17:08.940207  1899 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0826 04:18:03.227466  1899 solver.cpp:357] Iteration 62400 (1.84211 iter/s, 54.2854s/100 iters), loss = 0.00440374
I0826 04:18:03.227632  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00402314 (* 1 = 0.00402314 loss)
I0826 04:18:03.227644  1899 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0826 04:18:53.769819  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:18:54.923370  1899 solver.cpp:514] Iteration 62500, Testing net (#0)
I0826 04:19:32.150418  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:19:32.256175  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929903
I0826 04:19:32.256237  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.309431 (* 1 = 0.309431 loss)
I0826 04:19:32.815361  1899 solver.cpp:357] Iteration 62500 (1.11625 iter/s, 89.586s/100 iters), loss = 0.00420184
I0826 04:19:32.815433  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0056188 (* 1 = 0.0056188 loss)
I0826 04:19:32.815445  1899 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0826 04:20:29.506785  1899 solver.cpp:357] Iteration 62600 (1.76399 iter/s, 56.6895s/100 iters), loss = 0.00639777
I0826 04:20:29.506959  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00367128 (* 1 = 0.00367128 loss)
I0826 04:20:29.506973  1899 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0826 04:21:26.076243  1899 solver.cpp:357] Iteration 62700 (1.76773 iter/s, 56.5698s/100 iters), loss = 0.0019941
I0826 04:21:26.076402  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00178931 (* 1 = 0.00178931 loss)
I0826 04:21:26.076414  1899 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0826 04:22:22.665977  1899 solver.cpp:357] Iteration 62800 (1.7671 iter/s, 56.5898s/100 iters), loss = 0.00344923
I0826 04:22:22.666122  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00596845 (* 1 = 0.00596845 loss)
I0826 04:22:22.666133  1899 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0826 04:23:12.535058  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:23:19.318112  1899 solver.cpp:357] Iteration 62900 (1.7651 iter/s, 56.6541s/100 iters), loss = 0.0222041
I0826 04:23:19.318179  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.037335 (* 1 = 0.037335 loss)
I0826 04:23:19.318190  1899 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0826 04:24:15.427700  1899 solver.cpp:514] Iteration 63000, Testing net (#0)
I0826 04:24:45.827878  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:24:45.936643  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929603
I0826 04:24:45.936694  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.309525 (* 1 = 0.309525 loss)
I0826 04:24:46.372939  1899 solver.cpp:357] Iteration 63000 (1.14866 iter/s, 87.0577s/100 iters), loss = 0.00592051
I0826 04:24:46.373001  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00479124 (* 1 = 0.00479124 loss)
I0826 04:24:46.373016  1899 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0826 04:25:41.427475  1899 solver.cpp:357] Iteration 63100 (1.81633 iter/s, 55.0562s/100 iters), loss = 0.00604498
I0826 04:25:41.427639  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00344956 (* 1 = 0.00344956 loss)
I0826 04:25:41.427652  1899 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0826 04:26:37.875576  1899 solver.cpp:357] Iteration 63200 (1.77156 iter/s, 56.4476s/100 iters), loss = 0.0151107
I0826 04:26:37.875696  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0184232 (* 1 = 0.0184232 loss)
I0826 04:26:37.875710  1899 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0826 04:27:22.272995  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:27:34.472599  1899 solver.cpp:357] Iteration 63300 (1.7669 iter/s, 56.5964s/100 iters), loss = 0.00727661
I0826 04:27:34.472671  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00427622 (* 1 = 0.00427622 loss)
I0826 04:27:34.472682  1899 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0826 04:28:31.225194  1899 solver.cpp:357] Iteration 63400 (1.76206 iter/s, 56.7519s/100 iters), loss = 0.0130117
I0826 04:28:31.225309  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0156151 (* 1 = 0.0156151 loss)
I0826 04:28:31.225320  1899 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0826 04:29:27.382632  1899 solver.cpp:514] Iteration 63500, Testing net (#0)
I0826 04:30:04.125744  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:30:04.290254  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931902
I0826 04:30:04.290313  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.302185 (* 1 = 0.302185 loss)
I0826 04:30:04.815475  1899 solver.cpp:357] Iteration 63500 (1.06849 iter/s, 93.5903s/100 iters), loss = 0.0105383
I0826 04:30:04.815537  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0180489 (* 1 = 0.0180489 loss)
I0826 04:30:04.815548  1899 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0826 04:30:59.173319  1899 solver.cpp:357] Iteration 63600 (1.83962 iter/s, 54.3589s/100 iters), loss = 0.00298171
I0826 04:30:59.173442  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00324258 (* 1 = 0.00324258 loss)
I0826 04:30:59.173454  1899 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0826 04:31:33.893977  1913 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:31:51.257951  1899 solver.cpp:357] Iteration 63700 (1.91992 iter/s, 52.0856s/100 iters), loss = 0.00326078
I0826 04:31:51.258020  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00453365 (* 1 = 0.00453365 loss)
I0826 04:31:51.258033  1899 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0826 04:32:47.825657  1899 solver.cpp:357] Iteration 63800 (1.76783 iter/s, 56.5666s/100 iters), loss = 0.0147389
I0826 04:32:47.825804  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00475216 (* 1 = 0.00475216 loss)
I0826 04:32:47.825824  1899 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0826 04:33:44.484342  1899 solver.cpp:357] Iteration 63900 (1.76499 iter/s, 56.6576s/100 iters), loss = 0.00800249
I0826 04:33:44.484505  1899 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00287941 (* 1 = 0.00287941 loss)
I0826 04:33:44.484519  1899 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0826 04:34:40.631793  1899 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_44_iter_64000.caffemodel
I0826 04:34:40.654599  1899 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_44_iter_64000.solverstate
I0826 04:34:40.738149  1899 solver.cpp:472] Iteration 64000, loss = 0.00140235
I0826 04:34:40.738201  1899 solver.cpp:514] Iteration 64000, Testing net (#0)
I0826 04:35:17.879083  1925 data_layer.cpp:73] Restarting data prefetching from start.
I0826 04:35:18.050073  1899 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930602
I0826 04:35:18.050125  1899 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.313856 (* 1 = 0.313856 loss)
I0826 04:35:18.050132  1899 solver.cpp:479] Optimization Done.
I0826 04:35:18.050143  1899 caffe.cpp:326] Optimization Done.
