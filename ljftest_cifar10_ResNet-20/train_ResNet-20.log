WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 11:08:56.325980  1476 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0825 11:08:56.326272  1476 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0825 11:08:56.326293  1476 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0825 11:08:56.326308  1476 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0825 11:08:56.326323  1476 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0825 11:08:56.326400  1476 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0825 11:08:56.326565  1476 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0825 11:08:56.326907  1476 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0825 11:08:56.378890  1476 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0825 11:08:56.379004  1476 caffe.cpp:269] Using GPUs 0
I0825 11:08:56.581450  1476 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0825 11:08:58.734736  1476 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0825 11:08:58.734777  1476 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0825 11:08:59.100427  1476 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_20.prototxt"
test_net: "./test_ResNet_20.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_20"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
type: "Nesterov"
I0825 11:08:59.112442  1476 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_20.prototxt
I0825 11:08:59.125259  1476 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_20.prototxt
I0825 11:08:59.125337  1476 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:08:59.125952  1476 net.cpp:390] layer_param.include_size():1
I0825 11:08:59.125984  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126011  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126051  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126070  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126087  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126104  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126137  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126155  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126170  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126186  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126201  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126219  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126258  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126291  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126319  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126360  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126379  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126394  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126410  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126440  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126456  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126479  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126500  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126518  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126533  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126550  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126574  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126616  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126631  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126648  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126664  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126682  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126696  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126713  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126729  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126751  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126772  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126791  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126868  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126948  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.126973  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.126991  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127007  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127023  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127045  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127063  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127102  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127121  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127146  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127163  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127179  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127197  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127218  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127235  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127251  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127269  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127290  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127315  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127334  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127352  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127373  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127398  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127418  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127444  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127463  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127482  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127498  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127516  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127538  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127555  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127571  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127588  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127609  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127627  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127666  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127684  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127699  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127717  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127738  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127758  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127780  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127830  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127852  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127871  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127887  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127903  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127924  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127943  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127957  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.127975  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.127991  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128015  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128031  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128052  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128073  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128098  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128119  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128168  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128192  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128211  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128226  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128242  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128265  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128283  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128298  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128340  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128355  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128372  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128393  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128412  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128427  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128443  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128465  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128482  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128497  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128515  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128536  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128553  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128568  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128585  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128607  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128625  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128640  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128657  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128679  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128729  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128751  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128769  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128784  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128801  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128823  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128842  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128859  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128885  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128901  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128918  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128939  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128957  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.128979  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.128998  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129019  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129036  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129057  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129082  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129103  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129128  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129144  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129160  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129181  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129200  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129221  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129245  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129266  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129292  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129312  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129410  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129434  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129453  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129469  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129494  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129510  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129528  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129549  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129567  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129582  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129598  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129664  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129689  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129707  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129732  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129748  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129765  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129787  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129804  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129820  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129837  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129858  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129876  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129891  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129909  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129931  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129950  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.129964  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.129981  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.130003  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.130022  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.130038  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.130054  1476 net.cpp:390] layer_param.include_size():0
I0825 11:08:59.130075  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:08:59.132910  1476 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I0825 11:08:59.134713  1476 layer_factory.hpp:77] Creating layer Data1
I0825 11:08:59.254482  1476 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0825 11:08:59.261059  1476 net.cpp:128] Creating Layer Data1
I0825 11:08:59.261144  1476 net.cpp:522] Data1 -> Data1
I0825 11:08:59.261236  1476 net.cpp:522] Data1 -> Data2
I0825 11:08:59.264109  1476 data_layer.cpp:45] output data size: 128,3,32,32
I0825 11:08:59.273573  1476 net.cpp:172] Setting up Data1
I0825 11:08:59.273627  1476 net.cpp:186] Top shape: 128 3 32 32 (393216)
I0825 11:08:59.273634  1476 net.cpp:186] Top shape: 128 (128)
I0825 11:08:59.273638  1476 net.cpp:194] Memory required for data: 1573376
I0825 11:08:59.273653  1476 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:08:59.273689  1476 net.cpp:128] Creating Layer Convolution1
I0825 11:08:59.273697  1476 net.cpp:558] Convolution1 <- Data1
I0825 11:08:59.273716  1476 net.cpp:522] Convolution1 -> Convolution1
I0825 11:09:03.083650  1476 net.cpp:172] Setting up Convolution1
I0825 11:09:03.083750  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.083770  1476 net.cpp:194] Memory required for data: 9961984
I0825 11:09:03.083873  1476 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:09:03.083925  1476 net.cpp:128] Creating Layer BatchNorm1
I0825 11:09:03.083955  1476 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:09:03.083984  1476 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:09:03.084853  1476 net.cpp:172] Setting up BatchNorm1
I0825 11:09:03.084897  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.084914  1476 net.cpp:194] Memory required for data: 18350592
I0825 11:09:03.085033  1476 layer_factory.hpp:77] Creating layer Scale1
I0825 11:09:03.085078  1476 net.cpp:128] Creating Layer Scale1
I0825 11:09:03.085104  1476 net.cpp:558] Scale1 <- Convolution1
I0825 11:09:03.085130  1476 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:09:03.085391  1476 layer_factory.hpp:77] Creating layer Scale1
I0825 11:09:03.085875  1476 net.cpp:172] Setting up Scale1
I0825 11:09:03.085916  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.085942  1476 net.cpp:194] Memory required for data: 26739200
I0825 11:09:03.085978  1476 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:09:03.086015  1476 net.cpp:128] Creating Layer ReLU1
I0825 11:09:03.086045  1476 net.cpp:558] ReLU1 <- Convolution1
I0825 11:09:03.086071  1476 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:09:03.086944  1476 net.cpp:172] Setting up ReLU1
I0825 11:09:03.086984  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.087002  1476 net.cpp:194] Memory required for data: 35127808
I0825 11:09:03.087021  1476 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:09:03.087049  1476 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:09:03.087067  1476 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:09:03.087093  1476 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:09:03.087126  1476 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:09:03.087291  1476 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:09:03.087321  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.087347  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.087363  1476 net.cpp:194] Memory required for data: 51905024
I0825 11:09:03.087383  1476 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:09:03.087424  1476 net.cpp:128] Creating Layer Convolution2
I0825 11:09:03.087442  1476 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:09:03.087469  1476 net.cpp:522] Convolution2 -> Convolution2
I0825 11:09:03.094877  1476 net.cpp:172] Setting up Convolution2
I0825 11:09:03.094903  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.094908  1476 net.cpp:194] Memory required for data: 60293632
I0825 11:09:03.094920  1476 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:09:03.094933  1476 net.cpp:128] Creating Layer BatchNorm2
I0825 11:09:03.094938  1476 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:09:03.094949  1476 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:09:03.095167  1476 net.cpp:172] Setting up BatchNorm2
I0825 11:09:03.095178  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.095183  1476 net.cpp:194] Memory required for data: 68682240
I0825 11:09:03.095193  1476 layer_factory.hpp:77] Creating layer Scale2
I0825 11:09:03.095203  1476 net.cpp:128] Creating Layer Scale2
I0825 11:09:03.095208  1476 net.cpp:558] Scale2 <- Convolution2
I0825 11:09:03.095214  1476 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:09:03.095252  1476 layer_factory.hpp:77] Creating layer Scale2
I0825 11:09:03.095381  1476 net.cpp:172] Setting up Scale2
I0825 11:09:03.095393  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.095397  1476 net.cpp:194] Memory required for data: 77070848
I0825 11:09:03.095405  1476 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:09:03.095412  1476 net.cpp:128] Creating Layer ReLU2
I0825 11:09:03.095417  1476 net.cpp:558] ReLU2 <- Convolution2
I0825 11:09:03.095424  1476 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:09:03.095636  1476 net.cpp:172] Setting up ReLU2
I0825 11:09:03.095649  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.095654  1476 net.cpp:194] Memory required for data: 85459456
I0825 11:09:03.095659  1476 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:09:03.095670  1476 net.cpp:128] Creating Layer Convolution3
I0825 11:09:03.095675  1476 net.cpp:558] Convolution3 <- Convolution2
I0825 11:09:03.095685  1476 net.cpp:522] Convolution3 -> Convolution3
I0825 11:09:03.096868  1476 net.cpp:172] Setting up Convolution3
I0825 11:09:03.096896  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.096901  1476 net.cpp:194] Memory required for data: 93848064
I0825 11:09:03.096915  1476 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:09:03.096942  1476 net.cpp:128] Creating Layer BatchNorm3
I0825 11:09:03.096947  1476 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:09:03.096958  1476 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:09:03.097184  1476 net.cpp:172] Setting up BatchNorm3
I0825 11:09:03.097195  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.097200  1476 net.cpp:194] Memory required for data: 102236672
I0825 11:09:03.097213  1476 layer_factory.hpp:77] Creating layer Scale3
I0825 11:09:03.097221  1476 net.cpp:128] Creating Layer Scale3
I0825 11:09:03.097226  1476 net.cpp:558] Scale3 <- Convolution3
I0825 11:09:03.097232  1476 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:09:03.097270  1476 layer_factory.hpp:77] Creating layer Scale3
I0825 11:09:03.097398  1476 net.cpp:172] Setting up Scale3
I0825 11:09:03.097409  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.097412  1476 net.cpp:194] Memory required for data: 110625280
I0825 11:09:03.097420  1476 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:09:03.097429  1476 net.cpp:128] Creating Layer Eltwise1
I0825 11:09:03.097434  1476 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:09:03.097438  1476 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:09:03.097446  1476 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:09:03.097476  1476 net.cpp:172] Setting up Eltwise1
I0825 11:09:03.097483  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.097487  1476 net.cpp:194] Memory required for data: 119013888
I0825 11:09:03.097491  1476 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:09:03.097498  1476 net.cpp:128] Creating Layer ReLU3
I0825 11:09:03.097502  1476 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:09:03.097508  1476 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:09:03.097914  1476 net.cpp:172] Setting up ReLU3
I0825 11:09:03.097937  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.097942  1476 net.cpp:194] Memory required for data: 127402496
I0825 11:09:03.097947  1476 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:09:03.097956  1476 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:09:03.097961  1476 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:09:03.097970  1476 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:09:03.097980  1476 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:09:03.098027  1476 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:09:03.098038  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.098044  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.098048  1476 net.cpp:194] Memory required for data: 144179712
I0825 11:09:03.098053  1476 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:09:03.098068  1476 net.cpp:128] Creating Layer Convolution4
I0825 11:09:03.098073  1476 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:09:03.098080  1476 net.cpp:522] Convolution4 -> Convolution4
I0825 11:09:03.099293  1476 net.cpp:172] Setting up Convolution4
I0825 11:09:03.099319  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.099324  1476 net.cpp:194] Memory required for data: 152568320
I0825 11:09:03.099334  1476 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:09:03.099342  1476 net.cpp:128] Creating Layer BatchNorm4
I0825 11:09:03.099347  1476 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:09:03.099356  1476 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:09:03.099584  1476 net.cpp:172] Setting up BatchNorm4
I0825 11:09:03.099596  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.099599  1476 net.cpp:194] Memory required for data: 160956928
I0825 11:09:03.099609  1476 layer_factory.hpp:77] Creating layer Scale4
I0825 11:09:03.099617  1476 net.cpp:128] Creating Layer Scale4
I0825 11:09:03.099620  1476 net.cpp:558] Scale4 <- Convolution4
I0825 11:09:03.099628  1476 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:09:03.099681  1476 layer_factory.hpp:77] Creating layer Scale4
I0825 11:09:03.099812  1476 net.cpp:172] Setting up Scale4
I0825 11:09:03.099823  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.099828  1476 net.cpp:194] Memory required for data: 169345536
I0825 11:09:03.099836  1476 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:09:03.099844  1476 net.cpp:128] Creating Layer ReLU4
I0825 11:09:03.099848  1476 net.cpp:558] ReLU4 <- Convolution4
I0825 11:09:03.099854  1476 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:09:03.100082  1476 net.cpp:172] Setting up ReLU4
I0825 11:09:03.100095  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.100102  1476 net.cpp:194] Memory required for data: 177734144
I0825 11:09:03.100107  1476 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:09:03.100119  1476 net.cpp:128] Creating Layer Convolution5
I0825 11:09:03.100124  1476 net.cpp:558] Convolution5 <- Convolution4
I0825 11:09:03.100133  1476 net.cpp:522] Convolution5 -> Convolution5
I0825 11:09:03.101323  1476 net.cpp:172] Setting up Convolution5
I0825 11:09:03.101349  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.101353  1476 net.cpp:194] Memory required for data: 186122752
I0825 11:09:03.101363  1476 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:09:03.101374  1476 net.cpp:128] Creating Layer BatchNorm5
I0825 11:09:03.101379  1476 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:09:03.101390  1476 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:09:03.101620  1476 net.cpp:172] Setting up BatchNorm5
I0825 11:09:03.101631  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.101635  1476 net.cpp:194] Memory required for data: 194511360
I0825 11:09:03.101647  1476 layer_factory.hpp:77] Creating layer Scale5
I0825 11:09:03.101658  1476 net.cpp:128] Creating Layer Scale5
I0825 11:09:03.101663  1476 net.cpp:558] Scale5 <- Convolution5
I0825 11:09:03.101672  1476 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:09:03.101712  1476 layer_factory.hpp:77] Creating layer Scale5
I0825 11:09:03.101843  1476 net.cpp:172] Setting up Scale5
I0825 11:09:03.101853  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.101858  1476 net.cpp:194] Memory required for data: 202899968
I0825 11:09:03.101866  1476 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:09:03.101872  1476 net.cpp:128] Creating Layer Eltwise2
I0825 11:09:03.101877  1476 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:09:03.101882  1476 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:09:03.101889  1476 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:09:03.101915  1476 net.cpp:172] Setting up Eltwise2
I0825 11:09:03.101925  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.101930  1476 net.cpp:194] Memory required for data: 211288576
I0825 11:09:03.101934  1476 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:09:03.101940  1476 net.cpp:128] Creating Layer ReLU5
I0825 11:09:03.101945  1476 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:09:03.101950  1476 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:09:03.102171  1476 net.cpp:172] Setting up ReLU5
I0825 11:09:03.102185  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.102188  1476 net.cpp:194] Memory required for data: 219677184
I0825 11:09:03.102193  1476 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:09:03.102200  1476 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:09:03.102205  1476 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:09:03.102213  1476 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:09:03.102221  1476 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:09:03.102267  1476 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:09:03.102273  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.102279  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.102283  1476 net.cpp:194] Memory required for data: 236454400
I0825 11:09:03.102303  1476 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:09:03.102316  1476 net.cpp:128] Creating Layer Convolution6
I0825 11:09:03.102321  1476 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:09:03.102330  1476 net.cpp:522] Convolution6 -> Convolution6
I0825 11:09:03.103541  1476 net.cpp:172] Setting up Convolution6
I0825 11:09:03.103567  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.103571  1476 net.cpp:194] Memory required for data: 244843008
I0825 11:09:03.103582  1476 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:09:03.103592  1476 net.cpp:128] Creating Layer BatchNorm6
I0825 11:09:03.103597  1476 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:09:03.103606  1476 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:09:03.103837  1476 net.cpp:172] Setting up BatchNorm6
I0825 11:09:03.103848  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.103852  1476 net.cpp:194] Memory required for data: 253231616
I0825 11:09:03.103863  1476 layer_factory.hpp:77] Creating layer Scale6
I0825 11:09:03.103871  1476 net.cpp:128] Creating Layer Scale6
I0825 11:09:03.103876  1476 net.cpp:558] Scale6 <- Convolution6
I0825 11:09:03.103883  1476 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:09:03.103921  1476 layer_factory.hpp:77] Creating layer Scale6
I0825 11:09:03.104054  1476 net.cpp:172] Setting up Scale6
I0825 11:09:03.104064  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.104068  1476 net.cpp:194] Memory required for data: 261620224
I0825 11:09:03.104076  1476 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:09:03.104084  1476 net.cpp:128] Creating Layer ReLU6
I0825 11:09:03.104087  1476 net.cpp:558] ReLU6 <- Convolution6
I0825 11:09:03.104094  1476 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:09:03.104506  1476 net.cpp:172] Setting up ReLU6
I0825 11:09:03.104526  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.104531  1476 net.cpp:194] Memory required for data: 270008832
I0825 11:09:03.104537  1476 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:09:03.104549  1476 net.cpp:128] Creating Layer Convolution7
I0825 11:09:03.104555  1476 net.cpp:558] Convolution7 <- Convolution6
I0825 11:09:03.104568  1476 net.cpp:522] Convolution7 -> Convolution7
I0825 11:09:03.105787  1476 net.cpp:172] Setting up Convolution7
I0825 11:09:03.105813  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.105818  1476 net.cpp:194] Memory required for data: 278397440
I0825 11:09:03.105829  1476 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:09:03.105839  1476 net.cpp:128] Creating Layer BatchNorm7
I0825 11:09:03.105844  1476 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:09:03.105854  1476 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:09:03.106094  1476 net.cpp:172] Setting up BatchNorm7
I0825 11:09:03.106104  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106109  1476 net.cpp:194] Memory required for data: 286786048
I0825 11:09:03.106119  1476 layer_factory.hpp:77] Creating layer Scale7
I0825 11:09:03.106129  1476 net.cpp:128] Creating Layer Scale7
I0825 11:09:03.106134  1476 net.cpp:558] Scale7 <- Convolution7
I0825 11:09:03.106139  1476 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:09:03.106180  1476 layer_factory.hpp:77] Creating layer Scale7
I0825 11:09:03.106313  1476 net.cpp:172] Setting up Scale7
I0825 11:09:03.106326  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106333  1476 net.cpp:194] Memory required for data: 295174656
I0825 11:09:03.106355  1476 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:09:03.106364  1476 net.cpp:128] Creating Layer Eltwise3
I0825 11:09:03.106369  1476 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:09:03.106374  1476 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:09:03.106379  1476 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:09:03.106407  1476 net.cpp:172] Setting up Eltwise3
I0825 11:09:03.106415  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106433  1476 net.cpp:194] Memory required for data: 303563264
I0825 11:09:03.106438  1476 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:09:03.106447  1476 net.cpp:128] Creating Layer ReLU7
I0825 11:09:03.106452  1476 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:09:03.106458  1476 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:09:03.106688  1476 net.cpp:172] Setting up ReLU7
I0825 11:09:03.106703  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106707  1476 net.cpp:194] Memory required for data: 311951872
I0825 11:09:03.106711  1476 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:09:03.106720  1476 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:09:03.106725  1476 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:09:03.106732  1476 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:09:03.106740  1476 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:09:03.106791  1476 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:09:03.106799  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106804  1476 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0825 11:09:03.106808  1476 net.cpp:194] Memory required for data: 328729088
I0825 11:09:03.106813  1476 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:09:03.106825  1476 net.cpp:128] Creating Layer Convolution8
I0825 11:09:03.106830  1476 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:09:03.106839  1476 net.cpp:522] Convolution8 -> Convolution8
I0825 11:09:03.108207  1476 net.cpp:172] Setting up Convolution8
I0825 11:09:03.108233  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.108237  1476 net.cpp:194] Memory required for data: 332923392
I0825 11:09:03.108252  1476 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:09:03.108268  1476 net.cpp:128] Creating Layer BatchNorm8
I0825 11:09:03.108273  1476 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:09:03.108281  1476 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:09:03.108525  1476 net.cpp:172] Setting up BatchNorm8
I0825 11:09:03.108534  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.108539  1476 net.cpp:194] Memory required for data: 337117696
I0825 11:09:03.108549  1476 layer_factory.hpp:77] Creating layer Scale8
I0825 11:09:03.108559  1476 net.cpp:128] Creating Layer Scale8
I0825 11:09:03.108564  1476 net.cpp:558] Scale8 <- Convolution8
I0825 11:09:03.108569  1476 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:09:03.108610  1476 layer_factory.hpp:77] Creating layer Scale8
I0825 11:09:03.108747  1476 net.cpp:172] Setting up Scale8
I0825 11:09:03.108758  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.108762  1476 net.cpp:194] Memory required for data: 341312000
I0825 11:09:03.108770  1476 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:09:03.108783  1476 net.cpp:128] Creating Layer Convolution9
I0825 11:09:03.108788  1476 net.cpp:558] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0825 11:09:03.108798  1476 net.cpp:522] Convolution9 -> Convolution9
I0825 11:09:03.111412  1476 net.cpp:172] Setting up Convolution9
I0825 11:09:03.111434  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.111439  1476 net.cpp:194] Memory required for data: 345506304
I0825 11:09:03.111454  1476 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:09:03.111474  1476 net.cpp:128] Creating Layer BatchNorm9
I0825 11:09:03.111479  1476 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:09:03.111485  1476 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:09:03.111727  1476 net.cpp:172] Setting up BatchNorm9
I0825 11:09:03.111737  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.111742  1476 net.cpp:194] Memory required for data: 349700608
I0825 11:09:03.111752  1476 layer_factory.hpp:77] Creating layer Scale9
I0825 11:09:03.111758  1476 net.cpp:128] Creating Layer Scale9
I0825 11:09:03.111765  1476 net.cpp:558] Scale9 <- Convolution9
I0825 11:09:03.111793  1476 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:09:03.111835  1476 layer_factory.hpp:77] Creating layer Scale9
I0825 11:09:03.111970  1476 net.cpp:172] Setting up Scale9
I0825 11:09:03.111982  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.111987  1476 net.cpp:194] Memory required for data: 353894912
I0825 11:09:03.111995  1476 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:09:03.112002  1476 net.cpp:128] Creating Layer ReLU8
I0825 11:09:03.112006  1476 net.cpp:558] ReLU8 <- Convolution9
I0825 11:09:03.112011  1476 net.cpp:509] ReLU8 -> Convolution9 (in-place)
I0825 11:09:03.112242  1476 net.cpp:172] Setting up ReLU8
I0825 11:09:03.112257  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.112262  1476 net.cpp:194] Memory required for data: 358089216
I0825 11:09:03.112267  1476 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:09:03.112280  1476 net.cpp:128] Creating Layer Convolution10
I0825 11:09:03.112287  1476 net.cpp:558] Convolution10 <- Convolution9
I0825 11:09:03.112298  1476 net.cpp:522] Convolution10 -> Convolution10
I0825 11:09:03.113637  1476 net.cpp:172] Setting up Convolution10
I0825 11:09:03.113663  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.113668  1476 net.cpp:194] Memory required for data: 362283520
I0825 11:09:03.113692  1476 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:09:03.113705  1476 net.cpp:128] Creating Layer BatchNorm10
I0825 11:09:03.113713  1476 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:09:03.113719  1476 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:09:03.113956  1476 net.cpp:172] Setting up BatchNorm10
I0825 11:09:03.113968  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.113972  1476 net.cpp:194] Memory required for data: 366477824
I0825 11:09:03.113982  1476 layer_factory.hpp:77] Creating layer Scale10
I0825 11:09:03.113991  1476 net.cpp:128] Creating Layer Scale10
I0825 11:09:03.113996  1476 net.cpp:558] Scale10 <- Convolution10
I0825 11:09:03.114003  1476 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:09:03.114044  1476 layer_factory.hpp:77] Creating layer Scale10
I0825 11:09:03.114183  1476 net.cpp:172] Setting up Scale10
I0825 11:09:03.114197  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.114200  1476 net.cpp:194] Memory required for data: 370672128
I0825 11:09:03.114208  1476 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:09:03.114215  1476 net.cpp:128] Creating Layer Eltwise4
I0825 11:09:03.114220  1476 net.cpp:558] Eltwise4 <- Convolution8
I0825 11:09:03.114225  1476 net.cpp:558] Eltwise4 <- Convolution10
I0825 11:09:03.114233  1476 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:09:03.114256  1476 net.cpp:172] Setting up Eltwise4
I0825 11:09:03.114266  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.114270  1476 net.cpp:194] Memory required for data: 374866432
I0825 11:09:03.114274  1476 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:09:03.114280  1476 net.cpp:128] Creating Layer ReLU9
I0825 11:09:03.114285  1476 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:09:03.114293  1476 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:09:03.114540  1476 net.cpp:172] Setting up ReLU9
I0825 11:09:03.114555  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.114560  1476 net.cpp:194] Memory required for data: 379060736
I0825 11:09:03.114565  1476 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:09:03.114573  1476 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:09:03.114578  1476 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:09:03.114586  1476 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:09:03.114595  1476 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:09:03.114645  1476 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:09:03.114651  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.114657  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.114676  1476 net.cpp:194] Memory required for data: 387449344
I0825 11:09:03.114681  1476 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:09:03.114693  1476 net.cpp:128] Creating Layer Convolution11
I0825 11:09:03.114698  1476 net.cpp:558] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0825 11:09:03.114707  1476 net.cpp:522] Convolution11 -> Convolution11
I0825 11:09:03.116044  1476 net.cpp:172] Setting up Convolution11
I0825 11:09:03.116066  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.116071  1476 net.cpp:194] Memory required for data: 391643648
I0825 11:09:03.116081  1476 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:09:03.116089  1476 net.cpp:128] Creating Layer BatchNorm11
I0825 11:09:03.116099  1476 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:09:03.116108  1476 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:09:03.116348  1476 net.cpp:172] Setting up BatchNorm11
I0825 11:09:03.116358  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.116363  1476 net.cpp:194] Memory required for data: 395837952
I0825 11:09:03.116371  1476 layer_factory.hpp:77] Creating layer Scale11
I0825 11:09:03.116379  1476 net.cpp:128] Creating Layer Scale11
I0825 11:09:03.116384  1476 net.cpp:558] Scale11 <- Convolution11
I0825 11:09:03.116390  1476 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:09:03.116430  1476 layer_factory.hpp:77] Creating layer Scale11
I0825 11:09:03.116567  1476 net.cpp:172] Setting up Scale11
I0825 11:09:03.116577  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.116581  1476 net.cpp:194] Memory required for data: 400032256
I0825 11:09:03.116590  1476 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:09:03.116597  1476 net.cpp:128] Creating Layer ReLU10
I0825 11:09:03.116602  1476 net.cpp:558] ReLU10 <- Convolution11
I0825 11:09:03.116608  1476 net.cpp:509] ReLU10 -> Convolution11 (in-place)
I0825 11:09:03.117033  1476 net.cpp:172] Setting up ReLU10
I0825 11:09:03.117055  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.117059  1476 net.cpp:194] Memory required for data: 404226560
I0825 11:09:03.117064  1476 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:09:03.117080  1476 net.cpp:128] Creating Layer Convolution12
I0825 11:09:03.117086  1476 net.cpp:558] Convolution12 <- Convolution11
I0825 11:09:03.117094  1476 net.cpp:522] Convolution12 -> Convolution12
I0825 11:09:03.118474  1476 net.cpp:172] Setting up Convolution12
I0825 11:09:03.118495  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.118500  1476 net.cpp:194] Memory required for data: 408420864
I0825 11:09:03.118510  1476 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:09:03.118520  1476 net.cpp:128] Creating Layer BatchNorm12
I0825 11:09:03.118525  1476 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:09:03.118532  1476 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:09:03.118774  1476 net.cpp:172] Setting up BatchNorm12
I0825 11:09:03.118785  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.118789  1476 net.cpp:194] Memory required for data: 412615168
I0825 11:09:03.118799  1476 layer_factory.hpp:77] Creating layer Scale12
I0825 11:09:03.118808  1476 net.cpp:128] Creating Layer Scale12
I0825 11:09:03.118811  1476 net.cpp:558] Scale12 <- Convolution12
I0825 11:09:03.118819  1476 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:09:03.118858  1476 layer_factory.hpp:77] Creating layer Scale12
I0825 11:09:03.118997  1476 net.cpp:172] Setting up Scale12
I0825 11:09:03.119010  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.119015  1476 net.cpp:194] Memory required for data: 416809472
I0825 11:09:03.119024  1476 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:09:03.119030  1476 net.cpp:128] Creating Layer Eltwise5
I0825 11:09:03.119035  1476 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:09:03.119040  1476 net.cpp:558] Eltwise5 <- Convolution12
I0825 11:09:03.119048  1476 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:09:03.119086  1476 net.cpp:172] Setting up Eltwise5
I0825 11:09:03.119096  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.119101  1476 net.cpp:194] Memory required for data: 421003776
I0825 11:09:03.119104  1476 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:09:03.119110  1476 net.cpp:128] Creating Layer ReLU11
I0825 11:09:03.119115  1476 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:09:03.119123  1476 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:09:03.119355  1476 net.cpp:172] Setting up ReLU11
I0825 11:09:03.119369  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.119374  1476 net.cpp:194] Memory required for data: 425198080
I0825 11:09:03.119379  1476 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:09:03.119385  1476 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:09:03.119392  1476 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:09:03.119398  1476 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:09:03.119406  1476 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:09:03.119457  1476 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:09:03.119464  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.119470  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.119473  1476 net.cpp:194] Memory required for data: 433586688
I0825 11:09:03.119478  1476 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:09:03.119490  1476 net.cpp:128] Creating Layer Convolution13
I0825 11:09:03.119494  1476 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0825 11:09:03.119505  1476 net.cpp:522] Convolution13 -> Convolution13
I0825 11:09:03.120853  1476 net.cpp:172] Setting up Convolution13
I0825 11:09:03.120877  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.120882  1476 net.cpp:194] Memory required for data: 437780992
I0825 11:09:03.120892  1476 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:09:03.120906  1476 net.cpp:128] Creating Layer BatchNorm13
I0825 11:09:03.120911  1476 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:09:03.120918  1476 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:09:03.121163  1476 net.cpp:172] Setting up BatchNorm13
I0825 11:09:03.121174  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.121178  1476 net.cpp:194] Memory required for data: 441975296
I0825 11:09:03.121188  1476 layer_factory.hpp:77] Creating layer Scale13
I0825 11:09:03.121196  1476 net.cpp:128] Creating Layer Scale13
I0825 11:09:03.121201  1476 net.cpp:558] Scale13 <- Convolution13
I0825 11:09:03.121210  1476 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:09:03.121249  1476 layer_factory.hpp:77] Creating layer Scale13
I0825 11:09:03.121387  1476 net.cpp:172] Setting up Scale13
I0825 11:09:03.121399  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.121403  1476 net.cpp:194] Memory required for data: 446169600
I0825 11:09:03.121412  1476 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:09:03.121417  1476 net.cpp:128] Creating Layer ReLU12
I0825 11:09:03.121423  1476 net.cpp:558] ReLU12 <- Convolution13
I0825 11:09:03.121428  1476 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0825 11:09:03.121655  1476 net.cpp:172] Setting up ReLU12
I0825 11:09:03.121670  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.121675  1476 net.cpp:194] Memory required for data: 450363904
I0825 11:09:03.121680  1476 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:09:03.121692  1476 net.cpp:128] Creating Layer Convolution14
I0825 11:09:03.121697  1476 net.cpp:558] Convolution14 <- Convolution13
I0825 11:09:03.121706  1476 net.cpp:522] Convolution14 -> Convolution14
I0825 11:09:03.123112  1476 net.cpp:172] Setting up Convolution14
I0825 11:09:03.123134  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.123138  1476 net.cpp:194] Memory required for data: 454558208
I0825 11:09:03.123150  1476 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:09:03.123184  1476 net.cpp:128] Creating Layer BatchNorm14
I0825 11:09:03.123193  1476 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:09:03.123200  1476 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:09:03.123443  1476 net.cpp:172] Setting up BatchNorm14
I0825 11:09:03.123456  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.123461  1476 net.cpp:194] Memory required for data: 458752512
I0825 11:09:03.123471  1476 layer_factory.hpp:77] Creating layer Scale14
I0825 11:09:03.123479  1476 net.cpp:128] Creating Layer Scale14
I0825 11:09:03.123483  1476 net.cpp:558] Scale14 <- Convolution14
I0825 11:09:03.123489  1476 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:09:03.123530  1476 layer_factory.hpp:77] Creating layer Scale14
I0825 11:09:03.123669  1476 net.cpp:172] Setting up Scale14
I0825 11:09:03.123679  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.123683  1476 net.cpp:194] Memory required for data: 462946816
I0825 11:09:03.123692  1476 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:09:03.123700  1476 net.cpp:128] Creating Layer Eltwise6
I0825 11:09:03.123708  1476 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:09:03.123713  1476 net.cpp:558] Eltwise6 <- Convolution14
I0825 11:09:03.123719  1476 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:09:03.123741  1476 net.cpp:172] Setting up Eltwise6
I0825 11:09:03.123750  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.123754  1476 net.cpp:194] Memory required for data: 467141120
I0825 11:09:03.123759  1476 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:09:03.123765  1476 net.cpp:128] Creating Layer ReLU13
I0825 11:09:03.123769  1476 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:09:03.123780  1476 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:09:03.124207  1476 net.cpp:172] Setting up ReLU13
I0825 11:09:03.124228  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.124235  1476 net.cpp:194] Memory required for data: 471335424
I0825 11:09:03.124240  1476 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:09:03.124248  1476 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:09:03.124254  1476 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:09:03.124264  1476 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:09:03.124274  1476 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:09:03.124327  1476 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:09:03.124337  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.124343  1476 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0825 11:09:03.124347  1476 net.cpp:194] Memory required for data: 479724032
I0825 11:09:03.124351  1476 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:09:03.124366  1476 net.cpp:128] Creating Layer Convolution15
I0825 11:09:03.124370  1476 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0825 11:09:03.124378  1476 net.cpp:522] Convolution15 -> Convolution15
I0825 11:09:03.125628  1476 net.cpp:172] Setting up Convolution15
I0825 11:09:03.125650  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.125654  1476 net.cpp:194] Memory required for data: 481821184
I0825 11:09:03.125665  1476 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:09:03.125681  1476 net.cpp:128] Creating Layer BatchNorm15
I0825 11:09:03.125686  1476 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:09:03.125694  1476 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:09:03.125942  1476 net.cpp:172] Setting up BatchNorm15
I0825 11:09:03.125952  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.125957  1476 net.cpp:194] Memory required for data: 483918336
I0825 11:09:03.125967  1476 layer_factory.hpp:77] Creating layer Scale15
I0825 11:09:03.125973  1476 net.cpp:128] Creating Layer Scale15
I0825 11:09:03.125978  1476 net.cpp:558] Scale15 <- Convolution15
I0825 11:09:03.125983  1476 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:09:03.126026  1476 layer_factory.hpp:77] Creating layer Scale15
I0825 11:09:03.126186  1476 net.cpp:172] Setting up Scale15
I0825 11:09:03.126196  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.126200  1476 net.cpp:194] Memory required for data: 486015488
I0825 11:09:03.126209  1476 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:09:03.126224  1476 net.cpp:128] Creating Layer Convolution16
I0825 11:09:03.126235  1476 net.cpp:558] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0825 11:09:03.126243  1476 net.cpp:522] Convolution16 -> Convolution16
I0825 11:09:03.129272  1476 net.cpp:172] Setting up Convolution16
I0825 11:09:03.129300  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.129305  1476 net.cpp:194] Memory required for data: 488112640
I0825 11:09:03.129319  1476 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:09:03.129335  1476 net.cpp:128] Creating Layer BatchNorm16
I0825 11:09:03.129341  1476 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:09:03.129349  1476 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:09:03.129616  1476 net.cpp:172] Setting up BatchNorm16
I0825 11:09:03.129627  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.129632  1476 net.cpp:194] Memory required for data: 490209792
I0825 11:09:03.129642  1476 layer_factory.hpp:77] Creating layer Scale16
I0825 11:09:03.129649  1476 net.cpp:128] Creating Layer Scale16
I0825 11:09:03.129653  1476 net.cpp:558] Scale16 <- Convolution16
I0825 11:09:03.129662  1476 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:09:03.129703  1476 layer_factory.hpp:77] Creating layer Scale16
I0825 11:09:03.129848  1476 net.cpp:172] Setting up Scale16
I0825 11:09:03.129854  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.129858  1476 net.cpp:194] Memory required for data: 492306944
I0825 11:09:03.129866  1476 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:09:03.129873  1476 net.cpp:128] Creating Layer ReLU14
I0825 11:09:03.129878  1476 net.cpp:558] ReLU14 <- Convolution16
I0825 11:09:03.129885  1476 net.cpp:509] ReLU14 -> Convolution16 (in-place)
I0825 11:09:03.130113  1476 net.cpp:172] Setting up ReLU14
I0825 11:09:03.130131  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.130136  1476 net.cpp:194] Memory required for data: 494404096
I0825 11:09:03.130141  1476 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:09:03.130153  1476 net.cpp:128] Creating Layer Convolution17
I0825 11:09:03.130162  1476 net.cpp:558] Convolution17 <- Convolution16
I0825 11:09:03.130169  1476 net.cpp:522] Convolution17 -> Convolution17
I0825 11:09:03.132501  1476 net.cpp:172] Setting up Convolution17
I0825 11:09:03.132531  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.132536  1476 net.cpp:194] Memory required for data: 496501248
I0825 11:09:03.132550  1476 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:09:03.132562  1476 net.cpp:128] Creating Layer BatchNorm17
I0825 11:09:03.132568  1476 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:09:03.132585  1476 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:09:03.132853  1476 net.cpp:172] Setting up BatchNorm17
I0825 11:09:03.132863  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.132867  1476 net.cpp:194] Memory required for data: 498598400
I0825 11:09:03.132876  1476 layer_factory.hpp:77] Creating layer Scale17
I0825 11:09:03.132886  1476 net.cpp:128] Creating Layer Scale17
I0825 11:09:03.132894  1476 net.cpp:558] Scale17 <- Convolution17
I0825 11:09:03.132901  1476 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:09:03.132942  1476 layer_factory.hpp:77] Creating layer Scale17
I0825 11:09:03.133092  1476 net.cpp:172] Setting up Scale17
I0825 11:09:03.133105  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.133110  1476 net.cpp:194] Memory required for data: 500695552
I0825 11:09:03.133117  1476 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:09:03.133124  1476 net.cpp:128] Creating Layer Eltwise7
I0825 11:09:03.133129  1476 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:09:03.133149  1476 net.cpp:558] Eltwise7 <- Convolution17
I0825 11:09:03.133158  1476 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:09:03.133183  1476 net.cpp:172] Setting up Eltwise7
I0825 11:09:03.133191  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.133196  1476 net.cpp:194] Memory required for data: 502792704
I0825 11:09:03.133200  1476 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:09:03.133206  1476 net.cpp:128] Creating Layer ReLU15
I0825 11:09:03.133211  1476 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:09:03.133216  1476 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:09:03.133447  1476 net.cpp:172] Setting up ReLU15
I0825 11:09:03.133464  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.133468  1476 net.cpp:194] Memory required for data: 504889856
I0825 11:09:03.133473  1476 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:09:03.133481  1476 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:09:03.133486  1476 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:09:03.133491  1476 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:09:03.133502  1476 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:09:03.133553  1476 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:09:03.133563  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.133569  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.133572  1476 net.cpp:194] Memory required for data: 509084160
I0825 11:09:03.133576  1476 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:09:03.133590  1476 net.cpp:128] Creating Layer Convolution18
I0825 11:09:03.133595  1476 net.cpp:558] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0825 11:09:03.133602  1476 net.cpp:522] Convolution18 -> Convolution18
I0825 11:09:03.135398  1476 net.cpp:172] Setting up Convolution18
I0825 11:09:03.135424  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.135429  1476 net.cpp:194] Memory required for data: 511181312
I0825 11:09:03.135439  1476 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:09:03.135450  1476 net.cpp:128] Creating Layer BatchNorm18
I0825 11:09:03.135455  1476 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:09:03.135464  1476 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:09:03.135720  1476 net.cpp:172] Setting up BatchNorm18
I0825 11:09:03.135730  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.135735  1476 net.cpp:194] Memory required for data: 513278464
I0825 11:09:03.135746  1476 layer_factory.hpp:77] Creating layer Scale18
I0825 11:09:03.135753  1476 net.cpp:128] Creating Layer Scale18
I0825 11:09:03.135758  1476 net.cpp:558] Scale18 <- Convolution18
I0825 11:09:03.135764  1476 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:09:03.135807  1476 layer_factory.hpp:77] Creating layer Scale18
I0825 11:09:03.135962  1476 net.cpp:172] Setting up Scale18
I0825 11:09:03.135972  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.135975  1476 net.cpp:194] Memory required for data: 515375616
I0825 11:09:03.135984  1476 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:09:03.135990  1476 net.cpp:128] Creating Layer ReLU16
I0825 11:09:03.135995  1476 net.cpp:558] ReLU16 <- Convolution18
I0825 11:09:03.136003  1476 net.cpp:509] ReLU16 -> Convolution18 (in-place)
I0825 11:09:03.136417  1476 net.cpp:172] Setting up ReLU16
I0825 11:09:03.136438  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.136443  1476 net.cpp:194] Memory required for data: 517472768
I0825 11:09:03.136448  1476 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:09:03.136462  1476 net.cpp:128] Creating Layer Convolution19
I0825 11:09:03.136468  1476 net.cpp:558] Convolution19 <- Convolution18
I0825 11:09:03.136478  1476 net.cpp:522] Convolution19 -> Convolution19
I0825 11:09:03.138459  1476 net.cpp:172] Setting up Convolution19
I0825 11:09:03.138487  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.138491  1476 net.cpp:194] Memory required for data: 519569920
I0825 11:09:03.138516  1476 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:09:03.138525  1476 net.cpp:128] Creating Layer BatchNorm19
I0825 11:09:03.138530  1476 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:09:03.138540  1476 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:09:03.138810  1476 net.cpp:172] Setting up BatchNorm19
I0825 11:09:03.138823  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.138828  1476 net.cpp:194] Memory required for data: 521667072
I0825 11:09:03.138851  1476 layer_factory.hpp:77] Creating layer Scale19
I0825 11:09:03.138861  1476 net.cpp:128] Creating Layer Scale19
I0825 11:09:03.138866  1476 net.cpp:558] Scale19 <- Convolution19
I0825 11:09:03.138872  1476 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:09:03.138913  1476 layer_factory.hpp:77] Creating layer Scale19
I0825 11:09:03.139058  1476 net.cpp:172] Setting up Scale19
I0825 11:09:03.139070  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.139073  1476 net.cpp:194] Memory required for data: 523764224
I0825 11:09:03.139081  1476 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:09:03.139091  1476 net.cpp:128] Creating Layer Eltwise8
I0825 11:09:03.139096  1476 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:09:03.139101  1476 net.cpp:558] Eltwise8 <- Convolution19
I0825 11:09:03.139107  1476 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:09:03.139132  1476 net.cpp:172] Setting up Eltwise8
I0825 11:09:03.139138  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.139142  1476 net.cpp:194] Memory required for data: 525861376
I0825 11:09:03.139147  1476 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:09:03.139153  1476 net.cpp:128] Creating Layer ReLU17
I0825 11:09:03.139156  1476 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:09:03.139163  1476 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:09:03.139400  1476 net.cpp:172] Setting up ReLU17
I0825 11:09:03.139415  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.139420  1476 net.cpp:194] Memory required for data: 527958528
I0825 11:09:03.139425  1476 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:09:03.139432  1476 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:09:03.139437  1476 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:09:03.139446  1476 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:09:03.139454  1476 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:09:03.139503  1476 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:09:03.139514  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.139520  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.139524  1476 net.cpp:194] Memory required for data: 532152832
I0825 11:09:03.139528  1476 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:09:03.139542  1476 net.cpp:128] Creating Layer Convolution20
I0825 11:09:03.139547  1476 net.cpp:558] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0825 11:09:03.139554  1476 net.cpp:522] Convolution20 -> Convolution20
I0825 11:09:03.141337  1476 net.cpp:172] Setting up Convolution20
I0825 11:09:03.141360  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.141364  1476 net.cpp:194] Memory required for data: 534249984
I0825 11:09:03.141377  1476 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:09:03.141387  1476 net.cpp:128] Creating Layer BatchNorm20
I0825 11:09:03.141392  1476 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:09:03.141402  1476 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:09:03.141661  1476 net.cpp:172] Setting up BatchNorm20
I0825 11:09:03.141671  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.141676  1476 net.cpp:194] Memory required for data: 536347136
I0825 11:09:03.141686  1476 layer_factory.hpp:77] Creating layer Scale20
I0825 11:09:03.141693  1476 net.cpp:128] Creating Layer Scale20
I0825 11:09:03.141698  1476 net.cpp:558] Scale20 <- Convolution20
I0825 11:09:03.141718  1476 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:09:03.141762  1476 layer_factory.hpp:77] Creating layer Scale20
I0825 11:09:03.141916  1476 net.cpp:172] Setting up Scale20
I0825 11:09:03.141927  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.141932  1476 net.cpp:194] Memory required for data: 538444288
I0825 11:09:03.141939  1476 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:09:03.141947  1476 net.cpp:128] Creating Layer ReLU18
I0825 11:09:03.141952  1476 net.cpp:558] ReLU18 <- Convolution20
I0825 11:09:03.141959  1476 net.cpp:509] ReLU18 -> Convolution20 (in-place)
I0825 11:09:03.142189  1476 net.cpp:172] Setting up ReLU18
I0825 11:09:03.142204  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.142210  1476 net.cpp:194] Memory required for data: 540541440
I0825 11:09:03.142213  1476 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:09:03.142226  1476 net.cpp:128] Creating Layer Convolution21
I0825 11:09:03.142231  1476 net.cpp:558] Convolution21 <- Convolution20
I0825 11:09:03.142241  1476 net.cpp:522] Convolution21 -> Convolution21
I0825 11:09:03.144214  1476 net.cpp:172] Setting up Convolution21
I0825 11:09:03.144240  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.144245  1476 net.cpp:194] Memory required for data: 542638592
I0825 11:09:03.144258  1476 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:09:03.144266  1476 net.cpp:128] Creating Layer BatchNorm21
I0825 11:09:03.144271  1476 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:09:03.144285  1476 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:09:03.144568  1476 net.cpp:172] Setting up BatchNorm21
I0825 11:09:03.144578  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.144583  1476 net.cpp:194] Memory required for data: 544735744
I0825 11:09:03.144593  1476 layer_factory.hpp:77] Creating layer Scale21
I0825 11:09:03.144603  1476 net.cpp:128] Creating Layer Scale21
I0825 11:09:03.144608  1476 net.cpp:558] Scale21 <- Convolution21
I0825 11:09:03.144613  1476 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:09:03.144654  1476 layer_factory.hpp:77] Creating layer Scale21
I0825 11:09:03.144806  1476 net.cpp:172] Setting up Scale21
I0825 11:09:03.144816  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.144821  1476 net.cpp:194] Memory required for data: 546832896
I0825 11:09:03.144829  1476 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:09:03.144837  1476 net.cpp:128] Creating Layer Eltwise9
I0825 11:09:03.144843  1476 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:09:03.144848  1476 net.cpp:558] Eltwise9 <- Convolution21
I0825 11:09:03.144855  1476 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:09:03.144879  1476 net.cpp:172] Setting up Eltwise9
I0825 11:09:03.144886  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.144891  1476 net.cpp:194] Memory required for data: 548930048
I0825 11:09:03.144894  1476 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:09:03.144901  1476 net.cpp:128] Creating Layer ReLU19
I0825 11:09:03.144906  1476 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:09:03.144912  1476 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:09:03.145149  1476 net.cpp:172] Setting up ReLU19
I0825 11:09:03.145165  1476 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0825 11:09:03.145170  1476 net.cpp:194] Memory required for data: 551027200
I0825 11:09:03.145175  1476 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:09:03.145184  1476 net.cpp:128] Creating Layer Pooling1
I0825 11:09:03.145189  1476 net.cpp:558] Pooling1 <- Eltwise9
I0825 11:09:03.145197  1476 net.cpp:522] Pooling1 -> Pooling1
I0825 11:09:03.145680  1476 net.cpp:172] Setting up Pooling1
I0825 11:09:03.145704  1476 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0825 11:09:03.145709  1476 net.cpp:194] Memory required for data: 551059968
I0825 11:09:03.145712  1476 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:09:03.145723  1476 net.cpp:128] Creating Layer InnerProduct1
I0825 11:09:03.145742  1476 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:09:03.145752  1476 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:09:03.145922  1476 net.cpp:172] Setting up InnerProduct1
I0825 11:09:03.145929  1476 net.cpp:186] Top shape: 128 10 (1280)
I0825 11:09:03.145933  1476 net.cpp:194] Memory required for data: 551065088
I0825 11:09:03.145943  1476 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:09:03.145951  1476 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:09:03.145956  1476 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0825 11:09:03.145961  1476 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0825 11:09:03.145970  1476 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:09:03.145982  1476 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:09:03.146359  1476 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:09:03.146376  1476 net.cpp:186] Top shape: (1)
I0825 11:09:03.146381  1476 net.cpp:189]     with loss weight 1
I0825 11:09:03.146409  1476 net.cpp:194] Memory required for data: 551065092
I0825 11:09:03.146414  1476 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:09:03.146422  1476 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:09:03.146427  1476 net.cpp:301] Pooling1 needs backward computation.
I0825 11:09:03.146431  1476 net.cpp:301] ReLU19 needs backward computation.
I0825 11:09:03.146435  1476 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:09:03.146440  1476 net.cpp:301] Scale21 needs backward computation.
I0825 11:09:03.146445  1476 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:09:03.146448  1476 net.cpp:301] Convolution21 needs backward computation.
I0825 11:09:03.146453  1476 net.cpp:301] ReLU18 needs backward computation.
I0825 11:09:03.146457  1476 net.cpp:301] Scale20 needs backward computation.
I0825 11:09:03.146461  1476 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:09:03.146466  1476 net.cpp:301] Convolution20 needs backward computation.
I0825 11:09:03.146471  1476 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:09:03.146476  1476 net.cpp:301] ReLU17 needs backward computation.
I0825 11:09:03.146481  1476 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:09:03.146486  1476 net.cpp:301] Scale19 needs backward computation.
I0825 11:09:03.146489  1476 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:09:03.146493  1476 net.cpp:301] Convolution19 needs backward computation.
I0825 11:09:03.146497  1476 net.cpp:301] ReLU16 needs backward computation.
I0825 11:09:03.146502  1476 net.cpp:301] Scale18 needs backward computation.
I0825 11:09:03.146505  1476 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:09:03.146509  1476 net.cpp:301] Convolution18 needs backward computation.
I0825 11:09:03.146514  1476 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:09:03.146518  1476 net.cpp:301] ReLU15 needs backward computation.
I0825 11:09:03.146522  1476 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:09:03.146528  1476 net.cpp:301] Scale17 needs backward computation.
I0825 11:09:03.146531  1476 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:09:03.146535  1476 net.cpp:301] Convolution17 needs backward computation.
I0825 11:09:03.146540  1476 net.cpp:301] ReLU14 needs backward computation.
I0825 11:09:03.146544  1476 net.cpp:301] Scale16 needs backward computation.
I0825 11:09:03.146548  1476 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:09:03.146553  1476 net.cpp:301] Convolution16 needs backward computation.
I0825 11:09:03.146558  1476 net.cpp:301] Scale15 needs backward computation.
I0825 11:09:03.146561  1476 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:09:03.146565  1476 net.cpp:301] Convolution15 needs backward computation.
I0825 11:09:03.146570  1476 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:09:03.146575  1476 net.cpp:301] ReLU13 needs backward computation.
I0825 11:09:03.146579  1476 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:09:03.146600  1476 net.cpp:301] Scale14 needs backward computation.
I0825 11:09:03.146605  1476 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:09:03.146610  1476 net.cpp:301] Convolution14 needs backward computation.
I0825 11:09:03.146613  1476 net.cpp:301] ReLU12 needs backward computation.
I0825 11:09:03.146618  1476 net.cpp:301] Scale13 needs backward computation.
I0825 11:09:03.146622  1476 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:09:03.146626  1476 net.cpp:301] Convolution13 needs backward computation.
I0825 11:09:03.146631  1476 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:09:03.146636  1476 net.cpp:301] ReLU11 needs backward computation.
I0825 11:09:03.146641  1476 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:09:03.146648  1476 net.cpp:301] Scale12 needs backward computation.
I0825 11:09:03.146653  1476 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:09:03.146657  1476 net.cpp:301] Convolution12 needs backward computation.
I0825 11:09:03.146662  1476 net.cpp:301] ReLU10 needs backward computation.
I0825 11:09:03.146667  1476 net.cpp:301] Scale11 needs backward computation.
I0825 11:09:03.146670  1476 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:09:03.146674  1476 net.cpp:301] Convolution11 needs backward computation.
I0825 11:09:03.146680  1476 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:09:03.146684  1476 net.cpp:301] ReLU9 needs backward computation.
I0825 11:09:03.146688  1476 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:09:03.146694  1476 net.cpp:301] Scale10 needs backward computation.
I0825 11:09:03.146698  1476 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:09:03.146703  1476 net.cpp:301] Convolution10 needs backward computation.
I0825 11:09:03.146708  1476 net.cpp:301] ReLU8 needs backward computation.
I0825 11:09:03.146713  1476 net.cpp:301] Scale9 needs backward computation.
I0825 11:09:03.146718  1476 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:09:03.146721  1476 net.cpp:301] Convolution9 needs backward computation.
I0825 11:09:03.146726  1476 net.cpp:301] Scale8 needs backward computation.
I0825 11:09:03.146730  1476 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:09:03.146734  1476 net.cpp:301] Convolution8 needs backward computation.
I0825 11:09:03.146740  1476 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:09:03.146744  1476 net.cpp:301] ReLU7 needs backward computation.
I0825 11:09:03.146749  1476 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:09:03.146754  1476 net.cpp:301] Scale7 needs backward computation.
I0825 11:09:03.146759  1476 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:09:03.146762  1476 net.cpp:301] Convolution7 needs backward computation.
I0825 11:09:03.146767  1476 net.cpp:301] ReLU6 needs backward computation.
I0825 11:09:03.146771  1476 net.cpp:301] Scale6 needs backward computation.
I0825 11:09:03.146776  1476 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:09:03.146780  1476 net.cpp:301] Convolution6 needs backward computation.
I0825 11:09:03.146785  1476 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:09:03.146790  1476 net.cpp:301] ReLU5 needs backward computation.
I0825 11:09:03.146793  1476 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:09:03.146798  1476 net.cpp:301] Scale5 needs backward computation.
I0825 11:09:03.146803  1476 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:09:03.146807  1476 net.cpp:301] Convolution5 needs backward computation.
I0825 11:09:03.146812  1476 net.cpp:301] ReLU4 needs backward computation.
I0825 11:09:03.146816  1476 net.cpp:301] Scale4 needs backward computation.
I0825 11:09:03.146821  1476 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:09:03.146826  1476 net.cpp:301] Convolution4 needs backward computation.
I0825 11:09:03.146831  1476 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:09:03.146842  1476 net.cpp:301] ReLU3 needs backward computation.
I0825 11:09:03.146847  1476 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:09:03.146852  1476 net.cpp:301] Scale3 needs backward computation.
I0825 11:09:03.146855  1476 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:09:03.146860  1476 net.cpp:301] Convolution3 needs backward computation.
I0825 11:09:03.146864  1476 net.cpp:301] ReLU2 needs backward computation.
I0825 11:09:03.146869  1476 net.cpp:301] Scale2 needs backward computation.
I0825 11:09:03.146873  1476 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:09:03.146878  1476 net.cpp:301] Convolution2 needs backward computation.
I0825 11:09:03.146886  1476 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:09:03.146891  1476 net.cpp:301] ReLU1 needs backward computation.
I0825 11:09:03.146895  1476 net.cpp:301] Scale1 needs backward computation.
I0825 11:09:03.146899  1476 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:09:03.146904  1476 net.cpp:301] Convolution1 needs backward computation.
I0825 11:09:03.146909  1476 net.cpp:303] Data1 does not need backward computation.
I0825 11:09:03.146914  1476 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:09:03.146971  1476 net.cpp:363] Network initialization done.
I0825 11:09:03.162897  1476 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_20.prototxt
I0825 11:09:03.162973  1476 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:09:03.163007  1476 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_20.prototxt
I0825 11:09:03.163342  1476 net.cpp:390] layer_param.include_size():1
I0825 11:09:03.163372  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163393  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163409  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163427  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163444  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163461  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163476  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163493  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163508  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163524  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163539  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163558  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163573  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163589  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163604  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163621  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163637  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163653  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163668  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163686  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163702  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163718  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163733  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163750  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163765  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163782  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163797  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163813  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163828  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163846  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163861  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163877  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163892  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163954  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.163971  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.163988  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164003  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164021  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164037  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164053  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164068  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164085  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164100  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164116  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164131  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164149  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164163  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164180  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164196  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164212  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164227  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164243  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164259  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164275  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164290  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164307  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164322  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164338  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164353  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164371  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164386  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164402  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164417  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164433  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164448  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164466  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164481  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164499  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164513  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164530  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164546  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164562  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164577  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164593  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164609  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164625  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164640  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164657  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164672  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164690  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164731  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164749  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164764  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164782  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164796  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164813  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164829  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164844  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164860  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164877  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164892  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164934  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164950  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164968  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.164983  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.164999  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165014  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165031  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165046  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165062  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165078  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165094  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165109  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165127  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165141  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165158  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165172  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165189  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165205  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165221  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165236  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165253  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165268  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165284  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165299  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165318  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165333  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165349  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165364  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165380  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165395  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165411  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165427  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165443  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165458  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165475  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165491  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165508  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165522  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165539  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165555  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165571  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165587  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165604  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165619  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165637  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165652  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165668  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165683  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165701  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165725  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165745  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165760  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165776  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165797  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165815  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165832  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165848  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165871  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165890  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165930  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165948  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165964  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.165980  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.165995  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166012  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166028  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166052  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166076  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166095  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166110  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166126  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166142  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166159  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166182  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166208  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166229  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166249  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166263  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166280  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166296  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166312  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166362  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166388  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166409  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166427  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166443  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166460  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166476  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166501  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166522  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166538  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166554  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166571  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166586  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166609  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166630  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166649  1476 net.cpp:390] layer_param.include_size():0
I0825 11:09:03.166673  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.166692  1476 net.cpp:390] layer_param.include_size():1
I0825 11:09:03.166707  1476 net.cpp:391] layer_param.exclude_size():0
I0825 11:09:03.169191  1476 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I0825 11:09:03.170645  1476 layer_factory.hpp:77] Creating layer Data1
I0825 11:09:03.216205  1476 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0825 11:09:03.221586  1476 net.cpp:128] Creating Layer Data1
I0825 11:09:03.221658  1476 net.cpp:522] Data1 -> Data1
I0825 11:09:03.221704  1476 net.cpp:522] Data1 -> Data2
I0825 11:09:03.222268  1476 data_layer.cpp:45] output data size: 10,3,32,32
I0825 11:09:03.224807  1476 net.cpp:172] Setting up Data1
I0825 11:09:03.224882  1476 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0825 11:09:03.224915  1476 net.cpp:186] Top shape: 10 (10)
I0825 11:09:03.224938  1476 net.cpp:194] Memory required for data: 122920
I0825 11:09:03.224961  1476 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0825 11:09:03.224994  1476 net.cpp:128] Creating Layer Data2_Data1_1_split
I0825 11:09:03.225021  1476 net.cpp:558] Data2_Data1_1_split <- Data2
I0825 11:09:03.225051  1476 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0825 11:09:03.225096  1476 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0825 11:09:03.225419  1476 net.cpp:172] Setting up Data2_Data1_1_split
I0825 11:09:03.225455  1476 net.cpp:186] Top shape: 10 (10)
I0825 11:09:03.225463  1476 net.cpp:186] Top shape: 10 (10)
I0825 11:09:03.225469  1476 net.cpp:194] Memory required for data: 123000
I0825 11:09:03.225478  1476 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:09:03.225517  1476 net.cpp:128] Creating Layer Convolution1
I0825 11:09:03.225525  1476 net.cpp:558] Convolution1 <- Data1
I0825 11:09:03.225543  1476 net.cpp:522] Convolution1 -> Convolution1
I0825 11:09:03.255484  1476 net.cpp:172] Setting up Convolution1
I0825 11:09:03.255556  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.255575  1476 net.cpp:194] Memory required for data: 778360
I0825 11:09:03.255626  1476 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:09:03.255666  1476 net.cpp:128] Creating Layer BatchNorm1
I0825 11:09:03.255693  1476 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:09:03.255728  1476 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:09:03.256858  1476 net.cpp:172] Setting up BatchNorm1
I0825 11:09:03.256902  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.256920  1476 net.cpp:194] Memory required for data: 1433720
I0825 11:09:03.256983  1476 layer_factory.hpp:77] Creating layer Scale1
I0825 11:09:03.257026  1476 net.cpp:128] Creating Layer Scale1
I0825 11:09:03.257050  1476 net.cpp:558] Scale1 <- Convolution1
I0825 11:09:03.257076  1476 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:09:03.257283  1476 layer_factory.hpp:77] Creating layer Scale1
I0825 11:09:03.257889  1476 net.cpp:172] Setting up Scale1
I0825 11:09:03.257928  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.257958  1476 net.cpp:194] Memory required for data: 2089080
I0825 11:09:03.257992  1476 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:09:03.258031  1476 net.cpp:128] Creating Layer ReLU1
I0825 11:09:03.258061  1476 net.cpp:558] ReLU1 <- Convolution1
I0825 11:09:03.258086  1476 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:09:03.259057  1476 net.cpp:172] Setting up ReLU1
I0825 11:09:03.259111  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.259130  1476 net.cpp:194] Memory required for data: 2744440
I0825 11:09:03.259150  1476 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:09:03.259189  1476 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:09:03.259217  1476 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:09:03.259251  1476 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:09:03.259291  1476 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:09:03.259501  1476 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:09:03.259544  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.259577  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.259598  1476 net.cpp:194] Memory required for data: 4055160
I0825 11:09:03.259618  1476 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:09:03.259666  1476 net.cpp:128] Creating Layer Convolution2
I0825 11:09:03.259687  1476 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:09:03.259737  1476 net.cpp:522] Convolution2 -> Convolution2
I0825 11:09:03.264734  1476 net.cpp:172] Setting up Convolution2
I0825 11:09:03.264809  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.264828  1476 net.cpp:194] Memory required for data: 4710520
I0825 11:09:03.264885  1476 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:09:03.264927  1476 net.cpp:128] Creating Layer BatchNorm2
I0825 11:09:03.264955  1476 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:09:03.264986  1476 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:09:03.266089  1476 net.cpp:172] Setting up BatchNorm2
I0825 11:09:03.266132  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.266150  1476 net.cpp:194] Memory required for data: 5365880
I0825 11:09:03.266192  1476 layer_factory.hpp:77] Creating layer Scale2
I0825 11:09:03.266221  1476 net.cpp:128] Creating Layer Scale2
I0825 11:09:03.266238  1476 net.cpp:558] Scale2 <- Convolution2
I0825 11:09:03.266279  1476 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:09:03.266537  1476 layer_factory.hpp:77] Creating layer Scale2
I0825 11:09:03.266773  1476 net.cpp:172] Setting up Scale2
I0825 11:09:03.266788  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.266791  1476 net.cpp:194] Memory required for data: 6021240
I0825 11:09:03.266800  1476 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:09:03.266806  1476 net.cpp:128] Creating Layer ReLU2
I0825 11:09:03.266813  1476 net.cpp:558] ReLU2 <- Convolution2
I0825 11:09:03.266819  1476 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:09:03.267257  1476 net.cpp:172] Setting up ReLU2
I0825 11:09:03.267278  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.267283  1476 net.cpp:194] Memory required for data: 6676600
I0825 11:09:03.267288  1476 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:09:03.267303  1476 net.cpp:128] Creating Layer Convolution3
I0825 11:09:03.267308  1476 net.cpp:558] Convolution3 <- Convolution2
I0825 11:09:03.267316  1476 net.cpp:522] Convolution3 -> Convolution3
I0825 11:09:03.268623  1476 net.cpp:172] Setting up Convolution3
I0825 11:09:03.268648  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.268653  1476 net.cpp:194] Memory required for data: 7331960
I0825 11:09:03.268663  1476 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:09:03.268673  1476 net.cpp:128] Creating Layer BatchNorm3
I0825 11:09:03.268678  1476 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:09:03.268692  1476 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:09:03.268962  1476 net.cpp:172] Setting up BatchNorm3
I0825 11:09:03.268975  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.268980  1476 net.cpp:194] Memory required for data: 7987320
I0825 11:09:03.268993  1476 layer_factory.hpp:77] Creating layer Scale3
I0825 11:09:03.269001  1476 net.cpp:128] Creating Layer Scale3
I0825 11:09:03.269004  1476 net.cpp:558] Scale3 <- Convolution3
I0825 11:09:03.269011  1476 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:09:03.269062  1476 layer_factory.hpp:77] Creating layer Scale3
I0825 11:09:03.269212  1476 net.cpp:172] Setting up Scale3
I0825 11:09:03.269219  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.269223  1476 net.cpp:194] Memory required for data: 8642680
I0825 11:09:03.269232  1476 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:09:03.269240  1476 net.cpp:128] Creating Layer Eltwise1
I0825 11:09:03.269245  1476 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:09:03.269250  1476 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:09:03.269256  1476 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:09:03.269286  1476 net.cpp:172] Setting up Eltwise1
I0825 11:09:03.269294  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.269297  1476 net.cpp:194] Memory required for data: 9298040
I0825 11:09:03.269302  1476 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:09:03.269307  1476 net.cpp:128] Creating Layer ReLU3
I0825 11:09:03.269312  1476 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:09:03.269320  1476 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:09:03.269558  1476 net.cpp:172] Setting up ReLU3
I0825 11:09:03.269573  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.269578  1476 net.cpp:194] Memory required for data: 9953400
I0825 11:09:03.269582  1476 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:09:03.269594  1476 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:09:03.269599  1476 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:09:03.269606  1476 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:09:03.269615  1476 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:09:03.269668  1476 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:09:03.269675  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.269681  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.269685  1476 net.cpp:194] Memory required for data: 11264120
I0825 11:09:03.269690  1476 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:09:03.269701  1476 net.cpp:128] Creating Layer Convolution4
I0825 11:09:03.269721  1476 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:09:03.269731  1476 net.cpp:522] Convolution4 -> Convolution4
I0825 11:09:03.271037  1476 net.cpp:172] Setting up Convolution4
I0825 11:09:03.271060  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.271064  1476 net.cpp:194] Memory required for data: 11919480
I0825 11:09:03.271075  1476 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:09:03.271082  1476 net.cpp:128] Creating Layer BatchNorm4
I0825 11:09:03.271087  1476 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:09:03.271096  1476 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:09:03.271370  1476 net.cpp:172] Setting up BatchNorm4
I0825 11:09:03.271384  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.271387  1476 net.cpp:194] Memory required for data: 12574840
I0825 11:09:03.271397  1476 layer_factory.hpp:77] Creating layer Scale4
I0825 11:09:03.271404  1476 net.cpp:128] Creating Layer Scale4
I0825 11:09:03.271409  1476 net.cpp:558] Scale4 <- Convolution4
I0825 11:09:03.271417  1476 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:09:03.271464  1476 layer_factory.hpp:77] Creating layer Scale4
I0825 11:09:03.271612  1476 net.cpp:172] Setting up Scale4
I0825 11:09:03.271622  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.271627  1476 net.cpp:194] Memory required for data: 13230200
I0825 11:09:03.271636  1476 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:09:03.271641  1476 net.cpp:128] Creating Layer ReLU4
I0825 11:09:03.271646  1476 net.cpp:558] ReLU4 <- Convolution4
I0825 11:09:03.271651  1476 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:09:03.271894  1476 net.cpp:172] Setting up ReLU4
I0825 11:09:03.271911  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.271916  1476 net.cpp:194] Memory required for data: 13885560
I0825 11:09:03.271920  1476 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:09:03.271934  1476 net.cpp:128] Creating Layer Convolution5
I0825 11:09:03.271939  1476 net.cpp:558] Convolution5 <- Convolution4
I0825 11:09:03.271948  1476 net.cpp:522] Convolution5 -> Convolution5
I0825 11:09:03.273234  1476 net.cpp:172] Setting up Convolution5
I0825 11:09:03.273257  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.273262  1476 net.cpp:194] Memory required for data: 14540920
I0825 11:09:03.273272  1476 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:09:03.273281  1476 net.cpp:128] Creating Layer BatchNorm5
I0825 11:09:03.273288  1476 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:09:03.273296  1476 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:09:03.273568  1476 net.cpp:172] Setting up BatchNorm5
I0825 11:09:03.273579  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.273583  1476 net.cpp:194] Memory required for data: 15196280
I0825 11:09:03.273598  1476 layer_factory.hpp:77] Creating layer Scale5
I0825 11:09:03.273607  1476 net.cpp:128] Creating Layer Scale5
I0825 11:09:03.273610  1476 net.cpp:558] Scale5 <- Convolution5
I0825 11:09:03.273617  1476 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:09:03.273666  1476 layer_factory.hpp:77] Creating layer Scale5
I0825 11:09:03.273816  1476 net.cpp:172] Setting up Scale5
I0825 11:09:03.273825  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.273829  1476 net.cpp:194] Memory required for data: 15851640
I0825 11:09:03.273836  1476 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:09:03.273845  1476 net.cpp:128] Creating Layer Eltwise2
I0825 11:09:03.273850  1476 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:09:03.273855  1476 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:09:03.273861  1476 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:09:03.273890  1476 net.cpp:172] Setting up Eltwise2
I0825 11:09:03.273896  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.273902  1476 net.cpp:194] Memory required for data: 16507000
I0825 11:09:03.273906  1476 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:09:03.273928  1476 net.cpp:128] Creating Layer ReLU5
I0825 11:09:03.273932  1476 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:09:03.273941  1476 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:09:03.274385  1476 net.cpp:172] Setting up ReLU5
I0825 11:09:03.274408  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.274413  1476 net.cpp:194] Memory required for data: 17162360
I0825 11:09:03.274418  1476 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:09:03.274427  1476 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:09:03.274433  1476 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:09:03.274441  1476 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:09:03.274448  1476 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:09:03.274507  1476 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:09:03.274514  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.274520  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.274524  1476 net.cpp:194] Memory required for data: 18473080
I0825 11:09:03.274528  1476 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:09:03.274541  1476 net.cpp:128] Creating Layer Convolution6
I0825 11:09:03.274547  1476 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:09:03.274554  1476 net.cpp:522] Convolution6 -> Convolution6
I0825 11:09:03.275921  1476 net.cpp:172] Setting up Convolution6
I0825 11:09:03.275944  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.275949  1476 net.cpp:194] Memory required for data: 19128440
I0825 11:09:03.275961  1476 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:09:03.275971  1476 net.cpp:128] Creating Layer BatchNorm6
I0825 11:09:03.275977  1476 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:09:03.275986  1476 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:09:03.276263  1476 net.cpp:172] Setting up BatchNorm6
I0825 11:09:03.276275  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.276279  1476 net.cpp:194] Memory required for data: 19783800
I0825 11:09:03.276289  1476 layer_factory.hpp:77] Creating layer Scale6
I0825 11:09:03.276301  1476 net.cpp:128] Creating Layer Scale6
I0825 11:09:03.276306  1476 net.cpp:558] Scale6 <- Convolution6
I0825 11:09:03.276312  1476 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:09:03.276362  1476 layer_factory.hpp:77] Creating layer Scale6
I0825 11:09:03.276511  1476 net.cpp:172] Setting up Scale6
I0825 11:09:03.276522  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.276527  1476 net.cpp:194] Memory required for data: 20439160
I0825 11:09:03.276535  1476 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:09:03.276541  1476 net.cpp:128] Creating Layer ReLU6
I0825 11:09:03.276546  1476 net.cpp:558] ReLU6 <- Convolution6
I0825 11:09:03.276553  1476 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:09:03.276787  1476 net.cpp:172] Setting up ReLU6
I0825 11:09:03.276803  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.276809  1476 net.cpp:194] Memory required for data: 21094520
I0825 11:09:03.276813  1476 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:09:03.276826  1476 net.cpp:128] Creating Layer Convolution7
I0825 11:09:03.276831  1476 net.cpp:558] Convolution7 <- Convolution6
I0825 11:09:03.276840  1476 net.cpp:522] Convolution7 -> Convolution7
I0825 11:09:03.278151  1476 net.cpp:172] Setting up Convolution7
I0825 11:09:03.278177  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.278182  1476 net.cpp:194] Memory required for data: 21749880
I0825 11:09:03.278192  1476 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:09:03.278205  1476 net.cpp:128] Creating Layer BatchNorm7
I0825 11:09:03.278210  1476 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:09:03.278220  1476 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:09:03.278517  1476 net.cpp:172] Setting up BatchNorm7
I0825 11:09:03.278528  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.278549  1476 net.cpp:194] Memory required for data: 22405240
I0825 11:09:03.278560  1476 layer_factory.hpp:77] Creating layer Scale7
I0825 11:09:03.278570  1476 net.cpp:128] Creating Layer Scale7
I0825 11:09:03.278575  1476 net.cpp:558] Scale7 <- Convolution7
I0825 11:09:03.278582  1476 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:09:03.278635  1476 layer_factory.hpp:77] Creating layer Scale7
I0825 11:09:03.278795  1476 net.cpp:172] Setting up Scale7
I0825 11:09:03.278803  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.278807  1476 net.cpp:194] Memory required for data: 23060600
I0825 11:09:03.278815  1476 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:09:03.278825  1476 net.cpp:128] Creating Layer Eltwise3
I0825 11:09:03.278829  1476 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:09:03.278834  1476 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:09:03.278841  1476 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:09:03.278869  1476 net.cpp:172] Setting up Eltwise3
I0825 11:09:03.278877  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.278880  1476 net.cpp:194] Memory required for data: 23715960
I0825 11:09:03.278885  1476 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:09:03.278892  1476 net.cpp:128] Creating Layer ReLU7
I0825 11:09:03.278895  1476 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:09:03.278903  1476 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:09:03.279145  1476 net.cpp:172] Setting up ReLU7
I0825 11:09:03.279158  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.279163  1476 net.cpp:194] Memory required for data: 24371320
I0825 11:09:03.279167  1476 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:09:03.279176  1476 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:09:03.279181  1476 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:09:03.279188  1476 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:09:03.279196  1476 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:09:03.279250  1476 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:09:03.279258  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.279263  1476 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:09:03.279268  1476 net.cpp:194] Memory required for data: 25682040
I0825 11:09:03.279273  1476 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:09:03.279285  1476 net.cpp:128] Creating Layer Convolution8
I0825 11:09:03.279290  1476 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:09:03.279299  1476 net.cpp:522] Convolution8 -> Convolution8
I0825 11:09:03.280596  1476 net.cpp:172] Setting up Convolution8
I0825 11:09:03.280622  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.280627  1476 net.cpp:194] Memory required for data: 26009720
I0825 11:09:03.280639  1476 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:09:03.280653  1476 net.cpp:128] Creating Layer BatchNorm8
I0825 11:09:03.280658  1476 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:09:03.280668  1476 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:09:03.280941  1476 net.cpp:172] Setting up BatchNorm8
I0825 11:09:03.280951  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.280954  1476 net.cpp:194] Memory required for data: 26337400
I0825 11:09:03.280964  1476 layer_factory.hpp:77] Creating layer Scale8
I0825 11:09:03.280972  1476 net.cpp:128] Creating Layer Scale8
I0825 11:09:03.280975  1476 net.cpp:558] Scale8 <- Convolution8
I0825 11:09:03.280982  1476 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:09:03.281031  1476 layer_factory.hpp:77] Creating layer Scale8
I0825 11:09:03.281184  1476 net.cpp:172] Setting up Scale8
I0825 11:09:03.281194  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.281198  1476 net.cpp:194] Memory required for data: 26665080
I0825 11:09:03.281206  1476 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:09:03.281219  1476 net.cpp:128] Creating Layer Convolution9
I0825 11:09:03.281239  1476 net.cpp:558] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0825 11:09:03.281249  1476 net.cpp:522] Convolution9 -> Convolution9
I0825 11:09:03.282619  1476 net.cpp:172] Setting up Convolution9
I0825 11:09:03.282645  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.282649  1476 net.cpp:194] Memory required for data: 26992760
I0825 11:09:03.282660  1476 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:09:03.282675  1476 net.cpp:128] Creating Layer BatchNorm9
I0825 11:09:03.282680  1476 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:09:03.282687  1476 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:09:03.282958  1476 net.cpp:172] Setting up BatchNorm9
I0825 11:09:03.282968  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.282971  1476 net.cpp:194] Memory required for data: 27320440
I0825 11:09:03.282981  1476 layer_factory.hpp:77] Creating layer Scale9
I0825 11:09:03.282990  1476 net.cpp:128] Creating Layer Scale9
I0825 11:09:03.282995  1476 net.cpp:558] Scale9 <- Convolution9
I0825 11:09:03.283000  1476 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:09:03.283053  1476 layer_factory.hpp:77] Creating layer Scale9
I0825 11:09:03.283207  1476 net.cpp:172] Setting up Scale9
I0825 11:09:03.283217  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.283221  1476 net.cpp:194] Memory required for data: 27648120
I0825 11:09:03.283229  1476 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:09:03.283236  1476 net.cpp:128] Creating Layer ReLU8
I0825 11:09:03.283241  1476 net.cpp:558] ReLU8 <- Convolution9
I0825 11:09:03.283248  1476 net.cpp:509] ReLU8 -> Convolution9 (in-place)
I0825 11:09:03.283486  1476 net.cpp:172] Setting up ReLU8
I0825 11:09:03.283499  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.283504  1476 net.cpp:194] Memory required for data: 27975800
I0825 11:09:03.283509  1476 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:09:03.283520  1476 net.cpp:128] Creating Layer Convolution10
I0825 11:09:03.283527  1476 net.cpp:558] Convolution10 <- Convolution9
I0825 11:09:03.283536  1476 net.cpp:522] Convolution10 -> Convolution10
I0825 11:09:03.284966  1476 net.cpp:172] Setting up Convolution10
I0825 11:09:03.284991  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.284996  1476 net.cpp:194] Memory required for data: 28303480
I0825 11:09:03.285023  1476 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:09:03.285034  1476 net.cpp:128] Creating Layer BatchNorm10
I0825 11:09:03.285040  1476 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:09:03.285048  1476 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:09:03.285320  1476 net.cpp:172] Setting up BatchNorm10
I0825 11:09:03.285329  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.285333  1476 net.cpp:194] Memory required for data: 28631160
I0825 11:09:03.285343  1476 layer_factory.hpp:77] Creating layer Scale10
I0825 11:09:03.285351  1476 net.cpp:128] Creating Layer Scale10
I0825 11:09:03.285356  1476 net.cpp:558] Scale10 <- Convolution10
I0825 11:09:03.285360  1476 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:09:03.285413  1476 layer_factory.hpp:77] Creating layer Scale10
I0825 11:09:03.285567  1476 net.cpp:172] Setting up Scale10
I0825 11:09:03.285576  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.285580  1476 net.cpp:194] Memory required for data: 28958840
I0825 11:09:03.285588  1476 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:09:03.285598  1476 net.cpp:128] Creating Layer Eltwise4
I0825 11:09:03.285603  1476 net.cpp:558] Eltwise4 <- Convolution8
I0825 11:09:03.285607  1476 net.cpp:558] Eltwise4 <- Convolution10
I0825 11:09:03.285614  1476 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:09:03.285636  1476 net.cpp:172] Setting up Eltwise4
I0825 11:09:03.285645  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.285650  1476 net.cpp:194] Memory required for data: 29286520
I0825 11:09:03.285655  1476 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:09:03.285678  1476 net.cpp:128] Creating Layer ReLU9
I0825 11:09:03.285686  1476 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:09:03.285692  1476 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:09:03.286134  1476 net.cpp:172] Setting up ReLU9
I0825 11:09:03.286156  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.286164  1476 net.cpp:194] Memory required for data: 29614200
I0825 11:09:03.286168  1476 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:09:03.286176  1476 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:09:03.286181  1476 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:09:03.286188  1476 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:09:03.286200  1476 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:09:03.286257  1476 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:09:03.286267  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.286273  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.286278  1476 net.cpp:194] Memory required for data: 30269560
I0825 11:09:03.286283  1476 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:09:03.286298  1476 net.cpp:128] Creating Layer Convolution11
I0825 11:09:03.286303  1476 net.cpp:558] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0825 11:09:03.286311  1476 net.cpp:522] Convolution11 -> Convolution11
I0825 11:09:03.287915  1476 net.cpp:172] Setting up Convolution11
I0825 11:09:03.287943  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.287947  1476 net.cpp:194] Memory required for data: 30597240
I0825 11:09:03.287961  1476 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:09:03.287968  1476 net.cpp:128] Creating Layer BatchNorm11
I0825 11:09:03.287976  1476 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:09:03.287984  1476 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:09:03.288261  1476 net.cpp:172] Setting up BatchNorm11
I0825 11:09:03.288272  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.288276  1476 net.cpp:194] Memory required for data: 30924920
I0825 11:09:03.288286  1476 layer_factory.hpp:77] Creating layer Scale11
I0825 11:09:03.288293  1476 net.cpp:128] Creating Layer Scale11
I0825 11:09:03.288297  1476 net.cpp:558] Scale11 <- Convolution11
I0825 11:09:03.288303  1476 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:09:03.288354  1476 layer_factory.hpp:77] Creating layer Scale11
I0825 11:09:03.288511  1476 net.cpp:172] Setting up Scale11
I0825 11:09:03.288520  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.288524  1476 net.cpp:194] Memory required for data: 31252600
I0825 11:09:03.288532  1476 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:09:03.288542  1476 net.cpp:128] Creating Layer ReLU10
I0825 11:09:03.288547  1476 net.cpp:558] ReLU10 <- Convolution11
I0825 11:09:03.288552  1476 net.cpp:509] ReLU10 -> Convolution11 (in-place)
I0825 11:09:03.288791  1476 net.cpp:172] Setting up ReLU10
I0825 11:09:03.288805  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.288808  1476 net.cpp:194] Memory required for data: 31580280
I0825 11:09:03.288813  1476 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:09:03.288826  1476 net.cpp:128] Creating Layer Convolution12
I0825 11:09:03.288832  1476 net.cpp:558] Convolution12 <- Convolution11
I0825 11:09:03.288841  1476 net.cpp:522] Convolution12 -> Convolution12
I0825 11:09:03.290274  1476 net.cpp:172] Setting up Convolution12
I0825 11:09:03.290298  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.290303  1476 net.cpp:194] Memory required for data: 31907960
I0825 11:09:03.290316  1476 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:09:03.290326  1476 net.cpp:128] Creating Layer BatchNorm12
I0825 11:09:03.290340  1476 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:09:03.290351  1476 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:09:03.290630  1476 net.cpp:172] Setting up BatchNorm12
I0825 11:09:03.290642  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.290660  1476 net.cpp:194] Memory required for data: 32235640
I0825 11:09:03.290671  1476 layer_factory.hpp:77] Creating layer Scale12
I0825 11:09:03.290679  1476 net.cpp:128] Creating Layer Scale12
I0825 11:09:03.290683  1476 net.cpp:558] Scale12 <- Convolution12
I0825 11:09:03.290689  1476 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:09:03.290742  1476 layer_factory.hpp:77] Creating layer Scale12
I0825 11:09:03.290902  1476 net.cpp:172] Setting up Scale12
I0825 11:09:03.290916  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.290920  1476 net.cpp:194] Memory required for data: 32563320
I0825 11:09:03.290928  1476 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:09:03.290935  1476 net.cpp:128] Creating Layer Eltwise5
I0825 11:09:03.290940  1476 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:09:03.290946  1476 net.cpp:558] Eltwise5 <- Convolution12
I0825 11:09:03.290953  1476 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:09:03.290977  1476 net.cpp:172] Setting up Eltwise5
I0825 11:09:03.290983  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.290987  1476 net.cpp:194] Memory required for data: 32891000
I0825 11:09:03.290993  1476 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:09:03.291003  1476 net.cpp:128] Creating Layer ReLU11
I0825 11:09:03.291008  1476 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:09:03.291013  1476 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:09:03.291249  1476 net.cpp:172] Setting up ReLU11
I0825 11:09:03.291263  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.291267  1476 net.cpp:194] Memory required for data: 33218680
I0825 11:09:03.291272  1476 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:09:03.291281  1476 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:09:03.291286  1476 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:09:03.291293  1476 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:09:03.291303  1476 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:09:03.291359  1476 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:09:03.291366  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.291373  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.291378  1476 net.cpp:194] Memory required for data: 33874040
I0825 11:09:03.291381  1476 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:09:03.291393  1476 net.cpp:128] Creating Layer Convolution13
I0825 11:09:03.291399  1476 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0825 11:09:03.291409  1476 net.cpp:522] Convolution13 -> Convolution13
I0825 11:09:03.292835  1476 net.cpp:172] Setting up Convolution13
I0825 11:09:03.292861  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.292866  1476 net.cpp:194] Memory required for data: 34201720
I0825 11:09:03.292876  1476 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:09:03.292886  1476 net.cpp:128] Creating Layer BatchNorm13
I0825 11:09:03.292892  1476 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:09:03.292898  1476 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:09:03.293174  1476 net.cpp:172] Setting up BatchNorm13
I0825 11:09:03.293184  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.293190  1476 net.cpp:194] Memory required for data: 34529400
I0825 11:09:03.293198  1476 layer_factory.hpp:77] Creating layer Scale13
I0825 11:09:03.293205  1476 net.cpp:128] Creating Layer Scale13
I0825 11:09:03.293210  1476 net.cpp:558] Scale13 <- Convolution13
I0825 11:09:03.293223  1476 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:09:03.293273  1476 layer_factory.hpp:77] Creating layer Scale13
I0825 11:09:03.293431  1476 net.cpp:172] Setting up Scale13
I0825 11:09:03.293443  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.293448  1476 net.cpp:194] Memory required for data: 34857080
I0825 11:09:03.293457  1476 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:09:03.293478  1476 net.cpp:128] Creating Layer ReLU12
I0825 11:09:03.293483  1476 net.cpp:558] ReLU12 <- Convolution13
I0825 11:09:03.293491  1476 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0825 11:09:03.293934  1476 net.cpp:172] Setting up ReLU12
I0825 11:09:03.293957  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.293962  1476 net.cpp:194] Memory required for data: 35184760
I0825 11:09:03.293967  1476 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:09:03.293987  1476 net.cpp:128] Creating Layer Convolution14
I0825 11:09:03.293992  1476 net.cpp:558] Convolution14 <- Convolution13
I0825 11:09:03.293999  1476 net.cpp:522] Convolution14 -> Convolution14
I0825 11:09:03.295477  1476 net.cpp:172] Setting up Convolution14
I0825 11:09:03.295503  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.295508  1476 net.cpp:194] Memory required for data: 35512440
I0825 11:09:03.295518  1476 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:09:03.295529  1476 net.cpp:128] Creating Layer BatchNorm14
I0825 11:09:03.295534  1476 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:09:03.295544  1476 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:09:03.295819  1476 net.cpp:172] Setting up BatchNorm14
I0825 11:09:03.295831  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.295835  1476 net.cpp:194] Memory required for data: 35840120
I0825 11:09:03.295845  1476 layer_factory.hpp:77] Creating layer Scale14
I0825 11:09:03.295852  1476 net.cpp:128] Creating Layer Scale14
I0825 11:09:03.295857  1476 net.cpp:558] Scale14 <- Convolution14
I0825 11:09:03.295866  1476 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:09:03.295914  1476 layer_factory.hpp:77] Creating layer Scale14
I0825 11:09:03.296073  1476 net.cpp:172] Setting up Scale14
I0825 11:09:03.296082  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.296087  1476 net.cpp:194] Memory required for data: 36167800
I0825 11:09:03.296093  1476 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:09:03.296100  1476 net.cpp:128] Creating Layer Eltwise6
I0825 11:09:03.296106  1476 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0825 11:09:03.296111  1476 net.cpp:558] Eltwise6 <- Convolution14
I0825 11:09:03.296119  1476 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:09:03.296142  1476 net.cpp:172] Setting up Eltwise6
I0825 11:09:03.296152  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.296157  1476 net.cpp:194] Memory required for data: 36495480
I0825 11:09:03.296161  1476 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:09:03.296167  1476 net.cpp:128] Creating Layer ReLU13
I0825 11:09:03.296171  1476 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:09:03.296177  1476 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:09:03.296422  1476 net.cpp:172] Setting up ReLU13
I0825 11:09:03.296442  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.296447  1476 net.cpp:194] Memory required for data: 36823160
I0825 11:09:03.296452  1476 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:09:03.296458  1476 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:09:03.296463  1476 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:09:03.296470  1476 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:09:03.296480  1476 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:09:03.296535  1476 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:09:03.296542  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.296548  1476 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:09:03.296552  1476 net.cpp:194] Memory required for data: 37478520
I0825 11:09:03.296556  1476 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:09:03.296571  1476 net.cpp:128] Creating Layer Convolution15
I0825 11:09:03.296576  1476 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0825 11:09:03.296583  1476 net.cpp:522] Convolution15 -> Convolution15
I0825 11:09:03.297919  1476 net.cpp:172] Setting up Convolution15
I0825 11:09:03.297958  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.297963  1476 net.cpp:194] Memory required for data: 37642360
I0825 11:09:03.297973  1476 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:09:03.297984  1476 net.cpp:128] Creating Layer BatchNorm15
I0825 11:09:03.297991  1476 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:09:03.297998  1476 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:09:03.298293  1476 net.cpp:172] Setting up BatchNorm15
I0825 11:09:03.298306  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.298310  1476 net.cpp:194] Memory required for data: 37806200
I0825 11:09:03.298321  1476 layer_factory.hpp:77] Creating layer Scale15
I0825 11:09:03.298327  1476 net.cpp:128] Creating Layer Scale15
I0825 11:09:03.298333  1476 net.cpp:558] Scale15 <- Convolution15
I0825 11:09:03.298347  1476 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:09:03.298401  1476 layer_factory.hpp:77] Creating layer Scale15
I0825 11:09:03.298570  1476 net.cpp:172] Setting up Scale15
I0825 11:09:03.298581  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.298585  1476 net.cpp:194] Memory required for data: 37970040
I0825 11:09:03.298593  1476 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:09:03.298605  1476 net.cpp:128] Creating Layer Convolution16
I0825 11:09:03.298612  1476 net.cpp:558] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0825 11:09:03.298622  1476 net.cpp:522] Convolution16 -> Convolution16
I0825 11:09:03.300232  1476 net.cpp:172] Setting up Convolution16
I0825 11:09:03.300257  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.300262  1476 net.cpp:194] Memory required for data: 38133880
I0825 11:09:03.300274  1476 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:09:03.300287  1476 net.cpp:128] Creating Layer BatchNorm16
I0825 11:09:03.300297  1476 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:09:03.300305  1476 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:09:03.300593  1476 net.cpp:172] Setting up BatchNorm16
I0825 11:09:03.300603  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.300607  1476 net.cpp:194] Memory required for data: 38297720
I0825 11:09:03.300617  1476 layer_factory.hpp:77] Creating layer Scale16
I0825 11:09:03.300626  1476 net.cpp:128] Creating Layer Scale16
I0825 11:09:03.300629  1476 net.cpp:558] Scale16 <- Convolution16
I0825 11:09:03.300637  1476 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:09:03.300688  1476 layer_factory.hpp:77] Creating layer Scale16
I0825 11:09:03.300851  1476 net.cpp:172] Setting up Scale16
I0825 11:09:03.300861  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.300865  1476 net.cpp:194] Memory required for data: 38461560
I0825 11:09:03.300873  1476 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:09:03.300880  1476 net.cpp:128] Creating Layer ReLU14
I0825 11:09:03.300884  1476 net.cpp:558] ReLU14 <- Convolution16
I0825 11:09:03.300892  1476 net.cpp:509] ReLU14 -> Convolution16 (in-place)
I0825 11:09:03.301141  1476 net.cpp:172] Setting up ReLU14
I0825 11:09:03.301156  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.301159  1476 net.cpp:194] Memory required for data: 38625400
I0825 11:09:03.301164  1476 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:09:03.301177  1476 net.cpp:128] Creating Layer Convolution17
I0825 11:09:03.301185  1476 net.cpp:558] Convolution17 <- Convolution16
I0825 11:09:03.301193  1476 net.cpp:522] Convolution17 -> Convolution17
I0825 11:09:03.303076  1476 net.cpp:172] Setting up Convolution17
I0825 11:09:03.303102  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.303107  1476 net.cpp:194] Memory required for data: 38789240
I0825 11:09:03.303120  1476 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:09:03.303130  1476 net.cpp:128] Creating Layer BatchNorm17
I0825 11:09:03.303143  1476 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:09:03.303151  1476 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:09:03.303450  1476 net.cpp:172] Setting up BatchNorm17
I0825 11:09:03.303460  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.303465  1476 net.cpp:194] Memory required for data: 38953080
I0825 11:09:03.303475  1476 layer_factory.hpp:77] Creating layer Scale17
I0825 11:09:03.303486  1476 net.cpp:128] Creating Layer Scale17
I0825 11:09:03.303493  1476 net.cpp:558] Scale17 <- Convolution17
I0825 11:09:03.303499  1476 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:09:03.303550  1476 layer_factory.hpp:77] Creating layer Scale17
I0825 11:09:03.303715  1476 net.cpp:172] Setting up Scale17
I0825 11:09:03.303726  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.303730  1476 net.cpp:194] Memory required for data: 39116920
I0825 11:09:03.303738  1476 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:09:03.303745  1476 net.cpp:128] Creating Layer Eltwise7
I0825 11:09:03.303750  1476 net.cpp:558] Eltwise7 <- Convolution15
I0825 11:09:03.303755  1476 net.cpp:558] Eltwise7 <- Convolution17
I0825 11:09:03.303762  1476 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:09:03.303791  1476 net.cpp:172] Setting up Eltwise7
I0825 11:09:03.303800  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.303804  1476 net.cpp:194] Memory required for data: 39280760
I0825 11:09:03.303809  1476 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:09:03.303817  1476 net.cpp:128] Creating Layer ReLU15
I0825 11:09:03.303822  1476 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:09:03.303828  1476 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:09:03.304265  1476 net.cpp:172] Setting up ReLU15
I0825 11:09:03.304288  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.304296  1476 net.cpp:194] Memory required for data: 39444600
I0825 11:09:03.304301  1476 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:09:03.304308  1476 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:09:03.304313  1476 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:09:03.304322  1476 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:09:03.304332  1476 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:09:03.304389  1476 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:09:03.304396  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.304402  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.304406  1476 net.cpp:194] Memory required for data: 39772280
I0825 11:09:03.304410  1476 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:09:03.304425  1476 net.cpp:128] Creating Layer Convolution18
I0825 11:09:03.304430  1476 net.cpp:558] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0825 11:09:03.304437  1476 net.cpp:522] Convolution18 -> Convolution18
I0825 11:09:03.306493  1476 net.cpp:172] Setting up Convolution18
I0825 11:09:03.306519  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.306524  1476 net.cpp:194] Memory required for data: 39936120
I0825 11:09:03.306540  1476 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:09:03.306547  1476 net.cpp:128] Creating Layer BatchNorm18
I0825 11:09:03.306555  1476 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:09:03.306563  1476 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:09:03.306862  1476 net.cpp:172] Setting up BatchNorm18
I0825 11:09:03.306874  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.306877  1476 net.cpp:194] Memory required for data: 40099960
I0825 11:09:03.306887  1476 layer_factory.hpp:77] Creating layer Scale18
I0825 11:09:03.306896  1476 net.cpp:128] Creating Layer Scale18
I0825 11:09:03.306901  1476 net.cpp:558] Scale18 <- Convolution18
I0825 11:09:03.306906  1476 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:09:03.306958  1476 layer_factory.hpp:77] Creating layer Scale18
I0825 11:09:03.307126  1476 net.cpp:172] Setting up Scale18
I0825 11:09:03.307137  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.307142  1476 net.cpp:194] Memory required for data: 40263800
I0825 11:09:03.307164  1476 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:09:03.307173  1476 net.cpp:128] Creating Layer ReLU16
I0825 11:09:03.307178  1476 net.cpp:558] ReLU16 <- Convolution18
I0825 11:09:03.307184  1476 net.cpp:509] ReLU16 -> Convolution18 (in-place)
I0825 11:09:03.307428  1476 net.cpp:172] Setting up ReLU16
I0825 11:09:03.307445  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.307449  1476 net.cpp:194] Memory required for data: 40427640
I0825 11:09:03.307454  1476 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:09:03.307471  1476 net.cpp:128] Creating Layer Convolution19
I0825 11:09:03.307476  1476 net.cpp:558] Convolution19 <- Convolution18
I0825 11:09:03.307483  1476 net.cpp:522] Convolution19 -> Convolution19
I0825 11:09:03.309376  1476 net.cpp:172] Setting up Convolution19
I0825 11:09:03.309401  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.309406  1476 net.cpp:194] Memory required for data: 40591480
I0825 11:09:03.309415  1476 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:09:03.309425  1476 net.cpp:128] Creating Layer BatchNorm19
I0825 11:09:03.309432  1476 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:09:03.309445  1476 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:09:03.309736  1476 net.cpp:172] Setting up BatchNorm19
I0825 11:09:03.309747  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.309751  1476 net.cpp:194] Memory required for data: 40755320
I0825 11:09:03.309774  1476 layer_factory.hpp:77] Creating layer Scale19
I0825 11:09:03.309784  1476 net.cpp:128] Creating Layer Scale19
I0825 11:09:03.309789  1476 net.cpp:558] Scale19 <- Convolution19
I0825 11:09:03.309795  1476 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:09:03.309849  1476 layer_factory.hpp:77] Creating layer Scale19
I0825 11:09:03.310010  1476 net.cpp:172] Setting up Scale19
I0825 11:09:03.310021  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.310025  1476 net.cpp:194] Memory required for data: 40919160
I0825 11:09:03.310034  1476 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:09:03.310042  1476 net.cpp:128] Creating Layer Eltwise8
I0825 11:09:03.310047  1476 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:09:03.310053  1476 net.cpp:558] Eltwise8 <- Convolution19
I0825 11:09:03.310060  1476 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:09:03.310091  1476 net.cpp:172] Setting up Eltwise8
I0825 11:09:03.310097  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.310101  1476 net.cpp:194] Memory required for data: 41083000
I0825 11:09:03.310106  1476 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:09:03.310111  1476 net.cpp:128] Creating Layer ReLU17
I0825 11:09:03.310117  1476 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:09:03.310123  1476 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:09:03.310390  1476 net.cpp:172] Setting up ReLU17
I0825 11:09:03.310405  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.310410  1476 net.cpp:194] Memory required for data: 41246840
I0825 11:09:03.310415  1476 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:09:03.310422  1476 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:09:03.310427  1476 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:09:03.310436  1476 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:09:03.310446  1476 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:09:03.310503  1476 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:09:03.310513  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.310519  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.310523  1476 net.cpp:194] Memory required for data: 41574520
I0825 11:09:03.310528  1476 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:09:03.310539  1476 net.cpp:128] Creating Layer Convolution20
I0825 11:09:03.310544  1476 net.cpp:558] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0825 11:09:03.310554  1476 net.cpp:522] Convolution20 -> Convolution20
I0825 11:09:03.312623  1476 net.cpp:172] Setting up Convolution20
I0825 11:09:03.312649  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.312654  1476 net.cpp:194] Memory required for data: 41738360
I0825 11:09:03.312664  1476 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:09:03.312674  1476 net.cpp:128] Creating Layer BatchNorm20
I0825 11:09:03.312682  1476 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:09:03.312692  1476 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:09:03.312994  1476 net.cpp:172] Setting up BatchNorm20
I0825 11:09:03.313009  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.313012  1476 net.cpp:194] Memory required for data: 41902200
I0825 11:09:03.313022  1476 layer_factory.hpp:77] Creating layer Scale20
I0825 11:09:03.313032  1476 net.cpp:128] Creating Layer Scale20
I0825 11:09:03.313036  1476 net.cpp:558] Scale20 <- Convolution20
I0825 11:09:03.313042  1476 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:09:03.313096  1476 layer_factory.hpp:77] Creating layer Scale20
I0825 11:09:03.313262  1476 net.cpp:172] Setting up Scale20
I0825 11:09:03.313273  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.313277  1476 net.cpp:194] Memory required for data: 42066040
I0825 11:09:03.313285  1476 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:09:03.313292  1476 net.cpp:128] Creating Layer ReLU18
I0825 11:09:03.313297  1476 net.cpp:558] ReLU18 <- Convolution20
I0825 11:09:03.313304  1476 net.cpp:509] ReLU18 -> Convolution20 (in-place)
I0825 11:09:03.313549  1476 net.cpp:172] Setting up ReLU18
I0825 11:09:03.313565  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.313568  1476 net.cpp:194] Memory required for data: 42229880
I0825 11:09:03.313573  1476 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:09:03.313585  1476 net.cpp:128] Creating Layer Convolution21
I0825 11:09:03.313591  1476 net.cpp:558] Convolution21 <- Convolution20
I0825 11:09:03.313599  1476 net.cpp:522] Convolution21 -> Convolution21
I0825 11:09:03.316834  1476 net.cpp:172] Setting up Convolution21
I0825 11:09:03.316860  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.316865  1476 net.cpp:194] Memory required for data: 42393720
I0825 11:09:03.316875  1476 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:09:03.316886  1476 net.cpp:128] Creating Layer BatchNorm21
I0825 11:09:03.316892  1476 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:09:03.316898  1476 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:09:03.317190  1476 net.cpp:172] Setting up BatchNorm21
I0825 11:09:03.317201  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.317206  1476 net.cpp:194] Memory required for data: 42557560
I0825 11:09:03.317216  1476 layer_factory.hpp:77] Creating layer Scale21
I0825 11:09:03.317225  1476 net.cpp:128] Creating Layer Scale21
I0825 11:09:03.317230  1476 net.cpp:558] Scale21 <- Convolution21
I0825 11:09:03.317236  1476 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:09:03.317294  1476 layer_factory.hpp:77] Creating layer Scale21
I0825 11:09:03.317466  1476 net.cpp:172] Setting up Scale21
I0825 11:09:03.317476  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.317481  1476 net.cpp:194] Memory required for data: 42721400
I0825 11:09:03.317488  1476 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:09:03.317497  1476 net.cpp:128] Creating Layer Eltwise9
I0825 11:09:03.317503  1476 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:09:03.317508  1476 net.cpp:558] Eltwise9 <- Convolution21
I0825 11:09:03.317517  1476 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:09:03.317545  1476 net.cpp:172] Setting up Eltwise9
I0825 11:09:03.317555  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.317559  1476 net.cpp:194] Memory required for data: 42885240
I0825 11:09:03.317564  1476 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:09:03.317572  1476 net.cpp:128] Creating Layer ReLU19
I0825 11:09:03.317577  1476 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:09:03.317582  1476 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:09:03.318040  1476 net.cpp:172] Setting up ReLU19
I0825 11:09:03.318064  1476 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:09:03.318069  1476 net.cpp:194] Memory required for data: 43049080
I0825 11:09:03.318074  1476 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:09:03.318084  1476 net.cpp:128] Creating Layer Pooling1
I0825 11:09:03.318089  1476 net.cpp:558] Pooling1 <- Eltwise9
I0825 11:09:03.318095  1476 net.cpp:522] Pooling1 -> Pooling1
I0825 11:09:03.318387  1476 net.cpp:172] Setting up Pooling1
I0825 11:09:03.318403  1476 net.cpp:186] Top shape: 10 64 1 1 (640)
I0825 11:09:03.318408  1476 net.cpp:194] Memory required for data: 43051640
I0825 11:09:03.318413  1476 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:09:03.318425  1476 net.cpp:128] Creating Layer InnerProduct1
I0825 11:09:03.318430  1476 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:09:03.318439  1476 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:09:03.318615  1476 net.cpp:172] Setting up InnerProduct1
I0825 11:09:03.318626  1476 net.cpp:186] Top shape: 10 10 (100)
I0825 11:09:03.318630  1476 net.cpp:194] Memory required for data: 43052040
I0825 11:09:03.318640  1476 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0825 11:09:03.318647  1476 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0825 11:09:03.318652  1476 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0825 11:09:03.318660  1476 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0825 11:09:03.318670  1476 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0825 11:09:03.318718  1476 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0825 11:09:03.318725  1476 net.cpp:186] Top shape: 10 10 (100)
I0825 11:09:03.318730  1476 net.cpp:186] Top shape: 10 10 (100)
I0825 11:09:03.318734  1476 net.cpp:194] Memory required for data: 43052840
I0825 11:09:03.318738  1476 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:09:03.318747  1476 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:09:03.318753  1476 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0825 11:09:03.318758  1476 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0825 11:09:03.318766  1476 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:09:03.318776  1476 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:09:03.319156  1476 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:09:03.319175  1476 net.cpp:186] Top shape: (1)
I0825 11:09:03.319178  1476 net.cpp:189]     with loss weight 1
I0825 11:09:03.319195  1476 net.cpp:194] Memory required for data: 43052844
I0825 11:09:03.319200  1476 layer_factory.hpp:77] Creating layer Accuracy1
I0825 11:09:03.319209  1476 net.cpp:128] Creating Layer Accuracy1
I0825 11:09:03.319216  1476 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0825 11:09:03.319222  1476 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0825 11:09:03.319232  1476 net.cpp:522] Accuracy1 -> Accuracy1
I0825 11:09:03.319242  1476 net.cpp:172] Setting up Accuracy1
I0825 11:09:03.319250  1476 net.cpp:186] Top shape: (1)
I0825 11:09:03.319254  1476 net.cpp:194] Memory required for data: 43052848
I0825 11:09:03.319259  1476 net.cpp:303] Accuracy1 does not need backward computation.
I0825 11:09:03.319264  1476 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:09:03.319270  1476 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0825 11:09:03.319274  1476 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:09:03.319279  1476 net.cpp:301] Pooling1 needs backward computation.
I0825 11:09:03.319283  1476 net.cpp:301] ReLU19 needs backward computation.
I0825 11:09:03.319288  1476 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:09:03.319293  1476 net.cpp:301] Scale21 needs backward computation.
I0825 11:09:03.319298  1476 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:09:03.319315  1476 net.cpp:301] Convolution21 needs backward computation.
I0825 11:09:03.319320  1476 net.cpp:301] ReLU18 needs backward computation.
I0825 11:09:03.319325  1476 net.cpp:301] Scale20 needs backward computation.
I0825 11:09:03.319329  1476 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:09:03.319334  1476 net.cpp:301] Convolution20 needs backward computation.
I0825 11:09:03.319339  1476 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:09:03.319344  1476 net.cpp:301] ReLU17 needs backward computation.
I0825 11:09:03.319347  1476 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:09:03.319352  1476 net.cpp:301] Scale19 needs backward computation.
I0825 11:09:03.319357  1476 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:09:03.319361  1476 net.cpp:301] Convolution19 needs backward computation.
I0825 11:09:03.319365  1476 net.cpp:301] ReLU16 needs backward computation.
I0825 11:09:03.319370  1476 net.cpp:301] Scale18 needs backward computation.
I0825 11:09:03.319375  1476 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:09:03.319380  1476 net.cpp:301] Convolution18 needs backward computation.
I0825 11:09:03.319384  1476 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:09:03.319389  1476 net.cpp:301] ReLU15 needs backward computation.
I0825 11:09:03.319394  1476 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:09:03.319399  1476 net.cpp:301] Scale17 needs backward computation.
I0825 11:09:03.319404  1476 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:09:03.319409  1476 net.cpp:301] Convolution17 needs backward computation.
I0825 11:09:03.319413  1476 net.cpp:301] ReLU14 needs backward computation.
I0825 11:09:03.319418  1476 net.cpp:301] Scale16 needs backward computation.
I0825 11:09:03.319422  1476 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:09:03.319427  1476 net.cpp:301] Convolution16 needs backward computation.
I0825 11:09:03.319434  1476 net.cpp:301] Scale15 needs backward computation.
I0825 11:09:03.319439  1476 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:09:03.319444  1476 net.cpp:301] Convolution15 needs backward computation.
I0825 11:09:03.319449  1476 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:09:03.319453  1476 net.cpp:301] ReLU13 needs backward computation.
I0825 11:09:03.319458  1476 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:09:03.319463  1476 net.cpp:301] Scale14 needs backward computation.
I0825 11:09:03.319468  1476 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:09:03.319473  1476 net.cpp:301] Convolution14 needs backward computation.
I0825 11:09:03.319478  1476 net.cpp:301] ReLU12 needs backward computation.
I0825 11:09:03.319483  1476 net.cpp:301] Scale13 needs backward computation.
I0825 11:09:03.319486  1476 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:09:03.319491  1476 net.cpp:301] Convolution13 needs backward computation.
I0825 11:09:03.319496  1476 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:09:03.319501  1476 net.cpp:301] ReLU11 needs backward computation.
I0825 11:09:03.319505  1476 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:09:03.319511  1476 net.cpp:301] Scale12 needs backward computation.
I0825 11:09:03.319515  1476 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:09:03.319519  1476 net.cpp:301] Convolution12 needs backward computation.
I0825 11:09:03.319525  1476 net.cpp:301] ReLU10 needs backward computation.
I0825 11:09:03.319530  1476 net.cpp:301] Scale11 needs backward computation.
I0825 11:09:03.319535  1476 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:09:03.319538  1476 net.cpp:301] Convolution11 needs backward computation.
I0825 11:09:03.319543  1476 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:09:03.319548  1476 net.cpp:301] ReLU9 needs backward computation.
I0825 11:09:03.319553  1476 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:09:03.319569  1476 net.cpp:301] Scale10 needs backward computation.
I0825 11:09:03.319574  1476 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:09:03.319581  1476 net.cpp:301] Convolution10 needs backward computation.
I0825 11:09:03.319587  1476 net.cpp:301] ReLU8 needs backward computation.
I0825 11:09:03.319594  1476 net.cpp:301] Scale9 needs backward computation.
I0825 11:09:03.319599  1476 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:09:03.319604  1476 net.cpp:301] Convolution9 needs backward computation.
I0825 11:09:03.319624  1476 net.cpp:301] Scale8 needs backward computation.
I0825 11:09:03.319628  1476 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:09:03.319633  1476 net.cpp:301] Convolution8 needs backward computation.
I0825 11:09:03.319638  1476 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:09:03.319643  1476 net.cpp:301] ReLU7 needs backward computation.
I0825 11:09:03.319653  1476 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:09:03.319659  1476 net.cpp:301] Scale7 needs backward computation.
I0825 11:09:03.319667  1476 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:09:03.319671  1476 net.cpp:301] Convolution7 needs backward computation.
I0825 11:09:03.319676  1476 net.cpp:301] ReLU6 needs backward computation.
I0825 11:09:03.319684  1476 net.cpp:301] Scale6 needs backward computation.
I0825 11:09:03.319687  1476 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:09:03.319692  1476 net.cpp:301] Convolution6 needs backward computation.
I0825 11:09:03.319701  1476 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:09:03.319706  1476 net.cpp:301] ReLU5 needs backward computation.
I0825 11:09:03.319713  1476 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:09:03.319718  1476 net.cpp:301] Scale5 needs backward computation.
I0825 11:09:03.319725  1476 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:09:03.319730  1476 net.cpp:301] Convolution5 needs backward computation.
I0825 11:09:03.319736  1476 net.cpp:301] ReLU4 needs backward computation.
I0825 11:09:03.319742  1476 net.cpp:301] Scale4 needs backward computation.
I0825 11:09:03.319747  1476 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:09:03.319751  1476 net.cpp:301] Convolution4 needs backward computation.
I0825 11:09:03.319756  1476 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:09:03.319761  1476 net.cpp:301] ReLU3 needs backward computation.
I0825 11:09:03.319766  1476 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:09:03.319772  1476 net.cpp:301] Scale3 needs backward computation.
I0825 11:09:03.319777  1476 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:09:03.319782  1476 net.cpp:301] Convolution3 needs backward computation.
I0825 11:09:03.319785  1476 net.cpp:301] ReLU2 needs backward computation.
I0825 11:09:03.319790  1476 net.cpp:301] Scale2 needs backward computation.
I0825 11:09:03.319795  1476 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:09:03.319800  1476 net.cpp:301] Convolution2 needs backward computation.
I0825 11:09:03.319805  1476 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:09:03.319810  1476 net.cpp:301] ReLU1 needs backward computation.
I0825 11:09:03.319815  1476 net.cpp:301] Scale1 needs backward computation.
I0825 11:09:03.319819  1476 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:09:03.319824  1476 net.cpp:301] Convolution1 needs backward computation.
I0825 11:09:03.319829  1476 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0825 11:09:03.319835  1476 net.cpp:303] Data1 does not need backward computation.
I0825 11:09:03.319839  1476 net.cpp:348] This network produces output Accuracy1
I0825 11:09:03.319844  1476 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:09:03.319905  1476 net.cpp:363] Network initialization done.
I0825 11:09:03.320236  1476 solver.cpp:110] Solver scaffolding done.
I0825 11:09:03.328243  1476 caffe.cpp:313] Starting Optimization
I0825 11:09:03.328264  1476 solver.cpp:425] Solving resnet_cifar10
I0825 11:09:03.328269  1476 solver.cpp:427] Learning Rate Policy: multistep
I0825 11:09:03.331374  1476 solver.cpp:514] Iteration 0, Testing net (#0)
I0825 11:09:03.398329  1476 blocking_queue.cpp:49] Waiting for data
I0825 11:09:06.594322  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:09:06.606199  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 1
I0825 11:09:06.606240  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 87.3361 (* 1 = 87.3361 loss)
I0825 11:09:06.705579  1476 solver.cpp:357] Iteration 0 (0 iter/s, 3.37893s/100 iters), loss = 2.60566
I0825 11:09:06.705629  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 2.60566 (* 1 = 2.60566 loss)
I0825 11:09:06.705654  1476 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0825 11:09:17.316627  1476 solver.cpp:357] Iteration 100 (9.42058 iter/s, 10.6151s/100 iters), loss = 1.53558
I0825 11:09:17.316686  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.53558 (* 1 = 1.53558 loss)
I0825 11:09:17.316695  1476 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0825 11:09:28.274556  1476 solver.cpp:357] Iteration 200 (9.12327 iter/s, 10.961s/100 iters), loss = 1.24382
I0825 11:09:28.274793  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.24382 (* 1 = 1.24382 loss)
I0825 11:09:28.274806  1476 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0825 11:09:39.267191  1476 solver.cpp:357] Iteration 300 (9.09506 iter/s, 10.995s/100 iters), loss = 1.29268
I0825 11:09:39.267251  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.29268 (* 1 = 1.29268 loss)
I0825 11:09:39.267262  1476 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0825 11:09:48.425393  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:09:49.906230  1476 solver.cpp:357] Iteration 400 (9.39741 iter/s, 10.6412s/100 iters), loss = 1.15368
I0825 11:09:49.906288  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.15368 (* 1 = 1.15368 loss)
I0825 11:09:49.906299  1476 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0825 11:10:00.454778  1476 solver.cpp:514] Iteration 500, Testing net (#0)
I0825 11:10:03.678583  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:10:03.690687  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.397299
I0825 11:10:03.690729  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.69907 (* 1 = 1.69907 loss)
I0825 11:10:03.795042  1476 solver.cpp:357] Iteration 500 (7.19863 iter/s, 13.8915s/100 iters), loss = 0.956326
I0825 11:10:03.795085  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.956326 (* 1 = 0.956326 loss)
I0825 11:10:03.795099  1476 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0825 11:10:14.514230  1476 solver.cpp:357] Iteration 600 (9.39619 iter/s, 10.6426s/100 iters), loss = 0.962277
I0825 11:10:14.514291  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.962277 (* 1 = 0.962277 loss)
I0825 11:10:14.514302  1476 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0825 11:10:25.183990  1476 solver.cpp:357] Iteration 700 (9.3954 iter/s, 10.6435s/100 iters), loss = 0.749558
I0825 11:10:25.184048  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.749558 (* 1 = 0.749558 loss)
I0825 11:10:25.184062  1476 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0825 11:10:34.925225  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:10:38.755822  1476 solver.cpp:357] Iteration 800 (7.37367 iter/s, 13.5618s/100 iters), loss = 0.824482
I0825 11:10:38.755892  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.824482 (* 1 = 0.824482 loss)
I0825 11:10:38.755903  1476 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0825 11:10:55.647825  1476 solver.cpp:357] Iteration 900 (5.92062 iter/s, 16.8901s/100 iters), loss = 0.892733
I0825 11:10:55.647897  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.892733 (* 1 = 0.892733 loss)
I0825 11:10:55.647909  1476 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0825 11:11:16.877403  1476 solver.cpp:514] Iteration 1000, Testing net (#0)
I0825 11:11:40.517398  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:11:40.612434  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1468
I0825 11:11:40.612491  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.22234 (* 1 = 3.22234 loss)
I0825 11:11:40.891270  1476 solver.cpp:357] Iteration 1000 (2.211 iter/s, 45.2284s/100 iters), loss = 0.801607
I0825 11:11:40.891338  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.801607 (* 1 = 0.801607 loss)
I0825 11:11:40.891351  1476 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0825 11:12:12.444890  1476 solver.cpp:357] Iteration 1100 (3.15511 iter/s, 31.6946s/100 iters), loss = 0.626594
I0825 11:12:12.445103  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.626594 (* 1 = 0.626594 loss)
I0825 11:12:12.445133  1476 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0825 11:12:36.867030  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:12:51.926383  1476 solver.cpp:357] Iteration 1200 (2.53211 iter/s, 39.4927s/100 iters), loss = 0.662135
I0825 11:12:51.926600  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.662135 (* 1 = 0.662135 loss)
I0825 11:12:51.926645  1476 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0825 11:13:38.435310  1476 solver.cpp:357] Iteration 1300 (2.15226 iter/s, 46.4628s/100 iters), loss = 0.722805
I0825 11:13:38.435504  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.722805 (* 1 = 0.722805 loss)
I0825 11:13:38.435550  1476 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0825 11:14:24.812763  1476 solver.cpp:357] Iteration 1400 (2.15653 iter/s, 46.3708s/100 iters), loss = 0.619417
I0825 11:14:24.813001  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.619417 (* 1 = 0.619417 loss)
I0825 11:14:24.813050  1476 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0825 11:15:11.770519  1476 solver.cpp:514] Iteration 1500, Testing net (#0)
I0825 11:15:52.770028  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:15:52.989686  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.117699
I0825 11:15:52.989821  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.53238 (* 1 = 3.53238 loss)
I0825 11:15:53.262871  1476 solver.cpp:357] Iteration 1500 (1.13066 iter/s, 88.4436s/100 iters), loss = 0.647449
I0825 11:15:53.263041  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.647449 (* 1 = 0.647449 loss)
I0825 11:15:53.263084  1476 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0825 11:16:20.631101  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:16:39.001143  1476 solver.cpp:357] Iteration 1600 (2.18655 iter/s, 45.7341s/100 iters), loss = 0.572375
I0825 11:16:39.001346  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.572375 (* 1 = 0.572375 loss)
I0825 11:16:39.001392  1476 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0825 11:17:24.448827  1476 solver.cpp:357] Iteration 1700 (2.20043 iter/s, 45.4457s/100 iters), loss = 0.723371
I0825 11:17:24.449066  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.723371 (* 1 = 0.723371 loss)
I0825 11:17:24.449113  1476 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0825 11:18:11.819988  1476 solver.cpp:357] Iteration 1800 (2.11143 iter/s, 47.3612s/100 iters), loss = 0.754044
I0825 11:18:11.820219  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.754044 (* 1 = 0.754044 loss)
I0825 11:18:11.820266  1476 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0825 11:18:59.259284  1476 solver.cpp:357] Iteration 1900 (2.10838 iter/s, 47.4298s/100 iters), loss = 0.589552
I0825 11:18:59.259546  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.589552 (* 1 = 0.589552 loss)
I0825 11:18:59.259577  1476 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0825 11:19:22.626348  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:19:46.313560  1476 solver.cpp:514] Iteration 2000, Testing net (#0)
I0825 11:20:27.177157  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:20:27.397800  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.126199
I0825 11:20:27.397927  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.13578 (* 1 = 3.13578 loss)
I0825 11:20:27.850085  1476 solver.cpp:357] Iteration 2000 (1.12892 iter/s, 88.5799s/100 iters), loss = 0.505596
I0825 11:20:27.850256  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.505596 (* 1 = 0.505596 loss)
I0825 11:20:27.850301  1476 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0825 11:21:15.061048  1476 solver.cpp:357] Iteration 2100 (2.1184 iter/s, 47.2054s/100 iters), loss = 0.704882
I0825 11:21:15.061287  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.704882 (* 1 = 0.704882 loss)
I0825 11:21:15.061333  1476 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0825 11:22:01.297209  1476 solver.cpp:357] Iteration 2200 (2.16306 iter/s, 46.2308s/100 iters), loss = 0.650588
I0825 11:22:01.297443  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.650588 (* 1 = 0.650588 loss)
I0825 11:22:01.297489  1476 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0825 11:22:46.553190  1476 solver.cpp:357] Iteration 2300 (2.20973 iter/s, 45.2544s/100 iters), loss = 0.513434
I0825 11:22:46.553354  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513434 (* 1 = 0.513434 loss)
I0825 11:22:46.553398  1476 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0825 11:23:05.358765  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:23:34.127928  1476 solver.cpp:357] Iteration 2400 (2.10206 iter/s, 47.5724s/100 iters), loss = 0.53941
I0825 11:23:34.128156  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53941 (* 1 = 0.53941 loss)
I0825 11:23:34.128203  1476 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0825 11:24:18.441211  1476 solver.cpp:514] Iteration 2500, Testing net (#0)
I0825 11:24:57.903488  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:24:58.023170  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2212
I0825 11:24:58.023286  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.49173 (* 1 = 2.49173 loss)
I0825 11:24:58.329066  1476 solver.cpp:357] Iteration 2500 (1.18767 iter/s, 84.1986s/100 iters), loss = 0.659402
I0825 11:24:58.329231  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.659402 (* 1 = 0.659402 loss)
I0825 11:24:58.329277  1476 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0825 11:25:45.071251  1476 solver.cpp:357] Iteration 2600 (2.13957 iter/s, 46.7383s/100 iters), loss = 0.687021
I0825 11:25:45.071496  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.687021 (* 1 = 0.687021 loss)
I0825 11:25:45.071543  1476 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0825 11:26:31.734949  1476 solver.cpp:357] Iteration 2700 (2.14328 iter/s, 46.6575s/100 iters), loss = 0.785994
I0825 11:26:31.735196  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.785994 (* 1 = 0.785994 loss)
I0825 11:26:31.735244  1476 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0825 11:26:46.106182  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:27:19.221992  1476 solver.cpp:357] Iteration 2800 (2.10612 iter/s, 47.4806s/100 iters), loss = 0.56135
I0825 11:27:19.222250  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.56135 (* 1 = 0.56135 loss)
I0825 11:27:19.222297  1476 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0825 11:28:06.606858  1476 solver.cpp:357] Iteration 2900 (2.11058 iter/s, 47.3804s/100 iters), loss = 0.527527
I0825 11:28:06.607033  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.527527 (* 1 = 0.527527 loss)
I0825 11:28:06.607064  1476 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0825 11:28:53.420724  1476 solver.cpp:514] Iteration 3000, Testing net (#0)
I0825 11:29:34.986444  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:29:35.235095  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2105
I0825 11:29:35.235415  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.54377 (* 1 = 2.54377 loss)
I0825 11:29:35.536967  1476 solver.cpp:357] Iteration 3000 (1.12457 iter/s, 88.9232s/100 iters), loss = 0.563417
I0825 11:29:35.537367  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.563417 (* 1 = 0.563417 loss)
I0825 11:29:35.537545  1476 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0825 11:30:22.862956  1476 solver.cpp:357] Iteration 3100 (2.11311 iter/s, 47.3235s/100 iters), loss = 0.464926
I0825 11:30:22.863178  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464926 (* 1 = 0.464926 loss)
I0825 11:30:22.863225  1476 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0825 11:30:32.581027  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:31:10.237869  1476 solver.cpp:357] Iteration 3200 (2.11123 iter/s, 47.3658s/100 iters), loss = 0.54449
I0825 11:31:10.238114  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.54449 (* 1 = 0.54449 loss)
I0825 11:31:10.238160  1476 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0825 11:31:53.872272  1476 solver.cpp:357] Iteration 3300 (2.29226 iter/s, 43.625s/100 iters), loss = 0.394187
I0825 11:31:53.872495  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394187 (* 1 = 0.394187 loss)
I0825 11:31:53.872541  1476 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0825 11:32:37.306099  1476 solver.cpp:357] Iteration 3400 (2.30282 iter/s, 43.425s/100 iters), loss = 0.488337
I0825 11:32:37.306349  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488337 (* 1 = 0.488337 loss)
I0825 11:32:37.306396  1476 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0825 11:33:23.633196  1476 solver.cpp:514] Iteration 3500, Testing net (#0)
I0825 11:34:06.217696  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:34:06.371692  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.417499
I0825 11:34:06.371788  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.77545 (* 1 = 1.77545 loss)
I0825 11:34:06.752471  1476 solver.cpp:357] Iteration 3500 (1.11814 iter/s, 89.4346s/100 iters), loss = 0.562971
I0825 11:34:06.752641  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.562971 (* 1 = 0.562971 loss)
I0825 11:34:06.752687  1476 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0825 11:34:12.165144  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:34:54.176573  1476 solver.cpp:357] Iteration 3600 (2.10891 iter/s, 47.4178s/100 iters), loss = 0.649171
I0825 11:34:54.176803  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.649171 (* 1 = 0.649171 loss)
I0825 11:34:54.176851  1476 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0825 11:35:41.309592  1476 solver.cpp:357] Iteration 3700 (2.12198 iter/s, 47.1257s/100 iters), loss = 0.443883
I0825 11:35:41.309969  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443883 (* 1 = 0.443883 loss)
I0825 11:35:41.310063  1476 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0825 11:36:28.611450  1476 solver.cpp:357] Iteration 3800 (2.11439 iter/s, 47.2949s/100 iters), loss = 0.402719
I0825 11:36:28.611631  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402719 (* 1 = 0.402719 loss)
I0825 11:36:28.611678  1476 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0825 11:37:15.955754  1476 solver.cpp:357] Iteration 3900 (2.11258 iter/s, 47.3355s/100 iters), loss = 0.672022
I0825 11:37:15.955971  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.672022 (* 1 = 0.672022 loss)
I0825 11:37:15.956015  1476 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0825 11:37:17.125982  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:38:02.777052  1476 solver.cpp:514] Iteration 4000, Testing net (#0)
I0825 11:38:43.239289  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:38:43.368414  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.352
I0825 11:38:43.368551  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.71942 (* 1 = 1.71942 loss)
I0825 11:38:43.695551  1476 solver.cpp:357] Iteration 4000 (1.13981 iter/s, 87.7338s/100 iters), loss = 0.5463
I0825 11:38:43.695739  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.5463 (* 1 = 0.5463 loss)
I0825 11:38:43.695785  1476 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0825 11:39:27.229729  1476 solver.cpp:357] Iteration 4100 (2.29736 iter/s, 43.5281s/100 iters), loss = 0.446714
I0825 11:39:27.229959  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446714 (* 1 = 0.446714 loss)
I0825 11:39:27.230005  1476 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0825 11:40:13.108419  1476 solver.cpp:357] Iteration 4200 (2.17995 iter/s, 45.8727s/100 iters), loss = 0.639578
I0825 11:40:13.108599  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.639578 (* 1 = 0.639578 loss)
I0825 11:40:13.108628  1476 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0825 11:40:56.948354  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:41:00.646633  1476 solver.cpp:357] Iteration 4300 (2.10383 iter/s, 47.5323s/100 iters), loss = 0.427236
I0825 11:41:00.646800  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.427236 (* 1 = 0.427236 loss)
I0825 11:41:00.646844  1476 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0825 11:41:47.930603  1476 solver.cpp:357] Iteration 4400 (2.11515 iter/s, 47.2781s/100 iters), loss = 0.52875
I0825 11:41:47.930830  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52875 (* 1 = 0.52875 loss)
I0825 11:41:47.930874  1476 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0825 11:42:32.766850  1476 solver.cpp:514] Iteration 4500, Testing net (#0)
I0825 11:43:14.370873  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:43:14.459614  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4705
I0825 11:43:14.459686  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.49002 (* 1 = 1.49002 loss)
I0825 11:43:14.749516  1476 solver.cpp:357] Iteration 4500 (1.15194 iter/s, 86.8099s/100 iters), loss = 0.596099
I0825 11:43:14.749676  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.596099 (* 1 = 0.596099 loss)
I0825 11:43:14.749722  1476 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0825 11:44:02.348886  1476 solver.cpp:357] Iteration 4600 (2.10112 iter/s, 47.5937s/100 iters), loss = 0.362858
I0825 11:44:02.349098  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.362858 (* 1 = 0.362858 loss)
I0825 11:44:02.349145  1476 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0825 11:44:41.896294  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:44:49.848132  1476 solver.cpp:357] Iteration 4700 (2.10555 iter/s, 47.4936s/100 iters), loss = 0.389823
I0825 11:44:49.848300  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389823 (* 1 = 0.389823 loss)
I0825 11:44:49.848345  1476 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0825 11:45:36.899242  1476 solver.cpp:357] Iteration 4800 (2.12551 iter/s, 47.0476s/100 iters), loss = 0.425553
I0825 11:45:36.899461  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425553 (* 1 = 0.425553 loss)
I0825 11:45:36.899519  1476 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0825 11:46:24.299960  1476 solver.cpp:357] Iteration 4900 (2.10983 iter/s, 47.3972s/100 iters), loss = 0.495101
I0825 11:46:24.300150  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495101 (* 1 = 0.495101 loss)
I0825 11:46:24.300180  1476 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0825 11:47:07.820065  1476 solver.cpp:514] Iteration 5000, Testing net (#0)
I0825 11:47:47.870640  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:47:48.011350  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.502
I0825 11:47:48.011447  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.70936 (* 1 = 1.70936 loss)
I0825 11:47:48.340014  1476 solver.cpp:357] Iteration 5000 (1.18998 iter/s, 84.0348s/100 iters), loss = 0.389723
I0825 11:47:48.340198  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389723 (* 1 = 0.389723 loss)
I0825 11:47:48.340245  1476 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0825 11:48:23.411371  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:48:35.552963  1476 solver.cpp:357] Iteration 5100 (2.11764 iter/s, 47.2224s/100 iters), loss = 0.600831
I0825 11:48:35.553344  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.600831 (* 1 = 0.600831 loss)
I0825 11:48:35.553514  1476 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0825 11:49:22.797188  1476 solver.cpp:357] Iteration 5200 (2.11616 iter/s, 47.2554s/100 iters), loss = 0.678875
I0825 11:49:22.797405  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.678875 (* 1 = 0.678875 loss)
I0825 11:49:22.797451  1476 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0825 11:50:10.143662  1476 solver.cpp:357] Iteration 5300 (2.11175 iter/s, 47.3541s/100 iters), loss = 0.528001
I0825 11:50:10.143873  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.528001 (* 1 = 0.528001 loss)
I0825 11:50:10.143919  1476 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0825 11:50:56.329919  1476 solver.cpp:357] Iteration 5400 (2.16507 iter/s, 46.1879s/100 iters), loss = 0.380515
I0825 11:50:56.330128  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380515 (* 1 = 0.380515 loss)
I0825 11:50:56.330174  1476 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0825 11:51:26.340565  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:51:42.701874  1476 solver.cpp:514] Iteration 5500, Testing net (#0)
I0825 11:52:23.860476  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:52:24.012804  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5845
I0825 11:52:24.012912  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.16028 (* 1 = 1.16028 loss)
I0825 11:52:24.254623  1476 solver.cpp:357] Iteration 5500 (1.13721 iter/s, 87.9348s/100 iters), loss = 0.464544
I0825 11:52:24.254789  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464544 (* 1 = 0.464544 loss)
I0825 11:52:24.254835  1476 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0825 11:53:09.917538  1476 solver.cpp:357] Iteration 5600 (2.18982 iter/s, 45.6658s/100 iters), loss = 0.468455
I0825 11:53:09.917762  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468455 (* 1 = 0.468455 loss)
I0825 11:53:09.917807  1476 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0825 11:53:56.847005  1476 solver.cpp:357] Iteration 5700 (2.13085 iter/s, 46.9296s/100 iters), loss = 0.470867
I0825 11:53:56.847148  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.470867 (* 1 = 0.470867 loss)
I0825 11:53:56.847193  1476 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0825 11:54:41.130872  1476 solver.cpp:357] Iteration 5800 (2.25819 iter/s, 44.2833s/100 iters), loss = 0.313191
I0825 11:54:41.131098  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313191 (* 1 = 0.313191 loss)
I0825 11:54:41.131145  1476 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0825 11:55:06.661082  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:55:27.683861  1476 solver.cpp:357] Iteration 5900 (2.14814 iter/s, 46.552s/100 iters), loss = 0.643674
I0825 11:55:27.684087  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.643674 (* 1 = 0.643674 loss)
I0825 11:55:27.684132  1476 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0825 11:56:14.664556  1476 solver.cpp:514] Iteration 6000, Testing net (#0)
I0825 11:56:57.673406  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:56:57.864095  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4195
I0825 11:56:57.864236  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.72117 (* 1 = 2.72117 loss)
I0825 11:56:58.235584  1476 solver.cpp:357] Iteration 6000 (1.10432 iter/s, 90.5531s/100 iters), loss = 0.448378
I0825 11:56:58.235752  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448378 (* 1 = 0.448378 loss)
I0825 11:56:58.235798  1476 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0825 11:57:45.703713  1476 solver.cpp:357] Iteration 6100 (2.10677 iter/s, 47.4661s/100 iters), loss = 0.51399
I0825 11:57:45.703896  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.51399 (* 1 = 0.51399 loss)
I0825 11:57:45.703923  1476 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0825 11:58:33.008683  1476 solver.cpp:357] Iteration 6200 (2.11404 iter/s, 47.3027s/100 iters), loss = 0.461309
I0825 11:58:33.008877  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461309 (* 1 = 0.461309 loss)
I0825 11:58:33.008906  1476 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0825 11:58:54.744210  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:59:20.597045  1476 solver.cpp:357] Iteration 6300 (2.10156 iter/s, 47.5838s/100 iters), loss = 0.461228
I0825 11:59:20.597224  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461228 (* 1 = 0.461228 loss)
I0825 11:59:20.597252  1476 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0825 12:00:08.336105  1476 solver.cpp:357] Iteration 6400 (2.09484 iter/s, 47.7364s/100 iters), loss = 0.489408
I0825 12:00:08.336324  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.489408 (* 1 = 0.489408 loss)
I0825 12:00:08.336369  1476 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0825 12:00:55.441484  1476 solver.cpp:514] Iteration 6500, Testing net (#0)
I0825 12:01:37.557582  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:01:37.706066  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4736
I0825 12:01:37.706207  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.92315 (* 1 = 1.92315 loss)
I0825 12:01:38.110942  1476 solver.cpp:357] Iteration 6500 (1.11392 iter/s, 89.7733s/100 iters), loss = 0.369508
I0825 12:01:38.111095  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369508 (* 1 = 0.369508 loss)
I0825 12:01:38.111141  1476 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0825 12:02:21.105202  1476 solver.cpp:357] Iteration 6600 (2.32606 iter/s, 42.9911s/100 iters), loss = 0.496596
I0825 12:02:21.105336  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496596 (* 1 = 0.496596 loss)
I0825 12:02:21.105366  1476 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0825 12:02:37.759080  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:03:05.548097  1476 solver.cpp:357] Iteration 6700 (2.25014 iter/s, 44.4418s/100 iters), loss = 0.495238
I0825 12:03:05.548602  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495238 (* 1 = 0.495238 loss)
I0825 12:03:05.548784  1476 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0825 12:03:51.176028  1476 solver.cpp:357] Iteration 6800 (2.19174 iter/s, 45.626s/100 iters), loss = 0.449279
I0825 12:03:51.176224  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449279 (* 1 = 0.449279 loss)
I0825 12:03:51.176254  1476 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0825 12:04:38.371155  1476 solver.cpp:357] Iteration 6900 (2.1191 iter/s, 47.1898s/100 iters), loss = 0.430965
I0825 12:04:38.371436  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430965 (* 1 = 0.430965 loss)
I0825 12:04:38.371465  1476 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0825 12:05:25.659205  1476 solver.cpp:514] Iteration 7000, Testing net (#0)
I0825 12:06:06.864284  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:06:07.102210  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4032
I0825 12:06:07.102313  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.9796 (* 1 = 1.9796 loss)
I0825 12:06:07.463261  1476 solver.cpp:357] Iteration 7000 (1.12251 iter/s, 89.0858s/100 iters), loss = 0.40978
I0825 12:06:07.463420  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40978 (* 1 = 0.40978 loss)
I0825 12:06:07.463465  1476 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0825 12:06:20.395473  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:06:55.100033  1476 solver.cpp:357] Iteration 7100 (2.09941 iter/s, 47.6325s/100 iters), loss = 0.543493
I0825 12:06:55.100579  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543493 (* 1 = 0.543493 loss)
I0825 12:06:55.100700  1476 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0825 12:07:42.431535  1476 solver.cpp:357] Iteration 7200 (2.11304 iter/s, 47.3252s/100 iters), loss = 0.675762
I0825 12:07:42.431749  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.675762 (* 1 = 0.675762 loss)
I0825 12:07:42.431794  1476 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0825 12:08:29.759109  1476 solver.cpp:357] Iteration 7300 (2.11312 iter/s, 47.3234s/100 iters), loss = 0.537813
I0825 12:08:29.759542  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.537813 (* 1 = 0.537813 loss)
I0825 12:08:29.759660  1476 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0825 12:09:16.405135  1476 solver.cpp:357] Iteration 7400 (2.14406 iter/s, 46.6405s/100 iters), loss = 0.499032
I0825 12:09:16.405330  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499032 (* 1 = 0.499032 loss)
I0825 12:09:16.405376  1476 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0825 12:09:24.143359  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:10:00.678544  1476 solver.cpp:514] Iteration 7500, Testing net (#0)
I0825 12:10:43.031266  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:10:43.251386  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5861
I0825 12:10:43.251529  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.46454 (* 1 = 1.46454 loss)
I0825 12:10:43.518429  1476 solver.cpp:357] Iteration 7500 (1.14801 iter/s, 87.1075s/100 iters), loss = 0.400388
I0825 12:10:43.518563  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400388 (* 1 = 0.400388 loss)
I0825 12:10:43.518604  1476 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0825 12:11:30.798518  1476 solver.cpp:357] Iteration 7600 (2.11505 iter/s, 47.2801s/100 iters), loss = 0.468682
I0825 12:11:30.798933  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468682 (* 1 = 0.468682 loss)
I0825 12:11:30.799048  1476 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0825 12:12:17.377642  1476 solver.cpp:357] Iteration 7700 (2.14716 iter/s, 46.5731s/100 iters), loss = 0.415012
I0825 12:12:17.377876  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415012 (* 1 = 0.415012 loss)
I0825 12:12:17.377923  1476 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0825 12:13:02.728268  1476 solver.cpp:357] Iteration 7800 (2.20524 iter/s, 45.3466s/100 iters), loss = 0.471831
I0825 12:13:02.728471  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471831 (* 1 = 0.471831 loss)
I0825 12:13:02.728515  1476 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0825 12:13:06.594791  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:13:49.517117  1476 solver.cpp:357] Iteration 7900 (2.13734 iter/s, 46.787s/100 iters), loss = 0.422236
I0825 12:13:49.517406  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422236 (* 1 = 0.422236 loss)
I0825 12:13:49.517469  1476 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0825 12:14:36.610287  1476 solver.cpp:514] Iteration 8000, Testing net (#0)
I0825 12:15:19.483183  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:15:19.711755  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6552
I0825 12:15:19.711863  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.993437 (* 1 = 0.993437 loss)
I0825 12:15:20.087973  1476 solver.cpp:357] Iteration 8000 (1.10415 iter/s, 90.5675s/100 iters), loss = 0.414199
I0825 12:15:20.088150  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414199 (* 1 = 0.414199 loss)
I0825 12:15:20.088196  1476 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0825 12:16:06.930585  1476 solver.cpp:357] Iteration 8100 (2.13508 iter/s, 46.8367s/100 iters), loss = 0.353602
I0825 12:16:06.930907  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353602 (* 1 = 0.353602 loss)
I0825 12:16:06.930955  1476 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0825 12:16:50.564939  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:16:50.924631  1476 solver.cpp:357] Iteration 8200 (2.27313 iter/s, 43.9922s/100 iters), loss = 0.456524
I0825 12:16:50.924796  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456524 (* 1 = 0.456524 loss)
I0825 12:16:50.924841  1476 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0825 12:17:36.187158  1476 solver.cpp:357] Iteration 8300 (2.20941 iter/s, 45.261s/100 iters), loss = 0.481966
I0825 12:17:36.187347  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.481966 (* 1 = 0.481966 loss)
I0825 12:17:36.187392  1476 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0825 12:18:23.914705  1476 solver.cpp:357] Iteration 8400 (2.0953 iter/s, 47.7259s/100 iters), loss = 0.437272
I0825 12:18:23.914978  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437272 (* 1 = 0.437272 loss)
I0825 12:18:23.915025  1476 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0825 12:19:10.942987  1476 solver.cpp:514] Iteration 8500, Testing net (#0)
I0825 12:19:52.061559  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:19:52.168412  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7275
I0825 12:19:52.168532  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.797426 (* 1 = 0.797426 loss)
I0825 12:19:52.435901  1476 solver.cpp:357] Iteration 8500 (1.12969 iter/s, 88.52s/100 iters), loss = 0.324011
I0825 12:19:52.436064  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.324011 (* 1 = 0.324011 loss)
I0825 12:19:52.436108  1476 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0825 12:20:34.841464  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:20:39.905067  1476 solver.cpp:357] Iteration 8600 (2.1068 iter/s, 47.4654s/100 iters), loss = 0.558251
I0825 12:20:39.905320  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.558251 (* 1 = 0.558251 loss)
I0825 12:20:39.905334  1476 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0825 12:21:27.484819  1476 solver.cpp:357] Iteration 8700 (2.102 iter/s, 47.5738s/100 iters), loss = 0.387588
I0825 12:21:27.485033  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387588 (* 1 = 0.387588 loss)
I0825 12:21:27.485080  1476 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0825 12:22:14.986130  1476 solver.cpp:357] Iteration 8800 (2.10537 iter/s, 47.4976s/100 iters), loss = 0.53049
I0825 12:22:14.986340  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53049 (* 1 = 0.53049 loss)
I0825 12:22:14.986388  1476 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0825 12:23:00.262635  1476 solver.cpp:357] Iteration 8900 (2.20894 iter/s, 45.2706s/100 iters), loss = 0.516032
I0825 12:23:00.262868  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.516032 (* 1 = 0.516032 loss)
I0825 12:23:00.262915  1476 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0825 12:23:37.422006  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:23:46.421506  1476 solver.cpp:514] Iteration 9000, Testing net (#0)
I0825 12:24:25.702450  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:24:25.846927  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.605001
I0825 12:24:25.847041  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.36948 (* 1 = 1.36948 loss)
I0825 12:24:26.130018  1476 solver.cpp:357] Iteration 9000 (1.16464 iter/s, 85.8637s/100 iters), loss = 0.296258
I0825 12:24:26.130187  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296258 (* 1 = 0.296258 loss)
I0825 12:24:26.130231  1476 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0825 12:25:11.891973  1476 solver.cpp:357] Iteration 9100 (2.1854 iter/s, 45.7582s/100 iters), loss = 0.466184
I0825 12:25:11.894091  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.466184 (* 1 = 0.466184 loss)
I0825 12:25:11.894278  1476 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0825 12:25:59.202600  1476 solver.cpp:357] Iteration 9200 (2.11395 iter/s, 47.3048s/100 iters), loss = 0.40404
I0825 12:25:59.202788  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40404 (* 1 = 0.40404 loss)
I0825 12:25:59.202832  1476 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0825 12:26:46.814642  1476 solver.cpp:357] Iteration 9300 (2.10029 iter/s, 47.6124s/100 iters), loss = 0.261782
I0825 12:26:46.815016  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.261782 (* 1 = 0.261782 loss)
I0825 12:26:46.815109  1476 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0825 12:27:20.146855  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:27:34.255749  1476 solver.cpp:357] Iteration 9400 (2.10787 iter/s, 47.4413s/100 iters), loss = 0.53758
I0825 12:27:34.255928  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53758 (* 1 = 0.53758 loss)
I0825 12:27:34.255976  1476 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0825 12:28:20.831567  1476 solver.cpp:514] Iteration 9500, Testing net (#0)
I0825 12:29:01.882159  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:29:02.078269  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6706
I0825 12:29:02.078392  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.97358 (* 1 = 0.97358 loss)
I0825 12:29:02.303220  1476 solver.cpp:357] Iteration 9500 (1.13582 iter/s, 88.0423s/100 iters), loss = 0.486431
I0825 12:29:02.303395  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486431 (* 1 = 0.486431 loss)
I0825 12:29:02.303442  1476 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0825 12:29:49.388094  1476 solver.cpp:357] Iteration 9600 (2.12399 iter/s, 47.0812s/100 iters), loss = 0.409604
I0825 12:29:49.388622  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409604 (* 1 = 0.409604 loss)
I0825 12:29:49.388800  1476 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0825 12:30:36.805619  1476 solver.cpp:357] Iteration 9700 (2.10918 iter/s, 47.4118s/100 iters), loss = 0.406568
I0825 12:30:36.805824  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.406568 (* 1 = 0.406568 loss)
I0825 12:30:36.805868  1476 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0825 12:31:06.041286  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:31:24.328608  1476 solver.cpp:357] Iteration 9800 (2.10432 iter/s, 47.5214s/100 iters), loss = 0.457692
I0825 12:31:24.329074  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457692 (* 1 = 0.457692 loss)
I0825 12:31:24.329250  1476 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0825 12:32:09.032675  1476 solver.cpp:357] Iteration 9900 (2.23701 iter/s, 44.7025s/100 iters), loss = 0.411
I0825 12:32:09.032897  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.411 (* 1 = 0.411 loss)
I0825 12:32:09.032943  1476 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0825 12:32:52.767866  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.caffemodel
I0825 12:32:52.835530  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.solverstate
I0825 12:32:52.838791  1476 solver.cpp:514] Iteration 10000, Testing net (#0)
I0825 12:33:34.430341  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:33:34.660385  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.563001
I0825 12:33:34.660532  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.98426 (* 1 = 1.98426 loss)
I0825 12:33:34.957310  1476 solver.cpp:357] Iteration 10000 (1.16385 iter/s, 85.9215s/100 iters), loss = 0.384114
I0825 12:33:34.957463  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384114 (* 1 = 0.384114 loss)
I0825 12:33:34.957509  1476 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0825 12:34:22.415865  1476 solver.cpp:357] Iteration 10100 (2.10717 iter/s, 47.457s/100 iters), loss = 0.474571
I0825 12:34:22.416188  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474571 (* 1 = 0.474571 loss)
I0825 12:34:22.416218  1476 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0825 12:34:47.096581  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:35:09.705305  1476 solver.cpp:357] Iteration 10200 (2.1148 iter/s, 47.2858s/100 iters), loss = 0.44778
I0825 12:35:09.705549  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44778 (* 1 = 0.44778 loss)
I0825 12:35:09.705595  1476 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0825 12:35:57.247575  1476 solver.cpp:357] Iteration 10300 (2.10356 iter/s, 47.5386s/100 iters), loss = 0.351721
I0825 12:35:57.247781  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351721 (* 1 = 0.351721 loss)
I0825 12:35:57.247825  1476 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0825 12:36:44.561453  1476 solver.cpp:357] Iteration 10400 (2.11361 iter/s, 47.3123s/100 iters), loss = 0.464026
I0825 12:36:44.561686  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464026 (* 1 = 0.464026 loss)
I0825 12:36:44.561731  1476 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0825 12:37:31.664965  1476 solver.cpp:514] Iteration 10500, Testing net (#0)
I0825 12:38:14.655839  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:38:14.794934  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6897
I0825 12:38:14.795032  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05612 (* 1 = 1.05612 loss)
I0825 12:38:15.149014  1476 solver.cpp:357] Iteration 10500 (1.10394 iter/s, 90.5845s/100 iters), loss = 0.525598
I0825 12:38:15.149179  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.525598 (* 1 = 0.525598 loss)
I0825 12:38:15.149224  1476 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0825 12:38:35.240597  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:39:02.585813  1476 solver.cpp:357] Iteration 10600 (2.10833 iter/s, 47.431s/100 iters), loss = 0.466935
I0825 12:39:02.586031  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.466935 (* 1 = 0.466935 loss)
I0825 12:39:02.586076  1476 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0825 12:39:46.808697  1476 solver.cpp:357] Iteration 10700 (2.26124 iter/s, 44.2236s/100 iters), loss = 0.374225
I0825 12:39:46.810431  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374225 (* 1 = 0.374225 loss)
I0825 12:39:46.810464  1476 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0825 12:40:33.637547  1476 solver.cpp:357] Iteration 10800 (2.13549 iter/s, 46.8277s/100 iters), loss = 0.348777
I0825 12:40:33.637742  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348777 (* 1 = 0.348777 loss)
I0825 12:40:33.637769  1476 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0825 12:41:20.659468  1476 solver.cpp:357] Iteration 10900 (2.12664 iter/s, 47.0226s/100 iters), loss = 0.442332
I0825 12:41:20.659624  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442332 (* 1 = 0.442332 loss)
I0825 12:41:20.659669  1476 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0825 12:41:36.066689  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:42:06.294816  1476 solver.cpp:514] Iteration 11000, Testing net (#0)
I0825 12:42:46.072319  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:42:46.266842  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5183
I0825 12:42:46.266993  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32493 (* 1 = 1.32493 loss)
I0825 12:42:46.591092  1476 solver.cpp:357] Iteration 11000 (1.16365 iter/s, 85.9364s/100 iters), loss = 0.351416
I0825 12:42:46.591269  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351416 (* 1 = 0.351416 loss)
I0825 12:42:46.591315  1476 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0825 12:43:31.898404  1476 solver.cpp:357] Iteration 11100 (2.2072 iter/s, 45.3063s/100 iters), loss = 0.446272
I0825 12:43:31.899001  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446272 (* 1 = 0.446272 loss)
I0825 12:43:31.899175  1476 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0825 12:44:19.305320  1476 solver.cpp:357] Iteration 11200 (2.10935 iter/s, 47.408s/100 iters), loss = 0.375156
I0825 12:44:19.305573  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375156 (* 1 = 0.375156 loss)
I0825 12:44:19.305621  1476 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0825 12:45:06.947398  1476 solver.cpp:357] Iteration 11300 (2.09901 iter/s, 47.6415s/100 iters), loss = 0.477569
I0825 12:45:06.953516  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477569 (* 1 = 0.477569 loss)
I0825 12:45:06.953621  1476 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0825 12:45:18.368171  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:45:54.377684  1476 solver.cpp:357] Iteration 11400 (2.10855 iter/s, 47.426s/100 iters), loss = 0.397448
I0825 12:45:54.377871  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397448 (* 1 = 0.397448 loss)
I0825 12:45:54.377899  1476 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0825 12:46:41.182536  1476 solver.cpp:514] Iteration 11500, Testing net (#0)
I0825 12:47:20.453080  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:47:20.648959  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.617201
I0825 12:47:20.649051  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.28016 (* 1 = 1.28016 loss)
I0825 12:47:21.037366  1476 solver.cpp:357] Iteration 11500 (1.15392 iter/s, 86.6614s/100 iters), loss = 0.477111
I0825 12:47:21.037516  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477111 (* 1 = 0.477111 loss)
I0825 12:47:21.037562  1476 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0825 12:48:08.246438  1476 solver.cpp:357] Iteration 11600 (2.1182 iter/s, 47.2099s/100 iters), loss = 0.514245
I0825 12:48:08.246613  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.514245 (* 1 = 0.514245 loss)
I0825 12:48:08.246642  1476 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0825 12:48:55.669842  1476 solver.cpp:357] Iteration 11700 (2.10882 iter/s, 47.4199s/100 iters), loss = 0.459818
I0825 12:48:55.670078  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.459818 (* 1 = 0.459818 loss)
I0825 12:48:55.670125  1476 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0825 12:49:02.584074  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:49:43.193488  1476 solver.cpp:357] Iteration 11800 (2.10428 iter/s, 47.5222s/100 iters), loss = 0.615621
I0825 12:49:43.193706  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.615621 (* 1 = 0.615621 loss)
I0825 12:49:43.193751  1476 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0825 12:50:30.610124  1476 solver.cpp:357] Iteration 11900 (2.10904 iter/s, 47.415s/100 iters), loss = 0.444304
I0825 12:50:30.610319  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444304 (* 1 = 0.444304 loss)
I0825 12:50:30.610357  1476 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0825 12:51:17.748777  1476 solver.cpp:514] Iteration 12000, Testing net (#0)
I0825 12:52:00.890544  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:52:01.063896  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7093
I0825 12:52:01.064003  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.974827 (* 1 = 0.974827 loss)
I0825 12:52:01.349201  1476 solver.cpp:357] Iteration 12000 (1.10208 iter/s, 90.7375s/100 iters), loss = 0.447593
I0825 12:52:01.349369  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.447593 (* 1 = 0.447593 loss)
I0825 12:52:01.349414  1476 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0825 12:52:48.022330  1476 solver.cpp:357] Iteration 12100 (2.14265 iter/s, 46.6712s/100 iters), loss = 0.457996
I0825 12:52:48.022637  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457996 (* 1 = 0.457996 loss)
I0825 12:52:48.022686  1476 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0825 12:52:50.459792  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:53:32.311866  1476 solver.cpp:357] Iteration 12200 (2.25798 iter/s, 44.2874s/100 iters), loss = 0.440445
I0825 12:53:32.312098  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440445 (* 1 = 0.440445 loss)
I0825 12:53:32.312144  1476 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0825 12:54:17.801515  1476 solver.cpp:357] Iteration 12300 (2.19841 iter/s, 45.4875s/100 iters), loss = 0.366552
I0825 12:54:17.801718  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366552 (* 1 = 0.366552 loss)
I0825 12:54:17.801764  1476 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0825 12:55:00.897284  1476 solver.cpp:357] Iteration 12400 (2.32053 iter/s, 43.0936s/100 iters), loss = 0.589652
I0825 12:55:00.897439  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.589652 (* 1 = 0.589652 loss)
I0825 12:55:00.897467  1476 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0825 12:55:46.008533  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:55:47.889983  1476 solver.cpp:514] Iteration 12500, Testing net (#0)
I0825 12:56:30.758471  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:56:30.932281  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5128
I0825 12:56:30.932411  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.72624 (* 1 = 1.72624 loss)
I0825 12:56:31.181599  1476 solver.cpp:357] Iteration 12500 (1.10762 iter/s, 90.284s/100 iters), loss = 0.425021
I0825 12:56:31.181763  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425021 (* 1 = 0.425021 loss)
I0825 12:56:31.181807  1476 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0825 12:57:18.693886  1476 solver.cpp:357] Iteration 12600 (2.10473 iter/s, 47.5121s/100 iters), loss = 0.415193
I0825 12:57:18.694056  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415193 (* 1 = 0.415193 loss)
I0825 12:57:18.694087  1476 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0825 12:58:05.697578  1476 solver.cpp:357] Iteration 12700 (2.1276 iter/s, 47.0013s/100 iters), loss = 0.544715
I0825 12:58:05.697821  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544715 (* 1 = 0.544715 loss)
I0825 12:58:05.697867  1476 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0825 12:58:53.139801  1476 solver.cpp:357] Iteration 12800 (2.10793 iter/s, 47.4399s/100 iters), loss = 0.44008
I0825 12:58:53.140027  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44008 (* 1 = 0.44008 loss)
I0825 12:58:53.140072  1476 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0825 12:59:34.399586  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:59:40.907564  1476 solver.cpp:357] Iteration 12900 (2.09362 iter/s, 47.7641s/100 iters), loss = 0.544978
I0825 12:59:40.907752  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544978 (* 1 = 0.544978 loss)
I0825 12:59:40.907796  1476 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0825 13:00:27.952466  1476 solver.cpp:514] Iteration 13000, Testing net (#0)
I0825 13:01:10.933876  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:01:11.137570  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5964
I0825 13:01:11.137673  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.67514 (* 1 = 1.67514 loss)
I0825 13:01:11.605935  1476 solver.cpp:357] Iteration 13000 (1.10254 iter/s, 90.6997s/100 iters), loss = 0.390061
I0825 13:01:11.606065  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390061 (* 1 = 0.390061 loss)
I0825 13:01:11.606097  1476 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0825 13:01:57.652047  1476 solver.cpp:357] Iteration 13100 (2.1719 iter/s, 46.0426s/100 iters), loss = 0.443009
I0825 13:01:57.652318  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443009 (* 1 = 0.443009 loss)
I0825 13:01:57.652367  1476 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0825 13:02:42.057502  1476 solver.cpp:357] Iteration 13200 (2.25211 iter/s, 44.4028s/100 iters), loss = 0.387277
I0825 13:02:42.057898  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387277 (* 1 = 0.387277 loss)
I0825 13:02:42.057992  1476 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0825 13:03:16.603963  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:03:26.935837  1476 solver.cpp:357] Iteration 13300 (2.22838 iter/s, 44.8757s/100 iters), loss = 0.416951
I0825 13:03:26.936008  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416951 (* 1 = 0.416951 loss)
I0825 13:03:26.936054  1476 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0825 13:04:13.422628  1476 solver.cpp:357] Iteration 13400 (2.15127 iter/s, 46.4842s/100 iters), loss = 0.565585
I0825 13:04:13.422811  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.565585 (* 1 = 0.565585 loss)
I0825 13:04:13.422840  1476 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0825 13:05:00.818919  1476 solver.cpp:514] Iteration 13500, Testing net (#0)
I0825 13:05:42.022879  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:05:42.244504  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6295
I0825 13:05:42.244647  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.22378 (* 1 = 1.22378 loss)
I0825 13:05:42.618141  1476 solver.cpp:357] Iteration 13500 (1.12117 iter/s, 89.1923s/100 iters), loss = 0.455738
I0825 13:05:42.618355  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.455738 (* 1 = 0.455738 loss)
I0825 13:05:42.618400  1476 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0825 13:06:28.181277  1476 solver.cpp:357] Iteration 13600 (2.19479 iter/s, 45.5624s/100 iters), loss = 0.336414
I0825 13:06:28.181828  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.336414 (* 1 = 0.336414 loss)
I0825 13:06:28.182008  1476 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0825 13:06:59.605774  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:07:14.975419  1476 solver.cpp:357] Iteration 13700 (2.13714 iter/s, 46.7914s/100 iters), loss = 0.445545
I0825 13:07:14.975677  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445545 (* 1 = 0.445545 loss)
I0825 13:07:14.975689  1476 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0825 13:08:02.743330  1476 solver.cpp:357] Iteration 13800 (2.09358 iter/s, 47.7652s/100 iters), loss = 0.349365
I0825 13:08:02.743867  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.349365 (* 1 = 0.349365 loss)
I0825 13:08:02.744047  1476 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0825 13:08:49.972998  1476 solver.cpp:357] Iteration 13900 (2.11753 iter/s, 47.2248s/100 iters), loss = 0.326349
I0825 13:08:49.973194  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326349 (* 1 = 0.326349 loss)
I0825 13:08:49.973232  1476 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0825 13:09:35.457872  1476 solver.cpp:514] Iteration 14000, Testing net (#0)
I0825 13:10:15.561506  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:10:15.689237  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7062
I0825 13:10:15.689337  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.973163 (* 1 = 0.973163 loss)
I0825 13:10:16.033972  1476 solver.cpp:357] Iteration 14000 (1.16196 iter/s, 86.0617s/100 iters), loss = 0.400679
I0825 13:10:16.034128  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400679 (* 1 = 0.400679 loss)
I0825 13:10:16.034171  1476 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0825 13:10:43.980963  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:11:03.753206  1476 solver.cpp:357] Iteration 14100 (2.09571 iter/s, 47.7165s/100 iters), loss = 0.312718
I0825 13:11:03.753479  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312718 (* 1 = 0.312718 loss)
I0825 13:11:03.753526  1476 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0825 13:11:50.846884  1476 solver.cpp:357] Iteration 14200 (2.12355 iter/s, 47.0909s/100 iters), loss = 0.435773
I0825 13:11:50.847074  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435773 (* 1 = 0.435773 loss)
I0825 13:11:50.847105  1476 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0825 13:12:38.231505  1476 solver.cpp:357] Iteration 14300 (2.11051 iter/s, 47.3819s/100 iters), loss = 0.546101
I0825 13:12:38.231748  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.546101 (* 1 = 0.546101 loss)
I0825 13:12:38.231794  1476 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0825 13:13:23.254863  1476 solver.cpp:357] Iteration 14400 (2.22121 iter/s, 45.0205s/100 iters), loss = 0.39372
I0825 13:13:23.255091  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39372 (* 1 = 0.39372 loss)
I0825 13:13:23.255138  1476 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0825 13:13:45.433403  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:14:09.005792  1476 solver.cpp:514] Iteration 14500, Testing net (#0)
I0825 13:14:51.932410  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:14:52.143261  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6785
I0825 13:14:52.143409  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.10753 (* 1 = 1.10753 loss)
I0825 13:14:52.430959  1476 solver.cpp:357] Iteration 14500 (1.12152 iter/s, 89.1649s/100 iters), loss = 0.359091
I0825 13:14:52.431092  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359091 (* 1 = 0.359091 loss)
I0825 13:14:52.431133  1476 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0825 13:15:39.841958  1476 solver.cpp:357] Iteration 14600 (2.10944 iter/s, 47.4059s/100 iters), loss = 0.537399
I0825 13:15:39.842124  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.537399 (* 1 = 0.537399 loss)
I0825 13:15:39.842154  1476 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0825 13:16:27.169450  1476 solver.cpp:357] Iteration 14700 (2.11342 iter/s, 47.3167s/100 iters), loss = 0.411614
I0825 13:16:27.169675  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.411614 (* 1 = 0.411614 loss)
I0825 13:16:27.169723  1476 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0825 13:17:12.344828  1476 solver.cpp:357] Iteration 14800 (2.21409 iter/s, 45.1652s/100 iters), loss = 0.338181
I0825 13:17:12.345058  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338181 (* 1 = 0.338181 loss)
I0825 13:17:12.345105  1476 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0825 13:17:29.625283  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:17:57.736733  1476 solver.cpp:357] Iteration 14900 (2.20341 iter/s, 45.3842s/100 iters), loss = 0.337331
I0825 13:17:57.736964  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.337331 (* 1 = 0.337331 loss)
I0825 13:17:57.737010  1476 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0825 13:18:44.428064  1476 solver.cpp:514] Iteration 15000, Testing net (#0)
I0825 13:19:26.135031  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:19:26.225174  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6547
I0825 13:19:26.225281  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.12619 (* 1 = 1.12619 loss)
I0825 13:19:26.558409  1476 solver.cpp:357] Iteration 15000 (1.12598 iter/s, 88.8117s/100 iters), loss = 0.513183
I0825 13:19:26.558593  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513183 (* 1 = 0.513183 loss)
I0825 13:19:26.558639  1476 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0825 13:20:14.107568  1476 solver.cpp:357] Iteration 15100 (2.10339 iter/s, 47.5422s/100 iters), loss = 0.453032
I0825 13:20:14.108170  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453032 (* 1 = 0.453032 loss)
I0825 13:20:14.108353  1476 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0825 13:21:01.986627  1476 solver.cpp:357] Iteration 15200 (2.08898 iter/s, 47.8703s/100 iters), loss = 0.601942
I0825 13:21:01.986837  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.601942 (* 1 = 0.601942 loss)
I0825 13:21:01.986884  1476 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0825 13:21:16.416260  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:21:49.322459  1476 solver.cpp:357] Iteration 15300 (2.11294 iter/s, 47.3273s/100 iters), loss = 0.361653
I0825 13:21:49.322934  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361653 (* 1 = 0.361653 loss)
I0825 13:21:49.323050  1476 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0825 13:22:36.815080  1476 solver.cpp:357] Iteration 15400 (2.10582 iter/s, 47.4873s/100 iters), loss = 0.463403
I0825 13:22:36.815286  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.463403 (* 1 = 0.463403 loss)
I0825 13:22:36.815320  1476 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0825 13:23:21.595317  1476 solver.cpp:514] Iteration 15500, Testing net (#0)
I0825 13:24:01.067577  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:24:01.229997  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6274
I0825 13:24:01.230121  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.10715 (* 1 = 1.10715 loss)
I0825 13:24:01.467166  1476 solver.cpp:357] Iteration 15500 (1.1814 iter/s, 84.6451s/100 iters), loss = 0.424638
I0825 13:24:01.467314  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424638 (* 1 = 0.424638 loss)
I0825 13:24:01.467358  1476 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0825 13:24:45.625335  1476 solver.cpp:357] Iteration 15600 (2.26476 iter/s, 44.1547s/100 iters), loss = 0.37207
I0825 13:24:45.625865  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.37207 (* 1 = 0.37207 loss)
I0825 13:24:45.626045  1476 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0825 13:24:54.599771  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:25:31.940958  1476 solver.cpp:357] Iteration 15700 (2.15935 iter/s, 46.3102s/100 iters), loss = 0.443623
I0825 13:25:31.941164  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443623 (* 1 = 0.443623 loss)
I0825 13:25:31.941210  1476 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0825 13:26:19.504228  1476 solver.cpp:357] Iteration 15800 (2.1026 iter/s, 47.5601s/100 iters), loss = 0.480327
I0825 13:26:19.504428  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480327 (* 1 = 0.480327 loss)
I0825 13:26:19.504474  1476 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0825 13:27:07.107326  1476 solver.cpp:357] Iteration 15900 (2.10102 iter/s, 47.5959s/100 iters), loss = 0.290287
I0825 13:27:07.109088  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.290287 (* 1 = 0.290287 loss)
I0825 13:27:07.109138  1476 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0825 13:27:53.807960  1476 solver.cpp:514] Iteration 16000, Testing net (#0)
I0825 13:28:35.283085  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:28:35.390470  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.535101
I0825 13:28:35.390600  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.98758 (* 1 = 1.98758 loss)
I0825 13:28:35.751528  1476 solver.cpp:357] Iteration 16000 (1.1282 iter/s, 88.6366s/100 iters), loss = 0.430995
I0825 13:28:35.751708  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430995 (* 1 = 0.430995 loss)
I0825 13:28:35.751754  1476 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0825 13:28:41.387508  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:29:23.291133  1476 solver.cpp:357] Iteration 16100 (2.10382 iter/s, 47.5327s/100 iters), loss = 0.516614
I0825 13:29:23.291394  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.516614 (* 1 = 0.516614 loss)
I0825 13:29:23.291440  1476 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0825 13:30:10.807875  1476 solver.cpp:357] Iteration 16200 (2.10482 iter/s, 47.5099s/100 iters), loss = 0.345755
I0825 13:30:10.808377  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.345755 (* 1 = 0.345755 loss)
I0825 13:30:10.808553  1476 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0825 13:30:58.193205  1476 solver.cpp:357] Iteration 16300 (2.11047 iter/s, 47.3828s/100 iters), loss = 0.359548
I0825 13:30:58.193424  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359548 (* 1 = 0.359548 loss)
I0825 13:30:58.193472  1476 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0825 13:31:44.290405  1476 solver.cpp:357] Iteration 16400 (2.16964 iter/s, 46.0906s/100 iters), loss = 0.443545
I0825 13:31:44.290611  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443546 (* 1 = 0.443546 loss)
I0825 13:31:44.290645  1476 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0825 13:31:45.367969  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:32:27.912560  1476 solver.cpp:514] Iteration 16500, Testing net (#0)
I0825 13:33:08.709615  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:33:08.903537  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4693
I0825 13:33:08.903682  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.53174 (* 1 = 2.53174 loss)
I0825 13:33:09.313352  1476 solver.cpp:357] Iteration 16500 (1.17624 iter/s, 85.0168s/100 iters), loss = 0.386379
I0825 13:33:09.313516  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386379 (* 1 = 0.386379 loss)
I0825 13:33:09.313560  1476 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0825 13:33:54.674816  1476 solver.cpp:357] Iteration 16600 (2.20473 iter/s, 45.3571s/100 iters), loss = 0.421611
I0825 13:33:54.675027  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421611 (* 1 = 0.421611 loss)
I0825 13:33:54.675073  1476 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0825 13:34:42.319991  1476 solver.cpp:357] Iteration 16700 (2.09904 iter/s, 47.6409s/100 iters), loss = 0.440751
I0825 13:34:42.320173  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440751 (* 1 = 0.440751 loss)
I0825 13:34:42.320204  1476 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0825 13:35:26.209481  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:35:29.752717  1476 solver.cpp:357] Iteration 16800 (2.10826 iter/s, 47.4326s/100 iters), loss = 0.424568
I0825 13:35:29.753141  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424568 (* 1 = 0.424568 loss)
I0825 13:35:29.753319  1476 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0825 13:36:17.453671  1476 solver.cpp:357] Iteration 16900 (2.09658 iter/s, 47.6968s/100 iters), loss = 0.388189
I0825 13:36:17.453896  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38819 (* 1 = 0.38819 loss)
I0825 13:36:17.453941  1476 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0825 13:37:04.602103  1476 solver.cpp:514] Iteration 17000, Testing net (#0)
I0825 13:37:45.548673  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:37:45.730798  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5548
I0825 13:37:45.730938  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.50798 (* 1 = 1.50798 loss)
I0825 13:37:46.064807  1476 solver.cpp:357] Iteration 17000 (1.12858 iter/s, 88.6073s/100 iters), loss = 0.399154
I0825 13:37:46.064975  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399154 (* 1 = 0.399154 loss)
I0825 13:37:46.065019  1476 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0825 13:38:33.720764  1476 solver.cpp:357] Iteration 17100 (2.09855 iter/s, 47.6519s/100 iters), loss = 0.42551
I0825 13:38:33.721063  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42551 (* 1 = 0.42551 loss)
I0825 13:38:33.721110  1476 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0825 13:39:13.147161  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:39:20.458483  1476 solver.cpp:357] Iteration 17200 (2.13979 iter/s, 46.7336s/100 iters), loss = 0.340698
I0825 13:39:20.458758  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340698 (* 1 = 0.340698 loss)
I0825 13:39:20.458853  1476 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0825 13:40:04.574105  1476 solver.cpp:357] Iteration 17300 (2.26708 iter/s, 44.1095s/100 iters), loss = 0.380995
I0825 13:40:04.574348  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380995 (* 1 = 0.380995 loss)
I0825 13:40:04.574395  1476 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0825 13:40:51.841042  1476 solver.cpp:357] Iteration 17400 (2.11584 iter/s, 47.2627s/100 iters), loss = 0.366747
I0825 13:40:51.841570  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366747 (* 1 = 0.366747 loss)
I0825 13:40:51.841747  1476 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0825 13:41:39.066785  1476 solver.cpp:514] Iteration 17500, Testing net (#0)
I0825 13:42:22.390117  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:42:22.570492  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6159
I0825 13:42:22.570629  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20191 (* 1 = 1.20191 loss)
I0825 13:42:22.852855  1476 solver.cpp:357] Iteration 17500 (1.09878 iter/s, 91.0104s/100 iters), loss = 0.306226
I0825 13:42:22.853015  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.306226 (* 1 = 0.306226 loss)
I0825 13:42:22.853060  1476 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0825 13:42:57.793762  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:43:09.347935  1476 solver.cpp:357] Iteration 17600 (2.15095 iter/s, 46.4911s/100 iters), loss = 0.351816
I0825 13:43:09.348124  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351816 (* 1 = 0.351816 loss)
I0825 13:43:09.348170  1476 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0825 13:43:48.906636  1476 solver.cpp:357] Iteration 17700 (2.52828 iter/s, 39.5526s/100 iters), loss = 0.62852
I0825 13:43:48.906826  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.62852 (* 1 = 0.62852 loss)
I0825 13:43:48.906860  1476 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0825 13:44:35.253021  1476 solver.cpp:357] Iteration 17800 (2.15775 iter/s, 46.3446s/100 iters), loss = 0.445544
I0825 13:44:35.253211  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445544 (* 1 = 0.445544 loss)
I0825 13:44:35.253242  1476 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0825 13:45:22.137667  1476 solver.cpp:357] Iteration 17900 (2.13307 iter/s, 46.8808s/100 iters), loss = 0.285818
I0825 13:45:22.137857  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.285818 (* 1 = 0.285818 loss)
I0825 13:45:22.137888  1476 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0825 13:45:52.795205  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:46:09.448398  1476 solver.cpp:514] Iteration 18000, Testing net (#0)
I0825 13:46:50.543607  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:46:50.660420  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6747
I0825 13:46:50.660559  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.10895 (* 1 = 1.10895 loss)
I0825 13:46:50.923137  1476 solver.cpp:357] Iteration 18000 (1.12638 iter/s, 88.78s/100 iters), loss = 0.316836
I0825 13:46:50.923283  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.316836 (* 1 = 0.316836 loss)
I0825 13:46:50.923328  1476 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0825 13:47:35.001229  1476 solver.cpp:357] Iteration 18100 (2.26879 iter/s, 44.0763s/100 iters), loss = 0.530282
I0825 13:47:35.001801  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530282 (* 1 = 0.530282 loss)
I0825 13:47:35.001982  1476 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0825 13:48:22.444944  1476 solver.cpp:357] Iteration 18200 (2.10792 iter/s, 47.4402s/100 iters), loss = 0.40291
I0825 13:48:22.445156  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40291 (* 1 = 0.40291 loss)
I0825 13:48:22.445199  1476 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0825 13:49:09.677880  1476 solver.cpp:357] Iteration 18300 (2.11704 iter/s, 47.2358s/100 iters), loss = 0.306551
I0825 13:49:09.678115  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.306551 (* 1 = 0.306551 loss)
I0825 13:49:09.678162  1476 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0825 13:49:36.064968  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:49:57.290231  1476 solver.cpp:357] Iteration 18400 (2.10035 iter/s, 47.6111s/100 iters), loss = 0.602306
I0825 13:49:57.290477  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.602306 (* 1 = 0.602306 loss)
I0825 13:49:57.290508  1476 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0825 13:50:44.044559  1476 solver.cpp:514] Iteration 18500, Testing net (#0)
I0825 13:51:25.700173  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:51:25.883774  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6574
I0825 13:51:25.884110  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05542 (* 1 = 1.05542 loss)
I0825 13:51:26.334203  1476 solver.cpp:357] Iteration 18500 (1.12303 iter/s, 89.0448s/100 iters), loss = 0.440052
I0825 13:51:26.334516  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440052 (* 1 = 0.440052 loss)
I0825 13:51:26.334611  1476 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0825 13:52:13.850479  1476 solver.cpp:357] Iteration 18600 (2.10462 iter/s, 47.5145s/100 iters), loss = 0.483662
I0825 13:52:13.850718  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483662 (* 1 = 0.483662 loss)
I0825 13:52:13.850764  1476 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0825 13:53:00.925374  1476 solver.cpp:357] Iteration 18700 (2.12435 iter/s, 47.0732s/100 iters), loss = 0.252787
I0825 13:53:00.925860  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.252787 (* 1 = 0.252787 loss)
I0825 13:53:00.926038  1476 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0825 13:53:21.564106  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:53:46.331836  1476 solver.cpp:357] Iteration 18800 (2.20233 iter/s, 45.4065s/100 iters), loss = 0.335844
I0825 13:53:46.333426  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335844 (* 1 = 0.335844 loss)
I0825 13:53:46.333475  1476 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0825 13:54:31.169823  1476 solver.cpp:357] Iteration 18900 (2.23026 iter/s, 44.8378s/100 iters), loss = 0.458199
I0825 13:54:31.170039  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458199 (* 1 = 0.458199 loss)
I0825 13:54:31.170086  1476 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0825 13:55:15.704366  1476 solver.cpp:514] Iteration 19000, Testing net (#0)
I0825 13:55:58.436678  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:55:58.576391  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5941
I0825 13:55:58.576514  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.33681 (* 1 = 1.33681 loss)
I0825 13:55:58.972643  1476 solver.cpp:357] Iteration 19000 (1.1389 iter/s, 87.8044s/100 iters), loss = 0.429824
I0825 13:55:58.972828  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429824 (* 1 = 0.429824 loss)
I0825 13:55:58.972873  1476 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0825 13:56:45.778548  1476 solver.cpp:357] Iteration 19100 (2.13669 iter/s, 46.8013s/100 iters), loss = 0.407141
I0825 13:56:45.779081  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407141 (* 1 = 0.407141 loss)
I0825 13:56:45.779273  1476 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0825 13:57:01.776485  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:57:31.782872  1476 solver.cpp:357] Iteration 19200 (2.17373 iter/s, 46.0038s/100 iters), loss = 0.449231
I0825 13:57:31.783192  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449231 (* 1 = 0.449231 loss)
I0825 13:57:31.783241  1476 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0825 13:58:19.197621  1476 solver.cpp:357] Iteration 19300 (2.10907 iter/s, 47.4143s/100 iters), loss = 0.29725
I0825 13:58:19.198184  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29725 (* 1 = 0.29725 loss)
I0825 13:58:19.198380  1476 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0825 13:59:06.655506  1476 solver.cpp:357] Iteration 19400 (2.10715 iter/s, 47.4574s/100 iters), loss = 0.349153
I0825 13:59:06.655670  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.349153 (* 1 = 0.349153 loss)
I0825 13:59:06.655696  1476 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0825 13:59:53.622092  1476 solver.cpp:514] Iteration 19500, Testing net (#0)
I0825 14:00:34.716886  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:00:34.901331  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5672
I0825 14:00:34.901432  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.45403 (* 1 = 1.45403 loss)
I0825 14:00:35.202051  1476 solver.cpp:357] Iteration 19500 (1.12935 iter/s, 88.5468s/100 iters), loss = 0.410817
I0825 14:00:35.202224  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410817 (* 1 = 0.410817 loss)
I0825 14:00:35.202271  1476 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0825 14:00:48.186731  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:01:22.535265  1476 solver.cpp:357] Iteration 19600 (2.1128 iter/s, 47.3305s/100 iters), loss = 0.369314
I0825 14:01:22.535504  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369314 (* 1 = 0.369314 loss)
I0825 14:01:22.535552  1476 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0825 14:02:08.544673  1476 solver.cpp:357] Iteration 19700 (2.17361 iter/s, 46.0065s/100 iters), loss = 0.496278
I0825 14:02:08.545210  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496278 (* 1 = 0.496278 loss)
I0825 14:02:08.545388  1476 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0825 14:02:53.401883  1476 solver.cpp:357] Iteration 19800 (2.22933 iter/s, 44.8564s/100 iters), loss = 0.355089
I0825 14:02:53.402112  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355089 (* 1 = 0.355089 loss)
I0825 14:02:53.402158  1476 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0825 14:03:38.661370  1476 solver.cpp:357] Iteration 19900 (2.20952 iter/s, 45.2586s/100 iters), loss = 0.435211
I0825 14:03:38.661919  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435211 (* 1 = 0.435211 loss)
I0825 14:03:38.662098  1476 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0825 14:03:46.343448  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:04:24.122355  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.caffemodel
I0825 14:04:24.136862  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.solverstate
I0825 14:04:24.140095  1476 solver.cpp:514] Iteration 20000, Testing net (#0)
I0825 14:05:07.165714  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:05:07.325052  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7247
I0825 14:05:07.325176  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.827372 (* 1 = 0.827372 loss)
I0825 14:05:07.706652  1476 solver.cpp:357] Iteration 20000 (1.12307 iter/s, 89.0415s/100 iters), loss = 0.408138
I0825 14:05:07.706758  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408138 (* 1 = 0.408138 loss)
I0825 14:05:07.706784  1476 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0825 14:05:55.352890  1476 solver.cpp:357] Iteration 20100 (2.09893 iter/s, 47.6433s/100 iters), loss = 0.372958
I0825 14:05:55.353186  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372958 (* 1 = 0.372958 loss)
I0825 14:05:55.353233  1476 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0825 14:06:43.073251  1476 solver.cpp:357] Iteration 20200 (2.09558 iter/s, 47.7195s/100 iters), loss = 0.452367
I0825 14:06:43.073421  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.452367 (* 1 = 0.452367 loss)
I0825 14:06:43.073449  1476 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0825 14:07:30.561923  1476 solver.cpp:357] Iteration 20300 (2.10581 iter/s, 47.4878s/100 iters), loss = 0.468246
I0825 14:07:30.562481  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468246 (* 1 = 0.468246 loss)
I0825 14:07:30.562667  1476 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0825 14:07:34.385619  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:08:17.873734  1476 solver.cpp:357] Iteration 20400 (2.11377 iter/s, 47.3088s/100 iters), loss = 0.31473
I0825 14:08:17.873950  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31473 (* 1 = 0.31473 loss)
I0825 14:08:17.873996  1476 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0825 14:09:03.459353  1476 solver.cpp:514] Iteration 20500, Testing net (#0)
I0825 14:09:41.941256  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:09:42.114737  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6719
I0825 14:09:42.114954  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.25107 (* 1 = 1.25107 loss)
I0825 14:09:42.470393  1476 solver.cpp:357] Iteration 20500 (1.18211 iter/s, 84.5946s/100 iters), loss = 0.348398
I0825 14:09:42.470558  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348398 (* 1 = 0.348398 loss)
I0825 14:09:42.470604  1476 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0825 14:10:27.702457  1476 solver.cpp:357] Iteration 20600 (2.21098 iter/s, 45.2289s/100 iters), loss = 0.34105
I0825 14:10:27.702672  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.34105 (* 1 = 0.34105 loss)
I0825 14:10:27.702720  1476 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0825 14:11:15.046450  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:11:15.321339  1476 solver.cpp:357] Iteration 20700 (2.10014 iter/s, 47.6158s/100 iters), loss = 0.449266
I0825 14:11:15.321518  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449266 (* 1 = 0.449266 loss)
I0825 14:11:15.321564  1476 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0825 14:12:02.283254  1476 solver.cpp:357] Iteration 20800 (2.12953 iter/s, 46.9588s/100 iters), loss = 0.421395
I0825 14:12:02.283507  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421395 (* 1 = 0.421395 loss)
I0825 14:12:02.283556  1476 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0825 14:12:49.647358  1476 solver.cpp:357] Iteration 20900 (2.11154 iter/s, 47.3589s/100 iters), loss = 0.397895
I0825 14:12:49.647877  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397895 (* 1 = 0.397895 loss)
I0825 14:12:49.648056  1476 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0825 14:13:35.286795  1476 solver.cpp:514] Iteration 21000, Testing net (#0)
I0825 14:14:15.569269  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:14:15.788812  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.425999
I0825 14:14:15.789153  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.68684 (* 1 = 2.68684 loss)
I0825 14:14:16.080601  1476 solver.cpp:357] Iteration 21000 (1.15696 iter/s, 86.4334s/100 iters), loss = 0.337413
I0825 14:14:16.080955  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.337414 (* 1 = 0.337414 loss)
I0825 14:14:16.081123  1476 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0825 14:14:58.450482  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:15:03.656719  1476 solver.cpp:357] Iteration 21100 (2.10186 iter/s, 47.5769s/100 iters), loss = 0.472204
I0825 14:15:03.656867  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472204 (* 1 = 0.472204 loss)
I0825 14:15:03.656903  1476 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0825 14:15:51.166016  1476 solver.cpp:357] Iteration 21200 (2.10508 iter/s, 47.504s/100 iters), loss = 0.371693
I0825 14:15:51.166265  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371693 (* 1 = 0.371693 loss)
I0825 14:15:51.166312  1476 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0825 14:16:38.793426  1476 solver.cpp:357] Iteration 21300 (2.09977 iter/s, 47.6243s/100 iters), loss = 0.434971
I0825 14:16:38.793663  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.434971 (* 1 = 0.434971 loss)
I0825 14:16:38.793709  1476 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0825 14:17:23.206732  1476 solver.cpp:357] Iteration 21400 (2.25174 iter/s, 44.41s/100 iters), loss = 0.482547
I0825 14:17:23.206918  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.482547 (* 1 = 0.482547 loss)
I0825 14:17:23.206948  1476 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0825 14:18:00.002013  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:18:08.844530  1476 solver.cpp:514] Iteration 21500, Testing net (#0)
I0825 14:18:50.816638  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:18:50.885972  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7177
I0825 14:18:50.886113  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.880534 (* 1 = 0.880534 loss)
I0825 14:18:51.339354  1476 solver.cpp:357] Iteration 21500 (1.13468 iter/s, 88.1305s/100 iters), loss = 0.301578
I0825 14:18:51.339529  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.301578 (* 1 = 0.301578 loss)
I0825 14:18:51.339574  1476 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0825 14:19:38.737748  1476 solver.cpp:357] Iteration 21600 (2.11001 iter/s, 47.3931s/100 iters), loss = 0.343398
I0825 14:19:38.737987  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343398 (* 1 = 0.343398 loss)
I0825 14:19:38.738031  1476 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0825 14:20:26.095701  1476 solver.cpp:357] Iteration 21700 (2.11172 iter/s, 47.3547s/100 iters), loss = 0.443544
I0825 14:20:26.095870  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443544 (* 1 = 0.443544 loss)
I0825 14:20:26.095898  1476 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0825 14:21:13.521353  1476 solver.cpp:357] Iteration 21800 (2.1088 iter/s, 47.4203s/100 iters), loss = 0.435797
I0825 14:21:13.521594  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435797 (* 1 = 0.435797 loss)
I0825 14:21:13.521641  1476 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0825 14:21:46.536535  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:22:00.132241  1476 solver.cpp:357] Iteration 21900 (2.14548 iter/s, 46.6097s/100 iters), loss = 0.496233
I0825 14:22:00.132659  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496233 (* 1 = 0.496233 loss)
I0825 14:22:00.132838  1476 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0825 14:22:46.134310  1476 solver.cpp:514] Iteration 22000, Testing net (#0)
I0825 14:23:27.336719  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:23:27.544980  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6797
I0825 14:23:27.545344  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05182 (* 1 = 1.05182 loss)
I0825 14:23:27.845403  1476 solver.cpp:357] Iteration 22000 (1.14011 iter/s, 87.7112s/100 iters), loss = 0.393774
I0825 14:23:27.845798  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393774 (* 1 = 0.393774 loss)
I0825 14:23:27.845973  1476 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0825 14:24:12.832824  1476 solver.cpp:357] Iteration 22100 (2.22291 iter/s, 44.9861s/100 iters), loss = 0.441528
I0825 14:24:12.833101  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441528 (* 1 = 0.441528 loss)
I0825 14:24:12.833137  1476 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0825 14:24:57.028789  1476 solver.cpp:357] Iteration 22200 (2.26293 iter/s, 44.1904s/100 iters), loss = 0.328692
I0825 14:24:57.028976  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328692 (* 1 = 0.328692 loss)
I0825 14:24:57.029006  1476 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0825 14:25:25.334422  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:25:43.616714  1476 solver.cpp:357] Iteration 22300 (2.14663 iter/s, 46.5846s/100 iters), loss = 0.477899
I0825 14:25:43.616946  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4779 (* 1 = 0.4779 loss)
I0825 14:25:43.616991  1476 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0825 14:26:31.090749  1476 solver.cpp:357] Iteration 22400 (2.10666 iter/s, 47.4686s/100 iters), loss = 0.325029
I0825 14:26:31.090970  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325029 (* 1 = 0.325029 loss)
I0825 14:26:31.091017  1476 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0825 14:27:18.090831  1476 solver.cpp:514] Iteration 22500, Testing net (#0)
I0825 14:28:00.872911  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:28:01.099920  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7624
I0825 14:28:01.100073  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.700828 (* 1 = 0.700828 loss)
I0825 14:28:01.513783  1476 solver.cpp:357] Iteration 22500 (1.10592 iter/s, 90.4227s/100 iters), loss = 0.32106
I0825 14:28:01.513926  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32106 (* 1 = 0.32106 loss)
I0825 14:28:01.513968  1476 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0825 14:28:48.968945  1476 solver.cpp:357] Iteration 22600 (2.10731 iter/s, 47.4539s/100 iters), loss = 0.415218
I0825 14:28:48.969538  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415218 (* 1 = 0.415218 loss)
I0825 14:28:48.969722  1476 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0825 14:29:13.624161  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:29:36.353649  1476 solver.cpp:357] Iteration 22700 (2.11053 iter/s, 47.3814s/100 iters), loss = 0.420273
I0825 14:29:36.353832  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.420273 (* 1 = 0.420273 loss)
I0825 14:29:36.353875  1476 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0825 14:30:24.086227  1476 solver.cpp:357] Iteration 22800 (2.09506 iter/s, 47.7314s/100 iters), loss = 0.372807
I0825 14:30:24.086443  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372808 (* 1 = 0.372808 loss)
I0825 14:30:24.086488  1476 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0825 14:31:11.400532  1476 solver.cpp:357] Iteration 22900 (2.11358 iter/s, 47.313s/100 iters), loss = 0.554784
I0825 14:31:11.400969  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.554784 (* 1 = 0.554784 loss)
I0825 14:31:11.401083  1476 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0825 14:31:58.086184  1476 solver.cpp:514] Iteration 23000, Testing net (#0)
I0825 14:32:36.648370  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:32:36.857136  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6163
I0825 14:32:36.857475  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.27857 (* 1 = 1.27857 loss)
I0825 14:32:37.185169  1476 solver.cpp:357] Iteration 23000 (1.16574 iter/s, 85.7821s/100 iters), loss = 0.386904
I0825 14:32:37.185606  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386904 (* 1 = 0.386904 loss)
I0825 14:32:37.185784  1476 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0825 14:32:56.595309  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:33:22.986703  1476 solver.cpp:357] Iteration 23100 (2.18339 iter/s, 45.8003s/100 iters), loss = 0.515442
I0825 14:33:22.986937  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.515442 (* 1 = 0.515442 loss)
I0825 14:33:22.986985  1476 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0825 14:34:04.385263  1476 solver.cpp:357] Iteration 23200 (2.41575 iter/s, 41.3951s/100 iters), loss = 0.371678
I0825 14:34:04.385915  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371678 (* 1 = 0.371678 loss)
I0825 14:34:04.386093  1476 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0825 14:34:48.719473  1476 solver.cpp:357] Iteration 23300 (2.25558 iter/s, 44.3345s/100 iters), loss = 0.456055
I0825 14:34:48.719631  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456056 (* 1 = 0.456056 loss)
I0825 14:34:48.719674  1476 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0825 14:35:35.953557  1476 solver.cpp:357] Iteration 23400 (2.1172 iter/s, 47.2323s/100 iters), loss = 0.357849
I0825 14:35:35.953809  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357849 (* 1 = 0.357849 loss)
I0825 14:35:35.953857  1476 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0825 14:35:51.875000  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:36:23.029405  1476 solver.cpp:514] Iteration 23500, Testing net (#0)
I0825 14:37:05.607697  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:37:05.780215  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.609
I0825 14:37:05.780489  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.41836 (* 1 = 1.41836 loss)
I0825 14:37:06.191416  1476 solver.cpp:357] Iteration 23500 (1.10822 iter/s, 90.2346s/100 iters), loss = 0.377878
I0825 14:37:06.191582  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377878 (* 1 = 0.377878 loss)
I0825 14:37:06.191624  1476 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0825 14:37:53.724262  1476 solver.cpp:357] Iteration 23600 (2.10386 iter/s, 47.5317s/100 iters), loss = 0.379173
I0825 14:37:53.724426  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379173 (* 1 = 0.379173 loss)
I0825 14:37:53.724454  1476 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0825 14:38:41.084175  1476 solver.cpp:357] Iteration 23700 (2.11155 iter/s, 47.3587s/100 iters), loss = 0.368735
I0825 14:38:41.084563  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368735 (* 1 = 0.368735 loss)
I0825 14:38:41.084657  1476 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0825 14:39:27.979908  1476 solver.cpp:357] Iteration 23800 (2.13254 iter/s, 46.8924s/100 iters), loss = 0.409571
I0825 14:39:27.980134  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409571 (* 1 = 0.409571 loss)
I0825 14:39:27.980178  1476 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0825 14:39:38.769690  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:40:11.757567  1476 solver.cpp:357] Iteration 23900 (2.28445 iter/s, 43.7743s/100 iters), loss = 0.383469
I0825 14:40:11.758103  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383469 (* 1 = 0.383469 loss)
I0825 14:40:11.758283  1476 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0825 14:40:58.465245  1476 solver.cpp:514] Iteration 24000, Testing net (#0)
I0825 14:41:41.655630  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:41:41.725564  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.641
I0825 14:41:41.725678  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.11439 (* 1 = 1.11439 loss)
I0825 14:41:42.062800  1476 solver.cpp:357] Iteration 24000 (1.10738 iter/s, 90.303s/100 iters), loss = 0.44663
I0825 14:41:42.062994  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44663 (* 1 = 0.44663 loss)
I0825 14:41:42.063041  1476 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0825 14:42:29.655833  1476 solver.cpp:357] Iteration 24100 (2.10139 iter/s, 47.5876s/100 iters), loss = 0.450608
I0825 14:42:29.656258  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450608 (* 1 = 0.450608 loss)
I0825 14:42:29.656359  1476 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0825 14:43:16.686450  1476 solver.cpp:357] Iteration 24200 (2.12652 iter/s, 47.0252s/100 iters), loss = 0.361048
I0825 14:43:16.686738  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361048 (* 1 = 0.361048 loss)
I0825 14:43:16.686786  1476 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0825 14:43:23.235944  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:44:01.806671  1476 solver.cpp:357] Iteration 24300 (2.21647 iter/s, 45.1169s/100 iters), loss = 0.419671
I0825 14:44:01.806874  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419671 (* 1 = 0.419671 loss)
I0825 14:44:01.806919  1476 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0825 14:44:48.315927  1476 solver.cpp:357] Iteration 24400 (2.15026 iter/s, 46.506s/100 iters), loss = 0.383765
I0825 14:44:48.316112  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383765 (* 1 = 0.383765 loss)
I0825 14:44:48.316143  1476 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0825 14:45:34.991041  1476 solver.cpp:514] Iteration 24500, Testing net (#0)
I0825 14:46:16.623600  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:46:16.792918  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6831
I0825 14:46:16.793154  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.0539 (* 1 = 1.0539 loss)
I0825 14:46:17.159612  1476 solver.cpp:357] Iteration 24500 (1.12563 iter/s, 88.8392s/100 iters), loss = 0.465324
I0825 14:46:17.159888  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.465324 (* 1 = 0.465324 loss)
I0825 14:46:17.159979  1476 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0825 14:47:02.842645  1476 solver.cpp:357] Iteration 24600 (2.18916 iter/s, 45.6797s/100 iters), loss = 0.39715
I0825 14:47:02.842880  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397151 (* 1 = 0.397151 loss)
I0825 14:47:02.842928  1476 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0825 14:47:05.250980  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:47:46.320164  1476 solver.cpp:357] Iteration 24700 (2.30022 iter/s, 43.4741s/100 iters), loss = 0.448082
I0825 14:47:46.320327  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448082 (* 1 = 0.448082 loss)
I0825 14:47:46.320370  1476 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0825 14:48:33.588078  1476 solver.cpp:357] Iteration 24800 (2.11575 iter/s, 47.2646s/100 iters), loss = 0.302148
I0825 14:48:33.588577  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302148 (* 1 = 0.302148 loss)
I0825 14:48:33.588750  1476 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0825 14:49:20.986402  1476 solver.cpp:357] Iteration 24900 (2.10997 iter/s, 47.394s/100 iters), loss = 0.565468
I0825 14:49:20.986627  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.565468 (* 1 = 0.565468 loss)
I0825 14:49:20.986675  1476 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0825 14:50:06.319185  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:50:08.125852  1476 solver.cpp:514] Iteration 25000, Testing net (#0)
I0825 14:50:49.423008  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:50:49.482033  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7124
I0825 14:50:49.482122  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.872351 (* 1 = 0.872351 loss)
I0825 14:50:49.896082  1476 solver.cpp:357] Iteration 25000 (1.12474 iter/s, 88.9095s/100 iters), loss = 0.478525
I0825 14:50:49.896261  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478525 (* 1 = 0.478525 loss)
I0825 14:50:49.896328  1476 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0825 14:51:37.248502  1476 solver.cpp:357] Iteration 25100 (2.11207 iter/s, 47.347s/100 iters), loss = 0.48763
I0825 14:51:37.248706  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48763 (* 1 = 0.48763 loss)
I0825 14:51:37.248751  1476 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0825 14:52:24.494613  1476 solver.cpp:357] Iteration 25200 (2.11667 iter/s, 47.244s/100 iters), loss = 0.461854
I0825 14:52:24.495189  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461854 (* 1 = 0.461854 loss)
I0825 14:52:24.495309  1476 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0825 14:53:11.899369  1476 solver.cpp:357] Iteration 25300 (2.10973 iter/s, 47.3994s/100 iters), loss = 0.458859
I0825 14:53:11.899610  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458859 (* 1 = 0.458859 loss)
I0825 14:53:11.899657  1476 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0825 14:53:51.063264  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:53:57.243154  1476 solver.cpp:357] Iteration 25400 (2.20564 iter/s, 45.3384s/100 iters), loss = 0.497328
I0825 14:53:57.243326  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497328 (* 1 = 0.497328 loss)
I0825 14:53:57.243371  1476 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0825 14:54:41.850728  1476 solver.cpp:514] Iteration 25500, Testing net (#0)
I0825 14:55:21.479753  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:55:21.671555  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6649
I0825 14:55:21.671706  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.13395 (* 1 = 1.13395 loss)
I0825 14:55:21.953025  1476 solver.cpp:357] Iteration 25500 (1.1805 iter/s, 84.7096s/100 iters), loss = 0.319988
I0825 14:55:21.953177  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.319988 (* 1 = 0.319988 loss)
I0825 14:55:21.953220  1476 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0825 14:56:09.346593  1476 solver.cpp:357] Iteration 25600 (2.10997 iter/s, 47.3941s/100 iters), loss = 0.378913
I0825 14:56:09.347098  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378913 (* 1 = 0.378913 loss)
I0825 14:56:09.347275  1476 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0825 14:56:56.715490  1476 solver.cpp:357] Iteration 25700 (2.11078 iter/s, 47.3757s/100 iters), loss = 0.420805
I0825 14:56:56.715678  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.420805 (* 1 = 0.420805 loss)
I0825 14:56:56.715708  1476 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0825 14:57:33.500466  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:57:44.140645  1476 solver.cpp:357] Iteration 25800 (2.10852 iter/s, 47.4266s/100 iters), loss = 0.441872
I0825 14:57:44.140787  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441872 (* 1 = 0.441872 loss)
I0825 14:57:44.140815  1476 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0825 14:58:31.083717  1476 solver.cpp:357] Iteration 25900 (2.1302 iter/s, 46.944s/100 iters), loss = 0.529848
I0825 14:58:31.084264  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.529848 (* 1 = 0.529848 loss)
I0825 14:58:31.084445  1476 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0825 14:59:16.678794  1476 solver.cpp:514] Iteration 26000, Testing net (#0)
I0825 14:59:58.826385  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:59:58.923389  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7015
I0825 14:59:58.923506  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05172 (* 1 = 1.05172 loss)
I0825 14:59:59.321483  1476 solver.cpp:357] Iteration 26000 (1.13317 iter/s, 88.2483s/100 iters), loss = 0.319292
I0825 14:59:59.321657  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.319292 (* 1 = 0.319292 loss)
I0825 14:59:59.321703  1476 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0825 15:00:46.755580  1476 solver.cpp:357] Iteration 26100 (2.1082 iter/s, 47.4338s/100 iters), loss = 0.269417
I0825 15:00:46.756011  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.269417 (* 1 = 0.269417 loss)
I0825 15:00:46.756184  1476 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0825 15:01:18.896432  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:01:34.246100  1476 solver.cpp:357] Iteration 26200 (2.10544 iter/s, 47.496s/100 iters), loss = 0.417596
I0825 15:01:34.246256  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417596 (* 1 = 0.417596 loss)
I0825 15:01:34.246291  1476 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0825 15:02:19.477607  1476 solver.cpp:357] Iteration 26300 (2.2108 iter/s, 45.2325s/100 iters), loss = 0.309646
I0825 15:02:19.477883  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.309646 (* 1 = 0.309646 loss)
I0825 15:02:19.477931  1476 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0825 15:03:05.077491  1476 solver.cpp:357] Iteration 26400 (2.19285 iter/s, 45.6028s/100 iters), loss = 0.399166
I0825 15:03:05.077708  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399166 (* 1 = 0.399166 loss)
I0825 15:03:05.077754  1476 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0825 15:03:49.772891  1476 solver.cpp:514] Iteration 26500, Testing net (#0)
I0825 15:04:29.791389  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:04:29.945061  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7296
I0825 15:04:29.945396  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.810501 (* 1 = 0.810501 loss)
I0825 15:04:30.262111  1476 solver.cpp:357] Iteration 26500 (1.17386 iter/s, 85.1892s/100 iters), loss = 0.292398
I0825 15:04:30.262516  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.292398 (* 1 = 0.292398 loss)
I0825 15:04:30.262693  1476 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0825 15:04:57.991703  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:05:17.733453  1476 solver.cpp:357] Iteration 26600 (2.10661 iter/s, 47.4695s/100 iters), loss = 0.295433
I0825 15:05:17.733692  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.295433 (* 1 = 0.295433 loss)
I0825 15:05:17.733741  1476 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0825 15:06:04.626813  1476 solver.cpp:357] Iteration 26700 (2.13259 iter/s, 46.8913s/100 iters), loss = 0.48299
I0825 15:06:04.627038  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48299 (* 1 = 0.48299 loss)
I0825 15:06:04.627082  1476 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0825 15:06:52.191213  1476 solver.cpp:357] Iteration 26800 (2.10242 iter/s, 47.5643s/100 iters), loss = 0.490364
I0825 15:06:52.192863  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490364 (* 1 = 0.490364 loss)
I0825 15:06:52.192911  1476 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0825 15:07:39.495921  1476 solver.cpp:357] Iteration 26900 (2.11406 iter/s, 47.3023s/100 iters), loss = 0.292224
I0825 15:07:39.496119  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.292224 (* 1 = 0.292224 loss)
I0825 15:07:39.496155  1476 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0825 15:08:03.138242  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:08:26.651717  1476 solver.cpp:514] Iteration 27000, Testing net (#0)
I0825 15:09:07.664813  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:09:07.818732  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5122
I0825 15:09:07.818840  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.0783 (* 1 = 2.0783 loss)
I0825 15:09:08.141041  1476 solver.cpp:357] Iteration 27000 (1.12806 iter/s, 88.648s/100 iters), loss = 0.358015
I0825 15:09:08.141207  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358015 (* 1 = 0.358015 loss)
I0825 15:09:08.141252  1476 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0825 15:09:53.543256  1476 solver.cpp:357] Iteration 27100 (2.20268 iter/s, 45.3992s/100 iters), loss = 0.509965
I0825 15:09:53.543483  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509966 (* 1 = 0.509966 loss)
I0825 15:09:53.543530  1476 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0825 15:10:39.437710  1476 solver.cpp:357] Iteration 27200 (2.17905 iter/s, 45.8915s/100 iters), loss = 0.490297
I0825 15:10:39.438230  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490297 (* 1 = 0.490297 loss)
I0825 15:10:39.438426  1476 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0825 15:11:26.289029  1476 solver.cpp:357] Iteration 27300 (2.13455 iter/s, 46.8483s/100 iters), loss = 0.377964
I0825 15:11:26.289330  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377964 (* 1 = 0.377964 loss)
I0825 15:11:26.289376  1476 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0825 15:11:44.250725  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:12:12.297616  1476 solver.cpp:357] Iteration 27400 (2.17355 iter/s, 46.0077s/100 iters), loss = 0.324929
I0825 15:12:12.297843  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.324929 (* 1 = 0.324929 loss)
I0825 15:12:12.297889  1476 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0825 15:12:59.351128  1476 solver.cpp:514] Iteration 27500, Testing net (#0)
I0825 15:13:39.837271  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:13:40.017905  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7164
I0825 15:13:40.017988  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.844739 (* 1 = 0.844739 loss)
I0825 15:13:40.304215  1476 solver.cpp:357] Iteration 27500 (1.13623 iter/s, 88.0104s/100 iters), loss = 0.495036
I0825 15:13:40.304376  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495036 (* 1 = 0.495036 loss)
I0825 15:13:40.304421  1476 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0825 15:14:25.567293  1476 solver.cpp:357] Iteration 27600 (2.2092 iter/s, 45.2654s/100 iters), loss = 0.504332
I0825 15:14:25.567477  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.504332 (* 1 = 0.504332 loss)
I0825 15:14:25.567512  1476 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0825 15:15:13.122553  1476 solver.cpp:357] Iteration 27700 (2.10278 iter/s, 47.556s/100 iters), loss = 0.559664
I0825 15:15:13.123075  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.559664 (* 1 = 0.559664 loss)
I0825 15:15:13.123250  1476 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0825 15:15:27.326781  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:16:00.408082  1476 solver.cpp:357] Iteration 27800 (2.11478 iter/s, 47.2863s/100 iters), loss = 0.423501
I0825 15:16:00.408303  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423501 (* 1 = 0.423501 loss)
I0825 15:16:00.408349  1476 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0825 15:16:47.482861  1476 solver.cpp:357] Iteration 27900 (2.12425 iter/s, 47.0754s/100 iters), loss = 0.462969
I0825 15:16:47.483062  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462969 (* 1 = 0.462969 loss)
I0825 15:16:47.483108  1476 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0825 15:17:32.134302  1476 solver.cpp:514] Iteration 28000, Testing net (#0)
I0825 15:18:13.696199  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:18:13.851490  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.678
I0825 15:18:13.851646  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.16648 (* 1 = 1.16648 loss)
I0825 15:18:14.290597  1476 solver.cpp:357] Iteration 28000 (1.15196 iter/s, 86.8086s/100 iters), loss = 0.422058
I0825 15:18:14.290771  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422058 (* 1 = 0.422058 loss)
I0825 15:18:14.290817  1476 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0825 15:19:01.676406  1476 solver.cpp:357] Iteration 28100 (2.1105 iter/s, 47.3821s/100 iters), loss = 0.388194
I0825 15:19:01.676910  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388194 (* 1 = 0.388194 loss)
I0825 15:19:01.677089  1476 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0825 15:19:11.333906  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:19:48.999541  1476 solver.cpp:357] Iteration 28200 (2.1133 iter/s, 47.3194s/100 iters), loss = 0.4575
I0825 15:19:49.001145  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4575 (* 1 = 0.4575 loss)
I0825 15:19:49.001194  1476 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0825 15:20:36.726861  1476 solver.cpp:357] Iteration 28300 (2.09531 iter/s, 47.7257s/100 iters), loss = 0.45128
I0825 15:20:36.727135  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45128 (* 1 = 0.45128 loss)
I0825 15:20:36.727181  1476 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0825 15:21:24.048750  1476 solver.cpp:357] Iteration 28400 (2.11309 iter/s, 47.3241s/100 iters), loss = 0.287792
I0825 15:21:24.049278  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.287792 (* 1 = 0.287792 loss)
I0825 15:21:24.049453  1476 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0825 15:22:11.090914  1476 solver.cpp:514] Iteration 28500, Testing net (#0)
I0825 15:22:54.133704  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:22:54.353575  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3724
I0825 15:22:54.353662  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.19583 (* 1 = 3.19583 loss)
I0825 15:22:54.719029  1476 solver.cpp:357] Iteration 28500 (1.10289 iter/s, 90.671s/100 iters), loss = 0.465816
I0825 15:22:54.719208  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.465816 (* 1 = 0.465816 loss)
I0825 15:22:54.719254  1476 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0825 15:23:00.144191  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:23:40.097378  1476 solver.cpp:357] Iteration 28600 (2.20389 iter/s, 45.3743s/100 iters), loss = 0.3931
I0825 15:23:40.103441  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3931 (* 1 = 0.3931 loss)
I0825 15:23:40.103626  1476 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0825 15:24:20.305490  1476 solver.cpp:357] Iteration 28700 (2.48746 iter/s, 40.2016s/100 iters), loss = 0.341295
I0825 15:24:20.305691  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341295 (* 1 = 0.341295 loss)
I0825 15:24:20.305737  1476 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0825 15:25:03.560928  1476 solver.cpp:357] Iteration 28800 (2.31196 iter/s, 43.2534s/100 iters), loss = 0.242324
I0825 15:25:03.561411  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.242324 (* 1 = 0.242324 loss)
I0825 15:25:03.561591  1476 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0825 15:25:50.120851  1476 solver.cpp:357] Iteration 28900 (2.14786 iter/s, 46.558s/100 iters), loss = 0.409887
I0825 15:25:50.121055  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409887 (* 1 = 0.409887 loss)
I0825 15:25:50.121084  1476 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0825 15:25:51.171985  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:26:37.411623  1476 solver.cpp:514] Iteration 29000, Testing net (#0)
I0825 15:27:18.312412  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:27:18.480746  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7568
I0825 15:27:18.480904  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.703192 (* 1 = 0.703192 loss)
I0825 15:27:18.935477  1476 solver.cpp:357] Iteration 29000 (1.12596 iter/s, 88.8128s/100 iters), loss = 0.453131
I0825 15:27:18.935637  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453131 (* 1 = 0.453131 loss)
I0825 15:27:18.935680  1476 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0825 15:28:06.377379  1476 solver.cpp:357] Iteration 29100 (2.10785 iter/s, 47.4417s/100 iters), loss = 0.453772
I0825 15:28:06.377638  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453772 (* 1 = 0.453772 loss)
I0825 15:28:06.377701  1476 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0825 15:28:54.194273  1476 solver.cpp:357] Iteration 29200 (2.09149 iter/s, 47.8129s/100 iters), loss = 0.373488
I0825 15:28:54.194507  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373488 (* 1 = 0.373488 loss)
I0825 15:28:54.194555  1476 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0825 15:29:37.935094  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:29:41.757033  1476 solver.cpp:357] Iteration 29300 (2.10257 iter/s, 47.5609s/100 iters), loss = 0.425351
I0825 15:29:41.757196  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425351 (* 1 = 0.425351 loss)
I0825 15:29:41.757242  1476 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0825 15:30:28.894958  1476 solver.cpp:357] Iteration 29400 (2.12162 iter/s, 47.1339s/100 iters), loss = 0.465126
I0825 15:30:28.895193  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.465126 (* 1 = 0.465126 loss)
I0825 15:30:28.895238  1476 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0825 15:31:15.976135  1476 solver.cpp:514] Iteration 29500, Testing net (#0)
I0825 15:31:56.776420  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:31:56.937233  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.720401
I0825 15:31:56.937343  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.909054 (* 1 = 0.909054 loss)
I0825 15:31:57.317807  1476 solver.cpp:357] Iteration 29500 (1.13092 iter/s, 88.4239s/100 iters), loss = 0.439154
I0825 15:31:57.317919  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439154 (* 1 = 0.439154 loss)
I0825 15:31:57.317945  1476 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0825 15:32:41.012737  1476 solver.cpp:357] Iteration 29600 (2.2887 iter/s, 43.6929s/100 iters), loss = 0.262055
I0825 15:32:41.012922  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.262055 (* 1 = 0.262055 loss)
I0825 15:32:41.012951  1476 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0825 15:33:19.883710  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:33:27.106676  1476 solver.cpp:357] Iteration 29700 (2.16948 iter/s, 46.094s/100 iters), loss = 0.297485
I0825 15:33:27.107087  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.297486 (* 1 = 0.297486 loss)
I0825 15:33:27.107269  1476 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0825 15:34:12.332502  1476 solver.cpp:357] Iteration 29800 (2.21122 iter/s, 45.2238s/100 iters), loss = 0.39689
I0825 15:34:12.332738  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39689 (* 1 = 0.39689 loss)
I0825 15:34:12.332785  1476 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0825 15:34:58.925415  1476 solver.cpp:357] Iteration 29900 (2.14634 iter/s, 46.5909s/100 iters), loss = 0.299796
I0825 15:34:58.925654  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.299796 (* 1 = 0.299796 loss)
I0825 15:34:58.925684  1476 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0825 15:35:46.020824  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.caffemodel
I0825 15:35:46.051221  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.solverstate
I0825 15:35:46.062381  1476 solver.cpp:514] Iteration 30000, Testing net (#0)
I0825 15:36:27.632885  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:36:27.735895  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6479
I0825 15:36:27.735982  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02203 (* 1 = 1.02203 loss)
I0825 15:36:28.131014  1476 solver.cpp:357] Iteration 30000 (1.12098 iter/s, 89.2079s/100 iters), loss = 0.273855
I0825 15:36:28.131139  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.273855 (* 1 = 0.273855 loss)
I0825 15:36:28.131165  1476 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0825 15:37:02.235563  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:37:13.588999  1476 solver.cpp:357] Iteration 30100 (2.19996 iter/s, 45.4553s/100 iters), loss = 0.428678
I0825 15:37:13.589133  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428678 (* 1 = 0.428678 loss)
I0825 15:37:13.589175  1476 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0825 15:38:01.106052  1476 solver.cpp:357] Iteration 30200 (2.1045 iter/s, 47.5172s/100 iters), loss = 0.445077
I0825 15:38:01.106353  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445077 (* 1 = 0.445077 loss)
I0825 15:38:01.106402  1476 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0825 15:38:48.534747  1476 solver.cpp:357] Iteration 30300 (2.10852 iter/s, 47.4267s/100 iters), loss = 0.525823
I0825 15:38:48.534922  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.525823 (* 1 = 0.525823 loss)
I0825 15:38:48.534951  1476 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0825 15:39:35.148882  1476 solver.cpp:357] Iteration 30400 (2.14536 iter/s, 46.6123s/100 iters), loss = 0.281183
I0825 15:39:35.149488  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281183 (* 1 = 0.281183 loss)
I0825 15:39:35.149538  1476 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0825 15:40:03.804086  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:40:19.167948  1476 solver.cpp:514] Iteration 30500, Testing net (#0)
I0825 15:41:02.058198  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:41:02.230789  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6754
I0825 15:41:02.230886  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.15091 (* 1 = 1.15091 loss)
I0825 15:41:02.524412  1476 solver.cpp:357] Iteration 30500 (1.14451 iter/s, 87.3736s/100 iters), loss = 0.351694
I0825 15:41:02.524579  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351695 (* 1 = 0.351695 loss)
I0825 15:41:02.524623  1476 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0825 15:41:50.004302  1476 solver.cpp:357] Iteration 30600 (2.10634 iter/s, 47.4758s/100 iters), loss = 0.390297
I0825 15:41:50.004844  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390297 (* 1 = 0.390297 loss)
I0825 15:41:50.005023  1476 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0825 15:42:37.408092  1476 solver.cpp:357] Iteration 30700 (2.10962 iter/s, 47.4018s/100 iters), loss = 0.423041
I0825 15:42:37.408290  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423041 (* 1 = 0.423041 loss)
I0825 15:42:37.408323  1476 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0825 15:43:24.620712  1476 solver.cpp:357] Iteration 30800 (2.11807 iter/s, 47.2127s/100 iters), loss = 0.338414
I0825 15:43:24.620893  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338414 (* 1 = 0.338414 loss)
I0825 15:43:24.620924  1476 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0825 15:43:49.671950  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:44:09.920550  1476 solver.cpp:357] Iteration 30900 (2.20751 iter/s, 45.2998s/100 iters), loss = 0.537258
I0825 15:44:09.920964  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.537259 (* 1 = 0.537259 loss)
I0825 15:44:09.921058  1476 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0825 15:44:55.821235  1476 solver.cpp:514] Iteration 31000, Testing net (#0)
I0825 15:45:39.023063  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:45:39.226052  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6102
I0825 15:45:39.226202  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.419 (* 1 = 1.419 loss)
I0825 15:45:39.599342  1476 solver.cpp:357] Iteration 31000 (1.11506 iter/s, 89.681s/100 iters), loss = 0.354223
I0825 15:45:39.599503  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.354223 (* 1 = 0.354223 loss)
I0825 15:45:39.599548  1476 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0825 15:46:27.095907  1476 solver.cpp:357] Iteration 31100 (2.1055 iter/s, 47.4946s/100 iters), loss = 0.375424
I0825 15:46:27.096320  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375424 (* 1 = 0.375424 loss)
I0825 15:46:27.096436  1476 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0825 15:47:12.753386  1476 solver.cpp:357] Iteration 31200 (2.19025 iter/s, 45.657s/100 iters), loss = 0.259806
I0825 15:47:12.753617  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.259806 (* 1 = 0.259806 loss)
I0825 15:47:12.753662  1476 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0825 15:47:32.970134  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:47:57.761971  1476 solver.cpp:357] Iteration 31300 (2.22334 iter/s, 44.9775s/100 iters), loss = 0.286066
I0825 15:47:57.762286  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.286066 (* 1 = 0.286066 loss)
I0825 15:47:57.762338  1476 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0825 15:48:44.840977  1476 solver.cpp:357] Iteration 31400 (2.12527 iter/s, 47.0529s/100 iters), loss = 0.413258
I0825 15:48:44.841151  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413258 (* 1 = 0.413258 loss)
I0825 15:48:44.841179  1476 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0825 15:49:30.569815  1476 solver.cpp:514] Iteration 31500, Testing net (#0)
I0825 15:50:12.965486  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:50:13.180703  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6882
I0825 15:50:13.181043  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.99812 (* 1 = 0.99812 loss)
I0825 15:50:13.606945  1476 solver.cpp:357] Iteration 31500 (1.12702 iter/s, 88.7294s/100 iters), loss = 0.335458
I0825 15:50:13.607343  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.335458 (* 1 = 0.335458 loss)
I0825 15:50:13.607520  1476 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0825 15:51:01.192112  1476 solver.cpp:357] Iteration 31600 (2.10229 iter/s, 47.5672s/100 iters), loss = 0.392836
I0825 15:51:01.192338  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392836 (* 1 = 0.392836 loss)
I0825 15:51:01.192385  1476 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0825 15:51:18.445472  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:51:48.661249  1476 solver.cpp:357] Iteration 31700 (2.10734 iter/s, 47.4533s/100 iters), loss = 0.348546
I0825 15:51:48.661458  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348546 (* 1 = 0.348546 loss)
I0825 15:51:48.661504  1476 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0825 15:52:36.144902  1476 solver.cpp:357] Iteration 31800 (2.10671 iter/s, 47.4674s/100 iters), loss = 0.364854
I0825 15:52:36.145454  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364854 (* 1 = 0.364854 loss)
I0825 15:52:36.145632  1476 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0825 15:53:23.808845  1476 solver.cpp:357] Iteration 31900 (2.09859 iter/s, 47.6511s/100 iters), loss = 0.40792
I0825 15:53:23.809046  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40792 (* 1 = 0.40792 loss)
I0825 15:53:23.809092  1476 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0825 15:54:08.231139  1476 solver.cpp:514] Iteration 32000, Testing net (#0)
I0825 15:54:46.877379  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:54:47.070888  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.646401
I0825 15:54:47.071233  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.27474 (* 1 = 1.27474 loss)
I0825 15:54:47.411617  1476 solver.cpp:357] Iteration 32000 (1.19634 iter/s, 83.5884s/100 iters), loss = 0.402711
I0825 15:54:47.412009  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402711 (* 1 = 0.402711 loss)
I0825 15:54:47.412180  1476 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0825 15:54:47.412324  1476 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0825 15:54:59.319692  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:55:32.718856  1476 solver.cpp:357] Iteration 32100 (2.20759 iter/s, 45.2983s/100 iters), loss = 0.299595
I0825 15:55:32.719077  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.299595 (* 1 = 0.299595 loss)
I0825 15:55:32.719123  1476 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0825 15:56:20.351601  1476 solver.cpp:357] Iteration 32200 (2.0999 iter/s, 47.6213s/100 iters), loss = 0.343582
I0825 15:56:20.351752  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343582 (* 1 = 0.343582 loss)
I0825 15:56:20.351778  1476 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0825 15:57:07.868885  1476 solver.cpp:357] Iteration 32300 (2.10489 iter/s, 47.5085s/100 iters), loss = 0.207091
I0825 15:57:07.869562  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.207091 (* 1 = 0.207091 loss)
I0825 15:57:07.869742  1476 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0825 15:57:55.256878  1476 solver.cpp:357] Iteration 32400 (2.1107 iter/s, 47.3776s/100 iters), loss = 0.260868
I0825 15:57:55.257122  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260868 (* 1 = 0.260868 loss)
I0825 15:57:55.257169  1476 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0825 15:58:03.554605  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:58:42.111904  1476 solver.cpp:514] Iteration 32500, Testing net (#0)
I0825 15:59:23.128314  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:59:23.297929  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.866301
I0825 15:59:23.298032  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.387488 (* 1 = 0.387488 loss)
I0825 15:59:23.635455  1476 solver.cpp:357] Iteration 32500 (1.13161 iter/s, 88.37s/100 iters), loss = 0.20413
I0825 15:59:23.635622  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.20413 (* 1 = 0.20413 loss)
I0825 15:59:23.635665  1476 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0825 16:00:11.081485  1476 solver.cpp:357] Iteration 32600 (2.10798 iter/s, 47.4387s/100 iters), loss = 0.203609
I0825 16:00:11.081715  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.203609 (* 1 = 0.203609 loss)
I0825 16:00:11.081759  1476 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0825 16:00:58.078851  1476 solver.cpp:357] Iteration 32700 (2.1281 iter/s, 46.9903s/100 iters), loss = 0.274085
I0825 16:00:58.079041  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.274085 (* 1 = 0.274085 loss)
I0825 16:00:58.079074  1476 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0825 16:01:44.939620  1476 solver.cpp:357] Iteration 32800 (2.1342 iter/s, 46.856s/100 iters), loss = 0.223426
I0825 16:01:44.939834  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.223427 (* 1 = 0.223427 loss)
I0825 16:01:44.939882  1476 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0825 16:01:48.787186  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:02:28.301075  1476 solver.cpp:357] Iteration 32900 (2.30643 iter/s, 43.3571s/100 iters), loss = 0.173352
I0825 16:02:28.301307  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.173352 (* 1 = 0.173352 loss)
I0825 16:02:28.301355  1476 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0825 16:03:13.689513  1476 solver.cpp:514] Iteration 33000, Testing net (#0)
I0825 16:03:53.846231  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:03:53.974869  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.818001
I0825 16:03:53.975006  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.529893 (* 1 = 0.529893 loss)
I0825 16:03:54.273866  1476 solver.cpp:357] Iteration 33000 (1.1633 iter/s, 85.9624s/100 iters), loss = 0.140018
I0825 16:03:54.274029  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.140018 (* 1 = 0.140018 loss)
I0825 16:03:54.274076  1476 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0825 16:04:39.512259  1476 solver.cpp:357] Iteration 33100 (2.21039 iter/s, 45.2408s/100 iters), loss = 0.19577
I0825 16:04:39.512503  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.19577 (* 1 = 0.19577 loss)
I0825 16:04:39.512550  1476 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0825 16:05:26.729935  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:05:26.992573  1476 solver.cpp:357] Iteration 33200 (2.10599 iter/s, 47.4837s/100 iters), loss = 0.166547
I0825 16:05:26.992753  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.166547 (* 1 = 0.166547 loss)
I0825 16:05:26.992799  1476 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0825 16:06:14.538547  1476 solver.cpp:357] Iteration 33300 (2.10315 iter/s, 47.5477s/100 iters), loss = 0.184281
I0825 16:06:14.538846  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.184281 (* 1 = 0.184281 loss)
I0825 16:06:14.538893  1476 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0825 16:07:01.983119  1476 solver.cpp:357] Iteration 33400 (2.10761 iter/s, 47.447s/100 iters), loss = 0.15893
I0825 16:07:01.983345  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158931 (* 1 = 0.158931 loss)
I0825 16:07:01.983374  1476 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0825 16:07:49.221498  1476 solver.cpp:514] Iteration 33500, Testing net (#0)
I0825 16:08:32.426368  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:08:32.636008  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.817401
I0825 16:08:32.636122  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.536069 (* 1 = 0.536069 loss)
I0825 16:08:33.008414  1476 solver.cpp:357] Iteration 33500 (1.09853 iter/s, 91.0309s/100 iters), loss = 0.142428
I0825 16:08:33.008584  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142428 (* 1 = 0.142428 loss)
I0825 16:08:33.008630  1476 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0825 16:09:15.391378  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:09:20.531566  1476 solver.cpp:357] Iteration 33600 (2.10435 iter/s, 47.5206s/100 iters), loss = 0.286147
I0825 16:09:20.532130  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.286147 (* 1 = 0.286147 loss)
I0825 16:09:20.532173  1476 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0825 16:10:05.192924  1476 solver.cpp:357] Iteration 33700 (2.23914 iter/s, 44.6601s/100 iters), loss = 0.220145
I0825 16:10:05.193148  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.220145 (* 1 = 0.220145 loss)
I0825 16:10:05.193197  1476 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0825 16:10:51.610330  1476 solver.cpp:357] Iteration 33800 (2.15435 iter/s, 46.4178s/100 iters), loss = 0.218588
I0825 16:10:51.610571  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.218588 (* 1 = 0.218588 loss)
I0825 16:10:51.610617  1476 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0825 16:11:39.155072  1476 solver.cpp:357] Iteration 33900 (2.10338 iter/s, 47.5426s/100 iters), loss = 0.199265
I0825 16:11:39.155558  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.199265 (* 1 = 0.199265 loss)
I0825 16:11:39.155736  1476 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0825 16:12:17.340855  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:12:26.241987  1476 solver.cpp:514] Iteration 34000, Testing net (#0)
I0825 16:13:08.335291  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:13:08.497122  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7491
I0825 16:13:08.497498  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.778276 (* 1 = 0.778276 loss)
I0825 16:13:08.961100  1476 solver.cpp:357] Iteration 34000 (1.11353 iter/s, 89.8042s/100 iters), loss = 0.106977
I0825 16:13:08.961503  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106977 (* 1 = 0.106977 loss)
I0825 16:13:08.961683  1476 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0825 16:13:54.499991  1476 solver.cpp:357] Iteration 34100 (2.19618 iter/s, 45.5336s/100 iters), loss = 0.130407
I0825 16:13:54.500252  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130407 (* 1 = 0.130407 loss)
I0825 16:13:54.500300  1476 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0825 16:14:37.689615  1476 solver.cpp:357] Iteration 34200 (2.31555 iter/s, 43.1862s/100 iters), loss = 0.151368
I0825 16:14:37.690141  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.151368 (* 1 = 0.151368 loss)
I0825 16:14:37.690317  1476 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0825 16:15:24.800343  1476 solver.cpp:357] Iteration 34300 (2.12272 iter/s, 47.1094s/100 iters), loss = 0.102307
I0825 16:15:24.800645  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102307 (* 1 = 0.102307 loss)
I0825 16:15:24.800693  1476 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0825 16:15:58.211184  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:16:12.130240  1476 solver.cpp:357] Iteration 34400 (2.1129 iter/s, 47.3283s/100 iters), loss = 0.219142
I0825 16:16:12.130439  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.219142 (* 1 = 0.219142 loss)
I0825 16:16:12.130486  1476 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0825 16:16:58.947271  1476 solver.cpp:514] Iteration 34500, Testing net (#0)
I0825 16:17:38.019001  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:17:38.208047  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7437
I0825 16:17:38.208187  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.776785 (* 1 = 0.776785 loss)
I0825 16:17:38.610472  1476 solver.cpp:357] Iteration 34500 (1.15641 iter/s, 86.4748s/100 iters), loss = 0.140143
I0825 16:17:38.610626  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.140143 (* 1 = 0.140143 loss)
I0825 16:17:38.610671  1476 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0825 16:18:25.325584  1476 solver.cpp:357] Iteration 34600 (2.14072 iter/s, 46.7133s/100 iters), loss = 0.226238
I0825 16:18:25.328591  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.226238 (* 1 = 0.226238 loss)
I0825 16:18:25.328610  1476 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0825 16:19:12.823720  1476 solver.cpp:357] Iteration 34700 (2.1055 iter/s, 47.4948s/100 iters), loss = 0.119892
I0825 16:19:12.823925  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119892 (* 1 = 0.119892 loss)
I0825 16:19:12.823969  1476 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0825 16:19:41.954720  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:20:00.308306  1476 solver.cpp:357] Iteration 34800 (2.10613 iter/s, 47.4804s/100 iters), loss = 0.0873842
I0825 16:20:00.310292  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0873841 (* 1 = 0.0873841 loss)
I0825 16:20:00.310504  1476 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0825 16:20:47.577092  1476 solver.cpp:357] Iteration 34900 (2.11575 iter/s, 47.2647s/100 iters), loss = 0.153661
I0825 16:20:47.582445  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.153661 (* 1 = 0.153661 loss)
I0825 16:20:47.582478  1476 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0825 16:21:34.345260  1476 solver.cpp:514] Iteration 35000, Testing net (#0)
I0825 16:22:16.428625  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:22:16.588501  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7843
I0825 16:22:16.588637  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.658343 (* 1 = 0.658343 loss)
I0825 16:22:16.921948  1476 solver.cpp:357] Iteration 35000 (1.11934 iter/s, 89.3388s/100 iters), loss = 0.111298
I0825 16:22:16.922116  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.111298 (* 1 = 0.111298 loss)
I0825 16:22:16.922163  1476 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0825 16:23:04.130674  1476 solver.cpp:357] Iteration 35100 (2.11843 iter/s, 47.2047s/100 iters), loss = 0.172187
I0825 16:23:04.131057  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.172187 (* 1 = 0.172187 loss)
I0825 16:23:04.131182  1476 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0825 16:23:28.762325  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:23:50.245290  1476 solver.cpp:357] Iteration 35200 (2.1686 iter/s, 46.1127s/100 iters), loss = 0.158021
I0825 16:23:50.245522  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158021 (* 1 = 0.158021 loss)
I0825 16:23:50.245568  1476 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0825 16:24:34.684460  1476 solver.cpp:357] Iteration 35300 (2.25039 iter/s, 44.4368s/100 iters), loss = 0.205551
I0825 16:24:34.685057  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.205551 (* 1 = 0.205551 loss)
I0825 16:24:34.685240  1476 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0825 16:25:17.986285  1476 solver.cpp:357] Iteration 35400 (2.30953 iter/s, 43.2989s/100 iters), loss = 0.120231
I0825 16:25:17.986474  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120231 (* 1 = 0.120231 loss)
I0825 16:25:17.986501  1476 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0825 16:26:03.897610  1476 solver.cpp:514] Iteration 35500, Testing net (#0)
I0825 16:26:44.504231  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:26:44.671892  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7721
I0825 16:26:44.672049  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.723122 (* 1 = 0.723122 loss)
I0825 16:26:45.084502  1476 solver.cpp:357] Iteration 35500 (1.14821 iter/s, 87.0923s/100 iters), loss = 0.185795
I0825 16:26:45.084683  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.185795 (* 1 = 0.185795 loss)
I0825 16:26:45.084729  1476 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0825 16:27:04.840387  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:27:31.895762  1476 solver.cpp:357] Iteration 35600 (2.13653 iter/s, 46.805s/100 iters), loss = 0.208368
I0825 16:27:31.896409  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.208368 (* 1 = 0.208368 loss)
I0825 16:27:31.896590  1476 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0825 16:28:19.377828  1476 solver.cpp:357] Iteration 35700 (2.10615 iter/s, 47.48s/100 iters), loss = 0.195849
I0825 16:28:19.378011  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.195849 (* 1 = 0.195849 loss)
I0825 16:28:19.378039  1476 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0825 16:29:06.843500  1476 solver.cpp:357] Iteration 35800 (2.10697 iter/s, 47.4614s/100 iters), loss = 0.110627
I0825 16:29:06.843734  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110627 (* 1 = 0.110627 loss)
I0825 16:29:06.843781  1476 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0825 16:29:54.069059  1476 solver.cpp:357] Iteration 35900 (2.11778 iter/s, 47.2193s/100 iters), loss = 0.119976
I0825 16:29:54.069272  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119976 (* 1 = 0.119976 loss)
I0825 16:29:54.069316  1476 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0825 16:30:09.845939  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:30:40.787751  1476 solver.cpp:514] Iteration 36000, Testing net (#0)
I0825 16:31:21.976012  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:31:22.075037  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7525
I0825 16:31:22.075137  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.765042 (* 1 = 0.765042 loss)
I0825 16:31:22.403412  1476 solver.cpp:357] Iteration 36000 (1.13209 iter/s, 88.3324s/100 iters), loss = 0.181642
I0825 16:31:22.403587  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.181642 (* 1 = 0.181642 loss)
I0825 16:31:22.403635  1476 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0825 16:32:08.968608  1476 solver.cpp:357] Iteration 36100 (2.14782 iter/s, 46.5587s/100 iters), loss = 0.206481
I0825 16:32:08.969136  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.206481 (* 1 = 0.206481 loss)
I0825 16:32:08.969344  1476 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0825 16:32:53.162444  1476 solver.cpp:357] Iteration 36200 (2.26287 iter/s, 44.1917s/100 iters), loss = 0.0664507
I0825 16:32:53.162648  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0664507 (* 1 = 0.0664507 loss)
I0825 16:32:53.162694  1476 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0825 16:33:40.018321  1476 solver.cpp:357] Iteration 36300 (2.1345 iter/s, 46.8495s/100 iters), loss = 0.163496
I0825 16:33:40.018932  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.163496 (* 1 = 0.163496 loss)
I0825 16:33:40.019114  1476 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0825 16:33:50.905943  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:34:25.116096  1476 solver.cpp:357] Iteration 36400 (2.21761 iter/s, 45.0936s/100 iters), loss = 0.144614
I0825 16:34:25.116320  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144614 (* 1 = 0.144614 loss)
I0825 16:34:25.116367  1476 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0825 16:35:11.780511  1476 solver.cpp:514] Iteration 36500, Testing net (#0)
I0825 16:35:52.942649  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:35:53.110625  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.799801
I0825 16:35:53.110715  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.602483 (* 1 = 0.602483 loss)
I0825 16:35:53.457633  1476 solver.cpp:357] Iteration 36500 (1.13202 iter/s, 88.3373s/100 iters), loss = 0.164445
I0825 16:35:53.457806  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.164445 (* 1 = 0.164445 loss)
I0825 16:35:53.457852  1476 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0825 16:36:40.922214  1476 solver.cpp:357] Iteration 36600 (2.10712 iter/s, 47.4581s/100 iters), loss = 0.112371
I0825 16:36:40.922770  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112371 (* 1 = 0.112371 loss)
I0825 16:36:40.922947  1476 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0825 16:37:28.321964  1476 solver.cpp:357] Iteration 36700 (2.10991 iter/s, 47.3955s/100 iters), loss = 0.113505
I0825 16:37:28.322216  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113505 (* 1 = 0.113505 loss)
I0825 16:37:28.322263  1476 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0825 16:37:35.110280  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:38:15.703613  1476 solver.cpp:357] Iteration 36800 (2.11081 iter/s, 47.3753s/100 iters), loss = 0.156659
I0825 16:38:15.703795  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156659 (* 1 = 0.156659 loss)
I0825 16:38:15.703824  1476 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0825 16:39:02.350013  1476 solver.cpp:357] Iteration 36900 (2.14399 iter/s, 46.6421s/100 iters), loss = 0.134721
I0825 16:39:02.350219  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.134721 (* 1 = 0.134721 loss)
I0825 16:39:02.350265  1476 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0825 16:39:46.573720  1476 solver.cpp:514] Iteration 37000, Testing net (#0)
I0825 16:40:25.326457  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:40:25.531275  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.809601
I0825 16:40:25.531411  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.591397 (* 1 = 0.591397 loss)
I0825 16:40:25.807453  1476 solver.cpp:357] Iteration 37000 (1.19828 iter/s, 83.4532s/100 iters), loss = 0.143798
I0825 16:40:25.807624  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143798 (* 1 = 0.143798 loss)
I0825 16:40:25.807672  1476 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0825 16:41:13.304738  1476 solver.cpp:357] Iteration 37100 (2.10558 iter/s, 47.493s/100 iters), loss = 0.0886492
I0825 16:41:13.304970  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0886492 (* 1 = 0.0886492 loss)
I0825 16:41:13.305016  1476 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0825 16:41:15.972795  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:42:00.690898  1476 solver.cpp:357] Iteration 37200 (2.11044 iter/s, 47.3834s/100 iters), loss = 0.125193
I0825 16:42:00.691093  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125193 (* 1 = 0.125193 loss)
I0825 16:42:00.691138  1476 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0825 16:42:47.889600  1476 solver.cpp:357] Iteration 37300 (2.1188 iter/s, 47.1964s/100 iters), loss = 0.0950485
I0825 16:42:47.889856  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0950484 (* 1 = 0.0950484 loss)
I0825 16:42:47.889884  1476 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0825 16:43:35.213218  1476 solver.cpp:357] Iteration 37400 (2.11321 iter/s, 47.3214s/100 iters), loss = 0.232022
I0825 16:43:35.213405  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.232022 (* 1 = 0.232022 loss)
I0825 16:43:35.213435  1476 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0825 16:44:18.192560  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:44:19.851501  1476 solver.cpp:514] Iteration 37500, Testing net (#0)
I0825 16:44:59.701864  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:44:59.929030  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.808
I0825 16:44:59.929177  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.586748 (* 1 = 0.586748 loss)
I0825 16:45:00.258280  1476 solver.cpp:357] Iteration 37500 (1.17594 iter/s, 85.0387s/100 iters), loss = 0.14509
I0825 16:45:00.258433  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14509 (* 1 = 0.14509 loss)
I0825 16:45:00.258476  1476 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0825 16:45:47.872268  1476 solver.cpp:357] Iteration 37600 (2.10032 iter/s, 47.6117s/100 iters), loss = 0.121554
I0825 16:45:47.872505  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121554 (* 1 = 0.121554 loss)
I0825 16:45:47.872552  1476 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0825 16:46:35.247092  1476 solver.cpp:357] Iteration 37700 (2.11102 iter/s, 47.3704s/100 iters), loss = 0.155001
I0825 16:46:35.247299  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155001 (* 1 = 0.155001 loss)
I0825 16:46:35.247344  1476 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0825 16:47:21.035925  1476 solver.cpp:357] Iteration 37800 (2.18419 iter/s, 45.7835s/100 iters), loss = 0.191713
I0825 16:47:21.036164  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.191713 (* 1 = 0.191713 loss)
I0825 16:47:21.036208  1476 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0825 16:47:59.791934  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:48:06.301177  1476 solver.cpp:357] Iteration 37900 (2.20941 iter/s, 45.2609s/100 iters), loss = 0.145996
I0825 16:48:06.301343  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145996 (* 1 = 0.145996 loss)
I0825 16:48:06.301388  1476 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0825 16:48:52.866482  1476 solver.cpp:514] Iteration 38000, Testing net (#0)
I0825 16:49:33.877029  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:49:33.973719  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.806701
I0825 16:49:33.973841  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.601298 (* 1 = 0.601298 loss)
I0825 16:49:34.374346  1476 solver.cpp:357] Iteration 38000 (1.13545 iter/s, 88.071s/100 iters), loss = 0.0862639
I0825 16:49:34.374527  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0862639 (* 1 = 0.0862639 loss)
I0825 16:49:34.374573  1476 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0825 16:50:21.674427  1476 solver.cpp:357] Iteration 38100 (2.11445 iter/s, 47.2936s/100 iters), loss = 0.153745
I0825 16:50:21.674664  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.153745 (* 1 = 0.153745 loss)
I0825 16:50:21.674710  1476 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0825 16:51:08.809631  1476 solver.cpp:357] Iteration 38200 (2.12171 iter/s, 47.1319s/100 iters), loss = 0.0896891
I0825 16:51:08.809823  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0896891 (* 1 = 0.0896891 loss)
I0825 16:51:08.809850  1476 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0825 16:51:43.835108  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:51:54.332689  1476 solver.cpp:357] Iteration 38300 (2.1968 iter/s, 45.5208s/100 iters), loss = 0.173609
I0825 16:51:54.332864  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.173609 (* 1 = 0.173609 loss)
I0825 16:51:54.332907  1476 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0825 16:52:41.421255  1476 solver.cpp:357] Iteration 38400 (2.12386 iter/s, 47.0842s/100 iters), loss = 0.101842
I0825 16:52:41.422793  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101842 (* 1 = 0.101842 loss)
I0825 16:52:41.422842  1476 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0825 16:53:28.185371  1476 solver.cpp:514] Iteration 38500, Testing net (#0)
I0825 16:54:07.833967  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:54:08.017591  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8193
I0825 16:54:08.017742  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.556961 (* 1 = 0.556961 loss)
I0825 16:54:08.320088  1476 solver.cpp:357] Iteration 38500 (1.15082 iter/s, 86.8945s/100 iters), loss = 0.1109
I0825 16:54:08.320255  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.1109 (* 1 = 0.1109 loss)
I0825 16:54:08.320299  1476 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0825 16:54:50.420173  1476 solver.cpp:357] Iteration 38600 (2.37554 iter/s, 42.0957s/100 iters), loss = 0.104991
I0825 16:54:50.420364  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104991 (* 1 = 0.104991 loss)
I0825 16:54:50.420408  1476 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0825 16:55:20.283834  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:55:35.713775  1476 solver.cpp:357] Iteration 38700 (2.20796 iter/s, 45.2907s/100 iters), loss = 0.103742
I0825 16:55:35.714021  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103742 (* 1 = 0.103742 loss)
I0825 16:55:35.714066  1476 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0825 16:56:23.011940  1476 solver.cpp:357] Iteration 38800 (2.11392 iter/s, 47.3055s/100 iters), loss = 0.102678
I0825 16:56:23.012172  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102678 (* 1 = 0.102678 loss)
I0825 16:56:23.012217  1476 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0825 16:57:10.290618  1476 solver.cpp:357] Iteration 38900 (2.1149 iter/s, 47.2835s/100 iters), loss = 0.114462
I0825 16:57:10.290863  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114462 (* 1 = 0.114462 loss)
I0825 16:57:10.290910  1476 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0825 16:57:57.751150  1476 solver.cpp:514] Iteration 39000, Testing net (#0)
I0825 16:58:38.886349  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:58:39.063263  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.787101
I0825 16:58:39.063408  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.702697 (* 1 = 0.702697 loss)
I0825 16:58:39.444263  1476 solver.cpp:357] Iteration 39000 (1.12155 iter/s, 89.1621s/100 iters), loss = 0.0852936
I0825 16:58:39.444432  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0852935 (* 1 = 0.0852935 loss)
I0825 16:58:39.444476  1476 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0825 16:59:06.975250  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:59:26.321921  1476 solver.cpp:357] Iteration 39100 (2.13303 iter/s, 46.8817s/100 iters), loss = 0.160594
I0825 16:59:26.322206  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160594 (* 1 = 0.160594 loss)
I0825 16:59:26.322252  1476 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0825 17:00:13.752738  1476 solver.cpp:357] Iteration 39200 (2.10815 iter/s, 47.435s/100 iters), loss = 0.178333
I0825 17:00:13.753001  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.178333 (* 1 = 0.178333 loss)
I0825 17:00:13.753046  1476 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0825 17:01:01.136739  1476 solver.cpp:357] Iteration 39300 (2.11031 iter/s, 47.3863s/100 iters), loss = 0.135921
I0825 17:01:01.136978  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.135921 (* 1 = 0.135921 loss)
I0825 17:01:01.137027  1476 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0825 17:01:48.591194  1476 solver.cpp:357] Iteration 39400 (2.10732 iter/s, 47.4536s/100 iters), loss = 0.0895977
I0825 17:01:48.591439  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0895977 (* 1 = 0.0895977 loss)
I0825 17:01:48.591466  1476 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0825 17:02:11.278602  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:02:33.034564  1476 solver.cpp:514] Iteration 39500, Testing net (#0)
I0825 17:03:12.752184  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:03:12.958974  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8066
I0825 17:03:12.959117  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.605263 (* 1 = 0.605263 loss)
I0825 17:03:13.259884  1476 solver.cpp:357] Iteration 39500 (1.18101 iter/s, 84.6734s/100 iters), loss = 0.135416
I0825 17:03:13.260042  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.135416 (* 1 = 0.135416 loss)
I0825 17:03:13.260087  1476 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0825 17:03:57.702230  1476 solver.cpp:357] Iteration 39600 (2.25001 iter/s, 44.4443s/100 iters), loss = 0.0979261
I0825 17:03:57.702710  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.097926 (* 1 = 0.097926 loss)
I0825 17:03:57.702883  1476 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0825 17:04:38.895769  1476 solver.cpp:357] Iteration 39700 (2.42737 iter/s, 41.1968s/100 iters), loss = 0.100317
I0825 17:04:38.895995  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100316 (* 1 = 0.100316 loss)
I0825 17:04:38.896041  1476 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0825 17:05:25.539381  1476 solver.cpp:357] Iteration 39800 (2.14384 iter/s, 46.6454s/100 iters), loss = 0.12855
I0825 17:05:25.539582  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12855 (* 1 = 0.12855 loss)
I0825 17:05:25.539628  1476 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0825 17:05:44.229176  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:06:12.671442  1476 solver.cpp:357] Iteration 39900 (2.12182 iter/s, 47.1294s/100 iters), loss = 0.094941
I0825 17:06:12.671658  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0949409 (* 1 = 0.0949409 loss)
I0825 17:06:12.671703  1476 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0825 17:06:59.621057  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.caffemodel
I0825 17:06:59.637130  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.solverstate
I0825 17:06:59.640259  1476 solver.cpp:514] Iteration 40000, Testing net (#0)
I0825 17:07:42.569526  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:07:42.665016  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.843601
I0825 17:07:42.665086  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.489867 (* 1 = 0.489867 loss)
I0825 17:07:43.066587  1476 solver.cpp:357] Iteration 40000 (1.1062 iter/s, 90.3993s/100 iters), loss = 0.174235
I0825 17:07:43.066722  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.174235 (* 1 = 0.174235 loss)
I0825 17:07:43.066750  1476 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0825 17:08:30.324607  1476 solver.cpp:357] Iteration 40100 (2.11619 iter/s, 47.2547s/100 iters), loss = 0.154724
I0825 17:08:30.324766  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.154724 (* 1 = 0.154724 loss)
I0825 17:08:30.324807  1476 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0825 17:09:17.394541  1476 solver.cpp:357] Iteration 40200 (2.12456 iter/s, 47.0685s/100 iters), loss = 0.145008
I0825 17:09:17.394769  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145008 (* 1 = 0.145008 loss)
I0825 17:09:17.394815  1476 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0825 17:09:31.773712  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:10:02.636358  1476 solver.cpp:357] Iteration 40300 (2.21032 iter/s, 45.2422s/100 iters), loss = 0.116535
I0825 17:10:02.636644  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.116535 (* 1 = 0.116535 loss)
I0825 17:10:02.636692  1476 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0825 17:10:48.243647  1476 solver.cpp:357] Iteration 40400 (2.19272 iter/s, 45.6055s/100 iters), loss = 0.155582
I0825 17:10:48.244043  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155582 (* 1 = 0.155582 loss)
I0825 17:10:48.244138  1476 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0825 17:11:35.558133  1476 solver.cpp:514] Iteration 40500, Testing net (#0)
I0825 17:12:16.602469  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:12:16.815336  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.837301
I0825 17:12:16.815465  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.499119 (* 1 = 0.499119 loss)
I0825 17:12:17.202567  1476 solver.cpp:357] Iteration 40500 (1.12408 iter/s, 88.9613s/100 iters), loss = 0.0962424
I0825 17:12:17.202719  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0962423 (* 1 = 0.0962423 loss)
I0825 17:12:17.202764  1476 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0825 17:13:04.833061  1476 solver.cpp:357] Iteration 40600 (2.09958 iter/s, 47.6285s/100 iters), loss = 0.10936
I0825 17:13:04.833266  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10936 (* 1 = 0.10936 loss)
I0825 17:13:04.833312  1476 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0825 17:13:14.531468  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:13:52.064312  1476 solver.cpp:357] Iteration 40700 (2.11743 iter/s, 47.227s/100 iters), loss = 0.110515
I0825 17:13:52.064483  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110515 (* 1 = 0.110515 loss)
I0825 17:13:52.064514  1476 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0825 17:14:36.987620  1476 solver.cpp:357] Iteration 40800 (2.22613 iter/s, 44.9209s/100 iters), loss = 0.050199
I0825 17:14:36.987828  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.050199 (* 1 = 0.050199 loss)
I0825 17:14:36.987861  1476 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0825 17:15:23.061017  1476 solver.cpp:357] Iteration 40900 (2.17056 iter/s, 46.071s/100 iters), loss = 0.15857
I0825 17:15:23.062608  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.15857 (* 1 = 0.15857 loss)
I0825 17:15:23.062641  1476 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0825 17:16:10.062502  1476 solver.cpp:514] Iteration 41000, Testing net (#0)
I0825 17:16:51.529016  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:16:51.659487  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.830701
I0825 17:16:51.659634  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.544981 (* 1 = 0.544981 loss)
I0825 17:16:51.980958  1476 solver.cpp:357] Iteration 41000 (1.12459 iter/s, 88.9213s/100 iters), loss = 0.132006
I0825 17:16:51.981127  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.132006 (* 1 = 0.132006 loss)
I0825 17:16:51.981173  1476 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0825 17:16:57.219369  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:17:36.217610  1476 solver.cpp:357] Iteration 41100 (2.2606 iter/s, 44.2361s/100 iters), loss = 0.136974
I0825 17:17:36.217792  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136974 (* 1 = 0.136974 loss)
I0825 17:17:36.217835  1476 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0825 17:18:22.584951  1476 solver.cpp:357] Iteration 41200 (2.15691 iter/s, 46.3627s/100 iters), loss = 0.0639355
I0825 17:18:22.585130  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0639355 (* 1 = 0.0639355 loss)
I0825 17:18:22.585157  1476 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0825 17:19:09.992229  1476 solver.cpp:357] Iteration 41300 (2.10954 iter/s, 47.4038s/100 iters), loss = 0.109689
I0825 17:19:09.992483  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.109689 (* 1 = 0.109689 loss)
I0825 17:19:09.992514  1476 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0825 17:19:57.462255  1476 solver.cpp:357] Iteration 41400 (2.10671 iter/s, 47.4674s/100 iters), loss = 0.184147
I0825 17:19:57.462512  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.184147 (* 1 = 0.184147 loss)
I0825 17:19:57.462560  1476 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0825 17:19:58.654745  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:20:44.302505  1476 solver.cpp:514] Iteration 41500, Testing net (#0)
I0825 17:21:27.082080  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:21:27.293184  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8201
I0825 17:21:27.293421  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.566949 (* 1 = 0.566949 loss)
I0825 17:21:27.609438  1476 solver.cpp:357] Iteration 41500 (1.10931 iter/s, 90.1459s/100 iters), loss = 0.0875646
I0825 17:21:27.609820  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0875646 (* 1 = 0.0875646 loss)
I0825 17:21:27.609997  1476 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0825 17:22:14.988201  1476 solver.cpp:357] Iteration 41600 (2.11068 iter/s, 47.378s/100 iters), loss = 0.177551
I0825 17:22:14.988430  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.177551 (* 1 = 0.177551 loss)
I0825 17:22:14.988476  1476 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0825 17:23:02.234694  1476 solver.cpp:357] Iteration 41700 (2.11678 iter/s, 47.2415s/100 iters), loss = 0.0835639
I0825 17:23:02.235159  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0835638 (* 1 = 0.0835638 loss)
I0825 17:23:02.235337  1476 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0825 17:23:45.856748  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:23:49.398494  1476 solver.cpp:357] Iteration 41800 (2.1204 iter/s, 47.1609s/100 iters), loss = 0.171792
I0825 17:23:49.398681  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.171792 (* 1 = 0.171792 loss)
I0825 17:23:49.398727  1476 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0825 17:24:34.494379  1476 solver.cpp:357] Iteration 41900 (2.21775 iter/s, 45.0908s/100 iters), loss = 0.10335
I0825 17:24:34.494881  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10335 (* 1 = 0.10335 loss)
I0825 17:24:34.495060  1476 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0825 17:25:15.404088  1476 solver.cpp:514] Iteration 42000, Testing net (#0)
I0825 17:25:55.797399  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:25:55.951359  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.844801
I0825 17:25:55.951447  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.499987 (* 1 = 0.499987 loss)
I0825 17:25:56.283193  1476 solver.cpp:357] Iteration 42000 (1.22266 iter/s, 81.789s/100 iters), loss = 0.067832
I0825 17:25:56.283375  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0678319 (* 1 = 0.0678319 loss)
I0825 17:25:56.283422  1476 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0825 17:26:43.899560  1476 solver.cpp:357] Iteration 42100 (2.10034 iter/s, 47.6112s/100 iters), loss = 0.086139
I0825 17:26:43.900055  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0861389 (* 1 = 0.0861389 loss)
I0825 17:26:43.900230  1476 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0825 17:27:23.319732  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:27:31.179688  1476 solver.cpp:357] Iteration 42200 (2.11512 iter/s, 47.2787s/100 iters), loss = 0.127326
I0825 17:27:31.179854  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127326 (* 1 = 0.127326 loss)
I0825 17:27:31.179899  1476 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0825 17:28:18.541674  1476 solver.cpp:357] Iteration 42300 (2.11137 iter/s, 47.3627s/100 iters), loss = 0.130525
I0825 17:28:18.542294  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130525 (* 1 = 0.130525 loss)
I0825 17:28:18.542488  1476 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0825 17:29:05.308485  1476 solver.cpp:357] Iteration 42400 (2.13841 iter/s, 46.7637s/100 iters), loss = 0.124526
I0825 17:29:05.308732  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.124526 (* 1 = 0.124526 loss)
I0825 17:29:05.308780  1476 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0825 17:29:51.651880  1476 solver.cpp:514] Iteration 42500, Testing net (#0)
I0825 17:30:34.224293  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:30:34.453590  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.830101
I0825 17:30:34.453732  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.544017 (* 1 = 0.544017 loss)
I0825 17:30:34.690948  1476 solver.cpp:357] Iteration 42500 (1.11881 iter/s, 89.3805s/100 iters), loss = 0.0833913
I0825 17:30:34.691105  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0833913 (* 1 = 0.0833913 loss)
I0825 17:30:34.691149  1476 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0825 17:31:09.940630  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:31:21.924783  1476 solver.cpp:357] Iteration 42600 (2.11721 iter/s, 47.232s/100 iters), loss = 0.126624
I0825 17:31:21.924962  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126624 (* 1 = 0.126624 loss)
I0825 17:31:21.925009  1476 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0825 17:32:08.628499  1476 solver.cpp:357] Iteration 42700 (2.14153 iter/s, 46.6956s/100 iters), loss = 0.1228
I0825 17:32:08.628736  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.1228 (* 1 = 0.1228 loss)
I0825 17:32:08.628782  1476 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0825 17:32:53.081557  1476 solver.cpp:357] Iteration 42800 (2.24994 iter/s, 44.4455s/100 iters), loss = 0.077026
I0825 17:32:53.081797  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.077026 (* 1 = 0.077026 loss)
I0825 17:32:53.081845  1476 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0825 17:33:40.154642  1476 solver.cpp:357] Iteration 42900 (2.12464 iter/s, 47.0668s/100 iters), loss = 0.0595203
I0825 17:33:40.154881  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0595203 (* 1 = 0.0595203 loss)
I0825 17:33:40.154929  1476 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0825 17:34:10.406092  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:34:26.192461  1476 solver.cpp:514] Iteration 43000, Testing net (#0)
I0825 17:35:06.348486  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:35:06.531074  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7987
I0825 17:35:06.531390  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.697198 (* 1 = 0.697198 loss)
I0825 17:35:06.871484  1476 solver.cpp:357] Iteration 43000 (1.15333 iter/s, 86.7055s/100 iters), loss = 0.11645
I0825 17:35:06.871850  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11645 (* 1 = 0.11645 loss)
I0825 17:35:06.872025  1476 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0825 17:35:54.024070  1476 solver.cpp:357] Iteration 43100 (2.12107 iter/s, 47.146s/100 iters), loss = 0.107989
I0825 17:35:54.024303  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107989 (* 1 = 0.107989 loss)
I0825 17:35:54.024333  1476 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0825 17:36:41.427495  1476 solver.cpp:357] Iteration 43200 (2.10994 iter/s, 47.3948s/100 iters), loss = 0.123237
I0825 17:36:41.428009  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.123237 (* 1 = 0.123237 loss)
I0825 17:36:41.428187  1476 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0825 17:37:28.714804  1476 solver.cpp:357] Iteration 43300 (2.11491 iter/s, 47.2832s/100 iters), loss = 0.0764246
I0825 17:37:28.715076  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0764246 (* 1 = 0.0764246 loss)
I0825 17:37:28.715123  1476 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0825 17:37:55.064672  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:38:16.071704  1476 solver.cpp:357] Iteration 43400 (2.1118 iter/s, 47.3529s/100 iters), loss = 0.199808
I0825 17:38:16.072289  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.199808 (* 1 = 0.199808 loss)
I0825 17:38:16.072388  1476 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0825 17:39:03.041723  1476 solver.cpp:514] Iteration 43500, Testing net (#0)
I0825 17:39:43.150728  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:39:43.347342  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.875201
I0825 17:39:43.347472  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.387943 (* 1 = 0.387943 loss)
I0825 17:39:43.636119  1476 solver.cpp:357] Iteration 43500 (1.14208 iter/s, 87.5596s/100 iters), loss = 0.125085
I0825 17:39:43.636283  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125085 (* 1 = 0.125085 loss)
I0825 17:39:43.636329  1476 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0825 17:40:27.870122  1476 solver.cpp:357] Iteration 43600 (2.26098 iter/s, 44.2286s/100 iters), loss = 0.178024
I0825 17:40:27.870308  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.178024 (* 1 = 0.178024 loss)
I0825 17:40:27.870342  1476 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0825 17:41:14.951946  1476 solver.cpp:357] Iteration 43700 (2.1243 iter/s, 47.0743s/100 iters), loss = 0.0766174
I0825 17:41:14.952204  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0766174 (* 1 = 0.0766174 loss)
I0825 17:41:14.952251  1476 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0825 17:41:36.216338  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:42:01.739686  1476 solver.cpp:357] Iteration 43800 (2.13765 iter/s, 46.7803s/100 iters), loss = 0.0616903
I0825 17:42:01.740160  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0616902 (* 1 = 0.0616902 loss)
I0825 17:42:01.740331  1476 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0825 17:42:48.342468  1476 solver.cpp:357] Iteration 43900 (2.14586 iter/s, 46.6014s/100 iters), loss = 0.0813697
I0825 17:42:48.342680  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0813697 (* 1 = 0.0813697 loss)
I0825 17:42:48.342725  1476 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0825 17:43:35.269611  1476 solver.cpp:514] Iteration 44000, Testing net (#0)
I0825 17:44:17.219007  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:44:17.365392  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.871302
I0825 17:44:17.365550  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.412484 (* 1 = 0.412484 loss)
I0825 17:44:17.784564  1476 solver.cpp:357] Iteration 44000 (1.11809 iter/s, 89.4386s/100 iters), loss = 0.108053
I0825 17:44:17.784737  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108053 (* 1 = 0.108053 loss)
I0825 17:44:17.784781  1476 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0825 17:45:03.102950  1476 solver.cpp:357] Iteration 44100 (2.20685 iter/s, 45.3135s/100 iters), loss = 0.098114
I0825 17:45:03.103188  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.098114 (* 1 = 0.098114 loss)
I0825 17:45:03.103232  1476 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0825 17:45:19.831986  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:45:49.794811  1476 solver.cpp:357] Iteration 44200 (2.14183 iter/s, 46.6891s/100 iters), loss = 0.157503
I0825 17:45:49.795296  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.157503 (* 1 = 0.157503 loss)
I0825 17:45:49.795478  1476 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0825 17:46:36.986738  1476 solver.cpp:357] Iteration 44300 (2.11922 iter/s, 47.1872s/100 iters), loss = 0.0999682
I0825 17:46:36.986949  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0999682 (* 1 = 0.0999682 loss)
I0825 17:46:36.986995  1476 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0825 17:47:22.878453  1476 solver.cpp:357] Iteration 44400 (2.17937 iter/s, 45.8849s/100 iters), loss = 0.125058
I0825 17:47:22.878753  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125058 (* 1 = 0.125058 loss)
I0825 17:47:22.878803  1476 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0825 17:48:07.152407  1476 solver.cpp:514] Iteration 44500, Testing net (#0)
I0825 17:48:48.102020  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:48:48.249027  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.853301
I0825 17:48:48.249125  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.479741 (* 1 = 0.479741 loss)
I0825 17:48:48.670317  1476 solver.cpp:357] Iteration 44500 (1.16568 iter/s, 85.787s/100 iters), loss = 0.0969731
I0825 17:48:48.670485  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0969731 (* 1 = 0.0969731 loss)
I0825 17:48:48.670531  1476 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0825 17:49:01.732771  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:49:36.046779  1476 solver.cpp:357] Iteration 44600 (2.11086 iter/s, 47.374s/100 iters), loss = 0.11588
I0825 17:49:36.046974  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11588 (* 1 = 0.11588 loss)
I0825 17:49:36.047020  1476 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0825 17:50:23.632426  1476 solver.cpp:357] Iteration 44700 (2.10177 iter/s, 47.579s/100 iters), loss = 0.174318
I0825 17:50:23.632658  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.174318 (* 1 = 0.174318 loss)
I0825 17:50:23.632704  1476 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0825 17:51:11.075558  1476 solver.cpp:357] Iteration 44800 (2.10799 iter/s, 47.4386s/100 iters), loss = 0.120141
I0825 17:51:11.075744  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120141 (* 1 = 0.120141 loss)
I0825 17:51:11.075774  1476 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0825 17:51:58.500594  1476 solver.cpp:357] Iteration 44900 (2.10888 iter/s, 47.4185s/100 iters), loss = 0.140857
I0825 17:51:58.500761  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.140857 (* 1 = 0.140857 loss)
I0825 17:51:58.500793  1476 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0825 17:52:06.741255  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:52:45.317312  1476 solver.cpp:514] Iteration 45000, Testing net (#0)
I0825 17:53:26.471251  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:53:26.597286  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7821
I0825 17:53:26.597362  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.739813 (* 1 = 0.739813 loss)
I0825 17:53:26.889539  1476 solver.cpp:357] Iteration 45000 (1.13139 iter/s, 88.3867s/100 iters), loss = 0.0502245
I0825 17:53:26.889709  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0502245 (* 1 = 0.0502245 loss)
I0825 17:53:26.889755  1476 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0825 17:54:12.394814  1476 solver.cpp:357] Iteration 45100 (2.19766 iter/s, 45.503s/100 iters), loss = 0.0846392
I0825 17:54:12.395268  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0846392 (* 1 = 0.0846392 loss)
I0825 17:54:12.395445  1476 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0825 17:54:54.490808  1476 solver.cpp:357] Iteration 45200 (2.37589 iter/s, 42.0896s/100 iters), loss = 0.106911
I0825 17:54:54.491058  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106911 (* 1 = 0.106911 loss)
I0825 17:54:54.491106  1476 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0825 17:55:38.857650  1476 solver.cpp:357] Iteration 45300 (2.25415 iter/s, 44.3626s/100 iters), loss = 0.0781598
I0825 17:55:38.857874  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0781598 (* 1 = 0.0781598 loss)
I0825 17:55:38.857921  1476 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0825 17:55:42.860584  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:56:25.993775  1476 solver.cpp:357] Iteration 45400 (2.1218 iter/s, 47.1298s/100 iters), loss = 0.0993179
I0825 17:56:25.994079  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0993179 (* 1 = 0.0993179 loss)
I0825 17:56:25.994128  1476 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0825 17:57:13.076382  1476 solver.cpp:514] Iteration 45500, Testing net (#0)
I0825 17:57:53.911465  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:57:54.122169  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.846501
I0825 17:57:54.122319  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.497414 (* 1 = 0.497414 loss)
I0825 17:57:54.545872  1476 solver.cpp:357] Iteration 45500 (1.1293 iter/s, 88.5501s/100 iters), loss = 0.200669
I0825 17:57:54.546037  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.200669 (* 1 = 0.200669 loss)
I0825 17:57:54.546082  1476 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0825 17:58:41.569061  1476 solver.cpp:357] Iteration 45600 (2.1268 iter/s, 47.019s/100 iters), loss = 0.0847934
I0825 17:58:41.569285  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0847934 (* 1 = 0.0847934 loss)
I0825 17:58:41.569330  1476 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0825 17:59:28.830662  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:59:29.104866  1476 solver.cpp:357] Iteration 45700 (2.10386 iter/s, 47.5316s/100 iters), loss = 0.120307
I0825 17:59:29.105094  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120307 (* 1 = 0.120307 loss)
I0825 17:59:29.105108  1476 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0825 18:00:16.155059  1476 solver.cpp:357] Iteration 45800 (2.12558 iter/s, 47.046s/100 iters), loss = 0.0733459
I0825 18:00:16.155256  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0733459 (* 1 = 0.0733459 loss)
I0825 18:00:16.155300  1476 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0825 18:01:03.489847  1476 solver.cpp:357] Iteration 45900 (2.11262 iter/s, 47.3347s/100 iters), loss = 0.107725
I0825 18:01:03.490046  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107725 (* 1 = 0.107725 loss)
I0825 18:01:03.490090  1476 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0825 18:01:50.374907  1476 solver.cpp:514] Iteration 46000, Testing net (#0)
I0825 18:02:29.750036  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:02:29.926736  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.839201
I0825 18:02:29.926846  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.546981 (* 1 = 0.546981 loss)
I0825 18:02:30.131666  1476 solver.cpp:357] Iteration 46000 (1.15426 iter/s, 86.6357s/100 iters), loss = 0.0969886
I0825 18:02:30.131829  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0969886 (* 1 = 0.0969886 loss)
I0825 18:02:30.131872  1476 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0825 18:03:10.718854  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:03:15.672994  1476 solver.cpp:357] Iteration 46100 (2.19601 iter/s, 45.5371s/100 iters), loss = 0.142682
I0825 18:03:15.673405  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142682 (* 1 = 0.142682 loss)
I0825 18:03:15.673583  1476 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0825 18:04:02.777606  1476 solver.cpp:357] Iteration 46200 (2.12312 iter/s, 47.1005s/100 iters), loss = 0.158349
I0825 18:04:02.777863  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158349 (* 1 = 0.158349 loss)
I0825 18:04:02.777910  1476 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0825 18:04:48.699446  1476 solver.cpp:357] Iteration 46300 (2.17791 iter/s, 45.9156s/100 iters), loss = 0.0955165
I0825 18:04:48.699614  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0955165 (* 1 = 0.0955165 loss)
I0825 18:04:48.699642  1476 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0825 18:05:34.130327  1476 solver.cpp:357] Iteration 46400 (2.20124 iter/s, 45.429s/100 iters), loss = 0.0959521
I0825 18:05:34.131927  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0959521 (* 1 = 0.0959521 loss)
I0825 18:05:34.131975  1476 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0825 18:06:12.152993  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:06:21.014951  1476 solver.cpp:514] Iteration 46500, Testing net (#0)
I0825 18:07:02.208654  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:07:02.398713  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.863201
I0825 18:07:02.398880  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.450385 (* 1 = 0.450385 loss)
I0825 18:07:02.725968  1476 solver.cpp:357] Iteration 46500 (1.12879 iter/s, 88.5905s/100 iters), loss = 0.0698216
I0825 18:07:02.726124  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0698215 (* 1 = 0.0698215 loss)
I0825 18:07:02.726167  1476 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0825 18:07:49.829515  1476 solver.cpp:357] Iteration 46600 (2.12305 iter/s, 47.102s/100 iters), loss = 0.165861
I0825 18:07:49.829704  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.165861 (* 1 = 0.165861 loss)
I0825 18:07:49.829732  1476 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0825 18:08:37.115694  1476 solver.cpp:357] Iteration 46700 (2.11495 iter/s, 47.2824s/100 iters), loss = 0.11255
I0825 18:08:37.116201  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11255 (* 1 = 0.11255 loss)
I0825 18:08:37.116379  1476 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0825 18:09:24.370038  1476 solver.cpp:357] Iteration 46800 (2.11637 iter/s, 47.2507s/100 iters), loss = 0.112963
I0825 18:09:24.370232  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112962 (* 1 = 0.112962 loss)
I0825 18:09:24.370267  1476 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0825 18:09:55.962448  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:10:09.068944  1476 solver.cpp:357] Iteration 46900 (2.23738 iter/s, 44.6951s/100 iters), loss = 0.200273
I0825 18:10:09.069128  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.200273 (* 1 = 0.200273 loss)
I0825 18:10:09.069173  1476 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0825 18:10:54.446734  1476 solver.cpp:514] Iteration 47000, Testing net (#0)
I0825 18:11:35.337816  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:11:35.559806  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.821001
I0825 18:11:35.559942  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.623348 (* 1 = 0.623348 loss)
I0825 18:11:36.008316  1476 solver.cpp:357] Iteration 47000 (1.15027 iter/s, 86.9361s/100 iters), loss = 0.0803868
I0825 18:11:36.008487  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0803867 (* 1 = 0.0803867 loss)
I0825 18:11:36.008533  1476 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0825 18:12:23.386039  1476 solver.cpp:357] Iteration 47100 (2.11092 iter/s, 47.3727s/100 iters), loss = 0.144593
I0825 18:12:23.386471  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144593 (* 1 = 0.144593 loss)
I0825 18:12:23.386569  1476 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0825 18:13:10.636626  1476 solver.cpp:357] Iteration 47200 (2.11655 iter/s, 47.2468s/100 iters), loss = 0.102333
I0825 18:13:10.636837  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102333 (* 1 = 0.102333 loss)
I0825 18:13:10.636898  1476 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0825 18:13:39.790720  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:13:58.114490  1476 solver.cpp:357] Iteration 47300 (2.10642 iter/s, 47.474s/100 iters), loss = 0.090959
I0825 18:13:58.114711  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0909589 (* 1 = 0.0909589 loss)
I0825 18:13:58.114755  1476 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0825 18:14:44.529311  1476 solver.cpp:357] Iteration 47400 (2.15458 iter/s, 46.4127s/100 iters), loss = 0.136361
I0825 18:14:44.529621  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136361 (* 1 = 0.136361 loss)
I0825 18:14:44.529670  1476 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0825 18:15:29.488904  1476 solver.cpp:514] Iteration 47500, Testing net (#0)
I0825 18:16:12.054256  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:16:12.150207  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.879001
I0825 18:16:12.150313  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.376111 (* 1 = 0.376111 loss)
I0825 18:16:12.543551  1476 solver.cpp:357] Iteration 47500 (1.1362 iter/s, 88.013s/100 iters), loss = 0.0643587
I0825 18:16:12.543715  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0643586 (* 1 = 0.0643586 loss)
I0825 18:16:12.543759  1476 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0825 18:17:00.047247  1476 solver.cpp:357] Iteration 47600 (2.10527 iter/s, 47.4998s/100 iters), loss = 0.101145
I0825 18:17:00.047489  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101145 (* 1 = 0.101145 loss)
I0825 18:17:00.047534  1476 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0825 18:17:23.360785  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:17:44.105967  1476 solver.cpp:357] Iteration 47700 (2.2699 iter/s, 44.0548s/100 iters), loss = 0.135343
I0825 18:17:44.106184  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.135343 (* 1 = 0.135343 loss)
I0825 18:17:44.106230  1476 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0825 18:18:30.667439  1476 solver.cpp:357] Iteration 47800 (2.14778 iter/s, 46.5597s/100 iters), loss = 0.157016
I0825 18:18:30.667675  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.157016 (* 1 = 0.157016 loss)
I0825 18:18:30.667722  1476 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0825 18:19:16.565011  1476 solver.cpp:357] Iteration 47900 (2.17885 iter/s, 45.8958s/100 iters), loss = 0.135155
I0825 18:19:16.565239  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.135155 (* 1 = 0.135155 loss)
I0825 18:19:16.565286  1476 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0825 18:20:02.821032  1476 solver.cpp:514] Iteration 48000, Testing net (#0)
I0825 18:20:44.764704  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:20:44.943578  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.866502
I0825 18:20:44.943943  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.464375 (* 1 = 0.464375 loss)
I0825 18:20:45.403740  1476 solver.cpp:357] Iteration 48000 (1.12568 iter/s, 88.8353s/100 iters), loss = 0.157766
I0825 18:20:45.404141  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.157766 (* 1 = 0.157766 loss)
I0825 18:20:45.404310  1476 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0825 18:20:45.404458  1476 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0825 18:21:05.496253  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:21:32.777525  1476 solver.cpp:357] Iteration 48100 (2.11113 iter/s, 47.368s/100 iters), loss = 0.121776
I0825 18:21:32.777703  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121775 (* 1 = 0.121775 loss)
I0825 18:21:32.777732  1476 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0825 18:22:20.077183  1476 solver.cpp:357] Iteration 48200 (2.11445 iter/s, 47.2936s/100 iters), loss = 0.0857746
I0825 18:22:20.077749  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0857745 (* 1 = 0.0857745 loss)
I0825 18:22:20.077927  1476 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0825 18:23:07.694892  1476 solver.cpp:357] Iteration 48300 (2.10023 iter/s, 47.6138s/100 iters), loss = 0.0504287
I0825 18:23:07.695143  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0504286 (* 1 = 0.0504286 loss)
I0825 18:23:07.695190  1476 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0825 18:23:55.028839  1476 solver.cpp:357] Iteration 48400 (2.11292 iter/s, 47.3279s/100 iters), loss = 0.0993032
I0825 18:23:55.029585  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0993031 (* 1 = 0.0993031 loss)
I0825 18:23:55.029770  1476 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0825 18:24:10.905390  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:24:41.053962  1476 solver.cpp:514] Iteration 48500, Testing net (#0)
I0825 18:25:17.389379  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:25:17.493338  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.913802
I0825 18:25:17.493475  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271594 (* 1 = 0.271594 loss)
I0825 18:25:17.839735  1476 solver.cpp:357] Iteration 48500 (1.20759 iter/s, 82.8094s/100 iters), loss = 0.0708031
I0825 18:25:17.839906  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.070803 (* 1 = 0.070803 loss)
I0825 18:25:17.839952  1476 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0825 18:26:04.453992  1476 solver.cpp:357] Iteration 48600 (2.14545 iter/s, 46.6103s/100 iters), loss = 0.0956123
I0825 18:26:04.454180  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0956122 (* 1 = 0.0956122 loss)
I0825 18:26:04.454206  1476 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0825 18:26:51.908593  1476 solver.cpp:357] Iteration 48700 (2.10755 iter/s, 47.4486s/100 iters), loss = 0.0499955
I0825 18:26:51.910270  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0499954 (* 1 = 0.0499954 loss)
I0825 18:26:51.910317  1476 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0825 18:27:39.229868  1476 solver.cpp:357] Iteration 48800 (2.1133 iter/s, 47.3195s/100 iters), loss = 0.0958826
I0825 18:27:39.230068  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0958825 (* 1 = 0.0958825 loss)
I0825 18:27:39.230098  1476 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0825 18:27:50.899672  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:28:26.705478  1476 solver.cpp:357] Iteration 48900 (2.10661 iter/s, 47.4696s/100 iters), loss = 0.0875033
I0825 18:28:26.706485  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0875032 (* 1 = 0.0875032 loss)
I0825 18:28:26.706516  1476 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0825 18:29:13.800983  1476 solver.cpp:514] Iteration 49000, Testing net (#0)
I0825 18:29:54.854624  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:29:54.997819  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.915402
I0825 18:29:54.997918  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265016 (* 1 = 0.265016 loss)
I0825 18:29:55.335392  1476 solver.cpp:357] Iteration 49000 (1.12831 iter/s, 88.6284s/100 iters), loss = 0.0618399
I0825 18:29:55.335554  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0618398 (* 1 = 0.0618398 loss)
I0825 18:29:55.335599  1476 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0825 18:30:42.683390  1476 solver.cpp:357] Iteration 49100 (2.1121 iter/s, 47.3462s/100 iters), loss = 0.0605042
I0825 18:30:42.683647  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0605041 (* 1 = 0.0605041 loss)
I0825 18:30:42.683693  1476 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0825 18:31:29.982215  1476 solver.cpp:357] Iteration 49200 (2.11439 iter/s, 47.2949s/100 iters), loss = 0.0474195
I0825 18:31:29.982452  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0474194 (* 1 = 0.0474194 loss)
I0825 18:31:29.982513  1476 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0825 18:31:36.386646  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:32:15.314085  1476 solver.cpp:357] Iteration 49300 (2.20604 iter/s, 45.33s/100 iters), loss = 0.0785766
I0825 18:32:15.314239  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0785766 (* 1 = 0.0785766 loss)
I0825 18:32:15.314268  1476 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0825 18:32:59.173369  1476 solver.cpp:357] Iteration 49400 (2.28012 iter/s, 43.8574s/100 iters), loss = 0.0373395
I0825 18:32:59.173677  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0373394 (* 1 = 0.0373394 loss)
I0825 18:32:59.173727  1476 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0825 18:33:45.875272  1476 solver.cpp:514] Iteration 49500, Testing net (#0)
I0825 18:34:27.008268  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:34:27.201576  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.915102
I0825 18:34:27.201925  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.262901 (* 1 = 0.262901 loss)
I0825 18:34:27.552999  1476 solver.cpp:357] Iteration 49500 (1.13153 iter/s, 88.376s/100 iters), loss = 0.0780479
I0825 18:34:27.553388  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0780477 (* 1 = 0.0780477 loss)
I0825 18:34:27.553563  1476 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0825 18:35:12.017076  1476 solver.cpp:357] Iteration 49600 (2.2491 iter/s, 44.4622s/100 iters), loss = 0.0376011
I0825 18:35:12.017264  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.037601 (* 1 = 0.037601 loss)
I0825 18:35:12.017297  1476 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0825 18:35:14.452947  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:35:58.228540  1476 solver.cpp:357] Iteration 49700 (2.16425 iter/s, 46.2054s/100 iters), loss = 0.0767849
I0825 18:35:58.229073  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0767848 (* 1 = 0.0767848 loss)
I0825 18:35:58.229249  1476 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0825 18:36:45.535549  1476 solver.cpp:357] Iteration 49800 (2.11403 iter/s, 47.3031s/100 iters), loss = 0.0777851
I0825 18:36:45.535791  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0777849 (* 1 = 0.0777849 loss)
I0825 18:36:45.535837  1476 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0825 18:37:32.560571  1476 solver.cpp:357] Iteration 49900 (2.12661 iter/s, 47.0231s/100 iters), loss = 0.0540176
I0825 18:37:32.561100  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0540175 (* 1 = 0.0540175 loss)
I0825 18:37:32.561280  1476 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0825 18:38:17.752895  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:38:19.513916  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.caffemodel
I0825 18:38:19.549240  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.solverstate
I0825 18:38:19.553588  1476 solver.cpp:514] Iteration 50000, Testing net (#0)
I0825 18:39:02.542572  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:39:02.774313  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918503
I0825 18:39:02.774646  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.253047 (* 1 = 0.253047 loss)
I0825 18:39:03.143540  1476 solver.cpp:357] Iteration 50000 (1.10398 iter/s, 90.5815s/100 iters), loss = 0.045035
I0825 18:39:03.143937  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0450349 (* 1 = 0.0450349 loss)
I0825 18:39:03.144110  1476 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0825 18:39:49.219749  1476 solver.cpp:357] Iteration 50100 (2.17049 iter/s, 46.0726s/100 iters), loss = 0.0653386
I0825 18:39:49.219972  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0653385 (* 1 = 0.0653385 loss)
I0825 18:39:49.220031  1476 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0825 18:40:33.294515  1476 solver.cpp:357] Iteration 50200 (2.26868 iter/s, 44.0785s/100 iters), loss = 0.0584384
I0825 18:40:33.296733  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0584383 (* 1 = 0.0584383 loss)
I0825 18:40:33.296780  1476 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0825 18:41:20.760687  1476 solver.cpp:357] Iteration 50300 (2.1066 iter/s, 47.4699s/100 iters), loss = 0.0505069
I0825 18:41:20.761001  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0505068 (* 1 = 0.0505068 loss)
I0825 18:41:20.761049  1476 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0825 18:42:01.961644  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:42:08.395586  1476 solver.cpp:357] Iteration 50400 (2.09925 iter/s, 47.6361s/100 iters), loss = 0.0813326
I0825 18:42:08.395759  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0813325 (* 1 = 0.0813325 loss)
I0825 18:42:08.395805  1476 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0825 18:42:55.344523  1476 solver.cpp:514] Iteration 50500, Testing net (#0)
I0825 18:43:38.341972  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:43:38.427057  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919602
I0825 18:43:38.427143  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.254697 (* 1 = 0.254697 loss)
I0825 18:43:38.821594  1476 solver.cpp:357] Iteration 50500 (1.10581 iter/s, 90.4311s/100 iters), loss = 0.0369182
I0825 18:43:38.821751  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0369181 (* 1 = 0.0369181 loss)
I0825 18:43:38.821796  1476 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0825 18:44:25.706588  1476 solver.cpp:357] Iteration 50600 (2.13287 iter/s, 46.8851s/100 iters), loss = 0.0486189
I0825 18:44:25.707115  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0486188 (* 1 = 0.0486188 loss)
I0825 18:44:25.707293  1476 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0825 18:45:06.749930  1476 solver.cpp:357] Iteration 50700 (2.43649 iter/s, 41.0427s/100 iters), loss = 0.0293733
I0825 18:45:06.750154  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0293731 (* 1 = 0.0293731 loss)
I0825 18:45:06.750201  1476 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0825 18:45:42.026069  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:45:52.683877  1476 solver.cpp:357] Iteration 50800 (2.17696 iter/s, 45.9355s/100 iters), loss = 0.0411827
I0825 18:45:52.684161  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0411826 (* 1 = 0.0411826 loss)
I0825 18:45:52.684253  1476 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0825 18:46:40.136549  1476 solver.cpp:357] Iteration 50900 (2.10739 iter/s, 47.452s/100 iters), loss = 0.0463974
I0825 18:46:40.136735  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0463973 (* 1 = 0.0463973 loss)
I0825 18:46:40.136764  1476 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0825 18:47:25.076277  1476 solver.cpp:514] Iteration 51000, Testing net (#0)
I0825 18:48:04.319152  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:48:04.471688  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919303
I0825 18:48:04.471806  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.256725 (* 1 = 0.256725 loss)
I0825 18:48:04.826344  1476 solver.cpp:357] Iteration 51000 (1.18076 iter/s, 84.6916s/100 iters), loss = 0.046647
I0825 18:48:04.826493  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0466469 (* 1 = 0.0466469 loss)
I0825 18:48:04.826536  1476 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0825 18:48:52.085484  1476 solver.cpp:357] Iteration 51100 (2.11595 iter/s, 47.26s/100 iters), loss = 0.079965
I0825 18:48:52.086005  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0799649 (* 1 = 0.0799649 loss)
I0825 18:48:52.086216  1476 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0825 18:49:24.226799  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:49:39.670296  1476 solver.cpp:357] Iteration 51200 (2.10148 iter/s, 47.5856s/100 iters), loss = 0.0569303
I0825 18:49:39.670469  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0569301 (* 1 = 0.0569301 loss)
I0825 18:49:39.670513  1476 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0825 18:50:27.022239  1476 solver.cpp:357] Iteration 51300 (2.11182 iter/s, 47.3525s/100 iters), loss = 0.0549139
I0825 18:50:27.022485  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0549138 (* 1 = 0.0549138 loss)
I0825 18:50:27.022513  1476 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0825 18:51:14.128334  1476 solver.cpp:357] Iteration 51400 (2.12304 iter/s, 47.1023s/100 iters), loss = 0.0199621
I0825 18:51:14.128572  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0199619 (* 1 = 0.0199619 loss)
I0825 18:51:14.128618  1476 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0825 18:52:01.020212  1476 solver.cpp:514] Iteration 51500, Testing net (#0)
I0825 18:52:42.006129  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:52:42.213618  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921603
I0825 18:52:42.213968  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.255528 (* 1 = 0.255528 loss)
I0825 18:52:42.475854  1476 solver.cpp:357] Iteration 51500 (1.13186 iter/s, 88.3498s/100 iters), loss = 0.0223841
I0825 18:52:42.476243  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0223839 (* 1 = 0.0223839 loss)
I0825 18:52:42.476415  1476 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0825 18:53:10.290940  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:53:29.864154  1476 solver.cpp:357] Iteration 51600 (2.11022 iter/s, 47.3883s/100 iters), loss = 0.0449935
I0825 18:53:29.864352  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0449934 (* 1 = 0.0449934 loss)
I0825 18:53:29.864382  1476 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0825 18:54:17.067102  1476 solver.cpp:357] Iteration 51700 (2.1187 iter/s, 47.1987s/100 iters), loss = 0.0622097
I0825 18:54:17.067576  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0622096 (* 1 = 0.0622096 loss)
I0825 18:54:17.067754  1476 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0825 18:55:01.239351  1476 solver.cpp:357] Iteration 51800 (2.26388 iter/s, 44.1719s/100 iters), loss = 0.0423532
I0825 18:55:01.239565  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.042353 (* 1 = 0.042353 loss)
I0825 18:55:01.239614  1476 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0825 18:55:43.868337  1476 solver.cpp:357] Iteration 51900 (2.34585 iter/s, 42.6285s/100 iters), loss = 0.0614641
I0825 18:55:43.868549  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.061464 (* 1 = 0.061464 loss)
I0825 18:55:43.868595  1476 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0825 18:56:07.129367  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:56:30.568889  1476 solver.cpp:514] Iteration 52000, Testing net (#0)
I0825 18:57:11.820287  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:57:11.987977  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920302
I0825 18:57:11.988140  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.253944 (* 1 = 0.253944 loss)
I0825 18:57:12.359539  1476 solver.cpp:357] Iteration 52000 (1.13009 iter/s, 88.4883s/100 iters), loss = 0.0602752
I0825 18:57:12.359717  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0602751 (* 1 = 0.0602751 loss)
I0825 18:57:12.359763  1476 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0825 18:57:59.593653  1476 solver.cpp:357] Iteration 52100 (2.11723 iter/s, 47.2315s/100 iters), loss = 0.0323674
I0825 18:57:59.593834  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0323673 (* 1 = 0.0323673 loss)
I0825 18:57:59.593891  1476 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0825 18:58:46.628448  1476 solver.cpp:357] Iteration 52200 (2.12602 iter/s, 47.0362s/100 iters), loss = 0.0220643
I0825 18:58:46.630105  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0220642 (* 1 = 0.0220642 loss)
I0825 18:58:46.630153  1476 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0825 18:59:33.838363  1476 solver.cpp:357] Iteration 52300 (2.11832 iter/s, 47.2072s/100 iters), loss = 0.0443384
I0825 18:59:33.839136  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0443382 (* 1 = 0.0443382 loss)
I0825 18:59:33.839259  1476 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0825 18:59:52.599429  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:00:21.126430  1476 solver.cpp:357] Iteration 52400 (2.11491 iter/s, 47.2833s/100 iters), loss = 0.038883
I0825 19:00:21.126678  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0388829 (* 1 = 0.0388829 loss)
I0825 19:00:21.126724  1476 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0825 19:01:08.312142  1476 solver.cpp:514] Iteration 52500, Testing net (#0)
I0825 19:01:49.282063  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:01:49.439512  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919102
I0825 19:01:49.439635  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.258132 (* 1 = 0.258132 loss)
I0825 19:01:49.813069  1476 solver.cpp:357] Iteration 52500 (1.12756 iter/s, 88.6874s/100 iters), loss = 0.0544927
I0825 19:01:49.813241  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0544925 (* 1 = 0.0544925 loss)
I0825 19:01:49.813288  1476 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0825 19:02:34.372653  1476 solver.cpp:357] Iteration 52600 (2.24444 iter/s, 44.5545s/100 iters), loss = 0.0266689
I0825 19:02:34.372836  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0266688 (* 1 = 0.0266688 loss)
I0825 19:02:34.372866  1476 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0825 19:03:19.967217  1476 solver.cpp:357] Iteration 52700 (2.19339 iter/s, 45.5916s/100 iters), loss = 0.0608983
I0825 19:03:19.967736  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0608982 (* 1 = 0.0608982 loss)
I0825 19:03:19.967916  1476 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0825 19:03:34.434202  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:04:07.443980  1476 solver.cpp:357] Iteration 52800 (2.10642 iter/s, 47.4739s/100 iters), loss = 0.0207843
I0825 19:04:07.444151  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0207842 (* 1 = 0.0207842 loss)
I0825 19:04:07.444180  1476 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0825 19:04:53.567253  1476 solver.cpp:357] Iteration 52900 (2.16825 iter/s, 46.1202s/100 iters), loss = 0.0725225
I0825 19:04:53.567796  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0725224 (* 1 = 0.0725224 loss)
I0825 19:04:53.567975  1476 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0825 19:05:38.471355  1476 solver.cpp:514] Iteration 53000, Testing net (#0)
I0825 19:06:21.186842  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:06:21.402829  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919502
I0825 19:06:21.402967  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.259625 (* 1 = 0.259625 loss)
I0825 19:06:21.766180  1476 solver.cpp:357] Iteration 53000 (1.13382 iter/s, 88.1972s/100 iters), loss = 0.0367593
I0825 19:06:21.766360  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0367592 (* 1 = 0.0367592 loss)
I0825 19:06:21.766407  1476 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0825 19:07:09.170145  1476 solver.cpp:357] Iteration 53100 (2.10966 iter/s, 47.4009s/100 iters), loss = 0.0331153
I0825 19:07:09.170379  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0331151 (* 1 = 0.0331151 loss)
I0825 19:07:09.170425  1476 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0825 19:07:18.726913  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:07:56.269492  1476 solver.cpp:357] Iteration 53200 (2.1234 iter/s, 47.0942s/100 iters), loss = 0.0541563
I0825 19:07:56.269739  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0541561 (* 1 = 0.0541561 loss)
I0825 19:07:56.269786  1476 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0825 19:08:43.618705  1476 solver.cpp:357] Iteration 53300 (2.11211 iter/s, 47.3461s/100 iters), loss = 0.0304829
I0825 19:08:43.619256  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0304828 (* 1 = 0.0304828 loss)
I0825 19:08:43.619374  1476 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0825 19:09:30.348162  1476 solver.cpp:357] Iteration 53400 (2.14021 iter/s, 46.7243s/100 iters), loss = 0.042543
I0825 19:09:30.348366  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0425428 (* 1 = 0.0425428 loss)
I0825 19:09:30.348400  1476 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0825 19:10:12.031282  1476 solver.cpp:514] Iteration 53500, Testing net (#0)
I0825 19:10:52.112802  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:10:52.279538  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920902
I0825 19:10:52.279650  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.258984 (* 1 = 0.258984 loss)
I0825 19:10:52.507212  1476 solver.cpp:357] Iteration 53500 (1.21718 iter/s, 82.1569s/100 iters), loss = 0.0452146
I0825 19:10:52.507371  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0452145 (* 1 = 0.0452145 loss)
I0825 19:10:52.507416  1476 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0825 19:10:58.057552  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:11:39.718112  1476 solver.cpp:357] Iteration 53600 (2.1183 iter/s, 47.2077s/100 iters), loss = 0.0184709
I0825 19:11:39.718298  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0184707 (* 1 = 0.0184707 loss)
I0825 19:11:39.718327  1476 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0825 19:12:26.844121  1476 solver.cpp:357] Iteration 53700 (2.12211 iter/s, 47.1228s/100 iters), loss = 0.0280219
I0825 19:12:26.844525  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0280218 (* 1 = 0.0280218 loss)
I0825 19:12:26.844697  1476 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0825 19:13:14.082312  1476 solver.cpp:357] Iteration 53800 (2.11698 iter/s, 47.2372s/100 iters), loss = 0.0190393
I0825 19:13:14.082536  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0190391 (* 1 = 0.0190391 loss)
I0825 19:13:14.082582  1476 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0825 19:14:01.235963  1476 solver.cpp:357] Iteration 53900 (2.12087 iter/s, 47.1504s/100 iters), loss = 0.0341896
I0825 19:14:01.236157  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0341894 (* 1 = 0.0341894 loss)
I0825 19:14:01.236186  1476 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0825 19:14:02.184206  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:14:47.594632  1476 solver.cpp:514] Iteration 54000, Testing net (#0)
I0825 19:15:27.884433  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:15:28.085171  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921703
I0825 19:15:28.085317  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.259892 (* 1 = 0.259892 loss)
I0825 19:15:28.410791  1476 solver.cpp:357] Iteration 54000 (1.14726 iter/s, 87.1645s/100 iters), loss = 0.0390488
I0825 19:15:28.410953  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0390486 (* 1 = 0.0390486 loss)
I0825 19:15:28.410997  1476 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0825 19:16:14.918421  1476 solver.cpp:357] Iteration 54100 (2.15051 iter/s, 46.5005s/100 iters), loss = 0.058671
I0825 19:16:14.918951  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0586708 (* 1 = 0.0586708 loss)
I0825 19:16:14.919163  1476 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0825 19:17:02.225735  1476 solver.cpp:357] Iteration 54200 (2.11415 iter/s, 47.3004s/100 iters), loss = 0.0294438
I0825 19:17:02.225950  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0294437 (* 1 = 0.0294437 loss)
I0825 19:17:02.225996  1476 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0825 19:17:42.356657  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:17:45.706095  1476 solver.cpp:357] Iteration 54300 (2.30023 iter/s, 43.4739s/100 iters), loss = 0.0382093
I0825 19:17:45.706262  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0382091 (* 1 = 0.0382091 loss)
I0825 19:17:45.706306  1476 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0825 19:18:32.133072  1476 solver.cpp:357] Iteration 54400 (2.1541 iter/s, 46.4232s/100 iters), loss = 0.0367026
I0825 19:18:32.133303  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0367025 (* 1 = 0.0367025 loss)
I0825 19:18:32.133348  1476 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0825 19:19:19.085019  1476 solver.cpp:514] Iteration 54500, Testing net (#0)
I0825 19:20:01.771980  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:20:01.973523  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920302
I0825 19:20:01.973747  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.259658 (* 1 = 0.259658 loss)
I0825 19:20:02.291626  1476 solver.cpp:357] Iteration 54500 (1.10925 iter/s, 90.1508s/100 iters), loss = 0.0373572
I0825 19:20:02.291896  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0373571 (* 1 = 0.0373571 loss)
I0825 19:20:02.291988  1476 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0825 19:20:49.894105  1476 solver.cpp:357] Iteration 54600 (2.1009 iter/s, 47.5987s/100 iters), loss = 0.02588
I0825 19:20:49.894348  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0258798 (* 1 = 0.0258798 loss)
I0825 19:20:49.894395  1476 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0825 19:21:28.787228  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:21:36.553428  1476 solver.cpp:357] Iteration 54700 (2.14346 iter/s, 46.6536s/100 iters), loss = 0.037945
I0825 19:21:36.553613  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0379448 (* 1 = 0.0379448 loss)
I0825 19:21:36.553660  1476 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0825 19:22:22.410554  1476 solver.cpp:357] Iteration 54800 (2.18098 iter/s, 45.851s/100 iters), loss = 0.0280426
I0825 19:22:22.411083  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0280425 (* 1 = 0.0280425 loss)
I0825 19:22:22.411262  1476 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0825 19:23:09.285007  1476 solver.cpp:357] Iteration 54900 (2.13351 iter/s, 46.8712s/100 iters), loss = 0.0466444
I0825 19:23:09.285209  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0466442 (* 1 = 0.0466442 loss)
I0825 19:23:09.285254  1476 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0825 19:23:56.402794  1476 solver.cpp:514] Iteration 55000, Testing net (#0)
I0825 19:24:36.870934  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:24:37.024112  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921002
I0825 19:24:37.024253  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.261716 (* 1 = 0.261716 loss)
I0825 19:24:37.385706  1476 solver.cpp:357] Iteration 55000 (1.13509 iter/s, 88.0988s/100 iters), loss = 0.0288581
I0825 19:24:37.385877  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288579 (* 1 = 0.0288579 loss)
I0825 19:24:37.385922  1476 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0825 19:25:06.943853  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:25:17.682962  1476 solver.cpp:357] Iteration 55100 (2.482 iter/s, 40.2902s/100 iters), loss = 0.0681098
I0825 19:25:17.683137  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0681096 (* 1 = 0.0681096 loss)
I0825 19:25:17.683194  1476 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0825 19:26:03.601624  1476 solver.cpp:357] Iteration 55200 (2.17781 iter/s, 45.9178s/100 iters), loss = 0.0330653
I0825 19:26:03.606714  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0330652 (* 1 = 0.0330652 loss)
I0825 19:26:03.606823  1476 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0825 19:26:51.047020  1476 solver.cpp:357] Iteration 55300 (2.10801 iter/s, 47.4381s/100 iters), loss = 0.0327596
I0825 19:26:51.047268  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0327594 (* 1 = 0.0327594 loss)
I0825 19:26:51.047313  1476 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0825 19:27:38.411586  1476 solver.cpp:357] Iteration 55400 (2.11132 iter/s, 47.3638s/100 iters), loss = 0.0342995
I0825 19:27:38.411825  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0342994 (* 1 = 0.0342994 loss)
I0825 19:27:38.411870  1476 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0825 19:28:08.836298  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:28:25.329922  1476 solver.cpp:514] Iteration 55500, Testing net (#0)
I0825 19:29:08.408114  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:29:08.566304  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918202
I0825 19:29:08.566457  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.267373 (* 1 = 0.267373 loss)
I0825 19:29:08.835809  1476 solver.cpp:357] Iteration 55500 (1.10596 iter/s, 90.4196s/100 iters), loss = 0.0428271
I0825 19:29:08.835955  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0428269 (* 1 = 0.0428269 loss)
I0825 19:29:08.835999  1476 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0825 19:29:56.210846  1476 solver.cpp:357] Iteration 55600 (2.11093 iter/s, 47.3724s/100 iters), loss = 0.0172714
I0825 19:29:56.211387  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0172712 (* 1 = 0.0172712 loss)
I0825 19:29:56.211567  1476 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0825 19:30:43.626547  1476 solver.cpp:357] Iteration 55700 (2.10931 iter/s, 47.409s/100 iters), loss = 0.044918
I0825 19:30:43.626794  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0449178 (* 1 = 0.0449178 loss)
I0825 19:30:43.626842  1476 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0825 19:31:31.087622  1476 solver.cpp:357] Iteration 55800 (2.1072 iter/s, 47.4565s/100 iters), loss = 0.022426
I0825 19:31:31.088143  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0224258 (* 1 = 0.0224258 loss)
I0825 19:31:31.088318  1476 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0825 19:31:57.462174  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:32:17.747929  1476 solver.cpp:357] Iteration 55900 (2.14336 iter/s, 46.6558s/100 iters), loss = 0.0890076
I0825 19:32:17.748162  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0890075 (* 1 = 0.0890075 loss)
I0825 19:32:17.748209  1476 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0825 19:33:01.232542  1476 solver.cpp:514] Iteration 56000, Testing net (#0)
I0825 19:33:42.269526  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:33:42.465706  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919202
I0825 19:33:42.465849  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.26461 (* 1 = 0.26461 loss)
I0825 19:33:42.838555  1476 solver.cpp:357] Iteration 56000 (1.17531 iter/s, 85.084s/100 iters), loss = 0.0409806
I0825 19:33:42.838721  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0409804 (* 1 = 0.0409804 loss)
I0825 19:33:42.838765  1476 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0825 19:34:27.760054  1476 solver.cpp:357] Iteration 56100 (2.22632 iter/s, 44.9171s/100 iters), loss = 0.0366305
I0825 19:34:27.760285  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0366303 (* 1 = 0.0366303 loss)
I0825 19:34:27.760331  1476 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0825 19:35:12.881711  1476 solver.cpp:357] Iteration 56200 (2.21644 iter/s, 45.1173s/100 iters), loss = 0.0357775
I0825 19:35:12.881896  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0357773 (* 1 = 0.0357773 loss)
I0825 19:35:12.881925  1476 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0825 19:35:33.097467  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:35:57.921726  1476 solver.cpp:357] Iteration 56300 (2.22036 iter/s, 45.0378s/100 iters), loss = 0.0267067
I0825 19:35:57.922034  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0267065 (* 1 = 0.0267065 loss)
I0825 19:35:57.922081  1476 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0825 19:36:45.246170  1476 solver.cpp:357] Iteration 56400 (2.11326 iter/s, 47.3202s/100 iters), loss = 0.0228565
I0825 19:36:45.246403  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0228563 (* 1 = 0.0228563 loss)
I0825 19:36:45.246449  1476 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0825 19:37:32.291625  1476 solver.cpp:514] Iteration 56500, Testing net (#0)
I0825 19:38:13.484088  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:38:13.647279  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923102
I0825 19:38:13.647378  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.263465 (* 1 = 0.263465 loss)
I0825 19:38:14.085892  1476 solver.cpp:357] Iteration 56500 (1.12565 iter/s, 88.8376s/100 iters), loss = 0.0187779
I0825 19:38:14.086014  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187778 (* 1 = 0.0187778 loss)
I0825 19:38:14.086040  1476 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0825 19:39:01.435120  1476 solver.cpp:357] Iteration 56600 (2.11206 iter/s, 47.3471s/100 iters), loss = 0.0271527
I0825 19:39:01.435330  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0271526 (* 1 = 0.0271526 loss)
I0825 19:39:01.435375  1476 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0825 19:39:18.709118  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:39:47.814705  1476 solver.cpp:357] Iteration 56700 (2.15632 iter/s, 46.3754s/100 iters), loss = 0.070665
I0825 19:39:47.814860  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0706649 (* 1 = 0.0706649 loss)
I0825 19:39:47.814889  1476 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0825 19:40:31.289881  1476 solver.cpp:357] Iteration 56800 (2.30027 iter/s, 43.4731s/100 iters), loss = 0.033051
I0825 19:40:31.290030  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0330508 (* 1 = 0.0330508 loss)
I0825 19:40:31.290057  1476 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0825 19:41:18.515799  1476 solver.cpp:357] Iteration 56900 (2.11757 iter/s, 47.2239s/100 iters), loss = 0.0453733
I0825 19:41:18.516026  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0453731 (* 1 = 0.0453731 loss)
I0825 19:41:18.516073  1476 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0825 19:42:05.324904  1476 solver.cpp:514] Iteration 57000, Testing net (#0)
I0825 19:42:46.636313  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:42:46.762874  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921902
I0825 19:42:46.762971  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.266126 (* 1 = 0.266126 loss)
I0825 19:42:47.109472  1476 solver.cpp:357] Iteration 57000 (1.1288 iter/s, 88.5897s/100 iters), loss = 0.0326178
I0825 19:42:47.109638  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0326177 (* 1 = 0.0326177 loss)
I0825 19:42:47.109686  1476 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0825 19:43:00.036345  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:43:34.257475  1476 solver.cpp:357] Iteration 57100 (2.12114 iter/s, 47.1445s/100 iters), loss = 0.0285879
I0825 19:43:34.257649  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0285877 (* 1 = 0.0285877 loss)
I0825 19:43:34.257681  1476 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0825 19:44:21.532728  1476 solver.cpp:357] Iteration 57200 (2.11536 iter/s, 47.2732s/100 iters), loss = 0.0436537
I0825 19:44:21.532984  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0436536 (* 1 = 0.0436536 loss)
I0825 19:44:21.533030  1476 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0825 19:45:07.191980  1476 solver.cpp:357] Iteration 57300 (2.19034 iter/s, 45.655s/100 iters), loss = 0.031335
I0825 19:45:07.192551  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0313348 (* 1 = 0.0313348 loss)
I0825 19:45:07.192585  1476 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0825 19:45:52.424013  1476 solver.cpp:357] Iteration 57400 (2.21092 iter/s, 45.2301s/100 iters), loss = 0.0261579
I0825 19:45:52.424172  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0261577 (* 1 = 0.0261577 loss)
I0825 19:45:52.424198  1476 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0825 19:46:00.764888  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:46:38.417039  1476 solver.cpp:514] Iteration 57500, Testing net (#0)
I0825 19:47:17.729832  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:47:17.901854  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920802
I0825 19:47:17.902004  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270008 (* 1 = 0.270008 loss)
I0825 19:47:18.265089  1476 solver.cpp:357] Iteration 57500 (1.16495 iter/s, 85.8408s/100 iters), loss = 0.0364551
I0825 19:47:18.265316  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0364549 (* 1 = 0.0364549 loss)
I0825 19:47:18.265328  1476 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0825 19:48:02.072245  1476 solver.cpp:357] Iteration 57600 (2.28284 iter/s, 43.805s/100 iters), loss = 0.0288565
I0825 19:48:02.072480  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288563 (* 1 = 0.0288563 loss)
I0825 19:48:02.072528  1476 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0825 19:48:49.625838  1476 solver.cpp:357] Iteration 57700 (2.10292 iter/s, 47.5529s/100 iters), loss = 0.0303827
I0825 19:48:49.626070  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0303825 (* 1 = 0.0303825 loss)
I0825 19:48:49.626117  1476 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0825 19:49:37.147671  1476 solver.cpp:357] Iteration 57800 (2.10443 iter/s, 47.5188s/100 iters), loss = 0.0477483
I0825 19:49:37.147866  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0477481 (* 1 = 0.0477481 loss)
I0825 19:49:37.147910  1476 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0825 19:49:41.100756  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:50:24.487352  1476 solver.cpp:357] Iteration 57900 (2.11235 iter/s, 47.3406s/100 iters), loss = 0.0327702
I0825 19:50:24.487576  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0327701 (* 1 = 0.0327701 loss)
I0825 19:50:24.487623  1476 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0825 19:51:11.228801  1476 solver.cpp:514] Iteration 58000, Testing net (#0)
I0825 19:51:52.143764  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:51:52.303153  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920002
I0825 19:51:52.303225  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.272513 (* 1 = 0.272513 loss)
I0825 19:51:52.660392  1476 solver.cpp:357] Iteration 58000 (1.13415 iter/s, 88.172s/100 iters), loss = 0.0207956
I0825 19:51:52.660573  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0207954 (* 1 = 0.0207954 loss)
I0825 19:51:52.660620  1476 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0825 19:52:40.130578  1476 solver.cpp:357] Iteration 58100 (2.10676 iter/s, 47.4664s/100 iters), loss = 0.0209218
I0825 19:52:40.131121  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0209216 (* 1 = 0.0209216 loss)
I0825 19:52:40.131299  1476 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0825 19:53:27.266726  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:53:27.564867  1476 solver.cpp:357] Iteration 58200 (2.10836 iter/s, 47.4303s/100 iters), loss = 0.0324456
I0825 19:53:27.565099  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0324455 (* 1 = 0.0324455 loss)
I0825 19:53:27.565112  1476 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0825 19:54:14.627167  1476 solver.cpp:357] Iteration 58300 (2.12494 iter/s, 47.0602s/100 iters), loss = 0.0301369
I0825 19:54:14.627840  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0301367 (* 1 = 0.0301367 loss)
I0825 19:54:14.628021  1476 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0825 19:54:59.998456  1476 solver.cpp:357] Iteration 58400 (2.20425 iter/s, 45.3669s/100 iters), loss = 0.0366859
I0825 19:54:59.998670  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0366858 (* 1 = 0.0366858 loss)
I0825 19:54:59.998718  1476 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0825 19:55:42.162725  1476 solver.cpp:514] Iteration 58500, Testing net (#0)
I0825 19:56:23.577759  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:56:23.813673  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919602
I0825 19:56:23.813805  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271512 (* 1 = 0.271512 loss)
I0825 19:56:24.069489  1476 solver.cpp:357] Iteration 58500 (1.18948 iter/s, 84.0704s/100 iters), loss = 0.0249901
I0825 19:56:24.069653  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.02499 (* 1 = 0.02499 loss)
I0825 19:56:24.069696  1476 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0825 19:57:06.411736  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:57:11.464834  1476 solver.cpp:357] Iteration 58600 (2.11002 iter/s, 47.3928s/100 iters), loss = 0.0431639
I0825 19:57:11.465251  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0431637 (* 1 = 0.0431637 loss)
I0825 19:57:11.465430  1476 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0825 19:57:59.030989  1476 solver.cpp:357] Iteration 58700 (2.10245 iter/s, 47.5636s/100 iters), loss = 0.018865
I0825 19:57:59.031208  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0188649 (* 1 = 0.0188649 loss)
I0825 19:57:59.031253  1476 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0825 19:58:46.479934  1476 solver.cpp:357] Iteration 58800 (2.10755 iter/s, 47.4484s/100 iters), loss = 0.0259239
I0825 19:58:46.480293  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0259238 (* 1 = 0.0259238 loss)
I0825 19:58:46.480383  1476 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0825 19:59:32.728934  1476 solver.cpp:357] Iteration 58900 (2.16216 iter/s, 46.2501s/100 iters), loss = 0.0218094
I0825 19:59:32.729140  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0218092 (* 1 = 0.0218092 loss)
I0825 19:59:32.729187  1476 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0825 20:00:10.095208  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:00:18.921535  1476 solver.cpp:514] Iteration 59000, Testing net (#0)
I0825 20:01:01.443372  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:01:01.583623  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922003
I0825 20:01:01.583778  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265803 (* 1 = 0.265803 loss)
I0825 20:01:01.960206  1476 solver.cpp:357] Iteration 59000 (1.1207 iter/s, 89.2297s/100 iters), loss = 0.0373244
I0825 20:01:01.960363  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0373242 (* 1 = 0.0373242 loss)
I0825 20:01:01.960410  1476 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0825 20:01:49.291471  1476 solver.cpp:357] Iteration 59100 (2.1129 iter/s, 47.3283s/100 iters), loss = 0.0330855
I0825 20:01:49.292012  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0330853 (* 1 = 0.0330853 loss)
I0825 20:01:49.292193  1476 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0825 20:02:34.634264  1476 solver.cpp:357] Iteration 59200 (2.20567 iter/s, 45.3376s/100 iters), loss = 0.0209927
I0825 20:02:34.634451  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0209925 (* 1 = 0.0209925 loss)
I0825 20:02:34.634479  1476 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0825 20:03:20.054347  1476 solver.cpp:357] Iteration 59300 (2.20182 iter/s, 45.417s/100 iters), loss = 0.0408225
I0825 20:03:20.054576  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0408223 (* 1 = 0.0408223 loss)
I0825 20:03:20.054622  1476 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0825 20:03:53.609947  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:04:07.632338  1476 solver.cpp:357] Iteration 59400 (2.10195 iter/s, 47.5749s/100 iters), loss = 0.0370575
I0825 20:04:07.632506  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0370574 (* 1 = 0.0370574 loss)
I0825 20:04:07.632551  1476 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0825 20:04:54.292776  1476 solver.cpp:514] Iteration 59500, Testing net (#0)
I0825 20:05:34.552928  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:05:34.679193  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918802
I0825 20:05:34.679286  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271668 (* 1 = 0.271668 loss)
I0825 20:05:35.018369  1476 solver.cpp:357] Iteration 59500 (1.14438 iter/s, 87.3839s/100 iters), loss = 0.0543259
I0825 20:05:35.018541  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0543257 (* 1 = 0.0543257 loss)
I0825 20:05:35.018586  1476 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0825 20:06:21.438510  1476 solver.cpp:357] Iteration 59600 (2.15429 iter/s, 46.419s/100 iters), loss = 0.0244231
I0825 20:06:21.438740  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0244229 (* 1 = 0.0244229 loss)
I0825 20:06:21.438786  1476 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0825 20:07:08.948854  1476 solver.cpp:357] Iteration 59700 (2.10504 iter/s, 47.505s/100 iters), loss = 0.0257699
I0825 20:07:08.949405  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0257698 (* 1 = 0.0257698 loss)
I0825 20:07:08.949589  1476 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0825 20:07:38.228809  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:07:56.486269  1476 solver.cpp:357] Iteration 59800 (2.10375 iter/s, 47.5341s/100 iters), loss = 0.0204848
I0825 20:07:56.486517  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0204847 (* 1 = 0.0204847 loss)
I0825 20:07:56.486562  1476 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0825 20:08:43.836529  1476 solver.cpp:357] Iteration 59900 (2.11198 iter/s, 47.349s/100 iters), loss = 0.0508828
I0825 20:08:43.837064  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0508827 (* 1 = 0.0508827 loss)
I0825 20:08:43.837242  1476 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0825 20:09:30.772136  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.caffemodel
I0825 20:09:30.788627  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.solverstate
I0825 20:09:30.791671  1476 solver.cpp:514] Iteration 60000, Testing net (#0)
I0825 20:10:09.529516  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:10:09.629966  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920102
I0825 20:10:09.630046  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270728 (* 1 = 0.270728 loss)
I0825 20:10:09.972753  1476 solver.cpp:357] Iteration 60000 (1.16099 iter/s, 86.1337s/100 iters), loss = 0.0344849
I0825 20:10:09.972932  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0344847 (* 1 = 0.0344847 loss)
I0825 20:10:09.972980  1476 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0825 20:10:55.330242  1476 solver.cpp:357] Iteration 60100 (2.20498 iter/s, 45.3519s/100 iters), loss = 0.0267413
I0825 20:10:55.330549  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0267411 (* 1 = 0.0267411 loss)
I0825 20:10:55.330596  1476 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0825 20:11:20.154759  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:11:42.330476  1476 solver.cpp:357] Iteration 60200 (2.1278 iter/s, 46.9968s/100 iters), loss = 0.0239526
I0825 20:11:42.330719  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0239525 (* 1 = 0.0239525 loss)
I0825 20:11:42.330766  1476 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0825 20:12:28.179028  1476 solver.cpp:357] Iteration 60300 (2.18126 iter/s, 45.8451s/100 iters), loss = 0.0311417
I0825 20:12:28.179289  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0311415 (* 1 = 0.0311415 loss)
I0825 20:12:28.179337  1476 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0825 20:13:15.404201  1476 solver.cpp:357] Iteration 60400 (2.11749 iter/s, 47.2257s/100 iters), loss = 0.0292115
I0825 20:13:15.404412  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0292113 (* 1 = 0.0292113 loss)
I0825 20:13:15.404458  1476 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0825 20:14:02.176465  1476 solver.cpp:514] Iteration 60500, Testing net (#0)
I0825 20:14:43.277508  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:14:43.405743  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923002
I0825 20:14:43.405863  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271592 (* 1 = 0.271592 loss)
I0825 20:14:43.773486  1476 solver.cpp:357] Iteration 60500 (1.13165 iter/s, 88.3666s/100 iters), loss = 0.0324556
I0825 20:14:43.773663  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0324554 (* 1 = 0.0324554 loss)
I0825 20:14:43.773707  1476 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0825 20:15:03.058914  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:15:28.583031  1476 solver.cpp:357] Iteration 60600 (2.23188 iter/s, 44.8052s/100 iters), loss = 0.0350862
I0825 20:15:28.583243  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.035086 (* 1 = 0.035086 loss)
I0825 20:15:28.583289  1476 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0825 20:16:14.519611  1476 solver.cpp:357] Iteration 60700 (2.1771 iter/s, 45.9326s/100 iters), loss = 0.0226335
I0825 20:16:14.519841  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0226334 (* 1 = 0.0226334 loss)
I0825 20:16:14.519888  1476 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0825 20:17:01.731910  1476 solver.cpp:357] Iteration 60800 (2.11825 iter/s, 47.2088s/100 iters), loss = 0.0192166
I0825 20:17:01.732101  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0192164 (* 1 = 0.0192164 loss)
I0825 20:17:01.732131  1476 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0825 20:17:46.525234  1476 solver.cpp:357] Iteration 60900 (2.23255 iter/s, 44.7919s/100 iters), loss = 0.0143786
I0825 20:17:46.525413  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0143784 (* 1 = 0.0143784 loss)
I0825 20:17:46.525441  1476 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0825 20:18:01.297695  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:18:32.196557  1476 solver.cpp:514] Iteration 61000, Testing net (#0)
I0825 20:19:13.313841  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:19:13.470762  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921103
I0825 20:19:13.470873  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275132 (* 1 = 0.275132 loss)
I0825 20:19:13.900408  1476 solver.cpp:357] Iteration 61000 (1.14453 iter/s, 87.3725s/100 iters), loss = 0.0462178
I0825 20:19:13.900573  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0462176 (* 1 = 0.0462176 loss)
I0825 20:19:13.900619  1476 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0825 20:20:01.400446  1476 solver.cpp:357] Iteration 61100 (2.10542 iter/s, 47.4964s/100 iters), loss = 0.0322399
I0825 20:20:01.400673  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0322397 (* 1 = 0.0322397 loss)
I0825 20:20:01.400718  1476 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0825 20:20:48.518749  1476 solver.cpp:357] Iteration 61200 (2.12238 iter/s, 47.1169s/100 iters), loss = 0.018137
I0825 20:20:48.518973  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0181368 (* 1 = 0.0181368 loss)
I0825 20:20:48.519018  1476 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0825 20:21:35.762795  1476 solver.cpp:357] Iteration 61300 (2.11683 iter/s, 47.2405s/100 iters), loss = 0.0267837
I0825 20:21:35.763069  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0267835 (* 1 = 0.0267835 loss)
I0825 20:21:35.763116  1476 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0825 20:21:47.305969  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:22:23.345036  1476 solver.cpp:357] Iteration 61400 (2.10164 iter/s, 47.5819s/100 iters), loss = 0.0276603
I0825 20:22:23.345257  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0276601 (* 1 = 0.0276601 loss)
I0825 20:22:23.345304  1476 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0825 20:23:10.345062  1476 solver.cpp:514] Iteration 61500, Testing net (#0)
I0825 20:23:53.117017  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:23:53.302567  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920202
I0825 20:23:53.302718  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.277851 (* 1 = 0.277851 loss)
I0825 20:23:53.646308  1476 solver.cpp:357] Iteration 61500 (1.1074 iter/s, 90.302s/100 iters), loss = 0.0236349
I0825 20:23:53.646486  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0236347 (* 1 = 0.0236347 loss)
I0825 20:23:53.646534  1476 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0825 20:24:40.495021  1476 solver.cpp:357] Iteration 61600 (2.13452 iter/s, 46.8489s/100 iters), loss = 0.0396487
I0825 20:24:40.495234  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0396485 (* 1 = 0.0396485 loss)
I0825 20:24:40.495278  1476 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0825 20:25:19.734817  1476 solver.cpp:357] Iteration 61700 (2.54859 iter/s, 39.2374s/100 iters), loss = 0.00946012
I0825 20:25:19.735286  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0094599 (* 1 = 0.0094599 loss)
I0825 20:25:19.735467  1476 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0825 20:25:25.425704  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:26:03.519300  1476 solver.cpp:357] Iteration 61800 (2.28399 iter/s, 43.7831s/100 iters), loss = 0.0464905
I0825 20:26:03.519472  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0464903 (* 1 = 0.0464903 loss)
I0825 20:26:03.519501  1476 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0825 20:26:51.162096  1476 solver.cpp:357] Iteration 61900 (2.09915 iter/s, 47.6384s/100 iters), loss = 0.0277136
I0825 20:26:51.162659  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0277134 (* 1 = 0.0277134 loss)
I0825 20:26:51.162839  1476 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0825 20:27:38.082283  1476 solver.cpp:514] Iteration 62000, Testing net (#0)
I0825 20:28:19.397724  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:28:19.627892  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923202
I0825 20:28:19.628248  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.27742 (* 1 = 0.27742 loss)
I0825 20:28:20.039305  1476 solver.cpp:357] Iteration 62000 (1.12513 iter/s, 88.8788s/100 iters), loss = 0.0357885
I0825 20:28:20.039706  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0357883 (* 1 = 0.0357883 loss)
I0825 20:28:20.039881  1476 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0825 20:29:07.564548  1476 solver.cpp:357] Iteration 62100 (2.10416 iter/s, 47.5249s/100 iters), loss = 0.0175095
I0825 20:29:07.564769  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0175093 (* 1 = 0.0175093 loss)
I0825 20:29:07.564803  1476 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0825 20:29:10.178752  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:29:54.950443  1476 solver.cpp:357] Iteration 62200 (2.11054 iter/s, 47.3813s/100 iters), loss = 0.0375842
I0825 20:29:54.954483  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0375839 (* 1 = 0.0375839 loss)
I0825 20:29:54.954510  1476 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0825 20:30:42.695823  1476 solver.cpp:357] Iteration 62300 (2.09465 iter/s, 47.7407s/100 iters), loss = 0.0261494
I0825 20:30:42.696138  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0261492 (* 1 = 0.0261492 loss)
I0825 20:30:42.696187  1476 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0825 20:31:30.055827  1476 solver.cpp:357] Iteration 62400 (2.1117 iter/s, 47.3552s/100 iters), loss = 0.0307542
I0825 20:31:30.057397  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.030754 (* 1 = 0.030754 loss)
I0825 20:31:30.057446  1476 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0825 20:32:14.971617  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:32:16.700621  1476 solver.cpp:514] Iteration 62500, Testing net (#0)
I0825 20:32:54.861407  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:32:55.066596  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920802
I0825 20:32:55.066692  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271036 (* 1 = 0.271036 loss)
I0825 20:32:55.354579  1476 solver.cpp:357] Iteration 62500 (1.17237 iter/s, 85.2973s/100 iters), loss = 0.0490188
I0825 20:32:55.354737  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0490186 (* 1 = 0.0490186 loss)
I0825 20:32:55.354782  1476 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0825 20:33:42.188490  1476 solver.cpp:357] Iteration 62600 (2.13524 iter/s, 46.8332s/100 iters), loss = 0.0496411
I0825 20:33:42.188717  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0496409 (* 1 = 0.0496409 loss)
I0825 20:33:42.188762  1476 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0825 20:34:29.766319  1476 solver.cpp:357] Iteration 62700 (2.102 iter/s, 47.5737s/100 iters), loss = 0.0335438
I0825 20:34:29.766530  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0335436 (* 1 = 0.0335436 loss)
I0825 20:34:29.766574  1476 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0825 20:35:16.238122  1476 solver.cpp:357] Iteration 62800 (2.15179 iter/s, 46.473s/100 iters), loss = 0.0496637
I0825 20:35:16.238641  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0496635 (* 1 = 0.0496635 loss)
I0825 20:35:16.238831  1476 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0825 20:35:55.032726  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:36:01.106695  1476 solver.cpp:357] Iteration 62900 (2.22878 iter/s, 44.8677s/100 iters), loss = 0.0381864
I0825 20:36:01.106856  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0381861 (* 1 = 0.0381861 loss)
I0825 20:36:01.106904  1476 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0825 20:36:47.196213  1476 solver.cpp:514] Iteration 63000, Testing net (#0)
I0825 20:37:28.878547  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:37:29.011673  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920902
I0825 20:37:29.011821  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.280446 (* 1 = 0.280446 loss)
I0825 20:37:29.386150  1476 solver.cpp:357] Iteration 63000 (1.13276 iter/s, 88.2798s/100 iters), loss = 0.030534
I0825 20:37:29.386329  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0305338 (* 1 = 0.0305338 loss)
I0825 20:37:29.386379  1476 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0825 20:38:16.522052  1476 solver.cpp:357] Iteration 63100 (2.12166 iter/s, 47.1329s/100 iters), loss = 0.0286014
I0825 20:38:16.522305  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0286012 (* 1 = 0.0286012 loss)
I0825 20:38:16.522359  1476 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0825 20:39:03.904335  1476 solver.cpp:357] Iteration 63200 (2.11062 iter/s, 47.3795s/100 iters), loss = 0.0295469
I0825 20:39:03.904528  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0295467 (* 1 = 0.0295467 loss)
I0825 20:39:03.904557  1476 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0825 20:39:40.472180  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:39:50.584120  1476 solver.cpp:357] Iteration 63300 (2.14249 iter/s, 46.6746s/100 iters), loss = 0.019228
I0825 20:39:50.584302  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0192278 (* 1 = 0.0192278 loss)
I0825 20:39:50.584350  1476 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0825 20:40:34.441617  1476 solver.cpp:357] Iteration 63400 (2.28039 iter/s, 43.8522s/100 iters), loss = 0.0222873
I0825 20:40:34.441836  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222871 (* 1 = 0.0222871 loss)
I0825 20:40:34.441881  1476 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0825 20:41:21.292032  1476 solver.cpp:514] Iteration 63500, Testing net (#0)
I0825 20:42:04.093039  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:42:04.278523  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919502
I0825 20:42:04.278657  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.279032 (* 1 = 0.279032 loss)
I0825 20:42:04.605505  1476 solver.cpp:357] Iteration 63500 (1.10912 iter/s, 90.1619s/100 iters), loss = 0.0202684
I0825 20:42:04.605672  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0202681 (* 1 = 0.0202681 loss)
I0825 20:42:04.605715  1476 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0825 20:42:51.802464  1476 solver.cpp:357] Iteration 63600 (2.11892 iter/s, 47.1938s/100 iters), loss = 0.0331996
I0825 20:42:51.802700  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0331994 (* 1 = 0.0331994 loss)
I0825 20:42:51.802747  1476 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0825 20:43:23.833746  1482 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:43:39.277122  1476 solver.cpp:357] Iteration 63700 (2.10662 iter/s, 47.4694s/100 iters), loss = 0.021927
I0825 20:43:39.277544  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0219268 (* 1 = 0.0219268 loss)
I0825 20:43:39.277720  1476 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0825 20:44:26.489576  1476 solver.cpp:357] Iteration 63800 (2.11832 iter/s, 47.2072s/100 iters), loss = 0.0260548
I0825 20:44:26.489790  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0260546 (* 1 = 0.0260546 loss)
I0825 20:44:26.489836  1476 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0825 20:45:13.362032  1476 solver.cpp:357] Iteration 63900 (2.1335 iter/s, 46.8714s/100 iters), loss = 0.0238132
I0825 20:45:13.362252  1476 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.023813 (* 1 = 0.023813 loss)
I0825 20:45:13.362298  1476 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0825 20:45:57.692209  1476 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.caffemodel
I0825 20:45:57.704063  1476 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.solverstate
I0825 20:45:57.813359  1476 solver.cpp:472] Iteration 64000, loss = 0.0160813
I0825 20:45:57.813508  1476 solver.cpp:514] Iteration 64000, Testing net (#0)
I0825 20:46:38.325554  1486 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:46:38.441427  1476 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919302
I0825 20:46:38.441560  1476 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.281006 (* 1 = 0.281006 loss)
I0825 20:46:38.441602  1476 solver.cpp:479] Optimization Done.
I0825 20:46:38.441635  1476 caffe.cpp:326] Optimization Done.
