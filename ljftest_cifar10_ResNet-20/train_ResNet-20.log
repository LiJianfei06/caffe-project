WARNING: Logging before InitGoogleLogging() is written to STDERR
I0817 23:56:11.052390 15109 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0817 23:56:11.052536 15109 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0817 23:56:11.052542 15109 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0817 23:56:11.052546 15109 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0817 23:56:11.052549 15109 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0817 23:56:11.052553 15109 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0817 23:56:11.052624 15109 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0817 23:56:11.052839 15109 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0817 23:56:11.054679 15109 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0817 23:56:11.054720 15109 caffe.cpp:269] Using GPUs 0
I0817 23:56:11.061153 15109 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0817 23:56:11.736513 15109 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0817 23:56:11.736567 15109 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0817 23:56:11.836082 15109 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_20.prototxt"
test_net: "./test_ResNet_20.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_20"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
type: "Nesterov"
I0817 23:56:11.836393 15109 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_20.prototxt
I0817 23:56:11.837308 15109 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_20.prototxt
I0817 23:56:11.837330 15109 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0817 23:56:11.837528 15109 net.cpp:390] layer_param.include_size():1
I0817 23:56:11.837539 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837548 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837553 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837556 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837560 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837564 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837568 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837572 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837576 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837579 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837584 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837589 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837594 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837597 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837601 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837605 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837610 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837613 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837617 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837621 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837625 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837630 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837633 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837637 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837641 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837644 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837649 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837652 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837656 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837661 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837664 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837668 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837672 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837677 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837680 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837684 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837688 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837692 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837724 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837729 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837733 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837736 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837741 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837745 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837749 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837754 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837757 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837761 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837765 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837769 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837774 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837777 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837780 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837785 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837788 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837792 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837797 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837801 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837805 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837810 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837813 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837817 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837821 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837826 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837829 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837833 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837836 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837841 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837844 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837848 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837852 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837857 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837860 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837864 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837868 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837873 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837877 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837882 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837884 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837889 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837893 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837896 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837900 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837904 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837908 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837913 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837915 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837919 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837924 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837927 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837932 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837936 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837940 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837944 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837949 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837952 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837955 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837968 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837972 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837976 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837981 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837985 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837990 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.837993 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.837997 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838001 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838004 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838008 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838012 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838016 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838021 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838024 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838028 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838032 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838035 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838039 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838044 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838048 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838052 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838057 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838062 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838065 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838068 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838073 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838076 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838081 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838084 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838088 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838093 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838096 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838100 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838104 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838109 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838112 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838115 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838119 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838124 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838129 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838132 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838136 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838140 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838145 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838148 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838152 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838156 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838161 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838165 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838169 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838172 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838177 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838181 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838186 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838189 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838193 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838196 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838208 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838212 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838217 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838219 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838224 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838228 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838232 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838235 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838239 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838243 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838248 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838251 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838255 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838260 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838263 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838268 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838271 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838275 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838279 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838284 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838287 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838291 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838296 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838299 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838304 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838307 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838312 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838316 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838320 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838323 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838328 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838332 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.838336 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:11.838340 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:11.839016 15109 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215684
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
I0817 23:56:11.839726 15109 layer_factory.hpp:77] Creating layer Data1
I0817 23:56:11.839949 15109 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0817 23:56:11.839998 15109 net.cpp:128] Creating Layer Data1
I0817 23:56:11.840009 15109 net.cpp:522] Data1 -> Data1
I0817 23:56:11.840044 15109 net.cpp:522] Data1 -> Data2
I0817 23:56:11.841907 15109 data_layer.cpp:45] output data size: 128,3,32,32
I0817 23:56:11.857535 15109 net.cpp:172] Setting up Data1
I0817 23:56:11.857595 15109 net.cpp:186] Top shape: 128 3 32 32 (393216)
I0817 23:56:11.857604 15109 net.cpp:186] Top shape: 128 (128)
I0817 23:56:11.857609 15109 net.cpp:194] Memory required for data: 1573376
I0817 23:56:11.857623 15109 layer_factory.hpp:77] Creating layer Convolution1
I0817 23:56:11.857668 15109 net.cpp:128] Creating Layer Convolution1
I0817 23:56:11.857730 15109 net.cpp:558] Convolution1 <- Data1
I0817 23:56:11.857772 15109 net.cpp:522] Convolution1 -> Convolution1
I0817 23:56:12.921633 15109 net.cpp:172] Setting up Convolution1
I0817 23:56:12.921702 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.921708 15109 net.cpp:194] Memory required for data: 9961984
I0817 23:56:12.921756 15109 layer_factory.hpp:77] Creating layer BatchNorm1
I0817 23:56:12.921782 15109 net.cpp:128] Creating Layer BatchNorm1
I0817 23:56:12.921788 15109 net.cpp:558] BatchNorm1 <- Convolution1
I0817 23:56:12.921798 15109 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0817 23:56:12.922040 15109 net.cpp:172] Setting up BatchNorm1
I0817 23:56:12.922053 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.922060 15109 net.cpp:194] Memory required for data: 18350592
I0817 23:56:12.922073 15109 layer_factory.hpp:77] Creating layer Scale1
I0817 23:56:12.922083 15109 net.cpp:128] Creating Layer Scale1
I0817 23:56:12.922088 15109 net.cpp:558] Scale1 <- Convolution1
I0817 23:56:12.922094 15109 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0817 23:56:12.922199 15109 layer_factory.hpp:77] Creating layer Scale1
I0817 23:56:12.922333 15109 net.cpp:172] Setting up Scale1
I0817 23:56:12.922344 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.922349 15109 net.cpp:194] Memory required for data: 26739200
I0817 23:56:12.922358 15109 layer_factory.hpp:77] Creating layer ReLU1
I0817 23:56:12.922369 15109 net.cpp:128] Creating Layer ReLU1
I0817 23:56:12.922374 15109 net.cpp:558] ReLU1 <- Convolution1
I0817 23:56:12.922379 15109 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0817 23:56:12.923281 15109 net.cpp:172] Setting up ReLU1
I0817 23:56:12.923300 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.923305 15109 net.cpp:194] Memory required for data: 35127808
I0817 23:56:12.923308 15109 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0817 23:56:12.923318 15109 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0817 23:56:12.923323 15109 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0817 23:56:12.923331 15109 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0817 23:56:12.923339 15109 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0817 23:56:12.923388 15109 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0817 23:56:12.923395 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.923401 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.923406 15109 net.cpp:194] Memory required for data: 51905024
I0817 23:56:12.923410 15109 layer_factory.hpp:77] Creating layer Convolution2
I0817 23:56:12.923426 15109 net.cpp:128] Creating Layer Convolution2
I0817 23:56:12.923430 15109 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0817 23:56:12.923437 15109 net.cpp:522] Convolution2 -> Convolution2
I0817 23:56:12.930009 15109 net.cpp:172] Setting up Convolution2
I0817 23:56:12.930037 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.930042 15109 net.cpp:194] Memory required for data: 60293632
I0817 23:56:12.930054 15109 layer_factory.hpp:77] Creating layer BatchNorm2
I0817 23:56:12.930065 15109 net.cpp:128] Creating Layer BatchNorm2
I0817 23:56:12.930070 15109 net.cpp:558] BatchNorm2 <- Convolution2
I0817 23:56:12.930078 15109 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0817 23:56:12.930297 15109 net.cpp:172] Setting up BatchNorm2
I0817 23:56:12.930310 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.930315 15109 net.cpp:194] Memory required for data: 68682240
I0817 23:56:12.930325 15109 layer_factory.hpp:77] Creating layer Scale2
I0817 23:56:12.930335 15109 net.cpp:128] Creating Layer Scale2
I0817 23:56:12.930339 15109 net.cpp:558] Scale2 <- Convolution2
I0817 23:56:12.930346 15109 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0817 23:56:12.930382 15109 layer_factory.hpp:77] Creating layer Scale2
I0817 23:56:12.930555 15109 net.cpp:172] Setting up Scale2
I0817 23:56:12.930569 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.930574 15109 net.cpp:194] Memory required for data: 77070848
I0817 23:56:12.930583 15109 layer_factory.hpp:77] Creating layer ReLU2
I0817 23:56:12.930590 15109 net.cpp:128] Creating Layer ReLU2
I0817 23:56:12.930594 15109 net.cpp:558] ReLU2 <- Convolution2
I0817 23:56:12.930601 15109 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0817 23:56:12.932108 15109 net.cpp:172] Setting up ReLU2
I0817 23:56:12.932127 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.932132 15109 net.cpp:194] Memory required for data: 85459456
I0817 23:56:12.932137 15109 layer_factory.hpp:77] Creating layer Convolution3
I0817 23:56:12.932147 15109 net.cpp:128] Creating Layer Convolution3
I0817 23:56:12.932152 15109 net.cpp:558] Convolution3 <- Convolution2
I0817 23:56:12.932160 15109 net.cpp:522] Convolution3 -> Convolution3
I0817 23:56:12.938840 15109 net.cpp:172] Setting up Convolution3
I0817 23:56:12.938866 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.938871 15109 net.cpp:194] Memory required for data: 93848064
I0817 23:56:12.938881 15109 layer_factory.hpp:77] Creating layer BatchNorm3
I0817 23:56:12.938913 15109 net.cpp:128] Creating Layer BatchNorm3
I0817 23:56:12.938918 15109 net.cpp:558] BatchNorm3 <- Convolution3
I0817 23:56:12.938926 15109 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0817 23:56:12.939172 15109 net.cpp:172] Setting up BatchNorm3
I0817 23:56:12.939184 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.939188 15109 net.cpp:194] Memory required for data: 102236672
I0817 23:56:12.939203 15109 layer_factory.hpp:77] Creating layer Scale3
I0817 23:56:12.939210 15109 net.cpp:128] Creating Layer Scale3
I0817 23:56:12.939215 15109 net.cpp:558] Scale3 <- Convolution3
I0817 23:56:12.939221 15109 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0817 23:56:12.939258 15109 layer_factory.hpp:77] Creating layer Scale3
I0817 23:56:12.939386 15109 net.cpp:172] Setting up Scale3
I0817 23:56:12.939397 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.939402 15109 net.cpp:194] Memory required for data: 110625280
I0817 23:56:12.939410 15109 layer_factory.hpp:77] Creating layer Eltwise1
I0817 23:56:12.939419 15109 net.cpp:128] Creating Layer Eltwise1
I0817 23:56:12.939424 15109 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0817 23:56:12.939429 15109 net.cpp:558] Eltwise1 <- Convolution3
I0817 23:56:12.939435 15109 net.cpp:522] Eltwise1 -> Eltwise1
I0817 23:56:12.939463 15109 net.cpp:172] Setting up Eltwise1
I0817 23:56:12.939503 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.939512 15109 net.cpp:194] Memory required for data: 119013888
I0817 23:56:12.939517 15109 layer_factory.hpp:77] Creating layer ReLU3
I0817 23:56:12.939524 15109 net.cpp:128] Creating Layer ReLU3
I0817 23:56:12.939529 15109 net.cpp:558] ReLU3 <- Eltwise1
I0817 23:56:12.939535 15109 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0817 23:56:12.940927 15109 net.cpp:172] Setting up ReLU3
I0817 23:56:12.940953 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.940958 15109 net.cpp:194] Memory required for data: 127402496
I0817 23:56:12.940963 15109 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0817 23:56:12.940971 15109 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0817 23:56:12.940976 15109 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0817 23:56:12.940984 15109 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0817 23:56:12.940996 15109 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0817 23:56:12.941042 15109 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0817 23:56:12.941051 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.941057 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.941061 15109 net.cpp:194] Memory required for data: 144179712
I0817 23:56:12.941066 15109 layer_factory.hpp:77] Creating layer Convolution4
I0817 23:56:12.941079 15109 net.cpp:128] Creating Layer Convolution4
I0817 23:56:12.941084 15109 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0817 23:56:12.941092 15109 net.cpp:522] Convolution4 -> Convolution4
I0817 23:56:12.947664 15109 net.cpp:172] Setting up Convolution4
I0817 23:56:12.947691 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.947696 15109 net.cpp:194] Memory required for data: 152568320
I0817 23:56:12.947706 15109 layer_factory.hpp:77] Creating layer BatchNorm4
I0817 23:56:12.947716 15109 net.cpp:128] Creating Layer BatchNorm4
I0817 23:56:12.947721 15109 net.cpp:558] BatchNorm4 <- Convolution4
I0817 23:56:12.947728 15109 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0817 23:56:12.947952 15109 net.cpp:172] Setting up BatchNorm4
I0817 23:56:12.947962 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.947968 15109 net.cpp:194] Memory required for data: 160956928
I0817 23:56:12.947978 15109 layer_factory.hpp:77] Creating layer Scale4
I0817 23:56:12.947986 15109 net.cpp:128] Creating Layer Scale4
I0817 23:56:12.947990 15109 net.cpp:558] Scale4 <- Convolution4
I0817 23:56:12.947996 15109 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0817 23:56:12.948051 15109 layer_factory.hpp:77] Creating layer Scale4
I0817 23:56:12.948176 15109 net.cpp:172] Setting up Scale4
I0817 23:56:12.948189 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.948194 15109 net.cpp:194] Memory required for data: 169345536
I0817 23:56:12.948201 15109 layer_factory.hpp:77] Creating layer ReLU4
I0817 23:56:12.948209 15109 net.cpp:128] Creating Layer ReLU4
I0817 23:56:12.948212 15109 net.cpp:558] ReLU4 <- Convolution4
I0817 23:56:12.948218 15109 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0817 23:56:12.949753 15109 net.cpp:172] Setting up ReLU4
I0817 23:56:12.949769 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.949774 15109 net.cpp:194] Memory required for data: 177734144
I0817 23:56:12.949779 15109 layer_factory.hpp:77] Creating layer Convolution5
I0817 23:56:12.949790 15109 net.cpp:128] Creating Layer Convolution5
I0817 23:56:12.949795 15109 net.cpp:558] Convolution5 <- Convolution4
I0817 23:56:12.949802 15109 net.cpp:522] Convolution5 -> Convolution5
I0817 23:56:12.956483 15109 net.cpp:172] Setting up Convolution5
I0817 23:56:12.956509 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.956514 15109 net.cpp:194] Memory required for data: 186122752
I0817 23:56:12.956524 15109 layer_factory.hpp:77] Creating layer BatchNorm5
I0817 23:56:12.956534 15109 net.cpp:128] Creating Layer BatchNorm5
I0817 23:56:12.956539 15109 net.cpp:558] BatchNorm5 <- Convolution5
I0817 23:56:12.956547 15109 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0817 23:56:12.956778 15109 net.cpp:172] Setting up BatchNorm5
I0817 23:56:12.956789 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.956794 15109 net.cpp:194] Memory required for data: 194511360
I0817 23:56:12.956805 15109 layer_factory.hpp:77] Creating layer Scale5
I0817 23:56:12.956813 15109 net.cpp:128] Creating Layer Scale5
I0817 23:56:12.956818 15109 net.cpp:558] Scale5 <- Convolution5
I0817 23:56:12.956825 15109 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0817 23:56:12.956862 15109 layer_factory.hpp:77] Creating layer Scale5
I0817 23:56:12.956987 15109 net.cpp:172] Setting up Scale5
I0817 23:56:12.956998 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.957003 15109 net.cpp:194] Memory required for data: 202899968
I0817 23:56:12.957011 15109 layer_factory.hpp:77] Creating layer Eltwise2
I0817 23:56:12.957018 15109 net.cpp:128] Creating Layer Eltwise2
I0817 23:56:12.957023 15109 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0817 23:56:12.957028 15109 net.cpp:558] Eltwise2 <- Convolution5
I0817 23:56:12.957034 15109 net.cpp:522] Eltwise2 -> Eltwise2
I0817 23:56:12.957059 15109 net.cpp:172] Setting up Eltwise2
I0817 23:56:12.957067 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.957070 15109 net.cpp:194] Memory required for data: 211288576
I0817 23:56:12.957074 15109 layer_factory.hpp:77] Creating layer ReLU5
I0817 23:56:12.957082 15109 net.cpp:128] Creating Layer ReLU5
I0817 23:56:12.957085 15109 net.cpp:558] ReLU5 <- Eltwise2
I0817 23:56:12.957092 15109 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0817 23:56:12.958573 15109 net.cpp:172] Setting up ReLU5
I0817 23:56:12.958590 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.958595 15109 net.cpp:194] Memory required for data: 219677184
I0817 23:56:12.958600 15109 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0817 23:56:12.958607 15109 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0817 23:56:12.958612 15109 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0817 23:56:12.958619 15109 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0817 23:56:12.958627 15109 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0817 23:56:12.958676 15109 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0817 23:56:12.958683 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.958689 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.958693 15109 net.cpp:194] Memory required for data: 236454400
I0817 23:56:12.958712 15109 layer_factory.hpp:77] Creating layer Convolution6
I0817 23:56:12.958724 15109 net.cpp:128] Creating Layer Convolution6
I0817 23:56:12.958729 15109 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0817 23:56:12.958735 15109 net.cpp:522] Convolution6 -> Convolution6
I0817 23:56:12.965335 15109 net.cpp:172] Setting up Convolution6
I0817 23:56:12.965363 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.965368 15109 net.cpp:194] Memory required for data: 244843008
I0817 23:56:12.965378 15109 layer_factory.hpp:77] Creating layer BatchNorm6
I0817 23:56:12.965389 15109 net.cpp:128] Creating Layer BatchNorm6
I0817 23:56:12.965394 15109 net.cpp:558] BatchNorm6 <- Convolution6
I0817 23:56:12.965401 15109 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0817 23:56:12.965642 15109 net.cpp:172] Setting up BatchNorm6
I0817 23:56:12.965651 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.965654 15109 net.cpp:194] Memory required for data: 253231616
I0817 23:56:12.965664 15109 layer_factory.hpp:77] Creating layer Scale6
I0817 23:56:12.965673 15109 net.cpp:128] Creating Layer Scale6
I0817 23:56:12.965678 15109 net.cpp:558] Scale6 <- Convolution6
I0817 23:56:12.965683 15109 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0817 23:56:12.965721 15109 layer_factory.hpp:77] Creating layer Scale6
I0817 23:56:12.965858 15109 net.cpp:172] Setting up Scale6
I0817 23:56:12.965865 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.965869 15109 net.cpp:194] Memory required for data: 261620224
I0817 23:56:12.965878 15109 layer_factory.hpp:77] Creating layer ReLU6
I0817 23:56:12.965883 15109 net.cpp:128] Creating Layer ReLU6
I0817 23:56:12.965888 15109 net.cpp:558] ReLU6 <- Convolution6
I0817 23:56:12.965893 15109 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0817 23:56:12.967416 15109 net.cpp:172] Setting up ReLU6
I0817 23:56:12.967433 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.967437 15109 net.cpp:194] Memory required for data: 270008832
I0817 23:56:12.967442 15109 layer_factory.hpp:77] Creating layer Convolution7
I0817 23:56:12.967458 15109 net.cpp:128] Creating Layer Convolution7
I0817 23:56:12.967463 15109 net.cpp:558] Convolution7 <- Convolution6
I0817 23:56:12.967478 15109 net.cpp:522] Convolution7 -> Convolution7
I0817 23:56:12.974169 15109 net.cpp:172] Setting up Convolution7
I0817 23:56:12.974229 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.974248 15109 net.cpp:194] Memory required for data: 278397440
I0817 23:56:12.974282 15109 layer_factory.hpp:77] Creating layer BatchNorm7
I0817 23:56:12.974309 15109 net.cpp:128] Creating Layer BatchNorm7
I0817 23:56:12.974328 15109 net.cpp:558] BatchNorm7 <- Convolution7
I0817 23:56:12.974347 15109 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0817 23:56:12.974611 15109 net.cpp:172] Setting up BatchNorm7
I0817 23:56:12.974632 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.974649 15109 net.cpp:194] Memory required for data: 286786048
I0817 23:56:12.974673 15109 layer_factory.hpp:77] Creating layer Scale7
I0817 23:56:12.974699 15109 net.cpp:128] Creating Layer Scale7
I0817 23:56:12.974714 15109 net.cpp:558] Scale7 <- Convolution7
I0817 23:56:12.974733 15109 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0817 23:56:12.974795 15109 layer_factory.hpp:77] Creating layer Scale7
I0817 23:56:12.974968 15109 net.cpp:172] Setting up Scale7
I0817 23:56:12.974994 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.975010 15109 net.cpp:194] Memory required for data: 295174656
I0817 23:56:12.975033 15109 layer_factory.hpp:77] Creating layer Eltwise3
I0817 23:56:12.975056 15109 net.cpp:128] Creating Layer Eltwise3
I0817 23:56:12.975073 15109 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0817 23:56:12.975091 15109 net.cpp:558] Eltwise3 <- Convolution7
I0817 23:56:12.975111 15109 net.cpp:522] Eltwise3 -> Eltwise3
I0817 23:56:12.975157 15109 net.cpp:172] Setting up Eltwise3
I0817 23:56:12.975178 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.975205 15109 net.cpp:194] Memory required for data: 303563264
I0817 23:56:12.975221 15109 layer_factory.hpp:77] Creating layer ReLU7
I0817 23:56:12.975242 15109 net.cpp:128] Creating Layer ReLU7
I0817 23:56:12.975258 15109 net.cpp:558] ReLU7 <- Eltwise3
I0817 23:56:12.975276 15109 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0817 23:56:12.976222 15109 net.cpp:172] Setting up ReLU7
I0817 23:56:12.976253 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.976272 15109 net.cpp:194] Memory required for data: 311951872
I0817 23:56:12.976287 15109 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0817 23:56:12.976307 15109 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0817 23:56:12.976325 15109 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0817 23:56:12.976346 15109 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0817 23:56:12.976368 15109 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0817 23:56:12.976440 15109 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0817 23:56:12.976452 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.976459 15109 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0817 23:56:12.976462 15109 net.cpp:194] Memory required for data: 328729088
I0817 23:56:12.976467 15109 layer_factory.hpp:77] Creating layer Convolution8
I0817 23:56:12.976481 15109 net.cpp:128] Creating Layer Convolution8
I0817 23:56:12.976486 15109 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0817 23:56:12.976495 15109 net.cpp:522] Convolution8 -> Convolution8
I0817 23:56:12.983328 15109 net.cpp:172] Setting up Convolution8
I0817 23:56:12.983361 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.983364 15109 net.cpp:194] Memory required for data: 332923392
I0817 23:56:12.983379 15109 layer_factory.hpp:77] Creating layer BatchNorm8
I0817 23:56:12.983393 15109 net.cpp:128] Creating Layer BatchNorm8
I0817 23:56:12.983398 15109 net.cpp:558] BatchNorm8 <- Convolution8
I0817 23:56:12.983405 15109 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0817 23:56:12.983660 15109 net.cpp:172] Setting up BatchNorm8
I0817 23:56:12.983670 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.983675 15109 net.cpp:194] Memory required for data: 337117696
I0817 23:56:12.983685 15109 layer_factory.hpp:77] Creating layer Scale8
I0817 23:56:12.983692 15109 net.cpp:128] Creating Layer Scale8
I0817 23:56:12.983696 15109 net.cpp:558] Scale8 <- Convolution8
I0817 23:56:12.983702 15109 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0817 23:56:12.983745 15109 layer_factory.hpp:77] Creating layer Scale8
I0817 23:56:12.983886 15109 net.cpp:172] Setting up Scale8
I0817 23:56:12.983896 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.983901 15109 net.cpp:194] Memory required for data: 341312000
I0817 23:56:12.983908 15109 layer_factory.hpp:77] Creating layer Convolution9
I0817 23:56:12.983923 15109 net.cpp:128] Creating Layer Convolution9
I0817 23:56:12.983928 15109 net.cpp:558] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0817 23:56:12.983938 15109 net.cpp:522] Convolution9 -> Convolution9
I0817 23:56:12.989684 15109 net.cpp:172] Setting up Convolution9
I0817 23:56:12.989722 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.989727 15109 net.cpp:194] Memory required for data: 345506304
I0817 23:56:12.989747 15109 layer_factory.hpp:77] Creating layer BatchNorm9
I0817 23:56:12.989789 15109 net.cpp:128] Creating Layer BatchNorm9
I0817 23:56:12.989796 15109 net.cpp:558] BatchNorm9 <- Convolution9
I0817 23:56:12.989806 15109 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0817 23:56:12.990054 15109 net.cpp:172] Setting up BatchNorm9
I0817 23:56:12.990067 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.990072 15109 net.cpp:194] Memory required for data: 349700608
I0817 23:56:12.990082 15109 layer_factory.hpp:77] Creating layer Scale9
I0817 23:56:12.990092 15109 net.cpp:128] Creating Layer Scale9
I0817 23:56:12.990095 15109 net.cpp:558] Scale9 <- Convolution9
I0817 23:56:12.990131 15109 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0817 23:56:12.990175 15109 layer_factory.hpp:77] Creating layer Scale9
I0817 23:56:12.990352 15109 net.cpp:172] Setting up Scale9
I0817 23:56:12.990367 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.990371 15109 net.cpp:194] Memory required for data: 353894912
I0817 23:56:12.990381 15109 layer_factory.hpp:77] Creating layer ReLU8
I0817 23:56:12.990401 15109 net.cpp:128] Creating Layer ReLU8
I0817 23:56:12.990406 15109 net.cpp:558] ReLU8 <- Convolution9
I0817 23:56:12.990411 15109 net.cpp:509] ReLU8 -> Convolution9 (in-place)
I0817 23:56:12.991737 15109 net.cpp:172] Setting up ReLU8
I0817 23:56:12.991760 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.991765 15109 net.cpp:194] Memory required for data: 358089216
I0817 23:56:12.991773 15109 layer_factory.hpp:77] Creating layer Convolution10
I0817 23:56:12.991787 15109 net.cpp:128] Creating Layer Convolution10
I0817 23:56:12.991792 15109 net.cpp:558] Convolution10 <- Convolution9
I0817 23:56:12.991799 15109 net.cpp:522] Convolution10 -> Convolution10
I0817 23:56:12.993291 15109 net.cpp:172] Setting up Convolution10
I0817 23:56:12.993317 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.993322 15109 net.cpp:194] Memory required for data: 362283520
I0817 23:56:12.993350 15109 layer_factory.hpp:77] Creating layer BatchNorm10
I0817 23:56:12.993360 15109 net.cpp:128] Creating Layer BatchNorm10
I0817 23:56:12.993366 15109 net.cpp:558] BatchNorm10 <- Convolution10
I0817 23:56:12.993371 15109 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0817 23:56:12.993607 15109 net.cpp:172] Setting up BatchNorm10
I0817 23:56:12.993618 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.993621 15109 net.cpp:194] Memory required for data: 366477824
I0817 23:56:12.993631 15109 layer_factory.hpp:77] Creating layer Scale10
I0817 23:56:12.993638 15109 net.cpp:128] Creating Layer Scale10
I0817 23:56:12.993643 15109 net.cpp:558] Scale10 <- Convolution10
I0817 23:56:12.993649 15109 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0817 23:56:12.993687 15109 layer_factory.hpp:77] Creating layer Scale10
I0817 23:56:12.993824 15109 net.cpp:172] Setting up Scale10
I0817 23:56:12.993834 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.993839 15109 net.cpp:194] Memory required for data: 370672128
I0817 23:56:12.993845 15109 layer_factory.hpp:77] Creating layer Eltwise4
I0817 23:56:12.993856 15109 net.cpp:128] Creating Layer Eltwise4
I0817 23:56:12.993861 15109 net.cpp:558] Eltwise4 <- Convolution8
I0817 23:56:12.993866 15109 net.cpp:558] Eltwise4 <- Convolution10
I0817 23:56:12.993872 15109 net.cpp:522] Eltwise4 -> Eltwise4
I0817 23:56:12.993893 15109 net.cpp:172] Setting up Eltwise4
I0817 23:56:12.993901 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.993904 15109 net.cpp:194] Memory required for data: 374866432
I0817 23:56:12.993908 15109 layer_factory.hpp:77] Creating layer ReLU9
I0817 23:56:12.993917 15109 net.cpp:128] Creating Layer ReLU9
I0817 23:56:12.993922 15109 net.cpp:558] ReLU9 <- Eltwise4
I0817 23:56:12.993930 15109 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0817 23:56:12.994161 15109 net.cpp:172] Setting up ReLU9
I0817 23:56:12.994174 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.994179 15109 net.cpp:194] Memory required for data: 379060736
I0817 23:56:12.994184 15109 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0817 23:56:12.994192 15109 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0817 23:56:12.994196 15109 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0817 23:56:12.994204 15109 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0817 23:56:12.994216 15109 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0817 23:56:12.994264 15109 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0817 23:56:12.994271 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.994277 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.994297 15109 net.cpp:194] Memory required for data: 387449344
I0817 23:56:12.994300 15109 layer_factory.hpp:77] Creating layer Convolution11
I0817 23:56:12.994313 15109 net.cpp:128] Creating Layer Convolution11
I0817 23:56:12.994318 15109 net.cpp:558] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0817 23:56:12.994328 15109 net.cpp:522] Convolution11 -> Convolution11
I0817 23:56:12.995790 15109 net.cpp:172] Setting up Convolution11
I0817 23:56:12.995820 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.995826 15109 net.cpp:194] Memory required for data: 391643648
I0817 23:56:12.995836 15109 layer_factory.hpp:77] Creating layer BatchNorm11
I0817 23:56:12.995846 15109 net.cpp:128] Creating Layer BatchNorm11
I0817 23:56:12.995849 15109 net.cpp:558] BatchNorm11 <- Convolution11
I0817 23:56:12.995859 15109 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0817 23:56:12.996095 15109 net.cpp:172] Setting up BatchNorm11
I0817 23:56:12.996104 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.996107 15109 net.cpp:194] Memory required for data: 395837952
I0817 23:56:12.996116 15109 layer_factory.hpp:77] Creating layer Scale11
I0817 23:56:12.996124 15109 net.cpp:128] Creating Layer Scale11
I0817 23:56:12.996127 15109 net.cpp:558] Scale11 <- Convolution11
I0817 23:56:12.996134 15109 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0817 23:56:12.996172 15109 layer_factory.hpp:77] Creating layer Scale11
I0817 23:56:12.996305 15109 net.cpp:172] Setting up Scale11
I0817 23:56:12.996314 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.996317 15109 net.cpp:194] Memory required for data: 400032256
I0817 23:56:12.996325 15109 layer_factory.hpp:77] Creating layer ReLU10
I0817 23:56:12.996333 15109 net.cpp:128] Creating Layer ReLU10
I0817 23:56:12.996337 15109 net.cpp:558] ReLU10 <- Convolution11
I0817 23:56:12.996343 15109 net.cpp:509] ReLU10 -> Convolution11 (in-place)
I0817 23:56:12.996861 15109 net.cpp:172] Setting up ReLU10
I0817 23:56:12.996878 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.996882 15109 net.cpp:194] Memory required for data: 404226560
I0817 23:56:12.996887 15109 layer_factory.hpp:77] Creating layer Convolution12
I0817 23:56:12.996901 15109 net.cpp:128] Creating Layer Convolution12
I0817 23:56:12.996906 15109 net.cpp:558] Convolution12 <- Convolution11
I0817 23:56:12.996915 15109 net.cpp:522] Convolution12 -> Convolution12
I0817 23:56:12.998378 15109 net.cpp:172] Setting up Convolution12
I0817 23:56:12.998404 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.998409 15109 net.cpp:194] Memory required for data: 408420864
I0817 23:56:12.998420 15109 layer_factory.hpp:77] Creating layer BatchNorm12
I0817 23:56:12.998427 15109 net.cpp:128] Creating Layer BatchNorm12
I0817 23:56:12.998432 15109 net.cpp:558] BatchNorm12 <- Convolution12
I0817 23:56:12.998445 15109 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0817 23:56:12.998682 15109 net.cpp:172] Setting up BatchNorm12
I0817 23:56:12.998690 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.998694 15109 net.cpp:194] Memory required for data: 412615168
I0817 23:56:12.998703 15109 layer_factory.hpp:77] Creating layer Scale12
I0817 23:56:12.998710 15109 net.cpp:128] Creating Layer Scale12
I0817 23:56:12.998714 15109 net.cpp:558] Scale12 <- Convolution12
I0817 23:56:12.998723 15109 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0817 23:56:12.998760 15109 layer_factory.hpp:77] Creating layer Scale12
I0817 23:56:12.998895 15109 net.cpp:172] Setting up Scale12
I0817 23:56:12.998906 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.998910 15109 net.cpp:194] Memory required for data: 416809472
I0817 23:56:12.998919 15109 layer_factory.hpp:77] Creating layer Eltwise5
I0817 23:56:12.998925 15109 net.cpp:128] Creating Layer Eltwise5
I0817 23:56:12.998947 15109 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0817 23:56:12.998952 15109 net.cpp:558] Eltwise5 <- Convolution12
I0817 23:56:12.998958 15109 net.cpp:522] Eltwise5 -> Eltwise5
I0817 23:56:12.999001 15109 net.cpp:172] Setting up Eltwise5
I0817 23:56:12.999008 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.999011 15109 net.cpp:194] Memory required for data: 421003776
I0817 23:56:12.999017 15109 layer_factory.hpp:77] Creating layer ReLU11
I0817 23:56:12.999022 15109 net.cpp:128] Creating Layer ReLU11
I0817 23:56:12.999027 15109 net.cpp:558] ReLU11 <- Eltwise5
I0817 23:56:12.999034 15109 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0817 23:56:12.999277 15109 net.cpp:172] Setting up ReLU11
I0817 23:56:12.999287 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.999291 15109 net.cpp:194] Memory required for data: 425198080
I0817 23:56:12.999296 15109 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0817 23:56:12.999303 15109 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0817 23:56:12.999307 15109 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0817 23:56:12.999316 15109 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0817 23:56:12.999325 15109 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0817 23:56:12.999373 15109 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0817 23:56:12.999380 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.999387 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:12.999390 15109 net.cpp:194] Memory required for data: 433586688
I0817 23:56:12.999394 15109 layer_factory.hpp:77] Creating layer Convolution13
I0817 23:56:12.999408 15109 net.cpp:128] Creating Layer Convolution13
I0817 23:56:12.999413 15109 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0817 23:56:12.999421 15109 net.cpp:522] Convolution13 -> Convolution13
I0817 23:56:13.000895 15109 net.cpp:172] Setting up Convolution13
I0817 23:56:13.000917 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.000921 15109 net.cpp:194] Memory required for data: 437780992
I0817 23:56:13.000931 15109 layer_factory.hpp:77] Creating layer BatchNorm13
I0817 23:56:13.000939 15109 net.cpp:128] Creating Layer BatchNorm13
I0817 23:56:13.000944 15109 net.cpp:558] BatchNorm13 <- Convolution13
I0817 23:56:13.000953 15109 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0817 23:56:13.001194 15109 net.cpp:172] Setting up BatchNorm13
I0817 23:56:13.001200 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.001206 15109 net.cpp:194] Memory required for data: 441975296
I0817 23:56:13.001215 15109 layer_factory.hpp:77] Creating layer Scale13
I0817 23:56:13.001222 15109 net.cpp:128] Creating Layer Scale13
I0817 23:56:13.001226 15109 net.cpp:558] Scale13 <- Convolution13
I0817 23:56:13.001235 15109 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0817 23:56:13.001271 15109 layer_factory.hpp:77] Creating layer Scale13
I0817 23:56:13.001407 15109 net.cpp:172] Setting up Scale13
I0817 23:56:13.001416 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.001420 15109 net.cpp:194] Memory required for data: 446169600
I0817 23:56:13.001428 15109 layer_factory.hpp:77] Creating layer ReLU12
I0817 23:56:13.001435 15109 net.cpp:128] Creating Layer ReLU12
I0817 23:56:13.001438 15109 net.cpp:558] ReLU12 <- Convolution13
I0817 23:56:13.001443 15109 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0817 23:56:13.001675 15109 net.cpp:172] Setting up ReLU12
I0817 23:56:13.001688 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.001691 15109 net.cpp:194] Memory required for data: 450363904
I0817 23:56:13.001696 15109 layer_factory.hpp:77] Creating layer Convolution14
I0817 23:56:13.001708 15109 net.cpp:128] Creating Layer Convolution14
I0817 23:56:13.001713 15109 net.cpp:558] Convolution14 <- Convolution13
I0817 23:56:13.001720 15109 net.cpp:522] Convolution14 -> Convolution14
I0817 23:56:13.006201 15109 net.cpp:172] Setting up Convolution14
I0817 23:56:13.006228 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.006232 15109 net.cpp:194] Memory required for data: 454558208
I0817 23:56:13.006243 15109 layer_factory.hpp:77] Creating layer BatchNorm14
I0817 23:56:13.006276 15109 net.cpp:128] Creating Layer BatchNorm14
I0817 23:56:13.006283 15109 net.cpp:558] BatchNorm14 <- Convolution14
I0817 23:56:13.006289 15109 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0817 23:56:13.006526 15109 net.cpp:172] Setting up BatchNorm14
I0817 23:56:13.006537 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.006541 15109 net.cpp:194] Memory required for data: 458752512
I0817 23:56:13.006552 15109 layer_factory.hpp:77] Creating layer Scale14
I0817 23:56:13.006561 15109 net.cpp:128] Creating Layer Scale14
I0817 23:56:13.006566 15109 net.cpp:558] Scale14 <- Convolution14
I0817 23:56:13.006572 15109 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0817 23:56:13.006611 15109 layer_factory.hpp:77] Creating layer Scale14
I0817 23:56:13.006744 15109 net.cpp:172] Setting up Scale14
I0817 23:56:13.006754 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.006758 15109 net.cpp:194] Memory required for data: 462946816
I0817 23:56:13.006767 15109 layer_factory.hpp:77] Creating layer Eltwise6
I0817 23:56:13.006775 15109 net.cpp:128] Creating Layer Eltwise6
I0817 23:56:13.006780 15109 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0817 23:56:13.006785 15109 net.cpp:558] Eltwise6 <- Convolution14
I0817 23:56:13.006791 15109 net.cpp:522] Eltwise6 -> Eltwise6
I0817 23:56:13.006811 15109 net.cpp:172] Setting up Eltwise6
I0817 23:56:13.006819 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.006821 15109 net.cpp:194] Memory required for data: 467141120
I0817 23:56:13.006826 15109 layer_factory.hpp:77] Creating layer ReLU13
I0817 23:56:13.006834 15109 net.cpp:128] Creating Layer ReLU13
I0817 23:56:13.006839 15109 net.cpp:558] ReLU13 <- Eltwise6
I0817 23:56:13.006844 15109 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0817 23:56:13.007391 15109 net.cpp:172] Setting up ReLU13
I0817 23:56:13.007414 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.007419 15109 net.cpp:194] Memory required for data: 471335424
I0817 23:56:13.007424 15109 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0817 23:56:13.007432 15109 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0817 23:56:13.007437 15109 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0817 23:56:13.007444 15109 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0817 23:56:13.007455 15109 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0817 23:56:13.007505 15109 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0817 23:56:13.007513 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.007519 15109 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0817 23:56:13.007522 15109 net.cpp:194] Memory required for data: 479724032
I0817 23:56:13.007526 15109 layer_factory.hpp:77] Creating layer Convolution15
I0817 23:56:13.007540 15109 net.cpp:128] Creating Layer Convolution15
I0817 23:56:13.007545 15109 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0817 23:56:13.007552 15109 net.cpp:522] Convolution15 -> Convolution15
I0817 23:56:13.011610 15109 net.cpp:172] Setting up Convolution15
I0817 23:56:13.011637 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.011641 15109 net.cpp:194] Memory required for data: 481821184
I0817 23:56:13.011652 15109 layer_factory.hpp:77] Creating layer BatchNorm15
I0817 23:56:13.011662 15109 net.cpp:128] Creating Layer BatchNorm15
I0817 23:56:13.011667 15109 net.cpp:558] BatchNorm15 <- Convolution15
I0817 23:56:13.011677 15109 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0817 23:56:13.011924 15109 net.cpp:172] Setting up BatchNorm15
I0817 23:56:13.011934 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.011939 15109 net.cpp:194] Memory required for data: 483918336
I0817 23:56:13.011948 15109 layer_factory.hpp:77] Creating layer Scale15
I0817 23:56:13.011955 15109 net.cpp:128] Creating Layer Scale15
I0817 23:56:13.011960 15109 net.cpp:558] Scale15 <- Convolution15
I0817 23:56:13.011965 15109 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0817 23:56:13.012006 15109 layer_factory.hpp:77] Creating layer Scale15
I0817 23:56:13.012164 15109 net.cpp:172] Setting up Scale15
I0817 23:56:13.012179 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.012183 15109 net.cpp:194] Memory required for data: 486015488
I0817 23:56:13.012192 15109 layer_factory.hpp:77] Creating layer Convolution16
I0817 23:56:13.012205 15109 net.cpp:128] Creating Layer Convolution16
I0817 23:56:13.012210 15109 net.cpp:558] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0817 23:56:13.012219 15109 net.cpp:522] Convolution16 -> Convolution16
I0817 23:56:13.015872 15109 net.cpp:172] Setting up Convolution16
I0817 23:56:13.015913 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.015918 15109 net.cpp:194] Memory required for data: 488112640
I0817 23:56:13.015933 15109 layer_factory.hpp:77] Creating layer BatchNorm16
I0817 23:56:13.015949 15109 net.cpp:128] Creating Layer BatchNorm16
I0817 23:56:13.015956 15109 net.cpp:558] BatchNorm16 <- Convolution16
I0817 23:56:13.015967 15109 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0817 23:56:13.016232 15109 net.cpp:172] Setting up BatchNorm16
I0817 23:56:13.016239 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.016242 15109 net.cpp:194] Memory required for data: 490209792
I0817 23:56:13.016253 15109 layer_factory.hpp:77] Creating layer Scale16
I0817 23:56:13.016261 15109 net.cpp:128] Creating Layer Scale16
I0817 23:56:13.016266 15109 net.cpp:558] Scale16 <- Convolution16
I0817 23:56:13.016273 15109 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0817 23:56:13.016316 15109 layer_factory.hpp:77] Creating layer Scale16
I0817 23:56:13.016463 15109 net.cpp:172] Setting up Scale16
I0817 23:56:13.016470 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.016474 15109 net.cpp:194] Memory required for data: 492306944
I0817 23:56:13.016482 15109 layer_factory.hpp:77] Creating layer ReLU14
I0817 23:56:13.016489 15109 net.cpp:128] Creating Layer ReLU14
I0817 23:56:13.016494 15109 net.cpp:558] ReLU14 <- Convolution16
I0817 23:56:13.016499 15109 net.cpp:509] ReLU14 -> Convolution16 (in-place)
I0817 23:56:13.016873 15109 net.cpp:172] Setting up ReLU14
I0817 23:56:13.016886 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.016891 15109 net.cpp:194] Memory required for data: 494404096
I0817 23:56:13.016896 15109 layer_factory.hpp:77] Creating layer Convolution17
I0817 23:56:13.016913 15109 net.cpp:128] Creating Layer Convolution17
I0817 23:56:13.016918 15109 net.cpp:558] Convolution17 <- Convolution16
I0817 23:56:13.016927 15109 net.cpp:522] Convolution17 -> Convolution17
I0817 23:56:13.020226 15109 net.cpp:172] Setting up Convolution17
I0817 23:56:13.020253 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.020262 15109 net.cpp:194] Memory required for data: 496501248
I0817 23:56:13.020273 15109 layer_factory.hpp:77] Creating layer BatchNorm17
I0817 23:56:13.020282 15109 net.cpp:128] Creating Layer BatchNorm17
I0817 23:56:13.020292 15109 net.cpp:558] BatchNorm17 <- Convolution17
I0817 23:56:13.020300 15109 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0817 23:56:13.020570 15109 net.cpp:172] Setting up BatchNorm17
I0817 23:56:13.020577 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.020581 15109 net.cpp:194] Memory required for data: 498598400
I0817 23:56:13.020591 15109 layer_factory.hpp:77] Creating layer Scale17
I0817 23:56:13.020597 15109 net.cpp:128] Creating Layer Scale17
I0817 23:56:13.020601 15109 net.cpp:558] Scale17 <- Convolution17
I0817 23:56:13.020608 15109 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0817 23:56:13.020648 15109 layer_factory.hpp:77] Creating layer Scale17
I0817 23:56:13.020797 15109 net.cpp:172] Setting up Scale17
I0817 23:56:13.020804 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.020808 15109 net.cpp:194] Memory required for data: 500695552
I0817 23:56:13.020817 15109 layer_factory.hpp:77] Creating layer Eltwise7
I0817 23:56:13.020824 15109 net.cpp:128] Creating Layer Eltwise7
I0817 23:56:13.020828 15109 net.cpp:558] Eltwise7 <- Convolution15
I0817 23:56:13.020856 15109 net.cpp:558] Eltwise7 <- Convolution17
I0817 23:56:13.020865 15109 net.cpp:522] Eltwise7 -> Eltwise7
I0817 23:56:13.020889 15109 net.cpp:172] Setting up Eltwise7
I0817 23:56:13.020896 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.020900 15109 net.cpp:194] Memory required for data: 502792704
I0817 23:56:13.020905 15109 layer_factory.hpp:77] Creating layer ReLU15
I0817 23:56:13.020915 15109 net.cpp:128] Creating Layer ReLU15
I0817 23:56:13.020918 15109 net.cpp:558] ReLU15 <- Eltwise7
I0817 23:56:13.020923 15109 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0817 23:56:13.021158 15109 net.cpp:172] Setting up ReLU15
I0817 23:56:13.021168 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.021173 15109 net.cpp:194] Memory required for data: 504889856
I0817 23:56:13.021176 15109 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0817 23:56:13.021186 15109 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0817 23:56:13.021190 15109 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0817 23:56:13.021196 15109 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0817 23:56:13.021206 15109 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0817 23:56:13.021256 15109 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0817 23:56:13.021263 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.021270 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.021273 15109 net.cpp:194] Memory required for data: 509084160
I0817 23:56:13.021277 15109 layer_factory.hpp:77] Creating layer Convolution18
I0817 23:56:13.021291 15109 net.cpp:128] Creating Layer Convolution18
I0817 23:56:13.021296 15109 net.cpp:558] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0817 23:56:13.021303 15109 net.cpp:522] Convolution18 -> Convolution18
I0817 23:56:13.023483 15109 net.cpp:172] Setting up Convolution18
I0817 23:56:13.023504 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.023509 15109 net.cpp:194] Memory required for data: 511181312
I0817 23:56:13.023519 15109 layer_factory.hpp:77] Creating layer BatchNorm18
I0817 23:56:13.023530 15109 net.cpp:128] Creating Layer BatchNorm18
I0817 23:56:13.023535 15109 net.cpp:558] BatchNorm18 <- Convolution18
I0817 23:56:13.023547 15109 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0817 23:56:13.023799 15109 net.cpp:172] Setting up BatchNorm18
I0817 23:56:13.023807 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.023810 15109 net.cpp:194] Memory required for data: 513278464
I0817 23:56:13.023819 15109 layer_factory.hpp:77] Creating layer Scale18
I0817 23:56:13.023828 15109 net.cpp:128] Creating Layer Scale18
I0817 23:56:13.023833 15109 net.cpp:558] Scale18 <- Convolution18
I0817 23:56:13.023838 15109 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0817 23:56:13.023880 15109 layer_factory.hpp:77] Creating layer Scale18
I0817 23:56:13.024027 15109 net.cpp:172] Setting up Scale18
I0817 23:56:13.024035 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.024039 15109 net.cpp:194] Memory required for data: 515375616
I0817 23:56:13.024046 15109 layer_factory.hpp:77] Creating layer ReLU16
I0817 23:56:13.024058 15109 net.cpp:128] Creating Layer ReLU16
I0817 23:56:13.024063 15109 net.cpp:558] ReLU16 <- Convolution18
I0817 23:56:13.024070 15109 net.cpp:509] ReLU16 -> Convolution18 (in-place)
I0817 23:56:13.024581 15109 net.cpp:172] Setting up ReLU16
I0817 23:56:13.024606 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.024610 15109 net.cpp:194] Memory required for data: 517472768
I0817 23:56:13.024616 15109 layer_factory.hpp:77] Creating layer Convolution19
I0817 23:56:13.024629 15109 net.cpp:128] Creating Layer Convolution19
I0817 23:56:13.024636 15109 net.cpp:558] Convolution19 <- Convolution18
I0817 23:56:13.024644 15109 net.cpp:522] Convolution19 -> Convolution19
I0817 23:56:13.027096 15109 net.cpp:172] Setting up Convolution19
I0817 23:56:13.027117 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.027122 15109 net.cpp:194] Memory required for data: 519569920
I0817 23:56:13.027150 15109 layer_factory.hpp:77] Creating layer BatchNorm19
I0817 23:56:13.027161 15109 net.cpp:128] Creating Layer BatchNorm19
I0817 23:56:13.027166 15109 net.cpp:558] BatchNorm19 <- Convolution19
I0817 23:56:13.027174 15109 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0817 23:56:13.027446 15109 net.cpp:172] Setting up BatchNorm19
I0817 23:56:13.027454 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.027458 15109 net.cpp:194] Memory required for data: 521667072
I0817 23:56:13.027482 15109 layer_factory.hpp:77] Creating layer Scale19
I0817 23:56:13.027492 15109 net.cpp:128] Creating Layer Scale19
I0817 23:56:13.027496 15109 net.cpp:558] Scale19 <- Convolution19
I0817 23:56:13.027501 15109 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0817 23:56:13.027542 15109 layer_factory.hpp:77] Creating layer Scale19
I0817 23:56:13.027688 15109 net.cpp:172] Setting up Scale19
I0817 23:56:13.027695 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.027699 15109 net.cpp:194] Memory required for data: 523764224
I0817 23:56:13.027707 15109 layer_factory.hpp:77] Creating layer Eltwise8
I0817 23:56:13.027714 15109 net.cpp:128] Creating Layer Eltwise8
I0817 23:56:13.027719 15109 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0817 23:56:13.027724 15109 net.cpp:558] Eltwise8 <- Convolution19
I0817 23:56:13.027731 15109 net.cpp:522] Eltwise8 -> Eltwise8
I0817 23:56:13.027752 15109 net.cpp:172] Setting up Eltwise8
I0817 23:56:13.027762 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.027766 15109 net.cpp:194] Memory required for data: 525861376
I0817 23:56:13.027770 15109 layer_factory.hpp:77] Creating layer ReLU17
I0817 23:56:13.027776 15109 net.cpp:128] Creating Layer ReLU17
I0817 23:56:13.027781 15109 net.cpp:558] ReLU17 <- Eltwise8
I0817 23:56:13.027786 15109 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0817 23:56:13.028028 15109 net.cpp:172] Setting up ReLU17
I0817 23:56:13.028040 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.028044 15109 net.cpp:194] Memory required for data: 527958528
I0817 23:56:13.028049 15109 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0817 23:56:13.028056 15109 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0817 23:56:13.028060 15109 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0817 23:56:13.028066 15109 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0817 23:56:13.028077 15109 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0817 23:56:13.028126 15109 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0817 23:56:13.028132 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.028138 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.028142 15109 net.cpp:194] Memory required for data: 532152832
I0817 23:56:13.028146 15109 layer_factory.hpp:77] Creating layer Convolution20
I0817 23:56:13.028159 15109 net.cpp:128] Creating Layer Convolution20
I0817 23:56:13.028164 15109 net.cpp:558] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0817 23:56:13.028172 15109 net.cpp:522] Convolution20 -> Convolution20
I0817 23:56:13.030100 15109 net.cpp:172] Setting up Convolution20
I0817 23:56:13.030127 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.030130 15109 net.cpp:194] Memory required for data: 534249984
I0817 23:56:13.030144 15109 layer_factory.hpp:77] Creating layer BatchNorm20
I0817 23:56:13.030154 15109 net.cpp:128] Creating Layer BatchNorm20
I0817 23:56:13.030159 15109 net.cpp:558] BatchNorm20 <- Convolution20
I0817 23:56:13.030169 15109 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0817 23:56:13.030426 15109 net.cpp:172] Setting up BatchNorm20
I0817 23:56:13.030436 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.030441 15109 net.cpp:194] Memory required for data: 536347136
I0817 23:56:13.030449 15109 layer_factory.hpp:77] Creating layer Scale20
I0817 23:56:13.030459 15109 net.cpp:128] Creating Layer Scale20
I0817 23:56:13.030463 15109 net.cpp:558] Scale20 <- Convolution20
I0817 23:56:13.030484 15109 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0817 23:56:13.030529 15109 layer_factory.hpp:77] Creating layer Scale20
I0817 23:56:13.030685 15109 net.cpp:172] Setting up Scale20
I0817 23:56:13.030695 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.030699 15109 net.cpp:194] Memory required for data: 538444288
I0817 23:56:13.030711 15109 layer_factory.hpp:77] Creating layer ReLU18
I0817 23:56:13.030719 15109 net.cpp:128] Creating Layer ReLU18
I0817 23:56:13.030725 15109 net.cpp:558] ReLU18 <- Convolution20
I0817 23:56:13.030731 15109 net.cpp:509] ReLU18 -> Convolution20 (in-place)
I0817 23:56:13.030978 15109 net.cpp:172] Setting up ReLU18
I0817 23:56:13.030992 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.030997 15109 net.cpp:194] Memory required for data: 540541440
I0817 23:56:13.031006 15109 layer_factory.hpp:77] Creating layer Convolution21
I0817 23:56:13.031019 15109 net.cpp:128] Creating Layer Convolution21
I0817 23:56:13.031026 15109 net.cpp:558] Convolution21 <- Convolution20
I0817 23:56:13.031036 15109 net.cpp:522] Convolution21 -> Convolution21
I0817 23:56:13.033227 15109 net.cpp:172] Setting up Convolution21
I0817 23:56:13.033255 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.033259 15109 net.cpp:194] Memory required for data: 542638592
I0817 23:56:13.033272 15109 layer_factory.hpp:77] Creating layer BatchNorm21
I0817 23:56:13.033280 15109 net.cpp:128] Creating Layer BatchNorm21
I0817 23:56:13.033285 15109 net.cpp:558] BatchNorm21 <- Convolution21
I0817 23:56:13.033294 15109 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0817 23:56:13.033572 15109 net.cpp:172] Setting up BatchNorm21
I0817 23:56:13.033583 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.033587 15109 net.cpp:194] Memory required for data: 544735744
I0817 23:56:13.033597 15109 layer_factory.hpp:77] Creating layer Scale21
I0817 23:56:13.033603 15109 net.cpp:128] Creating Layer Scale21
I0817 23:56:13.033608 15109 net.cpp:558] Scale21 <- Convolution21
I0817 23:56:13.033615 15109 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0817 23:56:13.033655 15109 layer_factory.hpp:77] Creating layer Scale21
I0817 23:56:13.033805 15109 net.cpp:172] Setting up Scale21
I0817 23:56:13.033815 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.033820 15109 net.cpp:194] Memory required for data: 546832896
I0817 23:56:13.033828 15109 layer_factory.hpp:77] Creating layer Eltwise9
I0817 23:56:13.033836 15109 net.cpp:128] Creating Layer Eltwise9
I0817 23:56:13.033841 15109 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0817 23:56:13.033846 15109 net.cpp:558] Eltwise9 <- Convolution21
I0817 23:56:13.033852 15109 net.cpp:522] Eltwise9 -> Eltwise9
I0817 23:56:13.033875 15109 net.cpp:172] Setting up Eltwise9
I0817 23:56:13.033882 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.033886 15109 net.cpp:194] Memory required for data: 548930048
I0817 23:56:13.033890 15109 layer_factory.hpp:77] Creating layer ReLU19
I0817 23:56:13.033896 15109 net.cpp:128] Creating Layer ReLU19
I0817 23:56:13.033900 15109 net.cpp:558] ReLU19 <- Eltwise9
I0817 23:56:13.033906 15109 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0817 23:56:13.034147 15109 net.cpp:172] Setting up ReLU19
I0817 23:56:13.034163 15109 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0817 23:56:13.034168 15109 net.cpp:194] Memory required for data: 551027200
I0817 23:56:13.034173 15109 layer_factory.hpp:77] Creating layer Pooling1
I0817 23:56:13.034181 15109 net.cpp:128] Creating Layer Pooling1
I0817 23:56:13.034185 15109 net.cpp:558] Pooling1 <- Eltwise9
I0817 23:56:13.034193 15109 net.cpp:522] Pooling1 -> Pooling1
I0817 23:56:13.034787 15109 net.cpp:172] Setting up Pooling1
I0817 23:56:13.034816 15109 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0817 23:56:13.034821 15109 net.cpp:194] Memory required for data: 551059968
I0817 23:56:13.034826 15109 layer_factory.hpp:77] Creating layer InnerProduct1
I0817 23:56:13.034837 15109 net.cpp:128] Creating Layer InnerProduct1
I0817 23:56:13.034862 15109 net.cpp:558] InnerProduct1 <- Pooling1
I0817 23:56:13.034873 15109 net.cpp:522] InnerProduct1 -> InnerProduct1
I0817 23:56:13.035063 15109 net.cpp:172] Setting up InnerProduct1
I0817 23:56:13.035075 15109 net.cpp:186] Top shape: 128 10 (1280)
I0817 23:56:13.035079 15109 net.cpp:194] Memory required for data: 551065088
I0817 23:56:13.035089 15109 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0817 23:56:13.035097 15109 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0817 23:56:13.035102 15109 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0817 23:56:13.035107 15109 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0817 23:56:13.035117 15109 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0817 23:56:13.035128 15109 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0817 23:56:13.035490 15109 net.cpp:172] Setting up SoftmaxWithLoss1
I0817 23:56:13.035506 15109 net.cpp:186] Top shape: (1)
I0817 23:56:13.035511 15109 net.cpp:189]     with loss weight 1
I0817 23:56:13.035543 15109 net.cpp:194] Memory required for data: 551065092
I0817 23:56:13.035550 15109 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0817 23:56:13.035557 15109 net.cpp:301] InnerProduct1 needs backward computation.
I0817 23:56:13.035562 15109 net.cpp:301] Pooling1 needs backward computation.
I0817 23:56:13.035567 15109 net.cpp:301] ReLU19 needs backward computation.
I0817 23:56:13.035570 15109 net.cpp:301] Eltwise9 needs backward computation.
I0817 23:56:13.035575 15109 net.cpp:301] Scale21 needs backward computation.
I0817 23:56:13.035579 15109 net.cpp:301] BatchNorm21 needs backward computation.
I0817 23:56:13.035583 15109 net.cpp:301] Convolution21 needs backward computation.
I0817 23:56:13.035588 15109 net.cpp:301] ReLU18 needs backward computation.
I0817 23:56:13.035593 15109 net.cpp:301] Scale20 needs backward computation.
I0817 23:56:13.035596 15109 net.cpp:301] BatchNorm20 needs backward computation.
I0817 23:56:13.035600 15109 net.cpp:301] Convolution20 needs backward computation.
I0817 23:56:13.035605 15109 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0817 23:56:13.035609 15109 net.cpp:301] ReLU17 needs backward computation.
I0817 23:56:13.035614 15109 net.cpp:301] Eltwise8 needs backward computation.
I0817 23:56:13.035624 15109 net.cpp:301] Scale19 needs backward computation.
I0817 23:56:13.035629 15109 net.cpp:301] BatchNorm19 needs backward computation.
I0817 23:56:13.035632 15109 net.cpp:301] Convolution19 needs backward computation.
I0817 23:56:13.035636 15109 net.cpp:301] ReLU16 needs backward computation.
I0817 23:56:13.035640 15109 net.cpp:301] Scale18 needs backward computation.
I0817 23:56:13.035645 15109 net.cpp:301] BatchNorm18 needs backward computation.
I0817 23:56:13.035648 15109 net.cpp:301] Convolution18 needs backward computation.
I0817 23:56:13.035652 15109 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0817 23:56:13.035657 15109 net.cpp:301] ReLU15 needs backward computation.
I0817 23:56:13.035661 15109 net.cpp:301] Eltwise7 needs backward computation.
I0817 23:56:13.035666 15109 net.cpp:301] Scale17 needs backward computation.
I0817 23:56:13.035670 15109 net.cpp:301] BatchNorm17 needs backward computation.
I0817 23:56:13.035675 15109 net.cpp:301] Convolution17 needs backward computation.
I0817 23:56:13.035679 15109 net.cpp:301] ReLU14 needs backward computation.
I0817 23:56:13.035683 15109 net.cpp:301] Scale16 needs backward computation.
I0817 23:56:13.035687 15109 net.cpp:301] BatchNorm16 needs backward computation.
I0817 23:56:13.035691 15109 net.cpp:301] Convolution16 needs backward computation.
I0817 23:56:13.035696 15109 net.cpp:301] Scale15 needs backward computation.
I0817 23:56:13.035701 15109 net.cpp:301] BatchNorm15 needs backward computation.
I0817 23:56:13.035704 15109 net.cpp:301] Convolution15 needs backward computation.
I0817 23:56:13.035709 15109 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0817 23:56:13.035714 15109 net.cpp:301] ReLU13 needs backward computation.
I0817 23:56:13.035718 15109 net.cpp:301] Eltwise6 needs backward computation.
I0817 23:56:13.035738 15109 net.cpp:301] Scale14 needs backward computation.
I0817 23:56:13.035743 15109 net.cpp:301] BatchNorm14 needs backward computation.
I0817 23:56:13.035748 15109 net.cpp:301] Convolution14 needs backward computation.
I0817 23:56:13.035753 15109 net.cpp:301] ReLU12 needs backward computation.
I0817 23:56:13.035756 15109 net.cpp:301] Scale13 needs backward computation.
I0817 23:56:13.035760 15109 net.cpp:301] BatchNorm13 needs backward computation.
I0817 23:56:13.035764 15109 net.cpp:301] Convolution13 needs backward computation.
I0817 23:56:13.035769 15109 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0817 23:56:13.035774 15109 net.cpp:301] ReLU11 needs backward computation.
I0817 23:56:13.035778 15109 net.cpp:301] Eltwise5 needs backward computation.
I0817 23:56:13.035784 15109 net.cpp:301] Scale12 needs backward computation.
I0817 23:56:13.035787 15109 net.cpp:301] BatchNorm12 needs backward computation.
I0817 23:56:13.035791 15109 net.cpp:301] Convolution12 needs backward computation.
I0817 23:56:13.035796 15109 net.cpp:301] ReLU10 needs backward computation.
I0817 23:56:13.035800 15109 net.cpp:301] Scale11 needs backward computation.
I0817 23:56:13.035804 15109 net.cpp:301] BatchNorm11 needs backward computation.
I0817 23:56:13.035809 15109 net.cpp:301] Convolution11 needs backward computation.
I0817 23:56:13.035814 15109 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0817 23:56:13.035821 15109 net.cpp:301] ReLU9 needs backward computation.
I0817 23:56:13.035825 15109 net.cpp:301] Eltwise4 needs backward computation.
I0817 23:56:13.035830 15109 net.cpp:301] Scale10 needs backward computation.
I0817 23:56:13.035835 15109 net.cpp:301] BatchNorm10 needs backward computation.
I0817 23:56:13.035840 15109 net.cpp:301] Convolution10 needs backward computation.
I0817 23:56:13.035843 15109 net.cpp:301] ReLU8 needs backward computation.
I0817 23:56:13.035848 15109 net.cpp:301] Scale9 needs backward computation.
I0817 23:56:13.035852 15109 net.cpp:301] BatchNorm9 needs backward computation.
I0817 23:56:13.035857 15109 net.cpp:301] Convolution9 needs backward computation.
I0817 23:56:13.035861 15109 net.cpp:301] Scale8 needs backward computation.
I0817 23:56:13.035866 15109 net.cpp:301] BatchNorm8 needs backward computation.
I0817 23:56:13.035871 15109 net.cpp:301] Convolution8 needs backward computation.
I0817 23:56:13.035876 15109 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0817 23:56:13.035881 15109 net.cpp:301] ReLU7 needs backward computation.
I0817 23:56:13.035884 15109 net.cpp:301] Eltwise3 needs backward computation.
I0817 23:56:13.035889 15109 net.cpp:301] Scale7 needs backward computation.
I0817 23:56:13.035893 15109 net.cpp:301] BatchNorm7 needs backward computation.
I0817 23:56:13.035898 15109 net.cpp:301] Convolution7 needs backward computation.
I0817 23:56:13.035902 15109 net.cpp:301] ReLU6 needs backward computation.
I0817 23:56:13.035907 15109 net.cpp:301] Scale6 needs backward computation.
I0817 23:56:13.035912 15109 net.cpp:301] BatchNorm6 needs backward computation.
I0817 23:56:13.035915 15109 net.cpp:301] Convolution6 needs backward computation.
I0817 23:56:13.035920 15109 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0817 23:56:13.035925 15109 net.cpp:301] ReLU5 needs backward computation.
I0817 23:56:13.035929 15109 net.cpp:301] Eltwise2 needs backward computation.
I0817 23:56:13.035934 15109 net.cpp:301] Scale5 needs backward computation.
I0817 23:56:13.035938 15109 net.cpp:301] BatchNorm5 needs backward computation.
I0817 23:56:13.035943 15109 net.cpp:301] Convolution5 needs backward computation.
I0817 23:56:13.035948 15109 net.cpp:301] ReLU4 needs backward computation.
I0817 23:56:13.035953 15109 net.cpp:301] Scale4 needs backward computation.
I0817 23:56:13.035956 15109 net.cpp:301] BatchNorm4 needs backward computation.
I0817 23:56:13.035960 15109 net.cpp:301] Convolution4 needs backward computation.
I0817 23:56:13.035965 15109 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0817 23:56:13.035976 15109 net.cpp:301] ReLU3 needs backward computation.
I0817 23:56:13.035981 15109 net.cpp:301] Eltwise1 needs backward computation.
I0817 23:56:13.035986 15109 net.cpp:301] Scale3 needs backward computation.
I0817 23:56:13.035991 15109 net.cpp:301] BatchNorm3 needs backward computation.
I0817 23:56:13.035995 15109 net.cpp:301] Convolution3 needs backward computation.
I0817 23:56:13.036000 15109 net.cpp:301] ReLU2 needs backward computation.
I0817 23:56:13.036005 15109 net.cpp:301] Scale2 needs backward computation.
I0817 23:56:13.036008 15109 net.cpp:301] BatchNorm2 needs backward computation.
I0817 23:56:13.036013 15109 net.cpp:301] Convolution2 needs backward computation.
I0817 23:56:13.036018 15109 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0817 23:56:13.036023 15109 net.cpp:301] ReLU1 needs backward computation.
I0817 23:56:13.036027 15109 net.cpp:301] Scale1 needs backward computation.
I0817 23:56:13.036031 15109 net.cpp:301] BatchNorm1 needs backward computation.
I0817 23:56:13.036036 15109 net.cpp:301] Convolution1 needs backward computation.
I0817 23:56:13.036044 15109 net.cpp:303] Data1 does not need backward computation.
I0817 23:56:13.036048 15109 net.cpp:348] This network produces output SoftmaxWithLoss1
I0817 23:56:13.036104 15109 net.cpp:363] Network initialization done.
I0817 23:56:13.037154 15109 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_20.prototxt
I0817 23:56:13.037175 15109 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0817 23:56:13.037184 15109 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_20.prototxt
I0817 23:56:13.037271 15109 net.cpp:390] layer_param.include_size():1
I0817 23:56:13.037279 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037286 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037289 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037293 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037297 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037302 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037304 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037308 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037312 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037317 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037320 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037324 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037328 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037331 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037335 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037339 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037343 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037348 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037350 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037354 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037358 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037364 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037366 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037370 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037374 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037379 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037381 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037385 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037389 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037394 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037397 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037400 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037405 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037422 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037426 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037431 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037434 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037438 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037442 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037446 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037449 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037453 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037457 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037461 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037464 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037468 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037472 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037477 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037479 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037484 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037488 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037492 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037495 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037501 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037504 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037508 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037513 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037516 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037520 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037523 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037528 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037531 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037535 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037539 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037542 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037546 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037550 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037554 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037559 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037562 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037567 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037571 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037575 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037578 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037583 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037587 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037591 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037595 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037598 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037602 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037606 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037611 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037613 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037618 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037622 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037626 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037629 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037633 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037637 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037642 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037645 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037655 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037659 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037663 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037667 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037672 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037674 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037678 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037683 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037686 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037690 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037694 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037698 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037701 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037705 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037709 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037712 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037717 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037720 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037724 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037729 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037732 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037735 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037739 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037744 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037748 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037752 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037756 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037760 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037763 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037767 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037772 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037776 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037780 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037783 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037787 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037791 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037796 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037798 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037802 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037806 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037811 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037816 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037819 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037823 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037827 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037830 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037834 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037838 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037842 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037847 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037850 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037853 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037858 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037861 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037865 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037868 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037873 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037876 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037880 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037891 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037896 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037899 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037904 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037907 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037911 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037914 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037919 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037922 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037926 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037930 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037935 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037937 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037941 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037945 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037950 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037952 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037956 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037961 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037964 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037968 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037972 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037976 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037979 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037983 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037987 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037991 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.037994 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.037998 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038002 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.038005 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038010 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.038014 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038018 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.038022 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038028 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.038030 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038034 15109 net.cpp:390] layer_param.include_size():0
I0817 23:56:13.038038 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038043 15109 net.cpp:390] layer_param.include_size():1
I0817 23:56:13.038046 15109 net.cpp:391] layer_param.exclude_size():0
I0817 23:56:13.038671 15109 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215684
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution9"
  top: "Convolution9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Convolution9"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution8"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution12"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Eltwise5"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution16"
  top: "Convolution16"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Convolution16"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution15"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution19"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution21"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Eltwise9"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling1"
  top: "InnerProduct1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "Data2"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I0817 23:56:13.039064 15109 layer_factory.hpp:77] Creating layer Data1
I0817 23:56:13.039160 15109 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0817 23:56:13.039180 15109 net.cpp:128] Creating Layer Data1
I0817 23:56:13.039188 15109 net.cpp:522] Data1 -> Data1
I0817 23:56:13.039198 15109 net.cpp:522] Data1 -> Data2
I0817 23:56:13.039347 15109 data_layer.cpp:45] output data size: 10,3,32,32
I0817 23:56:13.047003 15109 net.cpp:172] Setting up Data1
I0817 23:56:13.047026 15109 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0817 23:56:13.047032 15109 net.cpp:186] Top shape: 10 (10)
I0817 23:56:13.047036 15109 net.cpp:194] Memory required for data: 122920
I0817 23:56:13.047041 15109 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0817 23:56:13.047050 15109 net.cpp:128] Creating Layer Data2_Data1_1_split
I0817 23:56:13.047055 15109 net.cpp:558] Data2_Data1_1_split <- Data2
I0817 23:56:13.047062 15109 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0817 23:56:13.047070 15109 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0817 23:56:13.047160 15109 net.cpp:172] Setting up Data2_Data1_1_split
I0817 23:56:13.047171 15109 net.cpp:186] Top shape: 10 (10)
I0817 23:56:13.047176 15109 net.cpp:186] Top shape: 10 (10)
I0817 23:56:13.047180 15109 net.cpp:194] Memory required for data: 123000
I0817 23:56:13.047184 15109 layer_factory.hpp:77] Creating layer Convolution1
I0817 23:56:13.047219 15109 net.cpp:128] Creating Layer Convolution1
I0817 23:56:13.047227 15109 net.cpp:558] Convolution1 <- Data1
I0817 23:56:13.047236 15109 net.cpp:522] Convolution1 -> Convolution1
I0817 23:56:13.053762 15109 net.cpp:172] Setting up Convolution1
I0817 23:56:13.053786 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.053791 15109 net.cpp:194] Memory required for data: 778360
I0817 23:56:13.053809 15109 layer_factory.hpp:77] Creating layer BatchNorm1
I0817 23:56:13.053822 15109 net.cpp:128] Creating Layer BatchNorm1
I0817 23:56:13.053827 15109 net.cpp:558] BatchNorm1 <- Convolution1
I0817 23:56:13.053834 15109 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0817 23:56:13.054103 15109 net.cpp:172] Setting up BatchNorm1
I0817 23:56:13.054117 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.054121 15109 net.cpp:194] Memory required for data: 1433720
I0817 23:56:13.054133 15109 layer_factory.hpp:77] Creating layer Scale1
I0817 23:56:13.054144 15109 net.cpp:128] Creating Layer Scale1
I0817 23:56:13.054149 15109 net.cpp:558] Scale1 <- Convolution1
I0817 23:56:13.054155 15109 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0817 23:56:13.054206 15109 layer_factory.hpp:77] Creating layer Scale1
I0817 23:56:13.054361 15109 net.cpp:172] Setting up Scale1
I0817 23:56:13.054373 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.054378 15109 net.cpp:194] Memory required for data: 2089080
I0817 23:56:13.054386 15109 layer_factory.hpp:77] Creating layer ReLU1
I0817 23:56:13.054395 15109 net.cpp:128] Creating Layer ReLU1
I0817 23:56:13.054399 15109 net.cpp:558] ReLU1 <- Convolution1
I0817 23:56:13.054405 15109 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0817 23:56:13.055833 15109 net.cpp:172] Setting up ReLU1
I0817 23:56:13.055852 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.055866 15109 net.cpp:194] Memory required for data: 2744440
I0817 23:56:13.055871 15109 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0817 23:56:13.055886 15109 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0817 23:56:13.055893 15109 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0817 23:56:13.055902 15109 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0817 23:56:13.055918 15109 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0817 23:56:13.058092 15109 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0817 23:56:13.058122 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.058140 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.058145 15109 net.cpp:194] Memory required for data: 4055160
I0817 23:56:13.058158 15109 layer_factory.hpp:77] Creating layer Convolution2
I0817 23:56:13.058182 15109 net.cpp:128] Creating Layer Convolution2
I0817 23:56:13.058189 15109 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0817 23:56:13.058199 15109 net.cpp:522] Convolution2 -> Convolution2
I0817 23:56:13.063042 15109 net.cpp:172] Setting up Convolution2
I0817 23:56:13.063066 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.063072 15109 net.cpp:194] Memory required for data: 4710520
I0817 23:56:13.063087 15109 layer_factory.hpp:77] Creating layer BatchNorm2
I0817 23:56:13.063102 15109 net.cpp:128] Creating Layer BatchNorm2
I0817 23:56:13.063108 15109 net.cpp:558] BatchNorm2 <- Convolution2
I0817 23:56:13.063119 15109 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0817 23:56:13.063457 15109 net.cpp:172] Setting up BatchNorm2
I0817 23:56:13.063472 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.063477 15109 net.cpp:194] Memory required for data: 5365880
I0817 23:56:13.063489 15109 layer_factory.hpp:77] Creating layer Scale2
I0817 23:56:13.063498 15109 net.cpp:128] Creating Layer Scale2
I0817 23:56:13.063503 15109 net.cpp:558] Scale2 <- Convolution2
I0817 23:56:13.063518 15109 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0817 23:56:13.063576 15109 layer_factory.hpp:77] Creating layer Scale2
I0817 23:56:13.063786 15109 net.cpp:172] Setting up Scale2
I0817 23:56:13.063803 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.063809 15109 net.cpp:194] Memory required for data: 6021240
I0817 23:56:13.063819 15109 layer_factory.hpp:77] Creating layer ReLU2
I0817 23:56:13.063827 15109 net.cpp:128] Creating Layer ReLU2
I0817 23:56:13.063833 15109 net.cpp:558] ReLU2 <- Convolution2
I0817 23:56:13.063840 15109 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0817 23:56:13.064667 15109 net.cpp:172] Setting up ReLU2
I0817 23:56:13.064697 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.064702 15109 net.cpp:194] Memory required for data: 6676600
I0817 23:56:13.064707 15109 layer_factory.hpp:77] Creating layer Convolution3
I0817 23:56:13.064729 15109 net.cpp:128] Creating Layer Convolution3
I0817 23:56:13.064735 15109 net.cpp:558] Convolution3 <- Convolution2
I0817 23:56:13.064745 15109 net.cpp:522] Convolution3 -> Convolution3
I0817 23:56:13.071532 15109 net.cpp:172] Setting up Convolution3
I0817 23:56:13.071565 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.071573 15109 net.cpp:194] Memory required for data: 7331960
I0817 23:56:13.071588 15109 layer_factory.hpp:77] Creating layer BatchNorm3
I0817 23:56:13.071605 15109 net.cpp:128] Creating Layer BatchNorm3
I0817 23:56:13.071619 15109 net.cpp:558] BatchNorm3 <- Convolution3
I0817 23:56:13.071635 15109 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0817 23:56:13.072042 15109 net.cpp:172] Setting up BatchNorm3
I0817 23:56:13.072058 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.072065 15109 net.cpp:194] Memory required for data: 7987320
I0817 23:56:13.072090 15109 layer_factory.hpp:77] Creating layer Scale3
I0817 23:56:13.072103 15109 net.cpp:128] Creating Layer Scale3
I0817 23:56:13.072110 15109 net.cpp:558] Scale3 <- Convolution3
I0817 23:56:13.072119 15109 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0817 23:56:13.072196 15109 layer_factory.hpp:77] Creating layer Scale3
I0817 23:56:13.072428 15109 net.cpp:172] Setting up Scale3
I0817 23:56:13.072443 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.072450 15109 net.cpp:194] Memory required for data: 8642680
I0817 23:56:13.072464 15109 layer_factory.hpp:77] Creating layer Eltwise1
I0817 23:56:13.072480 15109 net.cpp:128] Creating Layer Eltwise1
I0817 23:56:13.072487 15109 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0817 23:56:13.072495 15109 net.cpp:558] Eltwise1 <- Convolution3
I0817 23:56:13.072510 15109 net.cpp:522] Eltwise1 -> Eltwise1
I0817 23:56:13.072554 15109 net.cpp:172] Setting up Eltwise1
I0817 23:56:13.072568 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.072574 15109 net.cpp:194] Memory required for data: 9298040
I0817 23:56:13.072580 15109 layer_factory.hpp:77] Creating layer ReLU3
I0817 23:56:13.072593 15109 net.cpp:128] Creating Layer ReLU3
I0817 23:56:13.072600 15109 net.cpp:558] ReLU3 <- Eltwise1
I0817 23:56:13.072608 15109 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0817 23:56:13.073487 15109 net.cpp:172] Setting up ReLU3
I0817 23:56:13.073510 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.073518 15109 net.cpp:194] Memory required for data: 9953400
I0817 23:56:13.073524 15109 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0817 23:56:13.073544 15109 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0817 23:56:13.073554 15109 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0817 23:56:13.073565 15109 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0817 23:56:13.073576 15109 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0817 23:56:13.073657 15109 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0817 23:56:13.073668 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.073678 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.073683 15109 net.cpp:194] Memory required for data: 11264120
I0817 23:56:13.073690 15109 layer_factory.hpp:77] Creating layer Convolution4
I0817 23:56:13.073709 15109 net.cpp:128] Creating Layer Convolution4
I0817 23:56:13.073735 15109 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0817 23:56:13.073752 15109 net.cpp:522] Convolution4 -> Convolution4
I0817 23:56:13.080160 15109 net.cpp:172] Setting up Convolution4
I0817 23:56:13.080190 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.080195 15109 net.cpp:194] Memory required for data: 11919480
I0817 23:56:13.080205 15109 layer_factory.hpp:77] Creating layer BatchNorm4
I0817 23:56:13.080214 15109 net.cpp:128] Creating Layer BatchNorm4
I0817 23:56:13.080224 15109 net.cpp:558] BatchNorm4 <- Convolution4
I0817 23:56:13.080232 15109 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0817 23:56:13.080520 15109 net.cpp:172] Setting up BatchNorm4
I0817 23:56:13.080531 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.080535 15109 net.cpp:194] Memory required for data: 12574840
I0817 23:56:13.080545 15109 layer_factory.hpp:77] Creating layer Scale4
I0817 23:56:13.080552 15109 net.cpp:128] Creating Layer Scale4
I0817 23:56:13.080556 15109 net.cpp:558] Scale4 <- Convolution4
I0817 23:56:13.080564 15109 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0817 23:56:13.080610 15109 layer_factory.hpp:77] Creating layer Scale4
I0817 23:56:13.080761 15109 net.cpp:172] Setting up Scale4
I0817 23:56:13.080775 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.080780 15109 net.cpp:194] Memory required for data: 13230200
I0817 23:56:13.080787 15109 layer_factory.hpp:77] Creating layer ReLU4
I0817 23:56:13.080795 15109 net.cpp:128] Creating Layer ReLU4
I0817 23:56:13.080798 15109 net.cpp:558] ReLU4 <- Convolution4
I0817 23:56:13.080804 15109 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0817 23:56:13.082229 15109 net.cpp:172] Setting up ReLU4
I0817 23:56:13.082250 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.082255 15109 net.cpp:194] Memory required for data: 13885560
I0817 23:56:13.082259 15109 layer_factory.hpp:77] Creating layer Convolution5
I0817 23:56:13.082275 15109 net.cpp:128] Creating Layer Convolution5
I0817 23:56:13.082299 15109 net.cpp:558] Convolution5 <- Convolution4
I0817 23:56:13.082307 15109 net.cpp:522] Convolution5 -> Convolution5
I0817 23:56:13.088961 15109 net.cpp:172] Setting up Convolution5
I0817 23:56:13.088990 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.088995 15109 net.cpp:194] Memory required for data: 14540920
I0817 23:56:13.089009 15109 layer_factory.hpp:77] Creating layer BatchNorm5
I0817 23:56:13.089020 15109 net.cpp:128] Creating Layer BatchNorm5
I0817 23:56:13.089025 15109 net.cpp:558] BatchNorm5 <- Convolution5
I0817 23:56:13.089031 15109 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0817 23:56:13.089304 15109 net.cpp:172] Setting up BatchNorm5
I0817 23:56:13.089315 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.089319 15109 net.cpp:194] Memory required for data: 15196280
I0817 23:56:13.089332 15109 layer_factory.hpp:77] Creating layer Scale5
I0817 23:56:13.089346 15109 net.cpp:128] Creating Layer Scale5
I0817 23:56:13.089351 15109 net.cpp:558] Scale5 <- Convolution5
I0817 23:56:13.089360 15109 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0817 23:56:13.089412 15109 layer_factory.hpp:77] Creating layer Scale5
I0817 23:56:13.089568 15109 net.cpp:172] Setting up Scale5
I0817 23:56:13.089579 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.089583 15109 net.cpp:194] Memory required for data: 15851640
I0817 23:56:13.089591 15109 layer_factory.hpp:77] Creating layer Eltwise2
I0817 23:56:13.089601 15109 net.cpp:128] Creating Layer Eltwise2
I0817 23:56:13.089607 15109 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0817 23:56:13.089612 15109 net.cpp:558] Eltwise2 <- Convolution5
I0817 23:56:13.089617 15109 net.cpp:522] Eltwise2 -> Eltwise2
I0817 23:56:13.089649 15109 net.cpp:172] Setting up Eltwise2
I0817 23:56:13.089659 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.089663 15109 net.cpp:194] Memory required for data: 16507000
I0817 23:56:13.089668 15109 layer_factory.hpp:77] Creating layer ReLU5
I0817 23:56:13.089694 15109 net.cpp:128] Creating Layer ReLU5
I0817 23:56:13.089699 15109 net.cpp:558] ReLU5 <- Eltwise2
I0817 23:56:13.089709 15109 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0817 23:56:13.091037 15109 net.cpp:172] Setting up ReLU5
I0817 23:56:13.091061 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.091066 15109 net.cpp:194] Memory required for data: 17162360
I0817 23:56:13.091071 15109 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0817 23:56:13.091079 15109 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0817 23:56:13.091084 15109 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0817 23:56:13.091094 15109 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0817 23:56:13.091104 15109 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0817 23:56:13.091166 15109 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0817 23:56:13.091177 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.091183 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.091188 15109 net.cpp:194] Memory required for data: 18473080
I0817 23:56:13.091192 15109 layer_factory.hpp:77] Creating layer Convolution6
I0817 23:56:13.091207 15109 net.cpp:128] Creating Layer Convolution6
I0817 23:56:13.091215 15109 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0817 23:56:13.091223 15109 net.cpp:522] Convolution6 -> Convolution6
I0817 23:56:13.097789 15109 net.cpp:172] Setting up Convolution6
I0817 23:56:13.097815 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.097820 15109 net.cpp:194] Memory required for data: 19128440
I0817 23:56:13.097832 15109 layer_factory.hpp:77] Creating layer BatchNorm6
I0817 23:56:13.097843 15109 net.cpp:128] Creating Layer BatchNorm6
I0817 23:56:13.097856 15109 net.cpp:558] BatchNorm6 <- Convolution6
I0817 23:56:13.097864 15109 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0817 23:56:13.098145 15109 net.cpp:172] Setting up BatchNorm6
I0817 23:56:13.098156 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.098161 15109 net.cpp:194] Memory required for data: 19783800
I0817 23:56:13.098170 15109 layer_factory.hpp:77] Creating layer Scale6
I0817 23:56:13.098191 15109 net.cpp:128] Creating Layer Scale6
I0817 23:56:13.098196 15109 net.cpp:558] Scale6 <- Convolution6
I0817 23:56:13.098202 15109 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0817 23:56:13.098254 15109 layer_factory.hpp:77] Creating layer Scale6
I0817 23:56:13.098410 15109 net.cpp:172] Setting up Scale6
I0817 23:56:13.098420 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.098424 15109 net.cpp:194] Memory required for data: 20439160
I0817 23:56:13.098433 15109 layer_factory.hpp:77] Creating layer ReLU6
I0817 23:56:13.098439 15109 net.cpp:128] Creating Layer ReLU6
I0817 23:56:13.098443 15109 net.cpp:558] ReLU6 <- Convolution6
I0817 23:56:13.098451 15109 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0817 23:56:13.099853 15109 net.cpp:172] Setting up ReLU6
I0817 23:56:13.099872 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.099879 15109 net.cpp:194] Memory required for data: 21094520
I0817 23:56:13.099884 15109 layer_factory.hpp:77] Creating layer Convolution7
I0817 23:56:13.099900 15109 net.cpp:128] Creating Layer Convolution7
I0817 23:56:13.099910 15109 net.cpp:558] Convolution7 <- Convolution6
I0817 23:56:13.099920 15109 net.cpp:522] Convolution7 -> Convolution7
I0817 23:56:13.106596 15109 net.cpp:172] Setting up Convolution7
I0817 23:56:13.106626 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.106629 15109 net.cpp:194] Memory required for data: 21749880
I0817 23:56:13.106643 15109 layer_factory.hpp:77] Creating layer BatchNorm7
I0817 23:56:13.106657 15109 net.cpp:128] Creating Layer BatchNorm7
I0817 23:56:13.106665 15109 net.cpp:558] BatchNorm7 <- Convolution7
I0817 23:56:13.106673 15109 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0817 23:56:13.106969 15109 net.cpp:172] Setting up BatchNorm7
I0817 23:56:13.106982 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.107013 15109 net.cpp:194] Memory required for data: 22405240
I0817 23:56:13.107024 15109 layer_factory.hpp:77] Creating layer Scale7
I0817 23:56:13.107035 15109 net.cpp:128] Creating Layer Scale7
I0817 23:56:13.107040 15109 net.cpp:558] Scale7 <- Convolution7
I0817 23:56:13.107045 15109 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0817 23:56:13.107100 15109 layer_factory.hpp:77] Creating layer Scale7
I0817 23:56:13.107266 15109 net.cpp:172] Setting up Scale7
I0817 23:56:13.107278 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.107283 15109 net.cpp:194] Memory required for data: 23060600
I0817 23:56:13.107291 15109 layer_factory.hpp:77] Creating layer Eltwise3
I0817 23:56:13.107301 15109 net.cpp:128] Creating Layer Eltwise3
I0817 23:56:13.107306 15109 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0817 23:56:13.107311 15109 net.cpp:558] Eltwise3 <- Convolution7
I0817 23:56:13.107317 15109 net.cpp:522] Eltwise3 -> Eltwise3
I0817 23:56:13.107347 15109 net.cpp:172] Setting up Eltwise3
I0817 23:56:13.107362 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.107367 15109 net.cpp:194] Memory required for data: 23715960
I0817 23:56:13.107370 15109 layer_factory.hpp:77] Creating layer ReLU7
I0817 23:56:13.107376 15109 net.cpp:128] Creating Layer ReLU7
I0817 23:56:13.107380 15109 net.cpp:558] ReLU7 <- Eltwise3
I0817 23:56:13.107388 15109 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0817 23:56:13.108667 15109 net.cpp:172] Setting up ReLU7
I0817 23:56:13.108685 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.108688 15109 net.cpp:194] Memory required for data: 24371320
I0817 23:56:13.108693 15109 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0817 23:56:13.108700 15109 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0817 23:56:13.108705 15109 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0817 23:56:13.108714 15109 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0817 23:56:13.108722 15109 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0817 23:56:13.108779 15109 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0817 23:56:13.108803 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.108809 15109 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0817 23:56:13.108814 15109 net.cpp:194] Memory required for data: 25682040
I0817 23:56:13.108819 15109 layer_factory.hpp:77] Creating layer Convolution8
I0817 23:56:13.108830 15109 net.cpp:128] Creating Layer Convolution8
I0817 23:56:13.108835 15109 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0817 23:56:13.108845 15109 net.cpp:522] Convolution8 -> Convolution8
I0817 23:56:13.115396 15109 net.cpp:172] Setting up Convolution8
I0817 23:56:13.115417 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.115422 15109 net.cpp:194] Memory required for data: 26009720
I0817 23:56:13.115432 15109 layer_factory.hpp:77] Creating layer BatchNorm8
I0817 23:56:13.115448 15109 net.cpp:128] Creating Layer BatchNorm8
I0817 23:56:13.115453 15109 net.cpp:558] BatchNorm8 <- Convolution8
I0817 23:56:13.115460 15109 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0817 23:56:13.115738 15109 net.cpp:172] Setting up BatchNorm8
I0817 23:56:13.115751 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.115754 15109 net.cpp:194] Memory required for data: 26337400
I0817 23:56:13.115764 15109 layer_factory.hpp:77] Creating layer Scale8
I0817 23:56:13.115772 15109 net.cpp:128] Creating Layer Scale8
I0817 23:56:13.115775 15109 net.cpp:558] Scale8 <- Convolution8
I0817 23:56:13.115780 15109 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0817 23:56:13.115833 15109 layer_factory.hpp:77] Creating layer Scale8
I0817 23:56:13.115991 15109 net.cpp:172] Setting up Scale8
I0817 23:56:13.116004 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.116008 15109 net.cpp:194] Memory required for data: 26665080
I0817 23:56:13.116016 15109 layer_factory.hpp:77] Creating layer Convolution9
I0817 23:56:13.116029 15109 net.cpp:128] Creating Layer Convolution9
I0817 23:56:13.116051 15109 net.cpp:558] Convolution9 <- Eltwise3_ReLU7_0_split_1
I0817 23:56:13.116062 15109 net.cpp:522] Convolution9 -> Convolution9
I0817 23:56:13.122022 15109 net.cpp:172] Setting up Convolution9
I0817 23:56:13.122048 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.122052 15109 net.cpp:194] Memory required for data: 26992760
I0817 23:56:13.122066 15109 layer_factory.hpp:77] Creating layer BatchNorm9
I0817 23:56:13.122077 15109 net.cpp:128] Creating Layer BatchNorm9
I0817 23:56:13.122092 15109 net.cpp:558] BatchNorm9 <- Convolution9
I0817 23:56:13.122097 15109 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0817 23:56:13.122371 15109 net.cpp:172] Setting up BatchNorm9
I0817 23:56:13.122382 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.122387 15109 net.cpp:194] Memory required for data: 27320440
I0817 23:56:13.122397 15109 layer_factory.hpp:77] Creating layer Scale9
I0817 23:56:13.122407 15109 net.cpp:128] Creating Layer Scale9
I0817 23:56:13.122412 15109 net.cpp:558] Scale9 <- Convolution9
I0817 23:56:13.122418 15109 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0817 23:56:13.122470 15109 layer_factory.hpp:77] Creating layer Scale9
I0817 23:56:13.122634 15109 net.cpp:172] Setting up Scale9
I0817 23:56:13.122647 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.122651 15109 net.cpp:194] Memory required for data: 27648120
I0817 23:56:13.122659 15109 layer_factory.hpp:77] Creating layer ReLU8
I0817 23:56:13.122666 15109 net.cpp:128] Creating Layer ReLU8
I0817 23:56:13.122670 15109 net.cpp:558] ReLU8 <- Convolution9
I0817 23:56:13.122678 15109 net.cpp:509] ReLU8 -> Convolution9 (in-place)
I0817 23:56:13.124085 15109 net.cpp:172] Setting up ReLU8
I0817 23:56:13.124104 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.124109 15109 net.cpp:194] Memory required for data: 27975800
I0817 23:56:13.124114 15109 layer_factory.hpp:77] Creating layer Convolution10
I0817 23:56:13.124127 15109 net.cpp:128] Creating Layer Convolution10
I0817 23:56:13.124132 15109 net.cpp:558] Convolution10 <- Convolution9
I0817 23:56:13.124142 15109 net.cpp:522] Convolution10 -> Convolution10
I0817 23:56:13.130820 15109 net.cpp:172] Setting up Convolution10
I0817 23:56:13.130846 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.130849 15109 net.cpp:194] Memory required for data: 28303480
I0817 23:56:13.130868 15109 layer_factory.hpp:77] Creating layer BatchNorm10
I0817 23:56:13.130880 15109 net.cpp:128] Creating Layer BatchNorm10
I0817 23:56:13.130885 15109 net.cpp:558] BatchNorm10 <- Convolution10
I0817 23:56:13.130892 15109 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0817 23:56:13.131186 15109 net.cpp:172] Setting up BatchNorm10
I0817 23:56:13.131199 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.131204 15109 net.cpp:194] Memory required for data: 28631160
I0817 23:56:13.131214 15109 layer_factory.hpp:77] Creating layer Scale10
I0817 23:56:13.131222 15109 net.cpp:128] Creating Layer Scale10
I0817 23:56:13.131227 15109 net.cpp:558] Scale10 <- Convolution10
I0817 23:56:13.131232 15109 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0817 23:56:13.131284 15109 layer_factory.hpp:77] Creating layer Scale10
I0817 23:56:13.131448 15109 net.cpp:172] Setting up Scale10
I0817 23:56:13.131458 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.131462 15109 net.cpp:194] Memory required for data: 28958840
I0817 23:56:13.131470 15109 layer_factory.hpp:77] Creating layer Eltwise4
I0817 23:56:13.131477 15109 net.cpp:128] Creating Layer Eltwise4
I0817 23:56:13.131481 15109 net.cpp:558] Eltwise4 <- Convolution8
I0817 23:56:13.131486 15109 net.cpp:558] Eltwise4 <- Convolution10
I0817 23:56:13.131495 15109 net.cpp:522] Eltwise4 -> Eltwise4
I0817 23:56:13.131518 15109 net.cpp:172] Setting up Eltwise4
I0817 23:56:13.131525 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.131530 15109 net.cpp:194] Memory required for data: 29286520
I0817 23:56:13.131534 15109 layer_factory.hpp:77] Creating layer ReLU9
I0817 23:56:13.131558 15109 net.cpp:128] Creating Layer ReLU9
I0817 23:56:13.131563 15109 net.cpp:558] ReLU9 <- Eltwise4
I0817 23:56:13.131569 15109 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0817 23:56:13.132884 15109 net.cpp:172] Setting up ReLU9
I0817 23:56:13.132910 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.132915 15109 net.cpp:194] Memory required for data: 29614200
I0817 23:56:13.132920 15109 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0817 23:56:13.132931 15109 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0817 23:56:13.132936 15109 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0817 23:56:13.132943 15109 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0817 23:56:13.132956 15109 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0817 23:56:13.133015 15109 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0817 23:56:13.133023 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.133029 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.133033 15109 net.cpp:194] Memory required for data: 30269560
I0817 23:56:13.133038 15109 layer_factory.hpp:77] Creating layer Convolution11
I0817 23:56:13.133052 15109 net.cpp:128] Creating Layer Convolution11
I0817 23:56:13.133059 15109 net.cpp:558] Convolution11 <- Eltwise4_ReLU9_0_split_0
I0817 23:56:13.133066 15109 net.cpp:522] Convolution11 -> Convolution11
I0817 23:56:13.139940 15109 net.cpp:172] Setting up Convolution11
I0817 23:56:13.139966 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.139971 15109 net.cpp:194] Memory required for data: 30597240
I0817 23:56:13.139981 15109 layer_factory.hpp:77] Creating layer BatchNorm11
I0817 23:56:13.139991 15109 net.cpp:128] Creating Layer BatchNorm11
I0817 23:56:13.139997 15109 net.cpp:558] BatchNorm11 <- Convolution11
I0817 23:56:13.140002 15109 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0817 23:56:13.140287 15109 net.cpp:172] Setting up BatchNorm11
I0817 23:56:13.140300 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.140303 15109 net.cpp:194] Memory required for data: 30924920
I0817 23:56:13.140313 15109 layer_factory.hpp:77] Creating layer Scale11
I0817 23:56:13.140321 15109 net.cpp:128] Creating Layer Scale11
I0817 23:56:13.140324 15109 net.cpp:558] Scale11 <- Convolution11
I0817 23:56:13.140331 15109 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0817 23:56:13.140384 15109 layer_factory.hpp:77] Creating layer Scale11
I0817 23:56:13.140547 15109 net.cpp:172] Setting up Scale11
I0817 23:56:13.140558 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.140561 15109 net.cpp:194] Memory required for data: 31252600
I0817 23:56:13.140569 15109 layer_factory.hpp:77] Creating layer ReLU10
I0817 23:56:13.140576 15109 net.cpp:128] Creating Layer ReLU10
I0817 23:56:13.140580 15109 net.cpp:558] ReLU10 <- Convolution11
I0817 23:56:13.140589 15109 net.cpp:509] ReLU10 -> Convolution11 (in-place)
I0817 23:56:13.141713 15109 net.cpp:172] Setting up ReLU10
I0817 23:56:13.141731 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.141736 15109 net.cpp:194] Memory required for data: 31580280
I0817 23:56:13.141739 15109 layer_factory.hpp:77] Creating layer Convolution12
I0817 23:56:13.141753 15109 net.cpp:128] Creating Layer Convolution12
I0817 23:56:13.141758 15109 net.cpp:558] Convolution12 <- Convolution11
I0817 23:56:13.141768 15109 net.cpp:522] Convolution12 -> Convolution12
I0817 23:56:13.148458 15109 net.cpp:172] Setting up Convolution12
I0817 23:56:13.148484 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.148489 15109 net.cpp:194] Memory required for data: 31907960
I0817 23:56:13.148499 15109 layer_factory.hpp:77] Creating layer BatchNorm12
I0817 23:56:13.148510 15109 net.cpp:128] Creating Layer BatchNorm12
I0817 23:56:13.148519 15109 net.cpp:558] BatchNorm12 <- Convolution12
I0817 23:56:13.148530 15109 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0817 23:56:13.148809 15109 net.cpp:172] Setting up BatchNorm12
I0817 23:56:13.148821 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.148855 15109 net.cpp:194] Memory required for data: 32235640
I0817 23:56:13.148867 15109 layer_factory.hpp:77] Creating layer Scale12
I0817 23:56:13.148877 15109 net.cpp:128] Creating Layer Scale12
I0817 23:56:13.148882 15109 net.cpp:558] Scale12 <- Convolution12
I0817 23:56:13.148887 15109 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0817 23:56:13.148941 15109 layer_factory.hpp:77] Creating layer Scale12
I0817 23:56:13.149104 15109 net.cpp:172] Setting up Scale12
I0817 23:56:13.149116 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.149121 15109 net.cpp:194] Memory required for data: 32563320
I0817 23:56:13.149127 15109 layer_factory.hpp:77] Creating layer Eltwise5
I0817 23:56:13.149137 15109 net.cpp:128] Creating Layer Eltwise5
I0817 23:56:13.149142 15109 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0817 23:56:13.149147 15109 net.cpp:558] Eltwise5 <- Convolution12
I0817 23:56:13.149157 15109 net.cpp:522] Eltwise5 -> Eltwise5
I0817 23:56:13.149180 15109 net.cpp:172] Setting up Eltwise5
I0817 23:56:13.149188 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.149191 15109 net.cpp:194] Memory required for data: 32891000
I0817 23:56:13.149196 15109 layer_factory.hpp:77] Creating layer ReLU11
I0817 23:56:13.149202 15109 net.cpp:128] Creating Layer ReLU11
I0817 23:56:13.149207 15109 net.cpp:558] ReLU11 <- Eltwise5
I0817 23:56:13.149215 15109 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0817 23:56:13.150522 15109 net.cpp:172] Setting up ReLU11
I0817 23:56:13.150539 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.150544 15109 net.cpp:194] Memory required for data: 33218680
I0817 23:56:13.150549 15109 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0817 23:56:13.150557 15109 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0817 23:56:13.150562 15109 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0817 23:56:13.150569 15109 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0817 23:56:13.150583 15109 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0817 23:56:13.150641 15109 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0817 23:56:13.150650 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.150655 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.150660 15109 net.cpp:194] Memory required for data: 33874040
I0817 23:56:13.150665 15109 layer_factory.hpp:77] Creating layer Convolution13
I0817 23:56:13.150677 15109 net.cpp:128] Creating Layer Convolution13
I0817 23:56:13.150682 15109 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_0
I0817 23:56:13.150692 15109 net.cpp:522] Convolution13 -> Convolution13
I0817 23:56:13.157315 15109 net.cpp:172] Setting up Convolution13
I0817 23:56:13.157341 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.157344 15109 net.cpp:194] Memory required for data: 34201720
I0817 23:56:13.157356 15109 layer_factory.hpp:77] Creating layer BatchNorm13
I0817 23:56:13.157364 15109 net.cpp:128] Creating Layer BatchNorm13
I0817 23:56:13.157369 15109 net.cpp:558] BatchNorm13 <- Convolution13
I0817 23:56:13.157378 15109 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0817 23:56:13.157663 15109 net.cpp:172] Setting up BatchNorm13
I0817 23:56:13.157675 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.157680 15109 net.cpp:194] Memory required for data: 34529400
I0817 23:56:13.157690 15109 layer_factory.hpp:77] Creating layer Scale13
I0817 23:56:13.157696 15109 net.cpp:128] Creating Layer Scale13
I0817 23:56:13.157701 15109 net.cpp:558] Scale13 <- Convolution13
I0817 23:56:13.157709 15109 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0817 23:56:13.157759 15109 layer_factory.hpp:77] Creating layer Scale13
I0817 23:56:13.157927 15109 net.cpp:172] Setting up Scale13
I0817 23:56:13.157938 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.157943 15109 net.cpp:194] Memory required for data: 34857080
I0817 23:56:13.157950 15109 layer_factory.hpp:77] Creating layer ReLU12
I0817 23:56:13.157974 15109 net.cpp:128] Creating Layer ReLU12
I0817 23:56:13.157979 15109 net.cpp:558] ReLU12 <- Convolution13
I0817 23:56:13.157984 15109 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0817 23:56:13.159380 15109 net.cpp:172] Setting up ReLU12
I0817 23:56:13.159406 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.159411 15109 net.cpp:194] Memory required for data: 35184760
I0817 23:56:13.159416 15109 layer_factory.hpp:77] Creating layer Convolution14
I0817 23:56:13.159440 15109 net.cpp:128] Creating Layer Convolution14
I0817 23:56:13.159446 15109 net.cpp:558] Convolution14 <- Convolution13
I0817 23:56:13.159454 15109 net.cpp:522] Convolution14 -> Convolution14
I0817 23:56:13.166160 15109 net.cpp:172] Setting up Convolution14
I0817 23:56:13.166187 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.166191 15109 net.cpp:194] Memory required for data: 35512440
I0817 23:56:13.166201 15109 layer_factory.hpp:77] Creating layer BatchNorm14
I0817 23:56:13.166213 15109 net.cpp:128] Creating Layer BatchNorm14
I0817 23:56:13.166218 15109 net.cpp:558] BatchNorm14 <- Convolution14
I0817 23:56:13.166224 15109 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0817 23:56:13.166512 15109 net.cpp:172] Setting up BatchNorm14
I0817 23:56:13.166524 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.166529 15109 net.cpp:194] Memory required for data: 35840120
I0817 23:56:13.166539 15109 layer_factory.hpp:77] Creating layer Scale14
I0817 23:56:13.166546 15109 net.cpp:128] Creating Layer Scale14
I0817 23:56:13.166550 15109 net.cpp:558] Scale14 <- Convolution14
I0817 23:56:13.166558 15109 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0817 23:56:13.166610 15109 layer_factory.hpp:77] Creating layer Scale14
I0817 23:56:13.166784 15109 net.cpp:172] Setting up Scale14
I0817 23:56:13.166795 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.166800 15109 net.cpp:194] Memory required for data: 36167800
I0817 23:56:13.166807 15109 layer_factory.hpp:77] Creating layer Eltwise6
I0817 23:56:13.166815 15109 net.cpp:128] Creating Layer Eltwise6
I0817 23:56:13.166821 15109 net.cpp:558] Eltwise6 <- Eltwise5_ReLU11_0_split_1
I0817 23:56:13.166826 15109 net.cpp:558] Eltwise6 <- Convolution14
I0817 23:56:13.166834 15109 net.cpp:522] Eltwise6 -> Eltwise6
I0817 23:56:13.166858 15109 net.cpp:172] Setting up Eltwise6
I0817 23:56:13.166865 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.166869 15109 net.cpp:194] Memory required for data: 36495480
I0817 23:56:13.166873 15109 layer_factory.hpp:77] Creating layer ReLU13
I0817 23:56:13.166883 15109 net.cpp:128] Creating Layer ReLU13
I0817 23:56:13.166888 15109 net.cpp:558] ReLU13 <- Eltwise6
I0817 23:56:13.166893 15109 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0817 23:56:13.168224 15109 net.cpp:172] Setting up ReLU13
I0817 23:56:13.168244 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.168248 15109 net.cpp:194] Memory required for data: 36823160
I0817 23:56:13.168253 15109 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0817 23:56:13.168264 15109 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0817 23:56:13.168270 15109 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0817 23:56:13.168277 15109 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0817 23:56:13.168284 15109 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0817 23:56:13.168344 15109 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0817 23:56:13.168351 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.168357 15109 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0817 23:56:13.168360 15109 net.cpp:194] Memory required for data: 37478520
I0817 23:56:13.168365 15109 layer_factory.hpp:77] Creating layer Convolution15
I0817 23:56:13.168377 15109 net.cpp:128] Creating Layer Convolution15
I0817 23:56:13.168382 15109 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0817 23:56:13.168395 15109 net.cpp:522] Convolution15 -> Convolution15
I0817 23:56:13.174998 15109 net.cpp:172] Setting up Convolution15
I0817 23:56:13.175038 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.175043 15109 net.cpp:194] Memory required for data: 37642360
I0817 23:56:13.175053 15109 layer_factory.hpp:77] Creating layer BatchNorm15
I0817 23:56:13.175065 15109 net.cpp:128] Creating Layer BatchNorm15
I0817 23:56:13.175071 15109 net.cpp:558] BatchNorm15 <- Convolution15
I0817 23:56:13.175083 15109 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0817 23:56:13.175375 15109 net.cpp:172] Setting up BatchNorm15
I0817 23:56:13.175387 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.175391 15109 net.cpp:194] Memory required for data: 37806200
I0817 23:56:13.175401 15109 layer_factory.hpp:77] Creating layer Scale15
I0817 23:56:13.175410 15109 net.cpp:128] Creating Layer Scale15
I0817 23:56:13.175415 15109 net.cpp:558] Scale15 <- Convolution15
I0817 23:56:13.175420 15109 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0817 23:56:13.175472 15109 layer_factory.hpp:77] Creating layer Scale15
I0817 23:56:13.175644 15109 net.cpp:172] Setting up Scale15
I0817 23:56:13.175657 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.175660 15109 net.cpp:194] Memory required for data: 37970040
I0817 23:56:13.175668 15109 layer_factory.hpp:77] Creating layer Convolution16
I0817 23:56:13.175680 15109 net.cpp:128] Creating Layer Convolution16
I0817 23:56:13.175686 15109 net.cpp:558] Convolution16 <- Eltwise6_ReLU13_0_split_1
I0817 23:56:13.175695 15109 net.cpp:522] Convolution16 -> Convolution16
I0817 23:56:13.181622 15109 net.cpp:172] Setting up Convolution16
I0817 23:56:13.181648 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.181653 15109 net.cpp:194] Memory required for data: 38133880
I0817 23:56:13.181663 15109 layer_factory.hpp:77] Creating layer BatchNorm16
I0817 23:56:13.181672 15109 net.cpp:128] Creating Layer BatchNorm16
I0817 23:56:13.181677 15109 net.cpp:558] BatchNorm16 <- Convolution16
I0817 23:56:13.181690 15109 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0817 23:56:13.181984 15109 net.cpp:172] Setting up BatchNorm16
I0817 23:56:13.181996 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.182001 15109 net.cpp:194] Memory required for data: 38297720
I0817 23:56:13.182011 15109 layer_factory.hpp:77] Creating layer Scale16
I0817 23:56:13.182018 15109 net.cpp:128] Creating Layer Scale16
I0817 23:56:13.182023 15109 net.cpp:558] Scale16 <- Convolution16
I0817 23:56:13.182030 15109 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0817 23:56:13.182080 15109 layer_factory.hpp:77] Creating layer Scale16
I0817 23:56:13.182248 15109 net.cpp:172] Setting up Scale16
I0817 23:56:13.182258 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.182262 15109 net.cpp:194] Memory required for data: 38461560
I0817 23:56:13.182271 15109 layer_factory.hpp:77] Creating layer ReLU14
I0817 23:56:13.182277 15109 net.cpp:128] Creating Layer ReLU14
I0817 23:56:13.182282 15109 net.cpp:558] ReLU14 <- Convolution16
I0817 23:56:13.182289 15109 net.cpp:509] ReLU14 -> Convolution16 (in-place)
I0817 23:56:13.183672 15109 net.cpp:172] Setting up ReLU14
I0817 23:56:13.183692 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.183696 15109 net.cpp:194] Memory required for data: 38625400
I0817 23:56:13.183701 15109 layer_factory.hpp:77] Creating layer Convolution17
I0817 23:56:13.183714 15109 net.cpp:128] Creating Layer Convolution17
I0817 23:56:13.183719 15109 net.cpp:558] Convolution17 <- Convolution16
I0817 23:56:13.183729 15109 net.cpp:522] Convolution17 -> Convolution17
I0817 23:56:13.190454 15109 net.cpp:172] Setting up Convolution17
I0817 23:56:13.190479 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.190484 15109 net.cpp:194] Memory required for data: 38789240
I0817 23:56:13.190493 15109 layer_factory.hpp:77] Creating layer BatchNorm17
I0817 23:56:13.190505 15109 net.cpp:128] Creating Layer BatchNorm17
I0817 23:56:13.190510 15109 net.cpp:558] BatchNorm17 <- Convolution17
I0817 23:56:13.190516 15109 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0817 23:56:13.190825 15109 net.cpp:172] Setting up BatchNorm17
I0817 23:56:13.190837 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.190841 15109 net.cpp:194] Memory required for data: 38953080
I0817 23:56:13.190851 15109 layer_factory.hpp:77] Creating layer Scale17
I0817 23:56:13.190861 15109 net.cpp:128] Creating Layer Scale17
I0817 23:56:13.190865 15109 net.cpp:558] Scale17 <- Convolution17
I0817 23:56:13.190871 15109 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0817 23:56:13.190924 15109 layer_factory.hpp:77] Creating layer Scale17
I0817 23:56:13.191103 15109 net.cpp:172] Setting up Scale17
I0817 23:56:13.191114 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.191119 15109 net.cpp:194] Memory required for data: 39116920
I0817 23:56:13.191128 15109 layer_factory.hpp:77] Creating layer Eltwise7
I0817 23:56:13.191136 15109 net.cpp:128] Creating Layer Eltwise7
I0817 23:56:13.191141 15109 net.cpp:558] Eltwise7 <- Convolution15
I0817 23:56:13.191146 15109 net.cpp:558] Eltwise7 <- Convolution17
I0817 23:56:13.191155 15109 net.cpp:522] Eltwise7 -> Eltwise7
I0817 23:56:13.191184 15109 net.cpp:172] Setting up Eltwise7
I0817 23:56:13.191191 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.191195 15109 net.cpp:194] Memory required for data: 39280760
I0817 23:56:13.191200 15109 layer_factory.hpp:77] Creating layer ReLU15
I0817 23:56:13.191208 15109 net.cpp:128] Creating Layer ReLU15
I0817 23:56:13.191213 15109 net.cpp:558] ReLU15 <- Eltwise7
I0817 23:56:13.191218 15109 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0817 23:56:13.192524 15109 net.cpp:172] Setting up ReLU15
I0817 23:56:13.192550 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.192554 15109 net.cpp:194] Memory required for data: 39444600
I0817 23:56:13.192559 15109 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0817 23:56:13.192571 15109 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0817 23:56:13.192576 15109 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0817 23:56:13.192584 15109 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0817 23:56:13.192600 15109 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0817 23:56:13.192657 15109 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0817 23:56:13.192665 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.192670 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.192674 15109 net.cpp:194] Memory required for data: 39772280
I0817 23:56:13.192679 15109 layer_factory.hpp:77] Creating layer Convolution18
I0817 23:56:13.192695 15109 net.cpp:128] Creating Layer Convolution18
I0817 23:56:13.192700 15109 net.cpp:558] Convolution18 <- Eltwise7_ReLU15_0_split_0
I0817 23:56:13.192708 15109 net.cpp:522] Convolution18 -> Convolution18
I0817 23:56:13.199586 15109 net.cpp:172] Setting up Convolution18
I0817 23:56:13.199615 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.199620 15109 net.cpp:194] Memory required for data: 39936120
I0817 23:56:13.199631 15109 layer_factory.hpp:77] Creating layer BatchNorm18
I0817 23:56:13.199640 15109 net.cpp:128] Creating Layer BatchNorm18
I0817 23:56:13.199645 15109 net.cpp:558] BatchNorm18 <- Convolution18
I0817 23:56:13.199658 15109 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0817 23:56:13.199962 15109 net.cpp:172] Setting up BatchNorm18
I0817 23:56:13.199975 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.199980 15109 net.cpp:194] Memory required for data: 40099960
I0817 23:56:13.199990 15109 layer_factory.hpp:77] Creating layer Scale18
I0817 23:56:13.199996 15109 net.cpp:128] Creating Layer Scale18
I0817 23:56:13.200001 15109 net.cpp:558] Scale18 <- Convolution18
I0817 23:56:13.200008 15109 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0817 23:56:13.200059 15109 layer_factory.hpp:77] Creating layer Scale18
I0817 23:56:13.200224 15109 net.cpp:172] Setting up Scale18
I0817 23:56:13.200235 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.200239 15109 net.cpp:194] Memory required for data: 40263800
I0817 23:56:13.200264 15109 layer_factory.hpp:77] Creating layer ReLU16
I0817 23:56:13.200274 15109 net.cpp:128] Creating Layer ReLU16
I0817 23:56:13.200279 15109 net.cpp:558] ReLU16 <- Convolution18
I0817 23:56:13.200284 15109 net.cpp:509] ReLU16 -> Convolution18 (in-place)
I0817 23:56:13.201362 15109 net.cpp:172] Setting up ReLU16
I0817 23:56:13.201380 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.201385 15109 net.cpp:194] Memory required for data: 40427640
I0817 23:56:13.201390 15109 layer_factory.hpp:77] Creating layer Convolution19
I0817 23:56:13.201407 15109 net.cpp:128] Creating Layer Convolution19
I0817 23:56:13.201412 15109 net.cpp:558] Convolution19 <- Convolution18
I0817 23:56:13.201419 15109 net.cpp:522] Convolution19 -> Convolution19
I0817 23:56:13.208137 15109 net.cpp:172] Setting up Convolution19
I0817 23:56:13.208163 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.208168 15109 net.cpp:194] Memory required for data: 40591480
I0817 23:56:13.208178 15109 layer_factory.hpp:77] Creating layer BatchNorm19
I0817 23:56:13.208189 15109 net.cpp:128] Creating Layer BatchNorm19
I0817 23:56:13.208194 15109 net.cpp:558] BatchNorm19 <- Convolution19
I0817 23:56:13.208207 15109 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0817 23:56:13.208498 15109 net.cpp:172] Setting up BatchNorm19
I0817 23:56:13.208511 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.208515 15109 net.cpp:194] Memory required for data: 40755320
I0817 23:56:13.208541 15109 layer_factory.hpp:77] Creating layer Scale19
I0817 23:56:13.208549 15109 net.cpp:128] Creating Layer Scale19
I0817 23:56:13.208554 15109 net.cpp:558] Scale19 <- Convolution19
I0817 23:56:13.208561 15109 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0817 23:56:13.208614 15109 layer_factory.hpp:77] Creating layer Scale19
I0817 23:56:13.208781 15109 net.cpp:172] Setting up Scale19
I0817 23:56:13.208791 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.208796 15109 net.cpp:194] Memory required for data: 40919160
I0817 23:56:13.208804 15109 layer_factory.hpp:77] Creating layer Eltwise8
I0817 23:56:13.208811 15109 net.cpp:128] Creating Layer Eltwise8
I0817 23:56:13.208817 15109 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0817 23:56:13.208822 15109 net.cpp:558] Eltwise8 <- Convolution19
I0817 23:56:13.208829 15109 net.cpp:522] Eltwise8 -> Eltwise8
I0817 23:56:13.208861 15109 net.cpp:172] Setting up Eltwise8
I0817 23:56:13.208868 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.208873 15109 net.cpp:194] Memory required for data: 41083000
I0817 23:56:13.208876 15109 layer_factory.hpp:77] Creating layer ReLU17
I0817 23:56:13.208883 15109 net.cpp:128] Creating Layer ReLU17
I0817 23:56:13.208887 15109 net.cpp:558] ReLU17 <- Eltwise8
I0817 23:56:13.208892 15109 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0817 23:56:13.210163 15109 net.cpp:172] Setting up ReLU17
I0817 23:56:13.210182 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.210186 15109 net.cpp:194] Memory required for data: 41246840
I0817 23:56:13.210191 15109 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0817 23:56:13.210197 15109 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0817 23:56:13.210202 15109 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0817 23:56:13.210211 15109 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0817 23:56:13.210219 15109 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0817 23:56:13.210274 15109 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0817 23:56:13.210283 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.210289 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.210292 15109 net.cpp:194] Memory required for data: 41574520
I0817 23:56:13.210296 15109 layer_factory.hpp:77] Creating layer Convolution20
I0817 23:56:13.210310 15109 net.cpp:128] Creating Layer Convolution20
I0817 23:56:13.210315 15109 net.cpp:558] Convolution20 <- Eltwise8_ReLU17_0_split_0
I0817 23:56:13.210325 15109 net.cpp:522] Convolution20 -> Convolution20
I0817 23:56:13.215669 15109 net.cpp:172] Setting up Convolution20
I0817 23:56:13.215695 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.215700 15109 net.cpp:194] Memory required for data: 41738360
I0817 23:56:13.215711 15109 layer_factory.hpp:77] Creating layer BatchNorm20
I0817 23:56:13.215719 15109 net.cpp:128] Creating Layer BatchNorm20
I0817 23:56:13.215724 15109 net.cpp:558] BatchNorm20 <- Convolution20
I0817 23:56:13.215734 15109 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0817 23:56:13.216045 15109 net.cpp:172] Setting up BatchNorm20
I0817 23:56:13.216058 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.216063 15109 net.cpp:194] Memory required for data: 41902200
I0817 23:56:13.216073 15109 layer_factory.hpp:77] Creating layer Scale20
I0817 23:56:13.216082 15109 net.cpp:128] Creating Layer Scale20
I0817 23:56:13.216086 15109 net.cpp:558] Scale20 <- Convolution20
I0817 23:56:13.216092 15109 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0817 23:56:13.216145 15109 layer_factory.hpp:77] Creating layer Scale20
I0817 23:56:13.216318 15109 net.cpp:172] Setting up Scale20
I0817 23:56:13.216329 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.216333 15109 net.cpp:194] Memory required for data: 42066040
I0817 23:56:13.216341 15109 layer_factory.hpp:77] Creating layer ReLU18
I0817 23:56:13.216351 15109 net.cpp:128] Creating Layer ReLU18
I0817 23:56:13.216354 15109 net.cpp:558] ReLU18 <- Convolution20
I0817 23:56:13.216361 15109 net.cpp:509] ReLU18 -> Convolution20 (in-place)
I0817 23:56:13.216609 15109 net.cpp:172] Setting up ReLU18
I0817 23:56:13.216624 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.216629 15109 net.cpp:194] Memory required for data: 42229880
I0817 23:56:13.216632 15109 layer_factory.hpp:77] Creating layer Convolution21
I0817 23:56:13.216646 15109 net.cpp:128] Creating Layer Convolution21
I0817 23:56:13.216652 15109 net.cpp:558] Convolution21 <- Convolution20
I0817 23:56:13.216660 15109 net.cpp:522] Convolution21 -> Convolution21
I0817 23:56:13.223515 15109 net.cpp:172] Setting up Convolution21
I0817 23:56:13.223567 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.223572 15109 net.cpp:194] Memory required for data: 42393720
I0817 23:56:13.223594 15109 layer_factory.hpp:77] Creating layer BatchNorm21
I0817 23:56:13.223610 15109 net.cpp:128] Creating Layer BatchNorm21
I0817 23:56:13.223618 15109 net.cpp:558] BatchNorm21 <- Convolution21
I0817 23:56:13.223634 15109 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0817 23:56:13.223942 15109 net.cpp:172] Setting up BatchNorm21
I0817 23:56:13.223953 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.223958 15109 net.cpp:194] Memory required for data: 42557560
I0817 23:56:13.223969 15109 layer_factory.hpp:77] Creating layer Scale21
I0817 23:56:13.223983 15109 net.cpp:128] Creating Layer Scale21
I0817 23:56:13.223986 15109 net.cpp:558] Scale21 <- Convolution21
I0817 23:56:13.223992 15109 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0817 23:56:13.224063 15109 layer_factory.hpp:77] Creating layer Scale21
I0817 23:56:13.224246 15109 net.cpp:172] Setting up Scale21
I0817 23:56:13.224258 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.224262 15109 net.cpp:194] Memory required for data: 42721400
I0817 23:56:13.224270 15109 layer_factory.hpp:77] Creating layer Eltwise9
I0817 23:56:13.224282 15109 net.cpp:128] Creating Layer Eltwise9
I0817 23:56:13.224289 15109 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0817 23:56:13.224295 15109 net.cpp:558] Eltwise9 <- Convolution21
I0817 23:56:13.224305 15109 net.cpp:522] Eltwise9 -> Eltwise9
I0817 23:56:13.224336 15109 net.cpp:172] Setting up Eltwise9
I0817 23:56:13.224344 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.224347 15109 net.cpp:194] Memory required for data: 42885240
I0817 23:56:13.224352 15109 layer_factory.hpp:77] Creating layer ReLU19
I0817 23:56:13.224364 15109 net.cpp:128] Creating Layer ReLU19
I0817 23:56:13.224369 15109 net.cpp:558] ReLU19 <- Eltwise9
I0817 23:56:13.224426 15109 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0817 23:56:13.225878 15109 net.cpp:172] Setting up ReLU19
I0817 23:56:13.225905 15109 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0817 23:56:13.225909 15109 net.cpp:194] Memory required for data: 43049080
I0817 23:56:13.225919 15109 layer_factory.hpp:77] Creating layer Pooling1
I0817 23:56:13.225931 15109 net.cpp:128] Creating Layer Pooling1
I0817 23:56:13.225936 15109 net.cpp:558] Pooling1 <- Eltwise9
I0817 23:56:13.225944 15109 net.cpp:522] Pooling1 -> Pooling1
I0817 23:56:13.227784 15109 net.cpp:172] Setting up Pooling1
I0817 23:56:13.227803 15109 net.cpp:186] Top shape: 10 64 1 1 (640)
I0817 23:56:13.227807 15109 net.cpp:194] Memory required for data: 43051640
I0817 23:56:13.227813 15109 layer_factory.hpp:77] Creating layer InnerProduct1
I0817 23:56:13.227824 15109 net.cpp:128] Creating Layer InnerProduct1
I0817 23:56:13.227830 15109 net.cpp:558] InnerProduct1 <- Pooling1
I0817 23:56:13.227839 15109 net.cpp:522] InnerProduct1 -> InnerProduct1
I0817 23:56:13.228029 15109 net.cpp:172] Setting up InnerProduct1
I0817 23:56:13.228046 15109 net.cpp:186] Top shape: 10 10 (100)
I0817 23:56:13.228050 15109 net.cpp:194] Memory required for data: 43052040
I0817 23:56:13.228060 15109 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0817 23:56:13.228068 15109 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0817 23:56:13.228073 15109 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0817 23:56:13.228087 15109 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0817 23:56:13.228099 15109 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0817 23:56:13.228153 15109 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0817 23:56:13.228163 15109 net.cpp:186] Top shape: 10 10 (100)
I0817 23:56:13.228168 15109 net.cpp:186] Top shape: 10 10 (100)
I0817 23:56:13.228174 15109 net.cpp:194] Memory required for data: 43052840
I0817 23:56:13.228179 15109 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0817 23:56:13.228193 15109 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0817 23:56:13.228200 15109 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0817 23:56:13.228206 15109 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0817 23:56:13.228215 15109 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0817 23:56:13.228229 15109 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0817 23:56:13.230067 15109 net.cpp:172] Setting up SoftmaxWithLoss1
I0817 23:56:13.230088 15109 net.cpp:186] Top shape: (1)
I0817 23:56:13.230093 15109 net.cpp:189]     with loss weight 1
I0817 23:56:13.230121 15109 net.cpp:194] Memory required for data: 43052844
I0817 23:56:13.230126 15109 layer_factory.hpp:77] Creating layer Accuracy1
I0817 23:56:13.230140 15109 net.cpp:128] Creating Layer Accuracy1
I0817 23:56:13.230145 15109 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0817 23:56:13.230151 15109 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0817 23:56:13.230161 15109 net.cpp:522] Accuracy1 -> Accuracy1
I0817 23:56:13.230175 15109 net.cpp:172] Setting up Accuracy1
I0817 23:56:13.230185 15109 net.cpp:186] Top shape: (1)
I0817 23:56:13.230190 15109 net.cpp:194] Memory required for data: 43052848
I0817 23:56:13.230195 15109 net.cpp:303] Accuracy1 does not need backward computation.
I0817 23:56:13.230201 15109 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0817 23:56:13.230206 15109 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0817 23:56:13.230209 15109 net.cpp:301] InnerProduct1 needs backward computation.
I0817 23:56:13.230214 15109 net.cpp:301] Pooling1 needs backward computation.
I0817 23:56:13.230218 15109 net.cpp:301] ReLU19 needs backward computation.
I0817 23:56:13.230222 15109 net.cpp:301] Eltwise9 needs backward computation.
I0817 23:56:13.230227 15109 net.cpp:301] Scale21 needs backward computation.
I0817 23:56:13.230232 15109 net.cpp:301] BatchNorm21 needs backward computation.
I0817 23:56:13.230254 15109 net.cpp:301] Convolution21 needs backward computation.
I0817 23:56:13.230260 15109 net.cpp:301] ReLU18 needs backward computation.
I0817 23:56:13.230264 15109 net.cpp:301] Scale20 needs backward computation.
I0817 23:56:13.230269 15109 net.cpp:301] BatchNorm20 needs backward computation.
I0817 23:56:13.230273 15109 net.cpp:301] Convolution20 needs backward computation.
I0817 23:56:13.230278 15109 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0817 23:56:13.230288 15109 net.cpp:301] ReLU17 needs backward computation.
I0817 23:56:13.230293 15109 net.cpp:301] Eltwise8 needs backward computation.
I0817 23:56:13.230298 15109 net.cpp:301] Scale19 needs backward computation.
I0817 23:56:13.230303 15109 net.cpp:301] BatchNorm19 needs backward computation.
I0817 23:56:13.230310 15109 net.cpp:301] Convolution19 needs backward computation.
I0817 23:56:13.230315 15109 net.cpp:301] ReLU16 needs backward computation.
I0817 23:56:13.230320 15109 net.cpp:301] Scale18 needs backward computation.
I0817 23:56:13.230325 15109 net.cpp:301] BatchNorm18 needs backward computation.
I0817 23:56:13.230329 15109 net.cpp:301] Convolution18 needs backward computation.
I0817 23:56:13.230338 15109 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0817 23:56:13.230343 15109 net.cpp:301] ReLU15 needs backward computation.
I0817 23:56:13.230348 15109 net.cpp:301] Eltwise7 needs backward computation.
I0817 23:56:13.230353 15109 net.cpp:301] Scale17 needs backward computation.
I0817 23:56:13.230362 15109 net.cpp:301] BatchNorm17 needs backward computation.
I0817 23:56:13.230366 15109 net.cpp:301] Convolution17 needs backward computation.
I0817 23:56:13.230372 15109 net.cpp:301] ReLU14 needs backward computation.
I0817 23:56:13.230376 15109 net.cpp:301] Scale16 needs backward computation.
I0817 23:56:13.230381 15109 net.cpp:301] BatchNorm16 needs backward computation.
I0817 23:56:13.230386 15109 net.cpp:301] Convolution16 needs backward computation.
I0817 23:56:13.230391 15109 net.cpp:301] Scale15 needs backward computation.
I0817 23:56:13.230399 15109 net.cpp:301] BatchNorm15 needs backward computation.
I0817 23:56:13.230404 15109 net.cpp:301] Convolution15 needs backward computation.
I0817 23:56:13.230409 15109 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0817 23:56:13.230414 15109 net.cpp:301] ReLU13 needs backward computation.
I0817 23:56:13.230422 15109 net.cpp:301] Eltwise6 needs backward computation.
I0817 23:56:13.230432 15109 net.cpp:301] Scale14 needs backward computation.
I0817 23:56:13.230439 15109 net.cpp:301] BatchNorm14 needs backward computation.
I0817 23:56:13.230443 15109 net.cpp:301] Convolution14 needs backward computation.
I0817 23:56:13.230448 15109 net.cpp:301] ReLU12 needs backward computation.
I0817 23:56:13.230453 15109 net.cpp:301] Scale13 needs backward computation.
I0817 23:56:13.230460 15109 net.cpp:301] BatchNorm13 needs backward computation.
I0817 23:56:13.230465 15109 net.cpp:301] Convolution13 needs backward computation.
I0817 23:56:13.230470 15109 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0817 23:56:13.230475 15109 net.cpp:301] ReLU11 needs backward computation.
I0817 23:56:13.230482 15109 net.cpp:301] Eltwise5 needs backward computation.
I0817 23:56:13.230489 15109 net.cpp:301] Scale12 needs backward computation.
I0817 23:56:13.230494 15109 net.cpp:301] BatchNorm12 needs backward computation.
I0817 23:56:13.230497 15109 net.cpp:301] Convolution12 needs backward computation.
I0817 23:56:13.230501 15109 net.cpp:301] ReLU10 needs backward computation.
I0817 23:56:13.230506 15109 net.cpp:301] Scale11 needs backward computation.
I0817 23:56:13.230515 15109 net.cpp:301] BatchNorm11 needs backward computation.
I0817 23:56:13.230518 15109 net.cpp:301] Convolution11 needs backward computation.
I0817 23:56:13.230525 15109 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0817 23:56:13.230530 15109 net.cpp:301] ReLU9 needs backward computation.
I0817 23:56:13.230537 15109 net.cpp:301] Eltwise4 needs backward computation.
I0817 23:56:13.230553 15109 net.cpp:301] Scale10 needs backward computation.
I0817 23:56:13.230558 15109 net.cpp:301] BatchNorm10 needs backward computation.
I0817 23:56:13.230563 15109 net.cpp:301] Convolution10 needs backward computation.
I0817 23:56:13.230568 15109 net.cpp:301] ReLU8 needs backward computation.
I0817 23:56:13.230576 15109 net.cpp:301] Scale9 needs backward computation.
I0817 23:56:13.230581 15109 net.cpp:301] BatchNorm9 needs backward computation.
I0817 23:56:13.230585 15109 net.cpp:301] Convolution9 needs backward computation.
I0817 23:56:13.230590 15109 net.cpp:301] Scale8 needs backward computation.
I0817 23:56:13.230598 15109 net.cpp:301] BatchNorm8 needs backward computation.
I0817 23:56:13.230603 15109 net.cpp:301] Convolution8 needs backward computation.
I0817 23:56:13.230608 15109 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0817 23:56:13.230618 15109 net.cpp:301] ReLU7 needs backward computation.
I0817 23:56:13.230621 15109 net.cpp:301] Eltwise3 needs backward computation.
I0817 23:56:13.230628 15109 net.cpp:301] Scale7 needs backward computation.
I0817 23:56:13.230636 15109 net.cpp:301] BatchNorm7 needs backward computation.
I0817 23:56:13.230641 15109 net.cpp:301] Convolution7 needs backward computation.
I0817 23:56:13.230645 15109 net.cpp:301] ReLU6 needs backward computation.
I0817 23:56:13.230650 15109 net.cpp:301] Scale6 needs backward computation.
I0817 23:56:13.230654 15109 net.cpp:301] BatchNorm6 needs backward computation.
I0817 23:56:13.230664 15109 net.cpp:301] Convolution6 needs backward computation.
I0817 23:56:13.230671 15109 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0817 23:56:13.230677 15109 net.cpp:301] ReLU5 needs backward computation.
I0817 23:56:13.230685 15109 net.cpp:301] Eltwise2 needs backward computation.
I0817 23:56:13.230691 15109 net.cpp:301] Scale5 needs backward computation.
I0817 23:56:13.230695 15109 net.cpp:301] BatchNorm5 needs backward computation.
I0817 23:56:13.230700 15109 net.cpp:301] Convolution5 needs backward computation.
I0817 23:56:13.230705 15109 net.cpp:301] ReLU4 needs backward computation.
I0817 23:56:13.230713 15109 net.cpp:301] Scale4 needs backward computation.
I0817 23:56:13.230718 15109 net.cpp:301] BatchNorm4 needs backward computation.
I0817 23:56:13.230722 15109 net.cpp:301] Convolution4 needs backward computation.
I0817 23:56:13.230727 15109 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0817 23:56:13.230732 15109 net.cpp:301] ReLU3 needs backward computation.
I0817 23:56:13.230741 15109 net.cpp:301] Eltwise1 needs backward computation.
I0817 23:56:13.230747 15109 net.cpp:301] Scale3 needs backward computation.
I0817 23:56:13.230752 15109 net.cpp:301] BatchNorm3 needs backward computation.
I0817 23:56:13.230756 15109 net.cpp:301] Convolution3 needs backward computation.
I0817 23:56:13.230762 15109 net.cpp:301] ReLU2 needs backward computation.
I0817 23:56:13.230769 15109 net.cpp:301] Scale2 needs backward computation.
I0817 23:56:13.230774 15109 net.cpp:301] BatchNorm2 needs backward computation.
I0817 23:56:13.230778 15109 net.cpp:301] Convolution2 needs backward computation.
I0817 23:56:13.230784 15109 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0817 23:56:13.230789 15109 net.cpp:301] ReLU1 needs backward computation.
I0817 23:56:13.230798 15109 net.cpp:301] Scale1 needs backward computation.
I0817 23:56:13.230803 15109 net.cpp:301] BatchNorm1 needs backward computation.
I0817 23:56:13.230806 15109 net.cpp:301] Convolution1 needs backward computation.
I0817 23:56:13.230813 15109 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0817 23:56:13.230818 15109 net.cpp:303] Data1 does not need backward computation.
I0817 23:56:13.230821 15109 net.cpp:348] This network produces output Accuracy1
I0817 23:56:13.230826 15109 net.cpp:348] This network produces output SoftmaxWithLoss1
I0817 23:56:13.230890 15109 net.cpp:363] Network initialization done.
I0817 23:56:13.231336 15109 solver.cpp:110] Solver scaffolding done.
I0817 23:56:13.239522 15109 caffe.cpp:313] Starting Optimization
I0817 23:56:13.239545 15109 solver.cpp:425] Solving resnet_cifar10
I0817 23:56:13.239550 15109 solver.cpp:427] Learning Rate Policy: multistep
I0817 23:56:13.242866 15109 solver.cpp:514] Iteration 0, Testing net (#0)
I0817 23:56:38.680385 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0817 23:56:38.781422 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 1
I0817 23:56:38.781481 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 87.3361 (* 1 = 87.3361 loss)
I0817 23:56:39.090987 15109 solver.cpp:357] Iteration 0 (0 iter/s, 25.8513s/100 iters), loss = 2.79571
I0817 23:56:39.091068 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 2.79571 (* 1 = 2.79571 loss)
I0817 23:56:39.091109 15109 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0817 23:57:05.802753 15109 solver.cpp:357] Iteration 100 (3.74384 iter/s, 26.7105s/100 iters), loss = 1.65913
I0817 23:57:05.802968 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.65913 (* 1 = 1.65913 loss)
I0817 23:57:05.802980 15109 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0817 23:57:32.590822 15109 solver.cpp:357] Iteration 200 (3.73318 iter/s, 26.7868s/100 iters), loss = 1.30061
I0817 23:57:32.590907 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.30061 (* 1 = 1.30061 loss)
I0817 23:57:32.590920 15109 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0817 23:57:59.166683 15109 solver.cpp:357] Iteration 300 (3.76299 iter/s, 26.5746s/100 iters), loss = 1.27323
I0817 23:57:59.166879 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.27323 (* 1 = 1.27323 loss)
I0817 23:57:59.166893 15109 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0817 23:58:22.111377 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0817 23:58:25.765223 15109 solver.cpp:357] Iteration 400 (3.75978 iter/s, 26.5973s/100 iters), loss = 1.04656
I0817 23:58:25.765306 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.04656 (* 1 = 1.04656 loss)
I0817 23:58:25.765319 15109 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0817 23:58:52.334496 15109 solver.cpp:514] Iteration 500, Testing net (#0)
I0817 23:59:17.552209 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0817 23:59:17.656888 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2272
I0817 23:59:17.656949 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.28772 (* 1 = 2.28772 loss)
I0817 23:59:17.893749 15109 solver.cpp:357] Iteration 500 (1.91835 iter/s, 52.1282s/100 iters), loss = 0.96087
I0817 23:59:17.893807 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.96087 (* 1 = 0.96087 loss)
I0817 23:59:17.893821 15109 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0817 23:59:44.416687 15109 solver.cpp:357] Iteration 600 (3.77051 iter/s, 26.5216s/100 iters), loss = 1.04442
I0817 23:59:44.416862 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.04442 (* 1 = 1.04442 loss)
I0817 23:59:44.416874 15109 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0818 00:00:11.048230 15109 solver.cpp:357] Iteration 700 (3.75513 iter/s, 26.6302s/100 iters), loss = 0.800676
I0818 00:00:11.048310 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.800676 (* 1 = 0.800676 loss)
I0818 00:00:11.048321 15109 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0818 00:00:31.743764 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:00:37.870968 15109 solver.cpp:357] Iteration 800 (3.72837 iter/s, 26.8214s/100 iters), loss = 0.931334
I0818 00:00:37.871034 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.931334 (* 1 = 0.931334 loss)
I0818 00:00:37.871044 15109 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0818 00:01:04.680387 15109 solver.cpp:357] Iteration 900 (3.72993 iter/s, 26.8102s/100 iters), loss = 0.864218
I0818 00:01:04.680627 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.864218 (* 1 = 0.864218 loss)
I0818 00:01:04.680644 15109 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0818 00:01:30.991430 15109 solver.cpp:514] Iteration 1000, Testing net (#0)
I0818 00:01:56.299516 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:01:56.408164 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.100099
I0818 00:01:56.408242 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 4.3821 (* 1 = 4.3821 loss)
I0818 00:01:56.636905 15109 solver.cpp:357] Iteration 1000 (1.92471 iter/s, 51.956s/100 iters), loss = 0.7843
I0818 00:01:56.636965 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.7843 (* 1 = 0.7843 loss)
I0818 00:01:56.636977 15109 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0818 00:02:23.260501 15109 solver.cpp:357] Iteration 1100 (3.75626 iter/s, 26.6222s/100 iters), loss = 0.657706
I0818 00:02:23.260576 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.657706 (* 1 = 0.657706 loss)
I0818 00:02:23.260586 15109 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0818 00:02:41.292824 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:02:50.043506 15109 solver.cpp:357] Iteration 1200 (3.7339 iter/s, 26.7816s/100 iters), loss = 0.700624
I0818 00:02:50.043584 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.700624 (* 1 = 0.700624 loss)
I0818 00:02:50.043596 15109 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0818 00:03:16.670207 15109 solver.cpp:357] Iteration 1300 (3.75583 iter/s, 26.6253s/100 iters), loss = 0.738528
I0818 00:03:16.670332 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.738528 (* 1 = 0.738528 loss)
I0818 00:03:16.670344 15109 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0818 00:03:43.214772 15109 solver.cpp:357] Iteration 1400 (3.76745 iter/s, 26.5431s/100 iters), loss = 0.705185
I0818 00:03:43.214843 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.705185 (* 1 = 0.705185 loss)
I0818 00:03:43.214855 15109 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0818 00:04:09.716702 15109 solver.cpp:514] Iteration 1500, Testing net (#0)
I0818 00:04:34.954077 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:04:35.059418 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.101799
I0818 00:04:35.059490 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.30009 (* 1 = 3.30009 loss)
I0818 00:04:35.289913 15109 solver.cpp:357] Iteration 1500 (1.92033 iter/s, 52.0745s/100 iters), loss = 0.664842
I0818 00:04:35.289971 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.664842 (* 1 = 0.664842 loss)
I0818 00:04:35.289988 15109 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0818 00:04:50.786274 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:05:01.837982 15109 solver.cpp:357] Iteration 1600 (3.76696 iter/s, 26.5466s/100 iters), loss = 0.713329
I0818 00:05:01.838060 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.713329 (* 1 = 0.713329 loss)
I0818 00:05:01.838073 15109 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0818 00:05:28.343295 15109 solver.cpp:357] Iteration 1700 (3.77303 iter/s, 26.5039s/100 iters), loss = 0.717301
I0818 00:05:28.343525 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.717301 (* 1 = 0.717301 loss)
I0818 00:05:28.343542 15109 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0818 00:05:55.175155 15109 solver.cpp:357] Iteration 1800 (3.72711 iter/s, 26.8304s/100 iters), loss = 0.739054
I0818 00:05:55.175233 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.739054 (* 1 = 0.739054 loss)
I0818 00:05:55.175247 15109 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0818 00:06:21.591248 15109 solver.cpp:357] Iteration 1900 (3.78578 iter/s, 26.4146s/100 iters), loss = 0.600538
I0818 00:06:21.591401 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.600538 (* 1 = 0.600538 loss)
I0818 00:06:21.591413 15109 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0818 00:06:34.821956 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:06:48.066833 15109 solver.cpp:514] Iteration 2000, Testing net (#0)
I0818 00:07:13.303186 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:07:13.348209 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.132099
I0818 00:07:13.348275 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.80744 (* 1 = 2.80744 loss)
I0818 00:07:13.525931 15109 solver.cpp:357] Iteration 2000 (1.92553 iter/s, 51.9338s/100 iters), loss = 0.605191
I0818 00:07:13.525996 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.605191 (* 1 = 0.605191 loss)
I0818 00:07:13.526013 15109 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0818 00:07:40.310920 15109 solver.cpp:357] Iteration 2100 (3.73364 iter/s, 26.7835s/100 iters), loss = 0.721013
I0818 00:07:40.311017 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.721013 (* 1 = 0.721013 loss)
I0818 00:07:40.311029 15109 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0818 00:08:06.904440 15109 solver.cpp:357] Iteration 2200 (3.76053 iter/s, 26.592s/100 iters), loss = 0.591255
I0818 00:08:06.904788 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.591255 (* 1 = 0.591255 loss)
I0818 00:08:06.904799 15109 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0818 00:08:33.473143 15109 solver.cpp:357] Iteration 2300 (3.76404 iter/s, 26.5672s/100 iters), loss = 0.506756
I0818 00:08:33.473220 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.506756 (* 1 = 0.506756 loss)
I0818 00:08:33.473232 15109 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0818 00:08:43.938488 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:09:00.224323 15109 solver.cpp:357] Iteration 2400 (3.73836 iter/s, 26.7497s/100 iters), loss = 0.525266
I0818 00:09:00.224397 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.525266 (* 1 = 0.525266 loss)
I0818 00:09:00.224409 15109 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0818 00:09:26.427927 15109 solver.cpp:514] Iteration 2500, Testing net (#0)
I0818 00:09:51.779980 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:09:51.890748 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1842
I0818 00:09:51.890799 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.71651 (* 1 = 2.71651 loss)
I0818 00:09:52.117781 15109 solver.cpp:357] Iteration 2500 (1.92705 iter/s, 51.8927s/100 iters), loss = 0.577835
I0818 00:09:52.117862 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.577835 (* 1 = 0.577835 loss)
I0818 00:09:52.117875 15109 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0818 00:10:18.589068 15109 solver.cpp:357] Iteration 2600 (3.7779 iter/s, 26.4698s/100 iters), loss = 0.789313
I0818 00:10:18.589226 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.789313 (* 1 = 0.789313 loss)
I0818 00:10:18.589239 15109 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0818 00:10:45.420130 15109 solver.cpp:357] Iteration 2700 (3.72723 iter/s, 26.8296s/100 iters), loss = 0.839859
I0818 00:10:45.420214 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.839859 (* 1 = 0.839859 loss)
I0818 00:10:45.420228 15109 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0818 00:10:53.372730 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:11:11.873762 15109 solver.cpp:357] Iteration 2800 (3.78042 iter/s, 26.4521s/100 iters), loss = 0.519282
I0818 00:11:11.873855 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.519282 (* 1 = 0.519282 loss)
I0818 00:11:11.873869 15109 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0818 00:11:38.492053 15109 solver.cpp:357] Iteration 2900 (3.75703 iter/s, 26.6168s/100 iters), loss = 0.569392
I0818 00:11:38.492300 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.569392 (* 1 = 0.569392 loss)
I0818 00:11:38.492331 15109 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0818 00:12:04.933267 15109 solver.cpp:514] Iteration 3000, Testing net (#0)
I0818 00:12:30.503129 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:12:30.531589 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1695
I0818 00:12:30.531628 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.79574 (* 1 = 2.79574 loss)
I0818 00:12:30.676286 15109 solver.cpp:357] Iteration 3000 (1.91632 iter/s, 52.1834s/100 iters), loss = 0.612957
I0818 00:12:30.676367 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.612957 (* 1 = 0.612957 loss)
I0818 00:12:30.676379 15109 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0818 00:12:57.341205 15109 solver.cpp:357] Iteration 3100 (3.75023 iter/s, 26.6651s/100 iters), loss = 0.621944
I0818 00:12:57.341279 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.621944 (* 1 = 0.621944 loss)
I0818 00:12:57.341292 15109 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0818 00:13:02.617544 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:13:23.899721 15109 solver.cpp:357] Iteration 3200 (3.76548 iter/s, 26.557s/100 iters), loss = 0.548961
I0818 00:13:23.899794 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.548961 (* 1 = 0.548961 loss)
I0818 00:13:23.899806 15109 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0818 00:13:50.508929 15109 solver.cpp:357] Iteration 3300 (3.75831 iter/s, 26.6077s/100 iters), loss = 0.518673
I0818 00:13:50.509083 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.518673 (* 1 = 0.518673 loss)
I0818 00:13:50.509095 15109 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0818 00:14:17.206881 15109 solver.cpp:357] Iteration 3400 (3.74555 iter/s, 26.6983s/100 iters), loss = 0.466286
I0818 00:14:17.206964 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.466286 (* 1 = 0.466286 loss)
I0818 00:14:17.206974 15109 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0818 00:14:43.391983 15109 solver.cpp:514] Iteration 3500, Testing net (#0)
I0818 00:15:08.871294 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:15:08.973050 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3504
I0818 00:15:08.973107 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.02042 (* 1 = 2.02042 loss)
I0818 00:15:09.212134 15109 solver.cpp:357] Iteration 3500 (1.92291 iter/s, 52.0044s/100 iters), loss = 0.633551
I0818 00:15:09.212222 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.633551 (* 1 = 0.633551 loss)
I0818 00:15:09.212236 15109 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0818 00:15:12.170541 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:15:35.756757 15109 solver.cpp:357] Iteration 3600 (3.76746 iter/s, 26.5431s/100 iters), loss = 0.580495
I0818 00:15:35.756892 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.580495 (* 1 = 0.580495 loss)
I0818 00:15:35.756906 15109 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0818 00:16:02.540894 15109 solver.cpp:357] Iteration 3700 (3.73377 iter/s, 26.7826s/100 iters), loss = 0.444749
I0818 00:16:02.540980 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444749 (* 1 = 0.444749 loss)
I0818 00:16:02.540993 15109 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0818 00:16:29.076180 15109 solver.cpp:357] Iteration 3800 (3.76879 iter/s, 26.5337s/100 iters), loss = 0.441915
I0818 00:16:29.076342 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441915 (* 1 = 0.441915 loss)
I0818 00:16:29.076356 15109 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0818 00:16:55.629432 15109 solver.cpp:357] Iteration 3900 (3.76624 iter/s, 26.5517s/100 iters), loss = 0.584164
I0818 00:16:55.629514 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.584164 (* 1 = 0.584164 loss)
I0818 00:16:55.629528 15109 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0818 00:16:56.258432 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:17:22.054131 15109 solver.cpp:514] Iteration 4000, Testing net (#0)
I0818 00:17:47.386984 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:17:47.486969 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5298
I0818 00:17:47.487020 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30905 (* 1 = 1.30905 loss)
I0818 00:17:47.709566 15109 solver.cpp:357] Iteration 4000 (1.92015 iter/s, 52.0792s/100 iters), loss = 0.630422
I0818 00:17:47.709610 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.630422 (* 1 = 0.630422 loss)
I0818 00:17:47.709625 15109 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0818 00:18:14.293676 15109 solver.cpp:357] Iteration 4100 (3.76157 iter/s, 26.5847s/100 iters), loss = 0.49255
I0818 00:18:14.293841 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.49255 (* 1 = 0.49255 loss)
I0818 00:18:14.293856 15109 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0818 00:18:40.850023 15109 solver.cpp:357] Iteration 4200 (3.76579 iter/s, 26.5548s/100 iters), loss = 0.54379
I0818 00:18:40.850101 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.54379 (* 1 = 0.54379 loss)
I0818 00:18:40.850113 15109 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0818 00:19:05.404359 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:19:07.544417 15109 solver.cpp:357] Iteration 4300 (3.74632 iter/s, 26.6928s/100 iters), loss = 0.578837
I0818 00:19:07.544482 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.578837 (* 1 = 0.578837 loss)
I0818 00:19:07.544492 15109 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0818 00:19:34.127574 15109 solver.cpp:357] Iteration 4400 (3.76171 iter/s, 26.5837s/100 iters), loss = 0.550018
I0818 00:19:34.127696 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.550018 (* 1 = 0.550018 loss)
I0818 00:19:34.127710 15109 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0818 00:20:00.940938 15109 solver.cpp:514] Iteration 4500, Testing net (#0)
I0818 00:20:26.740289 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:20:26.838650 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5745
I0818 00:20:26.838716 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.31624 (* 1 = 1.31624 loss)
I0818 00:20:27.064266 15109 solver.cpp:357] Iteration 4500 (1.88908 iter/s, 52.9358s/100 iters), loss = 0.657629
I0818 00:20:27.064359 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.657629 (* 1 = 0.657629 loss)
I0818 00:20:27.064373 15109 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0818 00:20:54.119374 15109 solver.cpp:357] Iteration 4600 (3.69637 iter/s, 27.0536s/100 iters), loss = 0.380547
I0818 00:20:54.119554 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380547 (* 1 = 0.380547 loss)
I0818 00:20:54.119567 15109 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0818 00:21:16.623309 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:21:21.149607 15109 solver.cpp:357] Iteration 4700 (3.69977 iter/s, 27.0287s/100 iters), loss = 0.423625
I0818 00:21:21.149693 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423625 (* 1 = 0.423625 loss)
I0818 00:21:21.149706 15109 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0818 00:21:48.177108 15109 solver.cpp:357] Iteration 4800 (3.70014 iter/s, 27.026s/100 iters), loss = 0.462132
I0818 00:21:48.177304 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462132 (* 1 = 0.462132 loss)
I0818 00:21:48.177320 15109 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0818 00:22:15.284704 15109 solver.cpp:357] Iteration 4900 (3.68922 iter/s, 27.106s/100 iters), loss = 0.54298
I0818 00:22:15.284801 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.54298 (* 1 = 0.54298 loss)
I0818 00:22:15.284816 15109 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0818 00:22:42.027812 15109 solver.cpp:514] Iteration 5000, Testing net (#0)
I0818 00:23:07.535143 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:23:07.647233 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3202
I0818 00:23:07.647285 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.20936 (* 1 = 2.20936 loss)
I0818 00:23:07.887472 15109 solver.cpp:357] Iteration 5000 (1.90107 iter/s, 52.6018s/100 iters), loss = 0.375844
I0818 00:23:07.887555 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375844 (* 1 = 0.375844 loss)
I0818 00:23:07.887568 15109 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0818 00:23:27.707631 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:23:34.451622 15109 solver.cpp:357] Iteration 5100 (3.76469 iter/s, 26.5626s/100 iters), loss = 0.49153
I0818 00:23:34.451706 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.49153 (* 1 = 0.49153 loss)
I0818 00:23:34.451719 15109 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0818 00:24:01.336571 15109 solver.cpp:357] Iteration 5200 (3.71976 iter/s, 26.8834s/100 iters), loss = 0.730092
I0818 00:24:01.336735 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.730092 (* 1 = 0.730092 loss)
I0818 00:24:01.336747 15109 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0818 00:24:27.946319 15109 solver.cpp:357] Iteration 5300 (3.75825 iter/s, 26.6082s/100 iters), loss = 0.547412
I0818 00:24:27.946466 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.547412 (* 1 = 0.547412 loss)
I0818 00:24:27.946483 15109 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0818 00:24:54.541517 15109 solver.cpp:357] Iteration 5400 (3.7603 iter/s, 26.5936s/100 iters), loss = 0.363762
I0818 00:24:54.541678 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363762 (* 1 = 0.363762 loss)
I0818 00:24:54.541692 15109 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0818 00:25:11.842170 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:25:21.132041 15109 solver.cpp:514] Iteration 5500, Testing net (#0)
I0818 00:25:46.359050 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:25:46.472136 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6975
I0818 00:25:46.472189 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.884648 (* 1 = 0.884648 loss)
I0818 00:25:46.699041 15109 solver.cpp:357] Iteration 5500 (1.9173 iter/s, 52.1566s/100 iters), loss = 0.420407
I0818 00:25:46.699118 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.420407 (* 1 = 0.420407 loss)
I0818 00:25:46.699132 15109 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0818 00:26:13.520692 15109 solver.cpp:357] Iteration 5600 (3.72852 iter/s, 26.8203s/100 iters), loss = 0.439194
I0818 00:26:13.520772 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439194 (* 1 = 0.439194 loss)
I0818 00:26:13.520784 15109 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0818 00:26:40.063794 15109 solver.cpp:357] Iteration 5700 (3.76759 iter/s, 26.5422s/100 iters), loss = 0.4822
I0818 00:26:40.063977 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4822 (* 1 = 0.4822 loss)
I0818 00:26:40.063992 15109 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0818 00:27:06.545167 15109 solver.cpp:357] Iteration 5800 (3.77638 iter/s, 26.4804s/100 iters), loss = 0.348842
I0818 00:27:06.545253 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348842 (* 1 = 0.348842 loss)
I0818 00:27:06.545267 15109 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0818 00:27:21.319494 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:27:33.293285 15109 solver.cpp:357] Iteration 5900 (3.73872 iter/s, 26.7471s/100 iters), loss = 0.649414
I0818 00:27:33.293375 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.649414 (* 1 = 0.649414 loss)
I0818 00:27:33.293388 15109 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0818 00:27:59.585929 15109 solver.cpp:514] Iteration 6000, Testing net (#0)
I0818 00:28:25.100097 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:28:25.199972 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4417
I0818 00:28:25.200033 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.90917 (* 1 = 1.90917 loss)
I0818 00:28:25.440229 15109 solver.cpp:357] Iteration 6000 (1.91765 iter/s, 52.1471s/100 iters), loss = 0.404788
I0818 00:28:25.440289 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404788 (* 1 = 0.404788 loss)
I0818 00:28:25.440301 15109 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0818 00:28:51.976258 15109 solver.cpp:357] Iteration 6100 (3.76861 iter/s, 26.535s/100 iters), loss = 0.428406
I0818 00:28:51.976501 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428406 (* 1 = 0.428406 loss)
I0818 00:28:51.976516 15109 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0818 00:29:18.602794 15109 solver.cpp:357] Iteration 6200 (3.75581 iter/s, 26.6255s/100 iters), loss = 0.417273
I0818 00:29:18.602877 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417273 (* 1 = 0.417273 loss)
I0818 00:29:18.602891 15109 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0818 00:29:30.749781 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:29:45.390238 15109 solver.cpp:357] Iteration 6300 (3.73299 iter/s, 26.7881s/100 iters), loss = 0.4605
I0818 00:29:45.390324 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4605 (* 1 = 0.4605 loss)
I0818 00:29:45.390337 15109 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0818 00:30:11.867600 15109 solver.cpp:357] Iteration 6400 (3.77697 iter/s, 26.4763s/100 iters), loss = 0.500179
I0818 00:30:11.867769 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.500179 (* 1 = 0.500179 loss)
I0818 00:30:11.867780 15109 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0818 00:30:38.308027 15109 solver.cpp:514] Iteration 6500, Testing net (#0)
I0818 00:31:03.523336 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:31:03.570390 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5384
I0818 00:31:03.570453 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.70007 (* 1 = 1.70007 loss)
I0818 00:31:03.756106 15109 solver.cpp:357] Iteration 6500 (1.92721 iter/s, 51.8883s/100 iters), loss = 0.419548
I0818 00:31:03.756165 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419548 (* 1 = 0.419548 loss)
I0818 00:31:03.756178 15109 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0818 00:31:30.571941 15109 solver.cpp:357] Iteration 6600 (3.72931 iter/s, 26.8146s/100 iters), loss = 0.487438
I0818 00:31:30.572021 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487438 (* 1 = 0.487438 loss)
I0818 00:31:30.572033 15109 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0818 00:31:40.229254 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:31:57.170503 15109 solver.cpp:357] Iteration 6700 (3.75977 iter/s, 26.5974s/100 iters), loss = 0.486514
I0818 00:31:57.170577 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486514 (* 1 = 0.486514 loss)
I0818 00:31:57.170586 15109 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0818 00:32:23.744186 15109 solver.cpp:357] Iteration 6800 (3.7632 iter/s, 26.5731s/100 iters), loss = 0.380272
I0818 00:32:23.744320 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380272 (* 1 = 0.380272 loss)
I0818 00:32:23.744334 15109 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0818 00:32:50.559489 15109 solver.cpp:357] Iteration 6900 (3.7291 iter/s, 26.8161s/100 iters), loss = 0.479939
I0818 00:32:50.559574 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.479939 (* 1 = 0.479939 loss)
I0818 00:32:50.559587 15109 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0818 00:33:16.786394 15109 solver.cpp:514] Iteration 7000, Testing net (#0)
I0818 00:33:42.172842 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:33:42.278353 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6119
I0818 00:33:42.278398 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.3063 (* 1 = 1.3063 loss)
I0818 00:33:42.510159 15109 solver.cpp:357] Iteration 7000 (1.92492 iter/s, 51.9503s/100 iters), loss = 0.435522
I0818 00:33:42.510232 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435522 (* 1 = 0.435522 loss)
I0818 00:33:42.510246 15109 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0818 00:33:49.680441 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:34:09.164006 15109 solver.cpp:357] Iteration 7100 (3.75198 iter/s, 26.6526s/100 iters), loss = 0.493258
I0818 00:34:09.164101 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.493258 (* 1 = 0.493258 loss)
I0818 00:34:09.164115 15109 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0818 00:34:35.713555 15109 solver.cpp:357] Iteration 7200 (3.76673 iter/s, 26.5483s/100 iters), loss = 0.616667
I0818 00:34:35.713718 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.616667 (* 1 = 0.616667 loss)
I0818 00:34:35.713729 15109 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0818 00:35:02.419239 15109 solver.cpp:357] Iteration 7300 (3.74446 iter/s, 26.7061s/100 iters), loss = 0.469327
I0818 00:35:02.419311 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.469327 (* 1 = 0.469327 loss)
I0818 00:35:02.419323 15109 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0818 00:35:29.015467 15109 solver.cpp:357] Iteration 7400 (3.76012 iter/s, 26.5949s/100 iters), loss = 0.555081
I0818 00:35:29.015627 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.555081 (* 1 = 0.555081 loss)
I0818 00:35:29.015642 15109 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0818 00:35:33.736618 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:35:55.405830 15109 solver.cpp:514] Iteration 7500, Testing net (#0)
I0818 00:36:20.563374 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:36:20.644445 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6457
I0818 00:36:20.644488 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02896 (* 1 = 1.02896 loss)
I0818 00:36:20.779872 15109 solver.cpp:357] Iteration 7500 (1.93185 iter/s, 51.764s/100 iters), loss = 0.437474
I0818 00:36:20.779945 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437474 (* 1 = 0.437474 loss)
I0818 00:36:20.779958 15109 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0818 00:36:47.501279 15109 solver.cpp:357] Iteration 7600 (3.74225 iter/s, 26.7219s/100 iters), loss = 0.41365
I0818 00:36:47.501359 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41365 (* 1 = 0.41365 loss)
I0818 00:36:47.501371 15109 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0818 00:37:14.011862 15109 solver.cpp:357] Iteration 7700 (3.77227 iter/s, 26.5092s/100 iters), loss = 0.479934
I0818 00:37:14.012007 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.479934 (* 1 = 0.479934 loss)
I0818 00:37:14.012019 15109 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0818 00:37:40.618129 15109 solver.cpp:357] Iteration 7800 (3.7587 iter/s, 26.6049s/100 iters), loss = 0.503988
I0818 00:37:40.618211 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503988 (* 1 = 0.503988 loss)
I0818 00:37:40.618223 15109 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0818 00:37:42.919430 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:38:07.341249 15109 solver.cpp:357] Iteration 7900 (3.74202 iter/s, 26.7235s/100 iters), loss = 0.360927
I0818 00:38:07.341392 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360927 (* 1 = 0.360927 loss)
I0818 00:38:07.341404 15109 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0818 00:38:33.661262 15109 solver.cpp:514] Iteration 8000, Testing net (#0)
I0818 00:38:59.064066 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:38:59.177073 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.639
I0818 00:38:59.177125 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.17968 (* 1 = 1.17968 loss)
I0818 00:38:59.418659 15109 solver.cpp:357] Iteration 8000 (1.92024 iter/s, 52.0768s/100 iters), loss = 0.44368
I0818 00:38:59.418741 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44368 (* 1 = 0.44368 loss)
I0818 00:38:59.418754 15109 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0818 00:39:26.105583 15109 solver.cpp:357] Iteration 8100 (3.74735 iter/s, 26.6855s/100 iters), loss = 0.432952
I0818 00:39:26.105659 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432952 (* 1 = 0.432952 loss)
I0818 00:39:26.105670 15109 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0818 00:39:52.628010 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:39:52.860056 15109 solver.cpp:357] Iteration 8200 (3.73789 iter/s, 26.7531s/100 iters), loss = 0.480306
I0818 00:39:52.860111 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480306 (* 1 = 0.480306 loss)
I0818 00:39:52.860123 15109 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0818 00:40:19.460896 15109 solver.cpp:357] Iteration 8300 (3.75948 iter/s, 26.5994s/100 iters), loss = 0.552972
I0818 00:40:19.460981 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.552972 (* 1 = 0.552972 loss)
I0818 00:40:19.460994 15109 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0818 00:40:46.006897 15109 solver.cpp:357] Iteration 8400 (3.76725 iter/s, 26.5446s/100 iters), loss = 0.442066
I0818 00:40:46.007076 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442066 (* 1 = 0.442066 loss)
I0818 00:40:46.007091 15109 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0818 00:41:12.558394 15109 solver.cpp:514] Iteration 8500, Testing net (#0)
I0818 00:41:37.783582 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:41:37.830030 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6118
I0818 00:41:37.830094 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32735 (* 1 = 1.32735 loss)
I0818 00:41:38.015345 15109 solver.cpp:357] Iteration 8500 (1.92279 iter/s, 52.0078s/100 iters), loss = 0.440851
I0818 00:41:38.015410 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440851 (* 1 = 0.440851 loss)
I0818 00:41:38.015424 15109 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0818 00:42:01.957487 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:42:04.765142 15109 solver.cpp:357] Iteration 8600 (3.73854 iter/s, 26.7484s/100 iters), loss = 0.453631
I0818 00:42:04.765220 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453631 (* 1 = 0.453631 loss)
I0818 00:42:04.765233 15109 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0818 00:42:31.391214 15109 solver.cpp:357] Iteration 8700 (3.75592 iter/s, 26.6246s/100 iters), loss = 0.412124
I0818 00:42:31.391387 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412124 (* 1 = 0.412124 loss)
I0818 00:42:31.391400 15109 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0818 00:42:57.870232 15109 solver.cpp:357] Iteration 8800 (3.77678 iter/s, 26.4776s/100 iters), loss = 0.432222
I0818 00:42:57.870312 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432222 (* 1 = 0.432222 loss)
I0818 00:42:57.870326 15109 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0818 00:43:24.743760 15109 solver.cpp:357] Iteration 8900 (3.72133 iter/s, 26.8721s/100 iters), loss = 0.498086
I0818 00:43:24.743932 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.498086 (* 1 = 0.498086 loss)
I0818 00:43:24.743943 15109 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0818 00:43:46.039731 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:43:50.998286 15109 solver.cpp:514] Iteration 9000, Testing net (#0)
I0818 00:44:16.588214 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:44:16.692945 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.724499
I0818 00:44:16.693017 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.868064 (* 1 = 0.868064 loss)
I0818 00:44:16.922677 15109 solver.cpp:357] Iteration 9000 (1.91651 iter/s, 52.1782s/100 iters), loss = 0.268442
I0818 00:44:16.922742 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.268442 (* 1 = 0.268442 loss)
I0818 00:44:16.922757 15109 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0818 00:44:43.495710 15109 solver.cpp:357] Iteration 9100 (3.76342 iter/s, 26.5716s/100 iters), loss = 0.469681
I0818 00:44:43.495786 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.469681 (* 1 = 0.469681 loss)
I0818 00:44:43.495800 15109 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0818 00:45:10.136557 15109 solver.cpp:357] Iteration 9200 (3.75384 iter/s, 26.6394s/100 iters), loss = 0.486527
I0818 00:45:10.136771 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486527 (* 1 = 0.486527 loss)
I0818 00:45:10.136783 15109 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0818 00:45:37.022277 15109 solver.cpp:357] Iteration 9300 (3.71942 iter/s, 26.8859s/100 iters), loss = 0.409367
I0818 00:45:37.022354 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409367 (* 1 = 0.409367 loss)
I0818 00:45:37.022367 15109 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0818 00:45:55.698366 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:46:03.566601 15109 solver.cpp:357] Iteration 9400 (3.76748 iter/s, 26.5429s/100 iters), loss = 0.62463
I0818 00:46:03.566680 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.62463 (* 1 = 0.62463 loss)
I0818 00:46:03.566694 15109 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0818 00:46:29.839196 15109 solver.cpp:514] Iteration 9500, Testing net (#0)
I0818 00:46:55.047482 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:46:55.115967 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5531
I0818 00:46:55.116019 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.54089 (* 1 = 1.54089 loss)
I0818 00:46:55.351485 15109 solver.cpp:357] Iteration 9500 (1.93109 iter/s, 51.7841s/100 iters), loss = 0.509118
I0818 00:46:55.351536 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509118 (* 1 = 0.509118 loss)
I0818 00:46:55.351552 15109 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0818 00:47:22.143308 15109 solver.cpp:357] Iteration 9600 (3.73268 iter/s, 26.7904s/100 iters), loss = 0.544324
I0818 00:47:22.143463 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544324 (* 1 = 0.544324 loss)
I0818 00:47:22.143481 15109 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0818 00:47:48.714143 15109 solver.cpp:357] Iteration 9700 (3.76373 iter/s, 26.5694s/100 iters), loss = 0.456348
I0818 00:47:48.714213 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456348 (* 1 = 0.456348 loss)
I0818 00:47:48.714226 15109 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0818 00:48:05.089959 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:48:15.219359 15109 solver.cpp:357] Iteration 9800 (3.77275 iter/s, 26.5058s/100 iters), loss = 0.487853
I0818 00:48:15.219455 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487853 (* 1 = 0.487853 loss)
I0818 00:48:15.219470 15109 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0818 00:48:41.996503 15109 solver.cpp:357] Iteration 9900 (3.73473 iter/s, 26.7757s/100 iters), loss = 0.392398
I0818 00:48:41.996706 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392398 (* 1 = 0.392398 loss)
I0818 00:48:41.996718 15109 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0818 00:49:08.348175 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.caffemodel
I0818 00:49:08.367080 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.solverstate
I0818 00:49:08.370131 15109 solver.cpp:514] Iteration 10000, Testing net (#0)
I0818 00:49:33.827884 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:49:33.922945 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.654099
I0818 00:49:33.923019 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05405 (* 1 = 1.05405 loss)
I0818 00:49:34.161098 15109 solver.cpp:357] Iteration 10000 (1.91704 iter/s, 52.1638s/100 iters), loss = 0.436173
I0818 00:49:34.161152 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.436173 (* 1 = 0.436173 loss)
I0818 00:49:34.161165 15109 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0818 00:50:00.738453 15109 solver.cpp:357] Iteration 10100 (3.76281 iter/s, 26.5759s/100 iters), loss = 0.428668
I0818 00:50:00.738528 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428668 (* 1 = 0.428668 loss)
I0818 00:50:00.738539 15109 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0818 00:50:14.714161 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:50:27.316634 15109 solver.cpp:357] Iteration 10200 (3.76262 iter/s, 26.5772s/100 iters), loss = 0.388676
I0818 00:50:27.316715 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388676 (* 1 = 0.388676 loss)
I0818 00:50:27.316727 15109 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0818 00:50:54.103772 15109 solver.cpp:357] Iteration 10300 (3.73309 iter/s, 26.7874s/100 iters), loss = 0.404183
I0818 00:50:54.103955 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404183 (* 1 = 0.404183 loss)
I0818 00:50:54.103969 15109 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0818 00:51:20.669798 15109 solver.cpp:357] Iteration 10400 (3.76441 iter/s, 26.5646s/100 iters), loss = 0.421745
I0818 00:51:20.669884 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421745 (* 1 = 0.421745 loss)
I0818 00:51:20.669896 15109 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0818 00:51:47.051156 15109 solver.cpp:514] Iteration 10500, Testing net (#0)
I0818 00:52:12.495780 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:52:12.606360 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6822
I0818 00:52:12.606410 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07933 (* 1 = 1.07933 loss)
I0818 00:52:12.842722 15109 solver.cpp:357] Iteration 10500 (1.91673 iter/s, 52.1721s/100 iters), loss = 0.614548
I0818 00:52:12.842793 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.614548 (* 1 = 0.614548 loss)
I0818 00:52:12.842804 15109 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0818 00:52:24.177234 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:52:39.548785 15109 solver.cpp:357] Iteration 10600 (3.74438 iter/s, 26.7067s/100 iters), loss = 0.45373
I0818 00:52:39.548861 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45373 (* 1 = 0.45373 loss)
I0818 00:52:39.548871 15109 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0818 00:53:06.286070 15109 solver.cpp:357] Iteration 10700 (3.74001 iter/s, 26.7379s/100 iters), loss = 0.417473
I0818 00:53:06.286239 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417473 (* 1 = 0.417473 loss)
I0818 00:53:06.286252 15109 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0818 00:53:32.699785 15109 solver.cpp:357] Iteration 10800 (3.78613 iter/s, 26.4122s/100 iters), loss = 0.439577
I0818 00:53:32.699872 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439577 (* 1 = 0.439577 loss)
I0818 00:53:32.699887 15109 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0818 00:53:59.409193 15109 solver.cpp:357] Iteration 10900 (3.74421 iter/s, 26.7079s/100 iters), loss = 0.395332
I0818 00:53:59.409337 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395332 (* 1 = 0.395332 loss)
I0818 00:53:59.409348 15109 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0818 00:54:08.311203 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:54:25.829547 15109 solver.cpp:514] Iteration 11000, Testing net (#0)
I0818 00:54:51.300117 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:54:51.394444 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5233
I0818 00:54:51.394493 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.50894 (* 1 = 1.50894 loss)
I0818 00:54:51.633323 15109 solver.cpp:357] Iteration 11000 (1.91478 iter/s, 52.2254s/100 iters), loss = 0.345841
I0818 00:54:51.633375 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.345841 (* 1 = 0.345841 loss)
I0818 00:54:51.633390 15109 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0818 00:55:18.203049 15109 solver.cpp:357] Iteration 11100 (3.76389 iter/s, 26.5682s/100 iters), loss = 0.459924
I0818 00:55:18.203138 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.459924 (* 1 = 0.459924 loss)
I0818 00:55:18.203152 15109 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0818 00:55:44.849557 15109 solver.cpp:357] Iteration 11200 (3.75305 iter/s, 26.645s/100 iters), loss = 0.282324
I0818 00:55:44.849802 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.282324 (* 1 = 0.282324 loss)
I0818 00:55:44.849819 15109 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0818 00:56:11.693377 15109 solver.cpp:357] Iteration 11300 (3.72546 iter/s, 26.8423s/100 iters), loss = 0.377255
I0818 00:56:11.693452 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377255 (* 1 = 0.377255 loss)
I0818 00:56:11.693462 15109 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0818 00:56:18.004331 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:56:38.290011 15109 solver.cpp:357] Iteration 11400 (3.76009 iter/s, 26.5951s/100 iters), loss = 0.429589
I0818 00:56:38.290104 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429589 (* 1 = 0.429589 loss)
I0818 00:56:38.290119 15109 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0818 00:57:04.593694 15109 solver.cpp:514] Iteration 11500, Testing net (#0)
I0818 00:57:30.150377 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:57:30.253254 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.568601
I0818 00:57:30.253307 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.54646 (* 1 = 1.54646 loss)
I0818 00:57:30.486038 15109 solver.cpp:357] Iteration 11500 (1.91588 iter/s, 52.1952s/100 iters), loss = 0.49858
I0818 00:57:30.486127 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.49858 (* 1 = 0.49858 loss)
I0818 00:57:30.486141 15109 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0818 00:57:56.988065 15109 solver.cpp:357] Iteration 11600 (3.77351 iter/s, 26.5005s/100 iters), loss = 0.418029
I0818 00:57:56.988185 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.418029 (* 1 = 0.418029 loss)
I0818 00:57:56.988198 15109 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0818 00:58:23.745976 15109 solver.cpp:357] Iteration 11700 (3.73742 iter/s, 26.7564s/100 iters), loss = 0.483544
I0818 00:58:23.746052 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483544 (* 1 = 0.483544 loss)
I0818 00:58:23.746063 15109 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0818 00:58:27.402024 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 00:58:50.340167 15109 solver.cpp:357] Iteration 11800 (3.76043 iter/s, 26.5927s/100 iters), loss = 0.562872
I0818 00:58:50.340243 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.562872 (* 1 = 0.562872 loss)
I0818 00:58:50.340255 15109 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0818 00:59:16.852604 15109 solver.cpp:357] Iteration 11900 (3.77203 iter/s, 26.5109s/100 iters), loss = 0.407803
I0818 00:59:16.852747 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407803 (* 1 = 0.407803 loss)
I0818 00:59:16.852761 15109 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0818 00:59:43.416471 15109 solver.cpp:514] Iteration 12000, Testing net (#0)
I0818 01:00:08.880992 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:00:08.988768 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6792
I0818 01:00:08.988816 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07388 (* 1 = 1.07388 loss)
I0818 01:00:09.167790 15109 solver.cpp:357] Iteration 12000 (1.91152 iter/s, 52.3144s/100 iters), loss = 0.421209
I0818 01:00:09.167834 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421209 (* 1 = 0.421209 loss)
I0818 01:00:09.167846 15109 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0818 01:00:35.793623 15109 solver.cpp:357] Iteration 12100 (3.75569 iter/s, 26.6263s/100 iters), loss = 0.416406
I0818 01:00:35.793710 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416406 (* 1 = 0.416406 loss)
I0818 01:00:35.793725 15109 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0818 01:00:37.055779 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:01:02.394850 15109 solver.cpp:357] Iteration 12200 (3.75946 iter/s, 26.5995s/100 iters), loss = 0.395116
I0818 01:01:02.395045 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395116 (* 1 = 0.395116 loss)
I0818 01:01:02.395058 15109 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0818 01:01:29.024128 15109 solver.cpp:357] Iteration 12300 (3.7555 iter/s, 26.6276s/100 iters), loss = 0.480164
I0818 01:01:29.024209 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480164 (* 1 = 0.480164 loss)
I0818 01:01:29.024221 15109 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0818 01:01:55.755882 15109 solver.cpp:357] Iteration 12400 (3.74086 iter/s, 26.7318s/100 iters), loss = 0.61687
I0818 01:01:55.756063 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.61687 (* 1 = 0.61687 loss)
I0818 01:01:55.756075 15109 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0818 01:02:21.096029 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:02:21.992107 15109 solver.cpp:514] Iteration 12500, Testing net (#0)
I0818 01:02:47.626787 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:02:47.734714 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.405099
I0818 01:02:47.734781 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.66156 (* 1 = 2.66156 loss)
I0818 01:02:47.973225 15109 solver.cpp:357] Iteration 12500 (1.91511 iter/s, 52.2162s/100 iters), loss = 0.35407
I0818 01:02:47.973282 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.35407 (* 1 = 0.35407 loss)
I0818 01:02:47.973299 15109 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0818 01:03:14.525527 15109 solver.cpp:357] Iteration 12600 (3.76639 iter/s, 26.5506s/100 iters), loss = 0.476653
I0818 01:03:14.525598 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.476653 (* 1 = 0.476653 loss)
I0818 01:03:14.525609 15109 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0818 01:03:41.284068 15109 solver.cpp:357] Iteration 12700 (3.73711 iter/s, 26.7587s/100 iters), loss = 0.576277
I0818 01:03:41.284219 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.576277 (* 1 = 0.576277 loss)
I0818 01:03:41.284229 15109 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0818 01:04:08.004966 15109 solver.cpp:357] Iteration 12800 (3.74234 iter/s, 26.7213s/100 iters), loss = 0.470448
I0818 01:04:08.005045 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.470448 (* 1 = 0.470448 loss)
I0818 01:04:08.005057 15109 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0818 01:04:30.846068 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:04:34.561460 15109 solver.cpp:357] Iteration 12900 (3.76579 iter/s, 26.5549s/100 iters), loss = 0.583937
I0818 01:04:34.561535 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.583937 (* 1 = 0.583937 loss)
I0818 01:04:34.561547 15109 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0818 01:05:01.045094 15109 solver.cpp:514] Iteration 13000, Testing net (#0)
I0818 01:05:26.528898 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:05:26.641878 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6456
I0818 01:05:26.641933 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.23099 (* 1 = 1.23099 loss)
I0818 01:05:26.882508 15109 solver.cpp:357] Iteration 13000 (1.91132 iter/s, 52.32s/100 iters), loss = 0.305176
I0818 01:05:26.882586 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305176 (* 1 = 0.305176 loss)
I0818 01:05:26.882599 15109 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0818 01:05:53.612641 15109 solver.cpp:357] Iteration 13100 (3.74131 iter/s, 26.7286s/100 iters), loss = 0.428448
I0818 01:05:53.612892 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428448 (* 1 = 0.428448 loss)
I0818 01:05:53.612905 15109 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0818 01:06:20.177464 15109 solver.cpp:357] Iteration 13200 (3.76461 iter/s, 26.5632s/100 iters), loss = 0.369171
I0818 01:06:20.177557 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369171 (* 1 = 0.369171 loss)
I0818 01:06:20.177570 15109 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0818 01:06:40.731356 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:06:46.796721 15109 solver.cpp:357] Iteration 13300 (3.75691 iter/s, 26.6176s/100 iters), loss = 0.530106
I0818 01:06:46.796798 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530106 (* 1 = 0.530106 loss)
I0818 01:06:46.796811 15109 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0818 01:07:13.519878 15109 solver.cpp:357] Iteration 13400 (3.7423 iter/s, 26.7216s/100 iters), loss = 0.675056
I0818 01:07:13.519991 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.675056 (* 1 = 0.675056 loss)
I0818 01:07:13.520001 15109 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0818 01:07:39.979964 15109 solver.cpp:514] Iteration 13500, Testing net (#0)
I0818 01:08:05.399456 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:08:05.507380 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6395
I0818 01:08:05.507447 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07889 (* 1 = 1.07889 loss)
I0818 01:08:05.735677 15109 solver.cpp:357] Iteration 13500 (1.91509 iter/s, 52.2168s/100 iters), loss = 0.414182
I0818 01:08:05.735736 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414182 (* 1 = 0.414182 loss)
I0818 01:08:05.735750 15109 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0818 01:08:32.347102 15109 solver.cpp:357] Iteration 13600 (3.75801 iter/s, 26.6098s/100 iters), loss = 0.326825
I0818 01:08:32.347185 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326825 (* 1 = 0.326825 loss)
I0818 01:08:32.347200 15109 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0818 01:08:50.209619 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:08:58.928140 15109 solver.cpp:357] Iteration 13700 (3.7623 iter/s, 26.5795s/100 iters), loss = 0.518871
I0818 01:08:58.928231 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.518871 (* 1 = 0.518871 loss)
I0818 01:08:58.928246 15109 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0818 01:09:25.693284 15109 solver.cpp:357] Iteration 13800 (3.73642 iter/s, 26.7636s/100 iters), loss = 0.403725
I0818 01:09:25.693470 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403725 (* 1 = 0.403725 loss)
I0818 01:09:25.693485 15109 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0818 01:09:52.048877 15109 solver.cpp:357] Iteration 13900 (3.79449 iter/s, 26.354s/100 iters), loss = 0.431517
I0818 01:09:52.048951 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431517 (* 1 = 0.431517 loss)
I0818 01:09:52.048964 15109 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0818 01:10:18.443773 15109 solver.cpp:514] Iteration 14000, Testing net (#0)
I0818 01:10:43.832608 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:10:43.939258 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7565
I0818 01:10:43.939319 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.811169 (* 1 = 0.811169 loss)
I0818 01:10:44.175151 15109 solver.cpp:357] Iteration 14000 (1.91838 iter/s, 52.1274s/100 iters), loss = 0.355745
I0818 01:10:44.175211 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355745 (* 1 = 0.355745 loss)
I0818 01:10:44.175223 15109 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0818 01:10:59.743918 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:11:11.147536 15109 solver.cpp:357] Iteration 14100 (3.70771 iter/s, 26.9708s/100 iters), loss = 0.356517
I0818 01:11:11.147624 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356517 (* 1 = 0.356517 loss)
I0818 01:11:11.147639 15109 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0818 01:11:38.285953 15109 solver.cpp:357] Iteration 14200 (3.68503 iter/s, 27.1368s/100 iters), loss = 0.543681
I0818 01:11:38.286104 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543681 (* 1 = 0.543681 loss)
I0818 01:11:38.286116 15109 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0818 01:12:05.323760 15109 solver.cpp:357] Iteration 14300 (3.69874 iter/s, 27.0362s/100 iters), loss = 0.543793
I0818 01:12:05.323856 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543793 (* 1 = 0.543793 loss)
I0818 01:12:05.323869 15109 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0818 01:12:32.376313 15109 solver.cpp:357] Iteration 14400 (3.69673 iter/s, 27.051s/100 iters), loss = 0.378348
I0818 01:12:32.376488 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378348 (* 1 = 0.378348 loss)
I0818 01:12:32.376502 15109 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0818 01:12:45.753702 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:12:59.198678 15109 solver.cpp:514] Iteration 14500, Testing net (#0)
I0818 01:13:25.153368 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:13:25.246265 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.633901
I0818 01:13:25.246338 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.19629 (* 1 = 1.19629 loss)
I0818 01:13:25.477761 15109 solver.cpp:357] Iteration 14500 (1.88322 iter/s, 53.1005s/100 iters), loss = 0.407337
I0818 01:13:25.477845 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407337 (* 1 = 0.407337 loss)
I0818 01:13:25.477859 15109 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0818 01:13:52.585546 15109 solver.cpp:357] Iteration 14600 (3.68919 iter/s, 27.1062s/100 iters), loss = 0.61364
I0818 01:13:52.585628 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.61364 (* 1 = 0.61364 loss)
I0818 01:13:52.585646 15109 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0818 01:14:19.625918 15109 solver.cpp:357] Iteration 14700 (3.69839 iter/s, 27.0388s/100 iters), loss = 0.440767
I0818 01:14:19.626096 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440767 (* 1 = 0.440767 loss)
I0818 01:14:19.626111 15109 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0818 01:14:46.265286 15109 solver.cpp:357] Iteration 14800 (3.75406 iter/s, 26.6378s/100 iters), loss = 0.386463
I0818 01:14:46.265359 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386463 (* 1 = 0.386463 loss)
I0818 01:14:46.265372 15109 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0818 01:14:56.687122 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:15:12.863116 15109 solver.cpp:357] Iteration 14900 (3.75993 iter/s, 26.5963s/100 iters), loss = 0.35666
I0818 01:15:12.863188 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.35666 (* 1 = 0.35666 loss)
I0818 01:15:12.863200 15109 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0818 01:15:39.382503 15109 solver.cpp:514] Iteration 15000, Testing net (#0)
I0818 01:16:04.764389 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:16:04.810822 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6396
I0818 01:16:04.810885 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.12065 (* 1 = 1.12065 loss)
I0818 01:16:04.987579 15109 solver.cpp:357] Iteration 15000 (1.91852 iter/s, 52.1235s/100 iters), loss = 0.500133
I0818 01:16:04.987648 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.500133 (* 1 = 0.500133 loss)
I0818 01:16:04.987659 15109 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0818 01:16:31.891909 15109 solver.cpp:357] Iteration 15100 (3.71709 iter/s, 26.9028s/100 iters), loss = 0.493253
I0818 01:16:31.892206 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.493253 (* 1 = 0.493253 loss)
I0818 01:16:31.892222 15109 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0818 01:16:58.464902 15109 solver.cpp:357] Iteration 15200 (3.76344 iter/s, 26.5714s/100 iters), loss = 0.578219
I0818 01:16:58.464987 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.578219 (* 1 = 0.578219 loss)
I0818 01:16:58.465000 15109 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0818 01:17:06.380385 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:17:25.074081 15109 solver.cpp:357] Iteration 15300 (3.75832 iter/s, 26.6076s/100 iters), loss = 0.425803
I0818 01:17:25.074167 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425803 (* 1 = 0.425803 loss)
I0818 01:17:25.074180 15109 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0818 01:17:51.797721 15109 solver.cpp:357] Iteration 15400 (3.74222 iter/s, 26.7221s/100 iters), loss = 0.436088
I0818 01:17:51.797917 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.436088 (* 1 = 0.436088 loss)
I0818 01:17:51.797931 15109 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0818 01:18:18.049883 15109 solver.cpp:514] Iteration 15500, Testing net (#0)
I0818 01:18:43.489743 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:18:43.602866 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5582
I0818 01:18:43.602921 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.43814 (* 1 = 1.43814 loss)
I0818 01:18:43.840663 15109 solver.cpp:357] Iteration 15500 (1.92153 iter/s, 52.042s/100 iters), loss = 0.418145
I0818 01:18:43.840734 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.418145 (* 1 = 0.418145 loss)
I0818 01:18:43.840745 15109 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0818 01:19:10.487990 15109 solver.cpp:357] Iteration 15600 (3.75265 iter/s, 26.6479s/100 iters), loss = 0.429868
I0818 01:19:10.488071 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429868 (* 1 = 0.429868 loss)
I0818 01:19:10.488085 15109 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0818 01:19:15.847975 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:19:37.118150 15109 solver.cpp:357] Iteration 15700 (3.75536 iter/s, 26.6286s/100 iters), loss = 0.426194
I0818 01:19:37.118216 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426194 (* 1 = 0.426194 loss)
I0818 01:19:37.118227 15109 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0818 01:20:03.826665 15109 solver.cpp:357] Iteration 15800 (3.74405 iter/s, 26.709s/100 iters), loss = 0.404986
I0818 01:20:03.826787 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404986 (* 1 = 0.404986 loss)
I0818 01:20:03.826800 15109 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0818 01:20:30.414185 15109 solver.cpp:357] Iteration 15900 (3.76138 iter/s, 26.586s/100 iters), loss = 0.306055
I0818 01:20:30.414261 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.306055 (* 1 = 0.306055 loss)
I0818 01:20:30.414273 15109 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0818 01:20:56.899055 15109 solver.cpp:514] Iteration 16000, Testing net (#0)
I0818 01:21:22.140713 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:21:22.186585 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6411
I0818 01:21:22.186643 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.19361 (* 1 = 1.19361 loss)
I0818 01:21:22.362907 15109 solver.cpp:357] Iteration 16000 (1.92501 iter/s, 51.9478s/100 iters), loss = 0.477096
I0818 01:21:22.362979 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477096 (* 1 = 0.477096 loss)
I0818 01:21:22.362994 15109 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0818 01:21:25.351575 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:21:49.181422 15109 solver.cpp:357] Iteration 16100 (3.72898 iter/s, 26.817s/100 iters), loss = 0.510852
I0818 01:21:49.181643 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.510852 (* 1 = 0.510852 loss)
I0818 01:21:49.181656 15109 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0818 01:22:15.824653 15109 solver.cpp:357] Iteration 16200 (3.75352 iter/s, 26.6417s/100 iters), loss = 0.343993
I0818 01:22:15.824740 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343993 (* 1 = 0.343993 loss)
I0818 01:22:15.824753 15109 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0818 01:22:42.402721 15109 solver.cpp:357] Iteration 16300 (3.76271 iter/s, 26.5766s/100 iters), loss = 0.401513
I0818 01:22:42.403101 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401513 (* 1 = 0.401513 loss)
I0818 01:22:42.403163 15109 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0818 01:23:09.242494 15109 solver.cpp:357] Iteration 16400 (3.72578 iter/s, 26.84s/100 iters), loss = 0.431082
I0818 01:23:09.242573 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431082 (* 1 = 0.431082 loss)
I0818 01:23:09.242586 15109 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0818 01:23:09.642226 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:23:35.632004 15109 solver.cpp:514] Iteration 16500, Testing net (#0)
I0818 01:24:01.058965 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:24:01.165673 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7243
I0818 01:24:01.165742 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.811088 (* 1 = 0.811088 loss)
I0818 01:24:01.403359 15109 solver.cpp:357] Iteration 16500 (1.91718 iter/s, 52.1599s/100 iters), loss = 0.413389
I0818 01:24:01.403419 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413389 (* 1 = 0.413389 loss)
I0818 01:24:01.403432 15109 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0818 01:24:27.960974 15109 solver.cpp:357] Iteration 16600 (3.76562 iter/s, 26.5561s/100 iters), loss = 0.396723
I0818 01:24:27.961150 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396723 (* 1 = 0.396723 loss)
I0818 01:24:27.961164 15109 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0818 01:24:54.561565 15109 solver.cpp:357] Iteration 16700 (3.75953 iter/s, 26.599s/100 iters), loss = 0.457656
I0818 01:24:54.561638 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457656 (* 1 = 0.457656 loss)
I0818 01:24:54.561651 15109 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0818 01:25:19.145648 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:25:21.391106 15109 solver.cpp:357] Iteration 16800 (3.72745 iter/s, 26.828s/100 iters), loss = 0.396651
I0818 01:25:21.391187 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396651 (* 1 = 0.396651 loss)
I0818 01:25:21.391199 15109 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0818 01:25:47.980934 15109 solver.cpp:357] Iteration 16900 (3.76105 iter/s, 26.5883s/100 iters), loss = 0.384529
I0818 01:25:47.981010 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384529 (* 1 = 0.384529 loss)
I0818 01:25:47.981021 15109 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0818 01:26:14.328860 15109 solver.cpp:514] Iteration 17000, Testing net (#0)
I0818 01:26:39.865794 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:26:39.975208 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5674
I0818 01:26:39.975268 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.62473 (* 1 = 1.62473 loss)
I0818 01:26:40.213701 15109 solver.cpp:357] Iteration 17000 (1.91454 iter/s, 52.2319s/100 iters), loss = 0.513297
I0818 01:26:40.213755 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513297 (* 1 = 0.513297 loss)
I0818 01:26:40.213768 15109 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0818 01:27:06.796479 15109 solver.cpp:357] Iteration 17100 (3.76205 iter/s, 26.5812s/100 iters), loss = 0.326745
I0818 01:27:06.796792 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326745 (* 1 = 0.326745 loss)
I0818 01:27:06.796803 15109 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0818 01:27:28.918781 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:27:33.517127 15109 solver.cpp:357] Iteration 17200 (3.74239 iter/s, 26.7209s/100 iters), loss = 0.295764
I0818 01:27:33.517204 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.295764 (* 1 = 0.295764 loss)
I0818 01:27:33.517215 15109 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0818 01:28:00.121161 15109 solver.cpp:357] Iteration 17300 (3.75905 iter/s, 26.6025s/100 iters), loss = 0.334573
I0818 01:28:00.121368 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334573 (* 1 = 0.334573 loss)
I0818 01:28:00.121382 15109 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0818 01:28:26.621737 15109 solver.cpp:357] Iteration 17400 (3.77372 iter/s, 26.4991s/100 iters), loss = 0.383606
I0818 01:28:26.621820 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383606 (* 1 = 0.383606 loss)
I0818 01:28:26.621834 15109 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0818 01:28:53.159401 15109 solver.cpp:514] Iteration 17500, Testing net (#0)
I0818 01:29:18.596107 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:29:18.708788 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6297
I0818 01:29:18.708834 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.39699 (* 1 = 1.39699 loss)
I0818 01:29:18.936902 15109 solver.cpp:357] Iteration 17500 (1.91146 iter/s, 52.3161s/100 iters), loss = 0.286953
I0818 01:29:18.936985 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.286953 (* 1 = 0.286953 loss)
I0818 01:29:18.936997 15109 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0818 01:29:38.619493 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:29:45.539800 15109 solver.cpp:357] Iteration 17600 (3.75921 iter/s, 26.6013s/100 iters), loss = 0.451638
I0818 01:29:45.539876 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.451638 (* 1 = 0.451638 loss)
I0818 01:29:45.539889 15109 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0818 01:30:12.253628 15109 solver.cpp:357] Iteration 17700 (3.7436 iter/s, 26.7123s/100 iters), loss = 0.509616
I0818 01:30:12.253770 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.509616 (* 1 = 0.509616 loss)
I0818 01:30:12.253783 15109 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0818 01:30:38.991973 15109 solver.cpp:357] Iteration 17800 (3.74016 iter/s, 26.7368s/100 iters), loss = 0.540151
I0818 01:30:38.992044 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.540151 (* 1 = 0.540151 loss)
I0818 01:30:38.992054 15109 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0818 01:31:05.746706 15109 solver.cpp:357] Iteration 17900 (3.73758 iter/s, 26.7553s/100 iters), loss = 0.289838
I0818 01:31:05.746850 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.289838 (* 1 = 0.289838 loss)
I0818 01:31:05.746860 15109 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0818 01:31:22.703027 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:31:32.061440 15109 solver.cpp:514] Iteration 18000, Testing net (#0)
I0818 01:31:57.538755 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:31:57.653556 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5771
I0818 01:31:57.653609 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.36581 (* 1 = 1.36581 loss)
I0818 01:31:57.880391 15109 solver.cpp:357] Iteration 18000 (1.91818 iter/s, 52.1328s/100 iters), loss = 0.339753
I0818 01:31:57.880471 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339753 (* 1 = 0.339753 loss)
I0818 01:31:57.880484 15109 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0818 01:32:24.520514 15109 solver.cpp:357] Iteration 18100 (3.75395 iter/s, 26.6386s/100 iters), loss = 0.353674
I0818 01:32:24.520591 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353674 (* 1 = 0.353674 loss)
I0818 01:32:24.520606 15109 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0818 01:32:51.287767 15109 solver.cpp:357] Iteration 18200 (3.73612 iter/s, 26.7657s/100 iters), loss = 0.394933
I0818 01:32:51.288034 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394933 (* 1 = 0.394933 loss)
I0818 01:32:51.288048 15109 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0818 01:33:17.894418 15109 solver.cpp:357] Iteration 18300 (3.75868 iter/s, 26.6051s/100 iters), loss = 0.328941
I0818 01:33:17.894497 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328941 (* 1 = 0.328941 loss)
I0818 01:33:17.894510 15109 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0818 01:33:32.599820 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:33:44.440172 15109 solver.cpp:357] Iteration 18400 (3.7673 iter/s, 26.5442s/100 iters), loss = 0.541008
I0818 01:33:44.440258 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.541008 (* 1 = 0.541008 loss)
I0818 01:33:44.440270 15109 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0818 01:34:11.032447 15109 solver.cpp:514] Iteration 18500, Testing net (#0)
I0818 01:34:36.437224 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:34:36.479924 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7023
I0818 01:34:36.479985 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.962092 (* 1 = 0.962092 loss)
I0818 01:34:36.644721 15109 solver.cpp:357] Iteration 18500 (1.91562 iter/s, 52.2025s/100 iters), loss = 0.360679
I0818 01:34:36.644783 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360679 (* 1 = 0.360679 loss)
I0818 01:34:36.644795 15109 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0818 01:35:03.459549 15109 solver.cpp:357] Iteration 18600 (3.72979 iter/s, 26.8112s/100 iters), loss = 0.387066
I0818 01:35:03.459728 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387066 (* 1 = 0.387066 loss)
I0818 01:35:03.459741 15109 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0818 01:35:30.129664 15109 solver.cpp:357] Iteration 18700 (3.75002 iter/s, 26.6666s/100 iters), loss = 0.358479
I0818 01:35:30.129739 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358479 (* 1 = 0.358479 loss)
I0818 01:35:30.129750 15109 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0818 01:35:42.178197 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:35:56.697875 15109 solver.cpp:357] Iteration 18800 (3.76439 iter/s, 26.5647s/100 iters), loss = 0.363374
I0818 01:35:56.697959 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363374 (* 1 = 0.363374 loss)
I0818 01:35:56.697970 15109 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0818 01:36:23.524816 15109 solver.cpp:357] Iteration 18900 (3.72807 iter/s, 26.8235s/100 iters), loss = 0.421103
I0818 01:36:23.524977 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421103 (* 1 = 0.421103 loss)
I0818 01:36:23.524989 15109 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0818 01:36:49.876312 15109 solver.cpp:514] Iteration 19000, Testing net (#0)
I0818 01:37:15.383826 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:37:15.480535 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6794
I0818 01:37:15.480607 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.963649 (* 1 = 0.963649 loss)
I0818 01:37:15.719470 15109 solver.cpp:357] Iteration 19000 (1.91599 iter/s, 52.1922s/100 iters), loss = 0.408146
I0818 01:37:15.719527 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408146 (* 1 = 0.408146 loss)
I0818 01:37:15.719539 15109 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0818 01:37:42.265767 15109 solver.cpp:357] Iteration 19100 (3.76745 iter/s, 26.5431s/100 iters), loss = 0.458348
I0818 01:37:42.265843 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458348 (* 1 = 0.458348 loss)
I0818 01:37:42.265856 15109 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0818 01:37:52.000672 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:38:08.936713 15109 solver.cpp:357] Iteration 19200 (3.74983 iter/s, 26.6679s/100 iters), loss = 0.45365
I0818 01:38:08.936794 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45365 (* 1 = 0.45365 loss)
I0818 01:38:08.936806 15109 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0818 01:38:35.710837 15109 solver.cpp:357] Iteration 19300 (3.73513 iter/s, 26.7728s/100 iters), loss = 0.39061
I0818 01:38:35.710995 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39061 (* 1 = 0.39061 loss)
I0818 01:38:35.711009 15109 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0818 01:39:02.312041 15109 solver.cpp:357] Iteration 19400 (3.75964 iter/s, 26.5983s/100 iters), loss = 0.478996
I0818 01:39:02.312120 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478996 (* 1 = 0.478996 loss)
I0818 01:39:02.312134 15109 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0818 01:39:28.755115 15109 solver.cpp:514] Iteration 19500, Testing net (#0)
I0818 01:39:54.291882 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:39:54.403055 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6084
I0818 01:39:54.403106 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.63916 (* 1 = 1.63916 loss)
I0818 01:39:54.644093 15109 solver.cpp:357] Iteration 19500 (1.911 iter/s, 52.3286s/100 iters), loss = 0.381713
I0818 01:39:54.644171 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381713 (* 1 = 0.381713 loss)
I0818 01:39:54.644186 15109 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0818 01:40:01.972990 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:40:21.424057 15109 solver.cpp:357] Iteration 19600 (3.73446 iter/s, 26.7777s/100 iters), loss = 0.462901
I0818 01:40:21.424123 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462901 (* 1 = 0.462901 loss)
I0818 01:40:21.424134 15109 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0818 01:40:48.111443 15109 solver.cpp:357] Iteration 19700 (3.74718 iter/s, 26.6867s/100 iters), loss = 0.684283
I0818 01:40:48.111637 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.684283 (* 1 = 0.684283 loss)
I0818 01:40:48.111651 15109 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0818 01:41:14.720883 15109 solver.cpp:357] Iteration 19800 (3.75844 iter/s, 26.6068s/100 iters), loss = 0.44389
I0818 01:41:14.720962 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44389 (* 1 = 0.44389 loss)
I0818 01:41:14.720975 15109 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0818 01:41:41.397951 15109 solver.cpp:357] Iteration 19900 (3.74891 iter/s, 26.6744s/100 iters), loss = 0.488406
I0818 01:41:41.398124 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488406 (* 1 = 0.488406 loss)
I0818 01:41:41.398135 15109 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0818 01:41:46.014183 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:42:07.934828 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.caffemodel
I0818 01:42:07.944914 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.solverstate
I0818 01:42:07.947835 15109 solver.cpp:514] Iteration 20000, Testing net (#0)
I0818 01:42:33.727047 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:42:33.837466 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5244
I0818 01:42:33.837513 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.90213 (* 1 = 1.90213 loss)
I0818 01:42:33.970752 15109 solver.cpp:357] Iteration 20000 (1.90216 iter/s, 52.5718s/100 iters), loss = 0.403529
I0818 01:42:33.970798 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403529 (* 1 = 0.403529 loss)
I0818 01:42:33.970811 15109 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0818 01:43:00.656414 15109 solver.cpp:357] Iteration 20100 (3.74739 iter/s, 26.6852s/100 iters), loss = 0.358358
I0818 01:43:00.656489 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358358 (* 1 = 0.358358 loss)
I0818 01:43:00.656502 15109 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0818 01:43:27.210999 15109 solver.cpp:357] Iteration 20200 (3.76618 iter/s, 26.5521s/100 iters), loss = 0.414389
I0818 01:43:27.211244 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414389 (* 1 = 0.414389 loss)
I0818 01:43:27.211263 15109 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0818 01:43:53.944934 15109 solver.cpp:357] Iteration 20300 (3.74091 iter/s, 26.7315s/100 iters), loss = 0.330466
I0818 01:43:53.944999 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330466 (* 1 = 0.330466 loss)
I0818 01:43:53.945010 15109 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0818 01:43:56.170938 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:44:20.471359 15109 solver.cpp:357] Iteration 20400 (3.76987 iter/s, 26.5261s/100 iters), loss = 0.29451
I0818 01:44:20.471560 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29451 (* 1 = 0.29451 loss)
I0818 01:44:20.471572 15109 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0818 01:44:46.835737 15109 solver.cpp:514] Iteration 20500, Testing net (#0)
I0818 01:45:12.346222 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:45:12.458447 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6871
I0818 01:45:12.458493 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.11606 (* 1 = 1.11606 loss)
I0818 01:45:12.688196 15109 solver.cpp:357] Iteration 20500 (1.91518 iter/s, 52.2143s/100 iters), loss = 0.39468
I0818 01:45:12.688279 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.39468 (* 1 = 0.39468 loss)
I0818 01:45:12.688292 15109 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0818 01:45:39.261440 15109 solver.cpp:357] Iteration 20600 (3.76351 iter/s, 26.5709s/100 iters), loss = 0.356478
I0818 01:45:39.261512 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356478 (* 1 = 0.356478 loss)
I0818 01:45:39.261524 15109 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0818 01:46:05.744774 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:46:05.967262 15109 solver.cpp:357] Iteration 20700 (3.74465 iter/s, 26.7048s/100 iters), loss = 0.455057
I0818 01:46:05.967324 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.455057 (* 1 = 0.455057 loss)
I0818 01:46:05.967339 15109 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0818 01:46:32.561306 15109 solver.cpp:357] Iteration 20800 (3.76056 iter/s, 26.5918s/100 iters), loss = 0.426824
I0818 01:46:32.561384 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426824 (* 1 = 0.426824 loss)
I0818 01:46:32.561398 15109 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0818 01:46:59.267876 15109 solver.cpp:357] Iteration 20900 (3.74471 iter/s, 26.7043s/100 iters), loss = 0.357802
I0818 01:46:59.268057 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357802 (* 1 = 0.357802 loss)
I0818 01:46:59.268069 15109 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0818 01:47:25.884124 15109 solver.cpp:514] Iteration 21000, Testing net (#0)
I0818 01:47:51.383508 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:47:51.491839 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7239
I0818 01:47:51.491910 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.813194 (* 1 = 0.813194 loss)
I0818 01:47:51.721132 15109 solver.cpp:357] Iteration 21000 (1.90647 iter/s, 52.4528s/100 iters), loss = 0.471571
I0818 01:47:51.721189 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471571 (* 1 = 0.471571 loss)
I0818 01:47:51.721200 15109 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0818 01:48:15.497539 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:48:18.366215 15109 solver.cpp:357] Iteration 21100 (3.75335 iter/s, 26.6429s/100 iters), loss = 0.374083
I0818 01:48:18.366293 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374083 (* 1 = 0.374083 loss)
I0818 01:48:18.366304 15109 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0818 01:48:44.992228 15109 solver.cpp:357] Iteration 21200 (3.75603 iter/s, 26.6238s/100 iters), loss = 0.3838
I0818 01:48:44.992501 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3838 (* 1 = 0.3838 loss)
I0818 01:48:44.992516 15109 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0818 01:49:11.774421 15109 solver.cpp:357] Iteration 21300 (3.73412 iter/s, 26.7801s/100 iters), loss = 0.426235
I0818 01:49:11.774488 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426235 (* 1 = 0.426235 loss)
I0818 01:49:11.774499 15109 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0818 01:49:38.441522 15109 solver.cpp:357] Iteration 21400 (3.74994 iter/s, 26.6671s/100 iters), loss = 0.360899
I0818 01:49:38.441674 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.360899 (* 1 = 0.360899 loss)
I0818 01:49:38.441687 15109 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0818 01:49:59.817898 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:50:04.746749 15109 solver.cpp:514] Iteration 21500, Testing net (#0)
I0818 01:50:30.307162 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:50:30.415177 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6622
I0818 01:50:30.415246 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.04273 (* 1 = 1.04273 loss)
I0818 01:50:30.643282 15109 solver.cpp:357] Iteration 21500 (1.91572 iter/s, 52.1998s/100 iters), loss = 0.260213
I0818 01:50:30.643337 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260213 (* 1 = 0.260213 loss)
I0818 01:50:30.643349 15109 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0818 01:50:57.128835 15109 solver.cpp:357] Iteration 21600 (3.77594 iter/s, 26.4835s/100 iters), loss = 0.445004
I0818 01:50:57.128918 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445004 (* 1 = 0.445004 loss)
I0818 01:50:57.128931 15109 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0818 01:51:23.920938 15109 solver.cpp:357] Iteration 21700 (3.73249 iter/s, 26.7918s/100 iters), loss = 0.494189
I0818 01:51:23.921120 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.494189 (* 1 = 0.494189 loss)
I0818 01:51:23.921134 15109 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0818 01:51:50.538995 15109 solver.cpp:357] Iteration 21800 (3.75714 iter/s, 26.616s/100 iters), loss = 0.386479
I0818 01:51:50.539077 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386479 (* 1 = 0.386479 loss)
I0818 01:51:50.539090 15109 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0818 01:52:09.296700 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:52:17.155613 15109 solver.cpp:357] Iteration 21900 (3.75734 iter/s, 26.6146s/100 iters), loss = 0.52213
I0818 01:52:17.155689 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52213 (* 1 = 0.52213 loss)
I0818 01:52:17.155700 15109 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0818 01:52:43.668236 15109 solver.cpp:514] Iteration 22000, Testing net (#0)
I0818 01:53:09.118916 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:53:09.233826 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6484
I0818 01:53:09.233873 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08023 (* 1 = 1.08023 loss)
I0818 01:53:09.460284 15109 solver.cpp:357] Iteration 22000 (1.91194 iter/s, 52.3028s/100 iters), loss = 0.450053
I0818 01:53:09.460371 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450053 (* 1 = 0.450053 loss)
I0818 01:53:09.460384 15109 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0818 01:53:36.098595 15109 solver.cpp:357] Iteration 22100 (3.75427 iter/s, 26.6363s/100 iters), loss = 0.436448
I0818 01:53:36.098775 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.436448 (* 1 = 0.436448 loss)
I0818 01:53:36.098788 15109 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0818 01:54:02.668076 15109 solver.cpp:357] Iteration 22200 (3.764 iter/s, 26.5675s/100 iters), loss = 0.355574
I0818 01:54:02.668161 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355574 (* 1 = 0.355574 loss)
I0818 01:54:02.668174 15109 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0818 01:54:19.013445 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:54:29.455107 15109 solver.cpp:357] Iteration 22300 (3.73343 iter/s, 26.785s/100 iters), loss = 0.357308
I0818 01:54:29.455188 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.357308 (* 1 = 0.357308 loss)
I0818 01:54:29.455201 15109 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0818 01:54:56.059237 15109 solver.cpp:357] Iteration 22400 (3.75909 iter/s, 26.6022s/100 iters), loss = 0.344794
I0818 01:54:56.059370 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344794 (* 1 = 0.344794 loss)
I0818 01:54:56.059383 15109 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0818 01:55:22.313522 15109 solver.cpp:514] Iteration 22500, Testing net (#0)
I0818 01:55:47.962358 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:55:48.074712 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6972
I0818 01:55:48.074756 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.901063 (* 1 = 0.901063 loss)
I0818 01:55:48.300432 15109 solver.cpp:357] Iteration 22500 (1.91426 iter/s, 52.2395s/100 iters), loss = 0.414913
I0818 01:55:48.300515 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414913 (* 1 = 0.414913 loss)
I0818 01:55:48.300529 15109 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0818 01:56:14.947510 15109 solver.cpp:357] Iteration 22600 (3.75303 iter/s, 26.6452s/100 iters), loss = 0.446887
I0818 01:56:14.947592 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446887 (* 1 = 0.446887 loss)
I0818 01:56:14.947603 15109 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0818 01:56:29.047682 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:56:41.871701 15109 solver.cpp:357] Iteration 22700 (3.7144 iter/s, 26.9222s/100 iters), loss = 0.372384
I0818 01:56:41.871781 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372384 (* 1 = 0.372384 loss)
I0818 01:56:41.871794 15109 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0818 01:57:08.401587 15109 solver.cpp:357] Iteration 22800 (3.7696 iter/s, 26.528s/100 iters), loss = 0.309054
I0818 01:57:08.401758 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.309054 (* 1 = 0.309054 loss)
I0818 01:57:08.401772 15109 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0818 01:57:34.998008 15109 solver.cpp:357] Iteration 22900 (3.76017 iter/s, 26.5945s/100 iters), loss = 0.530491
I0818 01:57:34.998083 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530491 (* 1 = 0.530491 loss)
I0818 01:57:34.998095 15109 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0818 01:58:01.510439 15109 solver.cpp:514] Iteration 23000, Testing net (#0)
I0818 01:58:27.051671 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:58:27.132879 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5642
I0818 01:58:27.132925 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.87702 (* 1 = 1.87702 loss)
I0818 01:58:27.247511 15109 solver.cpp:357] Iteration 23000 (1.91388 iter/s, 52.25s/100 iters), loss = 0.433502
I0818 01:58:27.247560 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.433502 (* 1 = 0.433502 loss)
I0818 01:58:27.247570 15109 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0818 01:58:38.562198 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 01:58:53.925297 15109 solver.cpp:357] Iteration 23100 (3.74853 iter/s, 26.6771s/100 iters), loss = 0.530388
I0818 01:58:53.925380 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.530388 (* 1 = 0.530388 loss)
I0818 01:58:53.925393 15109 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0818 01:59:20.748061 15109 solver.cpp:357] Iteration 23200 (3.72844 iter/s, 26.8209s/100 iters), loss = 0.365124
I0818 01:59:20.748278 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365124 (* 1 = 0.365124 loss)
I0818 01:59:20.748292 15109 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0818 01:59:47.401829 15109 solver.cpp:357] Iteration 23300 (3.75208 iter/s, 26.6519s/100 iters), loss = 0.437455
I0818 01:59:47.401906 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437455 (* 1 = 0.437455 loss)
I0818 01:59:47.401916 15109 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0818 02:00:14.406056 15109 solver.cpp:357] Iteration 23400 (3.70338 iter/s, 27.0023s/100 iters), loss = 0.464918
I0818 02:00:14.406203 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464918 (* 1 = 0.464918 loss)
I0818 02:00:14.406214 15109 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0818 02:00:23.205083 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:00:40.723307 15109 solver.cpp:514] Iteration 23500, Testing net (#0)
I0818 02:01:06.250991 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:01:06.365937 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6181
I0818 02:01:06.365986 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.1942 (* 1 = 1.1942 loss)
I0818 02:01:06.593763 15109 solver.cpp:357] Iteration 23500 (1.91622 iter/s, 52.1861s/100 iters), loss = 0.313904
I0818 02:01:06.593847 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.313904 (* 1 = 0.313904 loss)
I0818 02:01:06.593861 15109 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0818 02:01:33.249678 15109 solver.cpp:357] Iteration 23600 (3.75177 iter/s, 26.6541s/100 iters), loss = 0.297588
I0818 02:01:33.249758 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.297588 (* 1 = 0.297588 loss)
I0818 02:01:33.249770 15109 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0818 02:01:59.771328 15109 solver.cpp:357] Iteration 23700 (3.77077 iter/s, 26.5198s/100 iters), loss = 0.264423
I0818 02:01:59.771492 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.264423 (* 1 = 0.264423 loss)
I0818 02:01:59.771504 15109 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0818 02:02:26.585476 15109 solver.cpp:357] Iteration 23800 (3.72963 iter/s, 26.8123s/100 iters), loss = 0.390902
I0818 02:02:26.585562 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390902 (* 1 = 0.390902 loss)
I0818 02:02:26.585575 15109 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0818 02:02:33.082577 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:02:53.407439 15109 solver.cpp:357] Iteration 23900 (3.72854 iter/s, 26.8201s/100 iters), loss = 0.34842
I0818 02:02:53.407533 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.34842 (* 1 = 0.34842 loss)
I0818 02:02:53.407546 15109 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0818 02:03:20.383035 15109 solver.cpp:514] Iteration 24000, Testing net (#0)
I0818 02:03:46.342555 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:03:46.452922 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.714699
I0818 02:03:46.452981 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.915104 (* 1 = 0.915104 loss)
I0818 02:03:46.672675 15109 solver.cpp:357] Iteration 24000 (1.87745 iter/s, 53.2637s/100 iters), loss = 0.351364
I0818 02:03:46.672773 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351364 (* 1 = 0.351364 loss)
I0818 02:03:46.672787 15109 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0818 02:04:13.650081 15109 solver.cpp:357] Iteration 24100 (3.70706 iter/s, 26.9755s/100 iters), loss = 0.422983
I0818 02:04:13.650316 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422983 (* 1 = 0.422983 loss)
I0818 02:04:13.650331 15109 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0818 02:04:40.738544 15109 solver.cpp:357] Iteration 24200 (3.69186 iter/s, 27.0866s/100 iters), loss = 0.415885
I0818 02:04:40.738621 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415885 (* 1 = 0.415885 loss)
I0818 02:04:40.738633 15109 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0818 02:04:44.568866 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:05:07.745735 15109 solver.cpp:357] Iteration 24300 (3.70289 iter/s, 27.006s/100 iters), loss = 0.625308
I0818 02:05:07.745827 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.625308 (* 1 = 0.625308 loss)
I0818 02:05:07.745841 15109 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0818 02:05:35.008131 15109 solver.cpp:357] Iteration 24400 (3.6683 iter/s, 27.2606s/100 iters), loss = 0.3151
I0818 02:05:35.008301 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.3151 (* 1 = 0.3151 loss)
I0818 02:05:35.008318 15109 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0818 02:06:01.767565 15109 solver.cpp:514] Iteration 24500, Testing net (#0)
I0818 02:06:27.341739 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:06:27.456547 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4982
I0818 02:06:27.456595 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.5739 (* 1 = 2.5739 loss)
I0818 02:06:27.680306 15109 solver.cpp:357] Iteration 24500 (1.89859 iter/s, 52.6707s/100 iters), loss = 0.435822
I0818 02:06:27.680389 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435822 (* 1 = 0.435822 loss)
I0818 02:06:27.680402 15109 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0818 02:06:54.382565 15109 solver.cpp:357] Iteration 24600 (3.74526 iter/s, 26.7004s/100 iters), loss = 0.346619
I0818 02:06:54.382639 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346619 (* 1 = 0.346619 loss)
I0818 02:06:54.382650 15109 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0818 02:06:55.865515 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:07:21.183008 15109 solver.cpp:357] Iteration 24700 (3.73154 iter/s, 26.7986s/100 iters), loss = 0.383726
I0818 02:07:21.183161 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383726 (* 1 = 0.383726 loss)
I0818 02:07:21.183171 15109 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0818 02:07:47.866876 15109 solver.cpp:357] Iteration 24800 (3.74756 iter/s, 26.6841s/100 iters), loss = 0.363946
I0818 02:07:47.866964 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.363946 (* 1 = 0.363946 loss)
I0818 02:07:47.866978 15109 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0818 02:08:14.512831 15109 solver.cpp:357] Iteration 24900 (3.75317 iter/s, 26.6441s/100 iters), loss = 0.541608
I0818 02:08:14.513001 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.541608 (* 1 = 0.541608 loss)
I0818 02:08:14.513012 15109 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0818 02:08:39.992084 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:08:40.998196 15109 solver.cpp:514] Iteration 25000, Testing net (#0)
I0818 02:09:06.403560 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:09:06.504994 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.745499
I0818 02:09:06.505064 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.731915 (* 1 = 0.731915 loss)
I0818 02:09:06.734792 15109 solver.cpp:357] Iteration 25000 (1.91495 iter/s, 52.2206s/100 iters), loss = 0.407961
I0818 02:09:06.734853 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407961 (* 1 = 0.407961 loss)
I0818 02:09:06.734865 15109 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0818 02:09:33.592140 15109 solver.cpp:357] Iteration 25100 (3.72352 iter/s, 26.8563s/100 iters), loss = 0.400646
I0818 02:09:33.592221 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400646 (* 1 = 0.400646 loss)
I0818 02:09:33.592232 15109 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0818 02:10:00.245831 15109 solver.cpp:357] Iteration 25200 (3.75197 iter/s, 26.6526s/100 iters), loss = 0.568115
I0818 02:10:00.246074 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.568115 (* 1 = 0.568115 loss)
I0818 02:10:00.246088 15109 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0818 02:10:26.798171 15109 solver.cpp:357] Iteration 25300 (3.7663 iter/s, 26.5512s/100 iters), loss = 0.446646
I0818 02:10:26.798257 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446646 (* 1 = 0.446646 loss)
I0818 02:10:26.798270 15109 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0818 02:10:49.945899 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:10:53.537580 15109 solver.cpp:357] Iteration 25400 (3.73996 iter/s, 26.7383s/100 iters), loss = 0.490196
I0818 02:10:53.537653 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490196 (* 1 = 0.490196 loss)
I0818 02:10:53.537663 15109 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0818 02:11:19.979722 15109 solver.cpp:514] Iteration 25500, Testing net (#0)
I0818 02:11:45.550259 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:11:45.653820 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6098
I0818 02:11:45.653873 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.65306 (* 1 = 1.65306 loss)
I0818 02:11:45.895164 15109 solver.cpp:357] Iteration 25500 (1.90987 iter/s, 52.3595s/100 iters), loss = 0.302461
I0818 02:11:45.895236 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302461 (* 1 = 0.302461 loss)
I0818 02:11:45.895248 15109 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0818 02:12:12.410615 15109 solver.cpp:357] Iteration 25600 (3.77156 iter/s, 26.5142s/100 iters), loss = 0.375951
I0818 02:12:12.411015 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375951 (* 1 = 0.375951 loss)
I0818 02:12:12.411077 15109 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0818 02:12:39.034911 15109 solver.cpp:357] Iteration 25700 (3.75614 iter/s, 26.6231s/100 iters), loss = 0.44735
I0818 02:12:39.035001 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44735 (* 1 = 0.44735 loss)
I0818 02:12:39.035014 15109 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0818 02:12:59.781622 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:13:05.870908 15109 solver.cpp:357] Iteration 25800 (3.72651 iter/s, 26.8348s/100 iters), loss = 0.435201
I0818 02:13:05.870999 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435201 (* 1 = 0.435201 loss)
I0818 02:13:05.871013 15109 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0818 02:13:32.562952 15109 solver.cpp:357] Iteration 25900 (3.74661 iter/s, 26.6908s/100 iters), loss = 0.457632
I0818 02:13:32.563076 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457632 (* 1 = 0.457632 loss)
I0818 02:13:32.563087 15109 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0818 02:13:58.938825 15109 solver.cpp:514] Iteration 26000, Testing net (#0)
I0818 02:14:24.327484 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:14:24.441213 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5837
I0818 02:14:24.441262 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.40889 (* 1 = 1.40889 loss)
I0818 02:14:24.676270 15109 solver.cpp:357] Iteration 26000 (1.91891 iter/s, 52.1129s/100 iters), loss = 0.353534
I0818 02:14:24.676340 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353534 (* 1 = 0.353534 loss)
I0818 02:14:24.676352 15109 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0818 02:14:51.294646 15109 solver.cpp:357] Iteration 26100 (3.7567 iter/s, 26.6191s/100 iters), loss = 0.346421
I0818 02:14:51.294723 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346421 (* 1 = 0.346421 loss)
I0818 02:14:51.294734 15109 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0818 02:15:09.397457 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:15:18.022079 15109 solver.cpp:357] Iteration 26200 (3.74166 iter/s, 26.7261s/100 iters), loss = 0.407879
I0818 02:15:18.022207 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407879 (* 1 = 0.407879 loss)
I0818 02:15:18.022224 15109 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0818 02:15:44.595479 15109 solver.cpp:357] Iteration 26300 (3.76335 iter/s, 26.572s/100 iters), loss = 0.361521
I0818 02:15:44.595733 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361521 (* 1 = 0.361521 loss)
I0818 02:15:44.595747 15109 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0818 02:16:11.239603 15109 solver.cpp:357] Iteration 26400 (3.75337 iter/s, 26.6427s/100 iters), loss = 0.356619
I0818 02:16:11.239681 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356619 (* 1 = 0.356619 loss)
I0818 02:16:11.239692 15109 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0818 02:16:37.811611 15109 solver.cpp:514] Iteration 26500, Testing net (#0)
I0818 02:17:03.491662 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:17:03.569383 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5343
I0818 02:17:03.569428 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.90841 (* 1 = 1.90841 loss)
I0818 02:17:03.683724 15109 solver.cpp:357] Iteration 26500 (1.90681 iter/s, 52.4436s/100 iters), loss = 0.37336
I0818 02:17:03.683773 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.37336 (* 1 = 0.37336 loss)
I0818 02:17:03.683785 15109 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0818 02:17:19.129963 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:17:30.488296 15109 solver.cpp:357] Iteration 26600 (3.73073 iter/s, 26.8044s/100 iters), loss = 0.388181
I0818 02:17:30.488389 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388181 (* 1 = 0.388181 loss)
I0818 02:17:30.488402 15109 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0818 02:17:57.005162 15109 solver.cpp:357] Iteration 26700 (3.77139 iter/s, 26.5154s/100 iters), loss = 0.478482
I0818 02:17:57.005326 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478482 (* 1 = 0.478482 loss)
I0818 02:17:57.005337 15109 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0818 02:18:23.656527 15109 solver.cpp:357] Iteration 26800 (3.75236 iter/s, 26.6499s/100 iters), loss = 0.459897
I0818 02:18:23.656610 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.459897 (* 1 = 0.459897 loss)
I0818 02:18:23.656625 15109 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0818 02:18:50.539105 15109 solver.cpp:357] Iteration 26900 (3.72008 iter/s, 26.8811s/100 iters), loss = 0.443689
I0818 02:18:50.539268 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443689 (* 1 = 0.443689 loss)
I0818 02:18:50.539281 15109 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0818 02:19:03.509479 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:19:16.950392 15109 solver.cpp:514] Iteration 27000, Testing net (#0)
I0818 02:19:42.531697 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:19:42.634891 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6568
I0818 02:19:42.634963 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.03621 (* 1 = 1.03621 loss)
I0818 02:19:42.873366 15109 solver.cpp:357] Iteration 27000 (1.91082 iter/s, 52.3336s/100 iters), loss = 0.339779
I0818 02:19:42.873419 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339779 (* 1 = 0.339779 loss)
I0818 02:19:42.873433 15109 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0818 02:20:09.460363 15109 solver.cpp:357] Iteration 27100 (3.76145 iter/s, 26.5855s/100 iters), loss = 0.449549
I0818 02:20:09.460448 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.449549 (* 1 = 0.449549 loss)
I0818 02:20:09.460460 15109 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0818 02:20:36.021808 15109 solver.cpp:357] Iteration 27200 (3.76506 iter/s, 26.56s/100 iters), loss = 0.370565
I0818 02:20:36.021916 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.370565 (* 1 = 0.370565 loss)
I0818 02:20:36.021929 15109 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0818 02:21:02.793632 15109 solver.cpp:357] Iteration 27300 (3.73531 iter/s, 26.7716s/100 iters), loss = 0.367928
I0818 02:21:02.793709 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367928 (* 1 = 0.367928 loss)
I0818 02:21:02.793720 15109 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0818 02:21:13.094354 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:21:29.348049 15109 solver.cpp:357] Iteration 27400 (3.76606 iter/s, 26.5529s/100 iters), loss = 0.300461
I0818 02:21:29.348127 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300461 (* 1 = 0.300461 loss)
I0818 02:21:29.348140 15109 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0818 02:21:55.819604 15109 solver.cpp:514] Iteration 27500, Testing net (#0)
I0818 02:22:21.153259 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:22:21.267895 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5834
I0818 02:22:21.267940 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.83169 (* 1 = 1.83169 loss)
I0818 02:22:21.484921 15109 solver.cpp:357] Iteration 27500 (1.91805 iter/s, 52.1362s/100 iters), loss = 0.440997
I0818 02:22:21.485002 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440997 (* 1 = 0.440997 loss)
I0818 02:22:21.485014 15109 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0818 02:22:48.296589 15109 solver.cpp:357] Iteration 27600 (3.72993 iter/s, 26.8101s/100 iters), loss = 0.438535
I0818 02:22:48.296939 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438535 (* 1 = 0.438535 loss)
I0818 02:22:48.297005 15109 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0818 02:23:14.938784 15109 solver.cpp:357] Iteration 27700 (3.75366 iter/s, 26.6407s/100 iters), loss = 0.59212
I0818 02:23:14.938868 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.59212 (* 1 = 0.59212 loss)
I0818 02:23:14.938880 15109 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0818 02:23:22.932559 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:23:41.495723 15109 solver.cpp:357] Iteration 27800 (3.76571 iter/s, 26.5554s/100 iters), loss = 0.40193
I0818 02:23:41.495797 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.40193 (* 1 = 0.40193 loss)
I0818 02:23:41.495810 15109 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0818 02:24:08.361907 15109 solver.cpp:357] Iteration 27900 (3.72236 iter/s, 26.8646s/100 iters), loss = 0.521533
I0818 02:24:08.362087 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.521533 (* 1 = 0.521533 loss)
I0818 02:24:08.362098 15109 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0818 02:24:34.739687 15109 solver.cpp:514] Iteration 28000, Testing net (#0)
I0818 02:25:00.035722 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:25:00.148681 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.593501
I0818 02:25:00.148753 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.61459 (* 1 = 1.61459 loss)
I0818 02:25:00.380313 15109 solver.cpp:357] Iteration 28000 (1.92243 iter/s, 52.0175s/100 iters), loss = 0.553063
I0818 02:25:00.380373 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.553063 (* 1 = 0.553063 loss)
I0818 02:25:00.380390 15109 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0818 02:25:26.909816 15109 solver.cpp:357] Iteration 28100 (3.76961 iter/s, 26.5279s/100 iters), loss = 0.325967
I0818 02:25:26.909899 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325967 (* 1 = 0.325967 loss)
I0818 02:25:26.909914 15109 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0818 02:25:32.240561 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:25:53.618784 15109 solver.cpp:357] Iteration 28200 (3.74428 iter/s, 26.7074s/100 iters), loss = 0.499235
I0818 02:25:53.618860 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499235 (* 1 = 0.499235 loss)
I0818 02:25:53.618870 15109 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0818 02:26:20.446997 15109 solver.cpp:357] Iteration 28300 (3.72735 iter/s, 26.8287s/100 iters), loss = 0.412624
I0818 02:26:20.447244 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412624 (* 1 = 0.412624 loss)
I0818 02:26:20.447259 15109 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0818 02:26:47.288105 15109 solver.cpp:357] Iteration 28400 (3.72584 iter/s, 26.8396s/100 iters), loss = 0.353919
I0818 02:26:47.288190 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353919 (* 1 = 0.353919 loss)
I0818 02:26:47.288203 15109 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0818 02:27:13.719635 15109 solver.cpp:514] Iteration 28500, Testing net (#0)
I0818 02:27:39.252710 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:27:39.363507 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6371
I0818 02:27:39.363554 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.25844 (* 1 = 1.25844 loss)
I0818 02:27:39.590217 15109 solver.cpp:357] Iteration 28500 (1.912 iter/s, 52.3011s/100 iters), loss = 0.377355
I0818 02:27:39.590296 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377355 (* 1 = 0.377355 loss)
I0818 02:27:39.590307 15109 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0818 02:27:42.591297 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:28:06.222205 15109 solver.cpp:357] Iteration 28600 (3.7551 iter/s, 26.6304s/100 iters), loss = 0.438676
I0818 02:28:06.222381 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438676 (* 1 = 0.438676 loss)
I0818 02:28:06.222394 15109 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0818 02:28:33.018683 15109 solver.cpp:357] Iteration 28700 (3.73205 iter/s, 26.7949s/100 iters), loss = 0.391732
I0818 02:28:33.018762 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.391732 (* 1 = 0.391732 loss)
I0818 02:28:33.018774 15109 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0818 02:28:59.510876 15109 solver.cpp:357] Iteration 28800 (3.77492 iter/s, 26.4906s/100 iters), loss = 0.366452
I0818 02:28:59.511065 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366452 (* 1 = 0.366452 loss)
I0818 02:28:59.511080 15109 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0818 02:29:26.184535 15109 solver.cpp:357] Iteration 28900 (3.74924 iter/s, 26.672s/100 iters), loss = 0.395321
I0818 02:29:26.184609 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395321 (* 1 = 0.395321 loss)
I0818 02:29:26.184620 15109 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0818 02:29:26.804288 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:29:52.785540 15109 solver.cpp:514] Iteration 29000, Testing net (#0)
I0818 02:30:18.271051 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:30:18.381332 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6126
I0818 02:30:18.381378 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.37231 (* 1 = 1.37231 loss)
I0818 02:30:18.539264 15109 solver.cpp:357] Iteration 29000 (1.91008 iter/s, 52.3537s/100 iters), loss = 0.51217
I0818 02:30:18.539312 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.51217 (* 1 = 0.51217 loss)
I0818 02:30:18.539326 15109 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0818 02:30:45.151599 15109 solver.cpp:357] Iteration 29100 (3.75759 iter/s, 26.6128s/100 iters), loss = 0.438353
I0818 02:30:45.151742 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438353 (* 1 = 0.438353 loss)
I0818 02:30:45.151757 15109 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0818 02:31:11.808974 15109 solver.cpp:357] Iteration 29200 (3.75153 iter/s, 26.6558s/100 iters), loss = 0.41183
I0818 02:31:11.809052 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41183 (* 1 = 0.41183 loss)
I0818 02:31:11.809064 15109 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0818 02:31:36.385318 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:31:38.465215 15109 solver.cpp:357] Iteration 29300 (3.75169 iter/s, 26.6546s/100 iters), loss = 0.37752
I0818 02:31:38.465281 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.37752 (* 1 = 0.37752 loss)
I0818 02:31:38.465291 15109 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0818 02:32:05.178895 15109 solver.cpp:357] Iteration 29400 (3.74364 iter/s, 26.712s/100 iters), loss = 0.327147
I0818 02:32:05.178983 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327147 (* 1 = 0.327147 loss)
I0818 02:32:05.178995 15109 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0818 02:32:31.464345 15109 solver.cpp:514] Iteration 29500, Testing net (#0)
I0818 02:32:57.136795 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:32:57.241436 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5629
I0818 02:32:57.241480 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.72692 (* 1 = 1.72692 loss)
I0818 02:32:57.466907 15109 solver.cpp:357] Iteration 29500 (1.91252 iter/s, 52.287s/100 iters), loss = 0.446159
I0818 02:32:57.466995 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446159 (* 1 = 0.446159 loss)
I0818 02:32:57.467010 15109 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0818 02:33:24.137545 15109 solver.cpp:357] Iteration 29600 (3.74967 iter/s, 26.669s/100 iters), loss = 0.312118
I0818 02:33:24.137694 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.312118 (* 1 = 0.312118 loss)
I0818 02:33:24.137708 15109 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0818 02:33:46.299875 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:33:50.864018 15109 solver.cpp:357] Iteration 29700 (3.74183 iter/s, 26.7249s/100 iters), loss = 0.268919
I0818 02:33:50.864099 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.268919 (* 1 = 0.268919 loss)
I0818 02:33:50.864111 15109 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0818 02:34:17.358685 15109 solver.cpp:357] Iteration 29800 (3.77458 iter/s, 26.493s/100 iters), loss = 0.393285
I0818 02:34:17.359060 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393285 (* 1 = 0.393285 loss)
I0818 02:34:17.359129 15109 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0818 02:34:43.963328 15109 solver.cpp:357] Iteration 29900 (3.75897 iter/s, 26.603s/100 iters), loss = 0.403198
I0818 02:34:43.963413 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403198 (* 1 = 0.403198 loss)
I0818 02:34:43.963426 15109 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0818 02:35:10.552445 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.caffemodel
I0818 02:35:10.568734 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.solverstate
I0818 02:35:10.572212 15109 solver.cpp:514] Iteration 30000, Testing net (#0)
I0818 02:35:36.008153 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:35:36.024848 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5919
I0818 02:35:36.024893 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.33731 (* 1 = 1.33731 loss)
I0818 02:35:36.161638 15109 solver.cpp:357] Iteration 30000 (1.91581 iter/s, 52.1972s/100 iters), loss = 0.29215
I0818 02:35:36.161720 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29215 (* 1 = 0.29215 loss)
I0818 02:35:36.161732 15109 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0818 02:35:56.040768 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:36:02.947304 15109 solver.cpp:357] Iteration 30100 (3.73331 iter/s, 26.7859s/100 iters), loss = 0.385593
I0818 02:36:02.947398 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385593 (* 1 = 0.385593 loss)
I0818 02:36:02.947412 15109 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0818 02:36:29.528017 15109 solver.cpp:357] Iteration 30200 (3.76236 iter/s, 26.5791s/100 iters), loss = 0.508118
I0818 02:36:29.528201 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.508118 (* 1 = 0.508118 loss)
I0818 02:36:29.528216 15109 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0818 02:36:56.131383 15109 solver.cpp:357] Iteration 30300 (3.75915 iter/s, 26.6017s/100 iters), loss = 0.486023
I0818 02:36:56.131462 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486023 (* 1 = 0.486023 loss)
I0818 02:36:56.131474 15109 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0818 02:37:23.082870 15109 solver.cpp:357] Iteration 30400 (3.7106 iter/s, 26.9499s/100 iters), loss = 0.256126
I0818 02:37:23.083106 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.256126 (* 1 = 0.256126 loss)
I0818 02:37:23.083118 15109 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0818 02:37:40.069281 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:37:49.384567 15109 solver.cpp:514] Iteration 30500, Testing net (#0)
I0818 02:38:15.047078 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:38:15.146194 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6687
I0818 02:38:15.146245 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30081 (* 1 = 1.30081 loss)
I0818 02:38:15.384835 15109 solver.cpp:357] Iteration 30500 (1.91201 iter/s, 52.3009s/100 iters), loss = 0.305538
I0818 02:38:15.384925 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305538 (* 1 = 0.305538 loss)
I0818 02:38:15.384939 15109 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0818 02:38:41.923393 15109 solver.cpp:357] Iteration 30600 (3.76834 iter/s, 26.5369s/100 iters), loss = 0.315478
I0818 02:38:41.923483 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.315478 (* 1 = 0.315478 loss)
I0818 02:38:41.923496 15109 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0818 02:39:08.514426 15109 solver.cpp:357] Iteration 30700 (3.7609 iter/s, 26.5894s/100 iters), loss = 0.379411
I0818 02:39:08.514616 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379411 (* 1 = 0.379411 loss)
I0818 02:39:08.514627 15109 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0818 02:39:35.337848 15109 solver.cpp:357] Iteration 30800 (3.72806 iter/s, 26.8236s/100 iters), loss = 0.299248
I0818 02:39:35.337924 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.299248 (* 1 = 0.299248 loss)
I0818 02:39:35.337934 15109 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0818 02:39:50.131813 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:40:02.054342 15109 solver.cpp:357] Iteration 30900 (3.74294 iter/s, 26.7169s/100 iters), loss = 0.517159
I0818 02:40:02.054419 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.517159 (* 1 = 0.517159 loss)
I0818 02:40:02.054431 15109 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0818 02:40:28.437284 15109 solver.cpp:514] Iteration 31000, Testing net (#0)
I0818 02:40:53.852721 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:40:53.963801 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.642601
I0818 02:40:53.963852 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.29665 (* 1 = 1.29665 loss)
I0818 02:40:54.190382 15109 solver.cpp:357] Iteration 31000 (1.9181 iter/s, 52.1349s/100 iters), loss = 0.399932
I0818 02:40:54.190469 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399932 (* 1 = 0.399932 loss)
I0818 02:40:54.190481 15109 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0818 02:41:20.911211 15109 solver.cpp:357] Iteration 31100 (3.74263 iter/s, 26.7192s/100 iters), loss = 0.495618
I0818 02:41:20.911357 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.495618 (* 1 = 0.495618 loss)
I0818 02:41:20.911367 15109 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0818 02:41:47.581522 15109 solver.cpp:357] Iteration 31200 (3.74944 iter/s, 26.6706s/100 iters), loss = 0.344748
I0818 02:41:47.581611 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344748 (* 1 = 0.344748 loss)
I0818 02:41:47.581625 15109 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0818 02:41:59.654472 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:42:14.211781 15109 solver.cpp:357] Iteration 31300 (3.75536 iter/s, 26.6286s/100 iters), loss = 0.300202
I0818 02:42:14.211856 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300202 (* 1 = 0.300202 loss)
I0818 02:42:14.211869 15109 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0818 02:42:40.874533 15109 solver.cpp:357] Iteration 31400 (3.75078 iter/s, 26.6611s/100 iters), loss = 0.462925
I0818 02:42:40.874646 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462925 (* 1 = 0.462925 loss)
I0818 02:42:40.874657 15109 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0818 02:43:07.284051 15109 solver.cpp:514] Iteration 31500, Testing net (#0)
I0818 02:43:32.945529 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:43:33.056033 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6149
I0818 02:43:33.056082 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.42525 (* 1 = 1.42525 loss)
I0818 02:43:33.284375 15109 solver.cpp:357] Iteration 31500 (1.90794 iter/s, 52.4126s/100 iters), loss = 0.443772
I0818 02:43:33.284451 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443772 (* 1 = 0.443772 loss)
I0818 02:43:33.284463 15109 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0818 02:43:59.883918 15109 solver.cpp:357] Iteration 31600 (3.75938 iter/s, 26.6001s/100 iters), loss = 0.428966
I0818 02:43:59.884001 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428966 (* 1 = 0.428966 loss)
I0818 02:43:59.884013 15109 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0818 02:44:09.538486 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:44:26.427784 15109 solver.cpp:357] Iteration 31700 (3.76728 iter/s, 26.5443s/100 iters), loss = 0.358952
I0818 02:44:26.427871 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358952 (* 1 = 0.358952 loss)
I0818 02:44:26.427886 15109 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0818 02:44:53.074532 15109 solver.cpp:357] Iteration 31800 (3.75275 iter/s, 26.6471s/100 iters), loss = 0.334576
I0818 02:44:53.074687 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334576 (* 1 = 0.334576 loss)
I0818 02:44:53.074699 15109 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0818 02:45:19.646490 15109 solver.cpp:357] Iteration 31900 (3.76333 iter/s, 26.5722s/100 iters), loss = 0.384207
I0818 02:45:19.646574 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384207 (* 1 = 0.384207 loss)
I0818 02:45:19.646587 15109 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0818 02:45:46.000821 15109 solver.cpp:514] Iteration 32000, Testing net (#0)
I0818 02:46:11.300357 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:46:11.416537 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.680199
I0818 02:46:11.416587 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02078 (* 1 = 1.02078 loss)
I0818 02:46:11.644909 15109 solver.cpp:357] Iteration 32000 (1.92305 iter/s, 52.0008s/100 iters), loss = 0.361301
I0818 02:46:11.644991 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361301 (* 1 = 0.361301 loss)
I0818 02:46:11.645002 15109 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0818 02:46:11.645009 15109 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0818 02:46:18.927115 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:46:38.431071 15109 solver.cpp:357] Iteration 32100 (3.73326 iter/s, 26.7862s/100 iters), loss = 0.270505
I0818 02:46:38.431159 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.270505 (* 1 = 0.270505 loss)
I0818 02:46:38.431171 15109 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0818 02:47:05.008357 15109 solver.cpp:357] Iteration 32200 (3.76261 iter/s, 26.5773s/100 iters), loss = 0.414747
I0818 02:47:05.008532 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414747 (* 1 = 0.414747 loss)
I0818 02:47:05.008544 15109 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0818 02:47:31.633165 15109 solver.cpp:357] Iteration 32300 (3.75591 iter/s, 26.6247s/100 iters), loss = 0.163694
I0818 02:47:31.633250 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.163694 (* 1 = 0.163694 loss)
I0818 02:47:31.633263 15109 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0818 02:47:58.372398 15109 solver.cpp:357] Iteration 32400 (3.73985 iter/s, 26.7391s/100 iters), loss = 0.234195
I0818 02:47:58.372663 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.234195 (* 1 = 0.234195 loss)
I0818 02:47:58.372676 15109 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0818 02:48:02.811862 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:48:24.765719 15109 solver.cpp:514] Iteration 32500, Testing net (#0)
I0818 02:48:50.133160 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:48:50.246572 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.882202
I0818 02:48:50.246619 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.359674 (* 1 = 0.359674 loss)
I0818 02:48:50.472681 15109 solver.cpp:357] Iteration 32500 (1.91932 iter/s, 52.1019s/100 iters), loss = 0.190019
I0818 02:48:50.472764 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.190019 (* 1 = 0.190019 loss)
I0818 02:48:50.472777 15109 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0818 02:49:17.124969 15109 solver.cpp:357] Iteration 32600 (3.75207 iter/s, 26.6519s/100 iters), loss = 0.206607
I0818 02:49:17.125047 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.206608 (* 1 = 0.206608 loss)
I0818 02:49:17.125059 15109 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0818 02:49:43.812371 15109 solver.cpp:357] Iteration 32700 (3.74714 iter/s, 26.687s/100 iters), loss = 0.252795
I0818 02:49:43.812530 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.252795 (* 1 = 0.252795 loss)
I0818 02:49:43.812541 15109 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0818 02:50:10.620611 15109 solver.cpp:357] Iteration 32800 (3.73003 iter/s, 26.8095s/100 iters), loss = 0.255712
I0818 02:50:10.620700 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.255712 (* 1 = 0.255712 loss)
I0818 02:50:10.620714 15109 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0818 02:50:12.769824 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:50:37.206413 15109 solver.cpp:357] Iteration 32900 (3.76147 iter/s, 26.5853s/100 iters), loss = 0.101597
I0818 02:50:37.206591 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101597 (* 1 = 0.101597 loss)
I0818 02:50:37.206604 15109 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0818 02:51:03.641343 15109 solver.cpp:514] Iteration 33000, Testing net (#0)
I0818 02:51:29.070080 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:51:29.168143 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.860501
I0818 02:51:29.168216 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.416306 (* 1 = 0.416306 loss)
I0818 02:51:29.402910 15109 solver.cpp:357] Iteration 33000 (1.9158 iter/s, 52.1975s/100 iters), loss = 0.202973
I0818 02:51:29.402972 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.202973 (* 1 = 0.202973 loss)
I0818 02:51:29.402987 15109 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0818 02:51:56.224977 15109 solver.cpp:357] Iteration 33100 (3.72836 iter/s, 26.8215s/100 iters), loss = 0.238045
I0818 02:51:56.225044 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.238045 (* 1 = 0.238045 loss)
I0818 02:51:56.225054 15109 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0818 02:52:22.520067 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:52:22.755089 15109 solver.cpp:357] Iteration 33200 (3.7691 iter/s, 26.5315s/100 iters), loss = 0.150135
I0818 02:52:22.755146 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150135 (* 1 = 0.150135 loss)
I0818 02:52:22.755157 15109 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0818 02:52:49.447090 15109 solver.cpp:357] Iteration 33300 (3.74654 iter/s, 26.6913s/100 iters), loss = 0.252305
I0818 02:52:49.447182 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.252305 (* 1 = 0.252305 loss)
I0818 02:52:49.447196 15109 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0818 02:53:16.170447 15109 solver.cpp:357] Iteration 33400 (3.74215 iter/s, 26.7226s/100 iters), loss = 0.144907
I0818 02:53:16.170785 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144907 (* 1 = 0.144907 loss)
I0818 02:53:16.170842 15109 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0818 02:53:42.618463 15109 solver.cpp:514] Iteration 33500, Testing net (#0)
I0818 02:54:08.145627 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:54:08.247822 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.842502
I0818 02:54:08.247875 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.469562 (* 1 = 0.469562 loss)
I0818 02:54:08.486162 15109 solver.cpp:357] Iteration 33500 (1.91138 iter/s, 52.3182s/100 iters), loss = 0.18275
I0818 02:54:08.486243 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.18275 (* 1 = 0.18275 loss)
I0818 02:54:08.486256 15109 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0818 02:54:32.659811 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:54:35.629081 15109 solver.cpp:357] Iteration 33600 (3.68431 iter/s, 27.1421s/100 iters), loss = 0.18843
I0818 02:54:35.629171 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.18843 (* 1 = 0.18843 loss)
I0818 02:54:35.629184 15109 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0818 02:55:02.812335 15109 solver.cpp:357] Iteration 33700 (3.67885 iter/s, 27.1824s/100 iters), loss = 0.208653
I0818 02:55:02.812477 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.208653 (* 1 = 0.208653 loss)
I0818 02:55:02.812491 15109 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0818 02:55:30.057389 15109 solver.cpp:357] Iteration 33800 (3.6705 iter/s, 27.2442s/100 iters), loss = 0.169162
I0818 02:55:30.057482 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169162 (* 1 = 0.169162 loss)
I0818 02:55:30.057497 15109 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0818 02:55:57.206743 15109 solver.cpp:357] Iteration 33900 (3.68345 iter/s, 27.1485s/100 iters), loss = 0.178494
I0818 02:55:57.206913 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.178494 (* 1 = 0.178494 loss)
I0818 02:55:57.206925 15109 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0818 02:56:19.035159 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:56:24.159698 15109 solver.cpp:514] Iteration 34000, Testing net (#0)
I0818 02:56:50.032299 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:56:50.138269 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.793201
I0818 02:56:50.138341 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.614379 (* 1 = 0.614379 loss)
I0818 02:56:50.361166 15109 solver.cpp:357] Iteration 34000 (1.88131 iter/s, 53.1546s/100 iters), loss = 0.110629
I0818 02:56:50.361253 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110629 (* 1 = 0.110629 loss)
I0818 02:56:50.361266 15109 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0818 02:57:17.536245 15109 solver.cpp:357] Iteration 34100 (3.67992 iter/s, 27.1745s/100 iters), loss = 0.181477
I0818 02:57:17.536339 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.181477 (* 1 = 0.181477 loss)
I0818 02:57:17.536352 15109 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0818 02:57:44.108321 15109 solver.cpp:357] Iteration 34200 (3.76349 iter/s, 26.5711s/100 iters), loss = 0.118669
I0818 02:57:44.108506 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118669 (* 1 = 0.118669 loss)
I0818 02:57:44.108520 15109 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0818 02:58:10.709759 15109 solver.cpp:357] Iteration 34300 (3.75934 iter/s, 26.6004s/100 iters), loss = 0.121109
I0818 02:58:10.709831 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121109 (* 1 = 0.121109 loss)
I0818 02:58:10.709843 15109 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0818 02:58:29.436096 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:58:37.532500 15109 solver.cpp:357] Iteration 34400 (3.72809 iter/s, 26.8234s/100 iters), loss = 0.2468
I0818 02:58:37.532582 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.2468 (* 1 = 0.2468 loss)
I0818 02:58:37.532594 15109 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0818 02:59:03.803912 15109 solver.cpp:514] Iteration 34500, Testing net (#0)
I0818 02:59:29.357184 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 02:59:29.461055 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.820102
I0818 02:59:29.461113 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.539034 (* 1 = 0.539034 loss)
I0818 02:59:29.694849 15109 solver.cpp:357] Iteration 34500 (1.91709 iter/s, 52.1624s/100 iters), loss = 0.160975
I0818 02:59:29.694907 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160975 (* 1 = 0.160975 loss)
I0818 02:59:29.694918 15109 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0818 02:59:56.331957 15109 solver.cpp:357] Iteration 34600 (3.75432 iter/s, 26.636s/100 iters), loss = 0.221895
I0818 02:59:56.332233 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.221895 (* 1 = 0.221895 loss)
I0818 02:59:56.332244 15109 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0818 03:00:23.093827 15109 solver.cpp:357] Iteration 34700 (3.73683 iter/s, 26.7607s/100 iters), loss = 0.166184
I0818 03:00:23.093906 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.166184 (* 1 = 0.166184 loss)
I0818 03:00:23.093919 15109 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0818 03:00:39.311888 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:00:49.764732 15109 solver.cpp:357] Iteration 34800 (3.74956 iter/s, 26.6698s/100 iters), loss = 0.175344
I0818 03:00:49.764819 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.175344 (* 1 = 0.175344 loss)
I0818 03:00:49.764832 15109 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0818 03:01:16.384261 15109 solver.cpp:357] Iteration 34900 (3.7568 iter/s, 26.6184s/100 iters), loss = 0.14225
I0818 03:01:16.384400 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14225 (* 1 = 0.14225 loss)
I0818 03:01:16.384413 15109 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0818 03:01:42.936553 15109 solver.cpp:514] Iteration 35000, Testing net (#0)
I0818 03:02:08.275172 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:02:08.308167 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7978
I0818 03:02:08.308233 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.594719 (* 1 = 0.594719 loss)
I0818 03:02:08.460124 15109 solver.cpp:357] Iteration 35000 (1.92028 iter/s, 52.0757s/100 iters), loss = 0.133268
I0818 03:02:08.460186 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.133268 (* 1 = 0.133268 loss)
I0818 03:02:08.460202 15109 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0818 03:02:35.248646 15109 solver.cpp:357] Iteration 35100 (3.7331 iter/s, 26.7874s/100 iters), loss = 0.220043
I0818 03:02:35.248729 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.220043 (* 1 = 0.220043 loss)
I0818 03:02:35.248742 15109 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0818 03:02:49.085556 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:03:01.821638 15109 solver.cpp:357] Iteration 35200 (3.76339 iter/s, 26.5718s/100 iters), loss = 0.182277
I0818 03:03:01.821723 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.182277 (* 1 = 0.182277 loss)
I0818 03:03:01.821738 15109 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0818 03:03:28.416919 15109 solver.cpp:357] Iteration 35300 (3.76024 iter/s, 26.5941s/100 iters), loss = 0.143965
I0818 03:03:28.417165 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143966 (* 1 = 0.143966 loss)
I0818 03:03:28.417179 15109 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0818 03:03:55.162940 15109 solver.cpp:357] Iteration 35400 (3.73879 iter/s, 26.7466s/100 iters), loss = 0.152354
I0818 03:03:55.163017 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.152354 (* 1 = 0.152354 loss)
I0818 03:03:55.163028 15109 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0818 03:04:21.467079 15109 solver.cpp:514] Iteration 35500, Testing net (#0)
I0818 03:04:47.081970 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:04:47.188068 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.799501
I0818 03:04:47.188125 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.598555 (* 1 = 0.598555 loss)
I0818 03:04:47.428118 15109 solver.cpp:357] Iteration 35500 (1.91333 iter/s, 52.2649s/100 iters), loss = 0.151694
I0818 03:04:47.428176 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.151694 (* 1 = 0.151694 loss)
I0818 03:04:47.428189 15109 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0818 03:04:58.591576 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:05:13.953497 15109 solver.cpp:357] Iteration 35600 (3.77015 iter/s, 26.5241s/100 iters), loss = 0.193137
I0818 03:05:13.953578 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.193137 (* 1 = 0.193137 loss)
I0818 03:05:13.953590 15109 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0818 03:05:40.779906 15109 solver.cpp:357] Iteration 35700 (3.72784 iter/s, 26.8252s/100 iters), loss = 0.187183
I0818 03:05:40.780086 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.187183 (* 1 = 0.187183 loss)
I0818 03:05:40.780099 15109 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0818 03:06:07.376502 15109 solver.cpp:357] Iteration 35800 (3.76006 iter/s, 26.5953s/100 iters), loss = 0.115042
I0818 03:06:07.376580 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.115042 (* 1 = 0.115042 loss)
I0818 03:06:07.376592 15109 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0818 03:06:33.890949 15109 solver.cpp:357] Iteration 35900 (3.77171 iter/s, 26.5132s/100 iters), loss = 0.164856
I0818 03:06:33.891301 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.164856 (* 1 = 0.164856 loss)
I0818 03:06:33.891371 15109 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0818 03:06:42.682427 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:07:00.463135 15109 solver.cpp:514] Iteration 36000, Testing net (#0)
I0818 03:07:25.975677 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:07:26.018227 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.786
I0818 03:07:26.018276 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.64478 (* 1 = 0.64478 loss)
I0818 03:07:26.165022 15109 solver.cpp:357] Iteration 36000 (1.91301 iter/s, 52.2737s/100 iters), loss = 0.242175
I0818 03:07:26.165104 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.242175 (* 1 = 0.242175 loss)
I0818 03:07:26.165117 15109 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0818 03:07:52.965883 15109 solver.cpp:357] Iteration 36100 (3.73116 iter/s, 26.8013s/100 iters), loss = 0.1272
I0818 03:07:52.965975 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.1272 (* 1 = 0.1272 loss)
I0818 03:07:52.965988 15109 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0818 03:08:19.507306 15109 solver.cpp:357] Iteration 36200 (3.76787 iter/s, 26.5402s/100 iters), loss = 0.0609685
I0818 03:08:19.507513 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0609685 (* 1 = 0.0609685 loss)
I0818 03:08:19.507529 15109 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0818 03:08:46.110100 15109 solver.cpp:357] Iteration 36300 (3.75919 iter/s, 26.6015s/100 iters), loss = 0.194938
I0818 03:08:46.110172 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.194938 (* 1 = 0.194938 loss)
I0818 03:08:46.110183 15109 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0818 03:08:52.589768 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:09:12.917570 15109 solver.cpp:357] Iteration 36400 (3.73019 iter/s, 26.8083s/100 iters), loss = 0.154821
I0818 03:09:12.917656 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.154821 (* 1 = 0.154821 loss)
I0818 03:09:12.917670 15109 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0818 03:09:39.335484 15109 solver.cpp:514] Iteration 36500, Testing net (#0)
I0818 03:10:04.926561 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:10:05.041908 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.811201
I0818 03:10:05.041954 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.575073 (* 1 = 0.575073 loss)
I0818 03:10:05.269647 15109 solver.cpp:357] Iteration 36500 (1.91016 iter/s, 52.3517s/100 iters), loss = 0.15511
I0818 03:10:05.269728 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.15511 (* 1 = 0.15511 loss)
I0818 03:10:05.269742 15109 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0818 03:10:31.885500 15109 solver.cpp:357] Iteration 36600 (3.75735 iter/s, 26.6145s/100 iters), loss = 0.126162
I0818 03:10:31.885649 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126162 (* 1 = 0.126162 loss)
I0818 03:10:31.885663 15109 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0818 03:10:58.594629 15109 solver.cpp:357] Iteration 36700 (3.74422 iter/s, 26.7078s/100 iters), loss = 0.0872509
I0818 03:10:58.594713 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.087251 (* 1 = 0.087251 loss)
I0818 03:10:58.594727 15109 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0818 03:11:02.405537 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:11:25.517637 15109 solver.cpp:357] Iteration 36800 (3.71423 iter/s, 26.9235s/100 iters), loss = 0.20102
I0818 03:11:25.517716 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.20102 (* 1 = 0.20102 loss)
I0818 03:11:25.517727 15109 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0818 03:11:52.129742 15109 solver.cpp:357] Iteration 36900 (3.75787 iter/s, 26.6108s/100 iters), loss = 0.116003
I0818 03:11:52.129922 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.116003 (* 1 = 0.116003 loss)
I0818 03:11:52.129935 15109 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0818 03:12:18.596385 15109 solver.cpp:514] Iteration 37000, Testing net (#0)
I0818 03:12:44.363706 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:12:44.463881 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.792399
I0818 03:12:44.463937 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.614206 (* 1 = 0.614206 loss)
I0818 03:12:44.703649 15109 solver.cpp:357] Iteration 37000 (1.9021 iter/s, 52.5734s/100 iters), loss = 0.195478
I0818 03:12:44.703735 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.195478 (* 1 = 0.195478 loss)
I0818 03:12:44.703748 15109 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0818 03:13:11.304728 15109 solver.cpp:357] Iteration 37100 (3.75943 iter/s, 26.5997s/100 iters), loss = 0.11562
I0818 03:13:11.304816 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11562 (* 1 = 0.11562 loss)
I0818 03:13:11.304829 15109 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0818 03:13:12.797350 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:13:38.187826 15109 solver.cpp:357] Iteration 37200 (3.71999 iter/s, 26.8818s/100 iters), loss = 0.113149
I0818 03:13:38.188009 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113149 (* 1 = 0.113149 loss)
I0818 03:13:38.188020 15109 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0818 03:14:04.787986 15109 solver.cpp:357] Iteration 37300 (3.75957 iter/s, 26.5988s/100 iters), loss = 0.144292
I0818 03:14:04.788066 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144292 (* 1 = 0.144292 loss)
I0818 03:14:04.788079 15109 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0818 03:14:31.330987 15109 solver.cpp:357] Iteration 37400 (3.76766 iter/s, 26.5417s/100 iters), loss = 0.179852
I0818 03:14:31.331269 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.179852 (* 1 = 0.179852 loss)
I0818 03:14:31.331282 15109 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0818 03:14:56.854423 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:14:57.992034 15109 solver.cpp:514] Iteration 37500, Testing net (#0)
I0818 03:15:23.454114 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:15:23.473415 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.840001
I0818 03:15:23.473462 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.467903 (* 1 = 0.467903 loss)
I0818 03:15:23.621196 15109 solver.cpp:357] Iteration 37500 (1.91242 iter/s, 52.2897s/100 iters), loss = 0.0944372
I0818 03:15:23.621271 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0944373 (* 1 = 0.0944373 loss)
I0818 03:15:23.621284 15109 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0818 03:15:50.527956 15109 solver.cpp:357] Iteration 37600 (3.71647 iter/s, 26.9072s/100 iters), loss = 0.14188
I0818 03:15:50.528031 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14188 (* 1 = 0.14188 loss)
I0818 03:15:50.528044 15109 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0818 03:16:17.177053 15109 solver.cpp:357] Iteration 37700 (3.75266 iter/s, 26.6477s/100 iters), loss = 0.205535
I0818 03:16:17.177381 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.205535 (* 1 = 0.205535 loss)
I0818 03:16:17.177443 15109 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0818 03:16:43.734362 15109 solver.cpp:357] Iteration 37800 (3.76564 iter/s, 26.5559s/100 iters), loss = 0.157928
I0818 03:16:43.734439 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.157928 (* 1 = 0.157928 loss)
I0818 03:16:43.734449 15109 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0818 03:17:06.904377 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:17:10.621114 15109 solver.cpp:357] Iteration 37900 (3.71949 iter/s, 26.8854s/100 iters), loss = 0.170791
I0818 03:17:10.621191 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170791 (* 1 = 0.170791 loss)
I0818 03:17:10.621202 15109 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0818 03:17:37.037014 15109 solver.cpp:514] Iteration 38000, Testing net (#0)
I0818 03:18:02.544667 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:18:02.657845 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8167
I0818 03:18:02.657891 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.567849 (* 1 = 0.567849 loss)
I0818 03:18:02.880928 15109 solver.cpp:357] Iteration 38000 (1.91359 iter/s, 52.2577s/100 iters), loss = 0.106992
I0818 03:18:02.881005 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106992 (* 1 = 0.106992 loss)
I0818 03:18:02.881017 15109 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0818 03:18:29.445416 15109 solver.cpp:357] Iteration 38100 (3.76475 iter/s, 26.5622s/100 iters), loss = 0.145208
I0818 03:18:29.445598 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145208 (* 1 = 0.145208 loss)
I0818 03:18:29.445611 15109 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0818 03:18:55.997840 15109 solver.cpp:357] Iteration 38200 (3.76646 iter/s, 26.5502s/100 iters), loss = 0.106827
I0818 03:18:55.997923 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106827 (* 1 = 0.106827 loss)
I0818 03:18:55.997936 15109 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0818 03:19:16.778458 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:19:22.828594 15109 solver.cpp:357] Iteration 38300 (3.72737 iter/s, 26.8286s/100 iters), loss = 0.137139
I0818 03:19:22.828680 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137139 (* 1 = 0.137139 loss)
I0818 03:19:22.828691 15109 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0818 03:19:49.489393 15109 solver.cpp:357] Iteration 38400 (3.75113 iter/s, 26.6586s/100 iters), loss = 0.11113
I0818 03:19:49.489615 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11113 (* 1 = 0.11113 loss)
I0818 03:19:49.489627 15109 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0818 03:20:15.811884 15109 solver.cpp:514] Iteration 38500, Testing net (#0)
I0818 03:20:41.510844 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:20:41.623806 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8042
I0818 03:20:41.623857 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.60799 (* 1 = 0.60799 loss)
I0818 03:20:41.847862 15109 solver.cpp:357] Iteration 38500 (1.90999 iter/s, 52.3564s/100 iters), loss = 0.121302
I0818 03:20:41.847947 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121302 (* 1 = 0.121302 loss)
I0818 03:20:41.847961 15109 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0818 03:21:08.438364 15109 solver.cpp:357] Iteration 38600 (3.76104 iter/s, 26.5884s/100 iters), loss = 0.0921125
I0818 03:21:08.438438 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0921126 (* 1 = 0.0921126 loss)
I0818 03:21:08.438450 15109 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0818 03:21:26.602334 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:21:35.260563 15109 solver.cpp:357] Iteration 38700 (3.72835 iter/s, 26.8215s/100 iters), loss = 0.122453
I0818 03:21:35.260655 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122453 (* 1 = 0.122453 loss)
I0818 03:21:35.260668 15109 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0818 03:22:01.843567 15109 solver.cpp:357] Iteration 38800 (3.76209 iter/s, 26.5809s/100 iters), loss = 0.0667867
I0818 03:22:01.843695 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0667867 (* 1 = 0.0667867 loss)
I0818 03:22:01.843709 15109 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0818 03:22:28.489744 15109 solver.cpp:357] Iteration 38900 (3.75317 iter/s, 26.6441s/100 iters), loss = 0.139313
I0818 03:22:28.489821 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.139313 (* 1 = 0.139313 loss)
I0818 03:22:28.489832 15109 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0818 03:22:55.233577 15109 solver.cpp:514] Iteration 39000, Testing net (#0)
I0818 03:23:20.744781 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:23:20.801753 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.837402
I0818 03:23:20.801797 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.501443 (* 1 = 0.501443 loss)
I0818 03:23:20.935801 15109 solver.cpp:357] Iteration 39000 (1.90679 iter/s, 52.4443s/100 iters), loss = 0.0965323
I0818 03:23:20.935884 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0965323 (* 1 = 0.0965323 loss)
I0818 03:23:20.935897 15109 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0818 03:23:36.414186 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:23:47.715795 15109 solver.cpp:357] Iteration 39100 (3.73422 iter/s, 26.7793s/100 iters), loss = 0.17137
I0818 03:23:47.715867 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.17137 (* 1 = 0.17137 loss)
I0818 03:23:47.715878 15109 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0818 03:24:14.343646 15109 solver.cpp:357] Iteration 39200 (3.75574 iter/s, 26.6259s/100 iters), loss = 0.18815
I0818 03:24:14.343838 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.18815 (* 1 = 0.18815 loss)
I0818 03:24:14.343852 15109 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0818 03:24:40.978163 15109 solver.cpp:357] Iteration 39300 (3.7548 iter/s, 26.6326s/100 iters), loss = 0.161918
I0818 03:24:40.978253 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.161918 (* 1 = 0.161918 loss)
I0818 03:24:40.978266 15109 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0818 03:25:07.730393 15109 solver.cpp:357] Iteration 39400 (3.73827 iter/s, 26.7503s/100 iters), loss = 0.0872813
I0818 03:25:07.730684 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0872813 (* 1 = 0.0872813 loss)
I0818 03:25:07.730697 15109 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0818 03:25:20.777133 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:25:34.049329 15109 solver.cpp:514] Iteration 39500, Testing net (#0)
I0818 03:25:59.718657 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:25:59.823482 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.841701
I0818 03:25:59.823539 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.498517 (* 1 = 0.498517 loss)
I0818 03:26:00.061125 15109 solver.cpp:357] Iteration 39500 (1.91098 iter/s, 52.3292s/100 iters), loss = 0.136098
I0818 03:26:00.061202 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136098 (* 1 = 0.136098 loss)
I0818 03:26:00.061215 15109 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0818 03:26:26.592051 15109 solver.cpp:357] Iteration 39600 (3.76945 iter/s, 26.5291s/100 iters), loss = 0.114308
I0818 03:26:26.592128 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114308 (* 1 = 0.114308 loss)
I0818 03:26:26.592139 15109 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0818 03:26:53.320020 15109 solver.cpp:357] Iteration 39700 (3.74166 iter/s, 26.7261s/100 iters), loss = 0.148805
I0818 03:26:53.320206 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.148805 (* 1 = 0.148805 loss)
I0818 03:26:53.320220 15109 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0818 03:27:19.854260 15109 solver.cpp:357] Iteration 39800 (3.76898 iter/s, 26.5324s/100 iters), loss = 0.0974471
I0818 03:27:19.854343 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0974471 (* 1 = 0.0974471 loss)
I0818 03:27:19.854357 15109 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0818 03:27:30.393451 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:27:46.449942 15109 solver.cpp:357] Iteration 39900 (3.76026 iter/s, 26.5939s/100 iters), loss = 0.0526497
I0818 03:27:46.450024 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0526497 (* 1 = 0.0526497 loss)
I0818 03:27:46.450037 15109 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0818 03:28:13.056790 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.caffemodel
I0818 03:28:13.068286 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.solverstate
I0818 03:28:13.071084 15109 solver.cpp:514] Iteration 40000, Testing net (#0)
I0818 03:28:38.443696 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:28:38.521590 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.847101
I0818 03:28:38.521636 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.480514 (* 1 = 0.480514 loss)
I0818 03:28:38.650696 15109 solver.cpp:357] Iteration 40000 (1.91573 iter/s, 52.1993s/100 iters), loss = 0.159682
I0818 03:28:38.650779 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.159682 (* 1 = 0.159682 loss)
I0818 03:28:38.650792 15109 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0818 03:29:05.453994 15109 solver.cpp:357] Iteration 40100 (3.73095 iter/s, 26.8028s/100 iters), loss = 0.139738
I0818 03:29:05.454259 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.139738 (* 1 = 0.139738 loss)
I0818 03:29:05.454293 15109 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0818 03:29:31.983412 15109 solver.cpp:357] Iteration 40200 (3.76965 iter/s, 26.5277s/100 iters), loss = 0.174723
I0818 03:29:31.983489 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.174723 (* 1 = 0.174723 loss)
I0818 03:29:31.983500 15109 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0818 03:29:40.035238 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:29:58.543488 15109 solver.cpp:357] Iteration 40300 (3.7653 iter/s, 26.5583s/100 iters), loss = 0.124322
I0818 03:29:58.543553 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.124322 (* 1 = 0.124322 loss)
I0818 03:29:58.543563 15109 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0818 03:30:25.152329 15109 solver.cpp:357] Iteration 40400 (3.7581 iter/s, 26.6092s/100 iters), loss = 0.15799
I0818 03:30:25.152482 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.15799 (* 1 = 0.15799 loss)
I0818 03:30:25.152495 15109 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0818 03:30:51.523403 15109 solver.cpp:514] Iteration 40500, Testing net (#0)
I0818 03:31:16.948024 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:31:17.062572 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.860402
I0818 03:31:17.062619 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.438457 (* 1 = 0.438457 loss)
I0818 03:31:17.285194 15109 solver.cpp:357] Iteration 40500 (1.91822 iter/s, 52.1316s/100 iters), loss = 0.166291
I0818 03:31:17.285279 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.166291 (* 1 = 0.166291 loss)
I0818 03:31:17.285291 15109 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0818 03:31:43.864502 15109 solver.cpp:357] Iteration 40600 (3.76256 iter/s, 26.5776s/100 iters), loss = 0.103362
I0818 03:31:43.864584 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103362 (* 1 = 0.103362 loss)
I0818 03:31:43.864596 15109 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0818 03:31:49.410661 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:32:10.618350 15109 solver.cpp:357] Iteration 40700 (3.73801 iter/s, 26.7522s/100 iters), loss = 0.152859
I0818 03:32:10.618434 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.152859 (* 1 = 0.152859 loss)
I0818 03:32:10.618448 15109 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0818 03:32:37.259138 15109 solver.cpp:357] Iteration 40800 (3.75388 iter/s, 26.6391s/100 iters), loss = 0.113455
I0818 03:32:37.259275 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113455 (* 1 = 0.113455 loss)
I0818 03:32:37.259289 15109 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0818 03:33:03.836544 15109 solver.cpp:357] Iteration 40900 (3.76284 iter/s, 26.5757s/100 iters), loss = 0.086666
I0818 03:33:03.836624 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0866661 (* 1 = 0.0866661 loss)
I0818 03:33:03.836637 15109 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0818 03:33:30.431355 15109 solver.cpp:514] Iteration 41000, Testing net (#0)
I0818 03:33:55.945600 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:33:56.053287 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.847801
I0818 03:33:56.053334 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.458983 (* 1 = 0.458983 loss)
I0818 03:33:56.230273 15109 solver.cpp:357] Iteration 41000 (1.90867 iter/s, 52.3926s/100 iters), loss = 0.0896637
I0818 03:33:56.230319 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0896638 (* 1 = 0.0896638 loss)
I0818 03:33:56.230331 15109 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0818 03:33:59.251060 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:34:22.918615 15109 solver.cpp:357] Iteration 41100 (3.74689 iter/s, 26.6888s/100 iters), loss = 0.138929
I0818 03:34:22.918756 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13893 (* 1 = 0.13893 loss)
I0818 03:34:22.918768 15109 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0818 03:34:49.563130 15109 solver.cpp:357] Iteration 41200 (3.75335 iter/s, 26.6428s/100 iters), loss = 0.0982587
I0818 03:34:49.563201 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0982588 (* 1 = 0.0982588 loss)
I0818 03:34:49.563212 15109 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0818 03:35:16.251327 15109 solver.cpp:357] Iteration 41300 (3.74721 iter/s, 26.6865s/100 iters), loss = 0.114839
I0818 03:35:16.251574 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114839 (* 1 = 0.114839 loss)
I0818 03:35:16.251586 15109 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0818 03:35:43.158954 15109 solver.cpp:357] Iteration 41400 (3.71665 iter/s, 26.906s/100 iters), loss = 0.146128
I0818 03:35:43.159034 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.146129 (* 1 = 0.146129 loss)
I0818 03:35:43.159044 15109 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0818 03:35:43.650014 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:36:09.476732 15109 solver.cpp:514] Iteration 41500, Testing net (#0)
I0818 03:36:34.902393 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:36:35.009948 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.854001
I0818 03:36:35.010015 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.445036 (* 1 = 0.445036 loss)
I0818 03:36:35.247825 15109 solver.cpp:357] Iteration 41500 (1.91984 iter/s, 52.0878s/100 iters), loss = 0.130038
I0818 03:36:35.247879 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130038 (* 1 = 0.130038 loss)
I0818 03:36:35.247891 15109 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0818 03:37:01.834141 15109 solver.cpp:357] Iteration 41600 (3.76157 iter/s, 26.5847s/100 iters), loss = 0.137384
I0818 03:37:01.834307 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137384 (* 1 = 0.137384 loss)
I0818 03:37:01.834321 15109 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0818 03:37:28.637276 15109 solver.cpp:357] Iteration 41700 (3.73113 iter/s, 26.8015s/100 iters), loss = 0.144295
I0818 03:37:28.637342 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144295 (* 1 = 0.144295 loss)
I0818 03:37:28.637351 15109 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0818 03:37:53.207891 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:37:55.304636 15109 solver.cpp:357] Iteration 41800 (3.74984 iter/s, 26.6678s/100 iters), loss = 0.153065
I0818 03:37:55.304726 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.153065 (* 1 = 0.153065 loss)
I0818 03:37:55.304739 15109 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0818 03:38:21.822360 15109 solver.cpp:357] Iteration 41900 (3.7713 iter/s, 26.5161s/100 iters), loss = 0.148083
I0818 03:38:21.822443 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.148083 (* 1 = 0.148083 loss)
I0818 03:38:21.822456 15109 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0818 03:38:48.342489 15109 solver.cpp:514] Iteration 42000, Testing net (#0)
I0818 03:39:13.718358 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:39:13.759721 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.839901
I0818 03:39:13.759768 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.524465 (* 1 = 0.524465 loss)
I0818 03:39:13.997660 15109 solver.cpp:357] Iteration 42000 (1.91665 iter/s, 52.1742s/100 iters), loss = 0.12391
I0818 03:39:13.997711 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12391 (* 1 = 0.12391 loss)
I0818 03:39:13.997723 15109 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0818 03:39:40.796110 15109 solver.cpp:357] Iteration 42100 (3.73179 iter/s, 26.7968s/100 iters), loss = 0.0784446
I0818 03:39:40.796288 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0784447 (* 1 = 0.0784447 loss)
I0818 03:39:40.796299 15109 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0818 03:40:02.877634 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:40:07.344363 15109 solver.cpp:357] Iteration 42200 (3.76696 iter/s, 26.5466s/100 iters), loss = 0.127474
I0818 03:40:07.344445 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127474 (* 1 = 0.127474 loss)
I0818 03:40:07.344458 15109 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0818 03:40:33.858830 15109 solver.cpp:357] Iteration 42300 (3.77176 iter/s, 26.5128s/100 iters), loss = 0.104417
I0818 03:40:33.859086 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104417 (* 1 = 0.104417 loss)
I0818 03:40:33.859098 15109 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0818 03:41:00.554774 15109 solver.cpp:357] Iteration 42400 (3.74611 iter/s, 26.6944s/100 iters), loss = 0.125778
I0818 03:41:00.554849 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125778 (* 1 = 0.125778 loss)
I0818 03:41:00.554860 15109 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0818 03:41:26.941365 15109 solver.cpp:514] Iteration 42500, Testing net (#0)
I0818 03:41:52.523018 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:41:52.621685 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.841501
I0818 03:41:52.621753 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.487113 (* 1 = 0.487113 loss)
I0818 03:41:52.860038 15109 solver.cpp:357] Iteration 42500 (1.91189 iter/s, 52.3042s/100 iters), loss = 0.0741352
I0818 03:41:52.860095 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0741353 (* 1 = 0.0741353 loss)
I0818 03:41:52.860107 15109 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0818 03:42:12.566476 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:42:19.519516 15109 solver.cpp:357] Iteration 42600 (3.75124 iter/s, 26.6579s/100 iters), loss = 0.15671
I0818 03:42:19.519606 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.15671 (* 1 = 0.15671 loss)
I0818 03:42:19.519620 15109 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0818 03:42:46.261768 15109 solver.cpp:357] Iteration 42700 (3.73962 iter/s, 26.7407s/100 iters), loss = 0.192033
I0818 03:42:46.261917 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.192033 (* 1 = 0.192033 loss)
I0818 03:42:46.261927 15109 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0818 03:43:13.102843 15109 solver.cpp:357] Iteration 42800 (3.72568 iter/s, 26.8407s/100 iters), loss = 0.101278
I0818 03:43:13.102941 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101278 (* 1 = 0.101278 loss)
I0818 03:43:13.102953 15109 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0818 03:43:39.622172 15109 solver.cpp:357] Iteration 42900 (3.77106 iter/s, 26.5177s/100 iters), loss = 0.0493716
I0818 03:43:39.622340 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0493717 (* 1 = 0.0493717 loss)
I0818 03:43:39.622352 15109 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0818 03:43:56.829311 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:44:06.094347 15109 solver.cpp:514] Iteration 43000, Testing net (#0)
I0818 03:44:31.628497 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:44:31.735134 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.878402
I0818 03:44:31.735188 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.371568 (* 1 = 0.371568 loss)
I0818 03:44:31.971546 15109 solver.cpp:357] Iteration 43000 (1.91028 iter/s, 52.3484s/100 iters), loss = 0.113639
I0818 03:44:31.971628 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113639 (* 1 = 0.113639 loss)
I0818 03:44:31.971642 15109 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0818 03:44:58.730043 15109 solver.cpp:357] Iteration 43100 (3.73735 iter/s, 26.7569s/100 iters), loss = 0.13881
I0818 03:44:58.730113 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13881 (* 1 = 0.13881 loss)
I0818 03:44:58.730123 15109 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0818 03:45:25.216694 15109 solver.cpp:357] Iteration 43200 (3.77572 iter/s, 26.485s/100 iters), loss = 0.132308
I0818 03:45:25.216861 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.132308 (* 1 = 0.132308 loss)
I0818 03:45:25.216873 15109 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0818 03:45:51.818320 15109 solver.cpp:357] Iteration 43300 (3.75939 iter/s, 26.6s/100 iters), loss = 0.084392
I0818 03:45:51.818405 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0843921 (* 1 = 0.0843921 loss)
I0818 03:45:51.818418 15109 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0818 03:46:06.815062 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:46:18.940675 15109 solver.cpp:357] Iteration 43400 (3.68721 iter/s, 27.1208s/100 iters), loss = 0.210952
I0818 03:46:18.940749 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.210952 (* 1 = 0.210952 loss)
I0818 03:46:18.940760 15109 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0818 03:46:45.842996 15109 solver.cpp:514] Iteration 43500, Testing net (#0)
I0818 03:47:12.089985 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:47:12.178639 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.866502
I0818 03:47:12.178702 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.403595 (* 1 = 0.403595 loss)
I0818 03:47:12.396648 15109 solver.cpp:357] Iteration 43500 (1.87066 iter/s, 53.4571s/100 iters), loss = 0.0880149
I0818 03:47:12.396730 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.088015 (* 1 = 0.088015 loss)
I0818 03:47:12.396744 15109 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0818 03:47:39.539167 15109 solver.cpp:357] Iteration 43600 (3.68446 iter/s, 27.141s/100 iters), loss = 0.152845
I0818 03:47:39.539304 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.152845 (* 1 = 0.152845 loss)
I0818 03:47:39.539317 15109 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0818 03:48:06.739151 15109 solver.cpp:357] Iteration 43700 (3.67668 iter/s, 27.1984s/100 iters), loss = 0.105
I0818 03:48:06.739234 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.105 (* 1 = 0.105 loss)
I0818 03:48:06.739248 15109 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0818 03:48:19.033967 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:48:33.882062 15109 solver.cpp:357] Iteration 43800 (3.6844 iter/s, 27.1414s/100 iters), loss = 0.118091
I0818 03:48:33.882153 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118091 (* 1 = 0.118091 loss)
I0818 03:48:33.882166 15109 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0818 03:49:00.946506 15109 solver.cpp:357] Iteration 43900 (3.69509 iter/s, 27.0629s/100 iters), loss = 0.129526
I0818 03:49:00.946635 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129526 (* 1 = 0.129526 loss)
I0818 03:49:00.946647 15109 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0818 03:49:27.559367 15109 solver.cpp:514] Iteration 44000, Testing net (#0)
I0818 03:49:52.983943 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:49:53.034765 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.867001
I0818 03:49:53.034821 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.427166 (* 1 = 0.427166 loss)
I0818 03:49:53.247303 15109 solver.cpp:357] Iteration 44000 (1.91205 iter/s, 52.2998s/100 iters), loss = 0.113144
I0818 03:49:53.247364 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113144 (* 1 = 0.113144 loss)
I0818 03:49:53.247376 15109 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0818 03:50:20.154844 15109 solver.cpp:357] Iteration 44100 (3.71665 iter/s, 26.906s/100 iters), loss = 0.151804
I0818 03:50:20.154939 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.151804 (* 1 = 0.151804 loss)
I0818 03:50:20.154953 15109 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0818 03:50:29.740425 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:50:46.704615 15109 solver.cpp:357] Iteration 44200 (3.76673 iter/s, 26.5482s/100 iters), loss = 0.0769854
I0818 03:50:46.704689 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0769855 (* 1 = 0.0769855 loss)
I0818 03:50:46.704699 15109 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0818 03:51:13.354995 15109 solver.cpp:357] Iteration 44300 (3.75251 iter/s, 26.6488s/100 iters), loss = 0.111445
I0818 03:51:13.355168 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.111445 (* 1 = 0.111445 loss)
I0818 03:51:13.355182 15109 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0818 03:51:40.134403 15109 solver.cpp:357] Iteration 44400 (3.73443 iter/s, 26.7779s/100 iters), loss = 0.131444
I0818 03:51:40.134490 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.131444 (* 1 = 0.131444 loss)
I0818 03:51:40.134503 15109 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0818 03:52:06.407624 15109 solver.cpp:514] Iteration 44500, Testing net (#0)
I0818 03:52:31.913745 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:52:32.026435 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.884101
I0818 03:52:32.026484 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.359438 (* 1 = 0.359438 loss)
I0818 03:52:32.251509 15109 solver.cpp:357] Iteration 44500 (1.91882 iter/s, 52.1154s/100 iters), loss = 0.0444174
I0818 03:52:32.251596 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0444175 (* 1 = 0.0444175 loss)
I0818 03:52:32.251610 15109 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0818 03:52:39.454871 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:52:58.818748 15109 solver.cpp:357] Iteration 44600 (3.76435 iter/s, 26.565s/100 iters), loss = 0.126275
I0818 03:52:58.818833 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126275 (* 1 = 0.126275 loss)
I0818 03:52:58.818846 15109 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0818 03:53:25.519237 15109 solver.cpp:357] Iteration 44700 (3.74555 iter/s, 26.6983s/100 iters), loss = 0.107746
I0818 03:53:25.519390 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107746 (* 1 = 0.107746 loss)
I0818 03:53:25.519400 15109 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0818 03:53:52.268219 15109 solver.cpp:357] Iteration 44800 (3.73848 iter/s, 26.7488s/100 iters), loss = 0.0892842
I0818 03:53:52.268296 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0892842 (* 1 = 0.0892842 loss)
I0818 03:53:52.268307 15109 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0818 03:54:18.856778 15109 solver.cpp:357] Iteration 44900 (3.76131 iter/s, 26.5865s/100 iters), loss = 0.152386
I0818 03:54:18.856947 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.152386 (* 1 = 0.152386 loss)
I0818 03:54:18.856959 15109 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0818 03:54:23.603937 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:54:45.359973 15109 solver.cpp:514] Iteration 45000, Testing net (#0)
I0818 03:55:10.935293 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:55:11.043910 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.861601
I0818 03:55:11.043962 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.44528 (* 1 = 0.44528 loss)
I0818 03:55:11.273686 15109 solver.cpp:357] Iteration 45000 (1.90785 iter/s, 52.415s/100 iters), loss = 0.0857147
I0818 03:55:11.273769 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0857147 (* 1 = 0.0857147 loss)
I0818 03:55:11.273782 15109 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0818 03:55:37.965535 15109 solver.cpp:357] Iteration 45100 (3.74675 iter/s, 26.6898s/100 iters), loss = 0.126295
I0818 03:55:37.965600 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126295 (* 1 = 0.126295 loss)
I0818 03:55:37.965610 15109 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0818 03:56:04.657989 15109 solver.cpp:357] Iteration 45200 (3.74637 iter/s, 26.6925s/100 iters), loss = 0.169355
I0818 03:56:04.658166 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169355 (* 1 = 0.169355 loss)
I0818 03:56:04.658179 15109 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0818 03:56:31.241995 15109 solver.cpp:357] Iteration 45300 (3.76194 iter/s, 26.582s/100 iters), loss = 0.123325
I0818 03:56:31.242074 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.123325 (* 1 = 0.123325 loss)
I0818 03:56:31.242086 15109 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0818 03:56:33.375133 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:56:57.909946 15109 solver.cpp:357] Iteration 45400 (3.7501 iter/s, 26.666s/100 iters), loss = 0.0684727
I0818 03:56:57.910171 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0684727 (* 1 = 0.0684727 loss)
I0818 03:56:57.910183 15109 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0818 03:57:24.491554 15109 solver.cpp:514] Iteration 45500, Testing net (#0)
I0818 03:57:49.831899 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:57:49.924906 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.853101
I0818 03:57:49.924974 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.478525 (* 1 = 0.478525 loss)
I0818 03:57:50.165390 15109 solver.cpp:357] Iteration 45500 (1.91369 iter/s, 52.2551s/100 iters), loss = 0.0773183
I0818 03:57:50.165446 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0773184 (* 1 = 0.0773184 loss)
I0818 03:57:50.165457 15109 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0818 03:58:16.790616 15109 solver.cpp:357] Iteration 45600 (3.75611 iter/s, 26.6233s/100 iters), loss = 0.0932712
I0818 03:58:16.790707 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0932712 (* 1 = 0.0932712 loss)
I0818 03:58:16.790721 15109 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0818 03:58:43.053256 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 03:58:43.288449 15109 solver.cpp:357] Iteration 45700 (3.77416 iter/s, 26.4959s/100 iters), loss = 0.108156
I0818 03:58:43.288508 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108156 (* 1 = 0.108156 loss)
I0818 03:58:43.288520 15109 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0818 03:59:10.031961 15109 solver.cpp:357] Iteration 45800 (3.73949 iter/s, 26.7416s/100 iters), loss = 0.169973
I0818 03:59:10.032040 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169974 (* 1 = 0.169974 loss)
I0818 03:59:10.032053 15109 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0818 03:59:36.567389 15109 solver.cpp:357] Iteration 45900 (3.76882 iter/s, 26.5335s/100 iters), loss = 0.126045
I0818 03:59:36.567589 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126045 (* 1 = 0.126045 loss)
I0818 03:59:36.567605 15109 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0818 04:00:02.950619 15109 solver.cpp:514] Iteration 46000, Testing net (#0)
I0818 04:00:28.361110 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:00:28.467619 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.868501
I0818 04:00:28.467686 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.429792 (* 1 = 0.429792 loss)
I0818 04:00:28.705858 15109 solver.cpp:357] Iteration 46000 (1.91803 iter/s, 52.1369s/100 iters), loss = 0.156029
I0818 04:00:28.705914 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156029 (* 1 = 0.156029 loss)
I0818 04:00:28.705930 15109 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0818 04:00:52.529016 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:00:55.524858 15109 solver.cpp:357] Iteration 46100 (3.72896 iter/s, 26.8171s/100 iters), loss = 0.137174
I0818 04:00:55.524926 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137174 (* 1 = 0.137174 loss)
I0818 04:00:55.524935 15109 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0818 04:01:22.194970 15109 solver.cpp:357] Iteration 46200 (3.74948 iter/s, 26.6703s/100 iters), loss = 0.094351
I0818 04:01:22.195147 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0943511 (* 1 = 0.0943511 loss)
I0818 04:01:22.195161 15109 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0818 04:01:48.740506 15109 solver.cpp:357] Iteration 46300 (3.76738 iter/s, 26.5437s/100 iters), loss = 0.091787
I0818 04:01:48.740593 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0917871 (* 1 = 0.0917871 loss)
I0818 04:01:48.740605 15109 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0818 04:02:15.442112 15109 solver.cpp:357] Iteration 46400 (3.74535 iter/s, 26.6998s/100 iters), loss = 0.193842
I0818 04:02:15.442317 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.193842 (* 1 = 0.193842 loss)
I0818 04:02:15.442327 15109 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0818 04:02:36.852740 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:02:41.857331 15109 solver.cpp:514] Iteration 46500, Testing net (#0)
I0818 04:03:07.296283 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:03:07.398914 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.849701
I0818 04:03:07.398983 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.486968 (* 1 = 0.486968 loss)
I0818 04:03:07.639154 15109 solver.cpp:357] Iteration 46500 (1.9158 iter/s, 52.1975s/100 iters), loss = 0.0505995
I0818 04:03:07.639210 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0505996 (* 1 = 0.0505996 loss)
I0818 04:03:07.639226 15109 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0818 04:03:34.276361 15109 solver.cpp:357] Iteration 46600 (3.7544 iter/s, 26.6354s/100 iters), loss = 0.150955
I0818 04:03:34.276448 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150955 (* 1 = 0.150955 loss)
I0818 04:03:34.276463 15109 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0818 04:04:00.868363 15109 solver.cpp:357] Iteration 46700 (3.76079 iter/s, 26.5902s/100 iters), loss = 0.0784622
I0818 04:04:00.868542 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0784622 (* 1 = 0.0784622 loss)
I0818 04:04:00.868556 15109 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0818 04:04:27.627058 15109 solver.cpp:357] Iteration 46800 (3.73736 iter/s, 26.7569s/100 iters), loss = 0.0651657
I0818 04:04:27.627141 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0651657 (* 1 = 0.0651657 loss)
I0818 04:04:27.627156 15109 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0818 04:04:46.298090 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:04:54.114769 15109 solver.cpp:357] Iteration 46900 (3.77559 iter/s, 26.4859s/100 iters), loss = 0.185003
I0818 04:04:54.114843 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.185003 (* 1 = 0.185003 loss)
I0818 04:04:54.114856 15109 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0818 04:05:20.474133 15109 solver.cpp:514] Iteration 47000, Testing net (#0)
I0818 04:05:45.957816 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:05:46.067776 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.845801
I0818 04:05:46.067821 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.524445 (* 1 = 0.524445 loss)
I0818 04:05:46.292304 15109 solver.cpp:357] Iteration 47000 (1.91658 iter/s, 52.1762s/100 iters), loss = 0.0725575
I0818 04:05:46.292379 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0725575 (* 1 = 0.0725575 loss)
I0818 04:05:46.292392 15109 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0818 04:06:12.991757 15109 solver.cpp:357] Iteration 47100 (3.74565 iter/s, 26.6977s/100 iters), loss = 0.135863
I0818 04:06:12.991873 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.135864 (* 1 = 0.135864 loss)
I0818 04:06:12.991883 15109 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0818 04:06:39.793594 15109 solver.cpp:357] Iteration 47200 (3.7311 iter/s, 26.8017s/100 iters), loss = 0.0969047
I0818 04:06:39.793668 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0969047 (* 1 = 0.0969047 loss)
I0818 04:06:39.793679 15109 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0818 04:06:55.984117 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:07:06.428498 15109 solver.cpp:357] Iteration 47300 (3.75443 iter/s, 26.6352s/100 iters), loss = 0.130963
I0818 04:07:06.428577 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130963 (* 1 = 0.130963 loss)
I0818 04:07:06.428589 15109 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0818 04:07:32.970551 15109 solver.cpp:357] Iteration 47400 (3.76786 iter/s, 26.5403s/100 iters), loss = 0.112896
I0818 04:07:32.970830 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112896 (* 1 = 0.112896 loss)
I0818 04:07:32.970844 15109 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0818 04:07:59.498210 15109 solver.cpp:514] Iteration 47500, Testing net (#0)
I0818 04:08:24.973253 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:08:25.082847 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.870502
I0818 04:08:25.082916 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.410213 (* 1 = 0.410213 loss)
I0818 04:08:25.313113 15109 solver.cpp:357] Iteration 47500 (1.91054 iter/s, 52.3413s/100 iters), loss = 0.128656
I0818 04:08:25.313172 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.128656 (* 1 = 0.128656 loss)
I0818 04:08:25.313184 15109 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0818 04:08:51.972915 15109 solver.cpp:357] Iteration 47600 (3.75121 iter/s, 26.658s/100 iters), loss = 0.139937
I0818 04:08:51.972995 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.139937 (* 1 = 0.139937 loss)
I0818 04:08:51.973008 15109 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0818 04:09:05.815887 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:09:18.542668 15109 solver.cpp:357] Iteration 47700 (3.76392 iter/s, 26.568s/100 iters), loss = 0.132826
I0818 04:09:18.542754 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.132826 (* 1 = 0.132826 loss)
I0818 04:09:18.542768 15109 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0818 04:09:45.295321 15109 solver.cpp:357] Iteration 47800 (3.73819 iter/s, 26.7509s/100 iters), loss = 0.154114
I0818 04:09:45.295495 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.154114 (* 1 = 0.154114 loss)
I0818 04:09:45.295506 15109 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0818 04:10:12.015677 15109 solver.cpp:357] Iteration 47900 (3.74243 iter/s, 26.7206s/100 iters), loss = 0.130993
I0818 04:10:12.015753 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130993 (* 1 = 0.130993 loss)
I0818 04:10:12.015764 15109 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0818 04:10:38.322772 15109 solver.cpp:514] Iteration 48000, Testing net (#0)
I0818 04:11:03.763128 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:11:03.858007 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.787301
I0818 04:11:03.858073 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.823723 (* 1 = 0.823723 loss)
I0818 04:11:04.096364 15109 solver.cpp:357] Iteration 48000 (1.92014 iter/s, 52.0794s/100 iters), loss = 0.143642
I0818 04:11:04.096416 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143642 (* 1 = 0.143642 loss)
I0818 04:11:04.096428 15109 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0818 04:11:04.096436 15109 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0818 04:11:15.260349 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:11:30.650805 15109 solver.cpp:357] Iteration 48100 (3.76609 iter/s, 26.5527s/100 iters), loss = 0.11775
I0818 04:11:30.650877 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11775 (* 1 = 0.11775 loss)
I0818 04:11:30.650888 15109 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0818 04:11:57.488605 15109 solver.cpp:357] Iteration 48200 (3.72633 iter/s, 26.8361s/100 iters), loss = 0.110257
I0818 04:11:57.488790 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110257 (* 1 = 0.110257 loss)
I0818 04:11:57.488803 15109 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0818 04:12:24.053390 15109 solver.cpp:357] Iteration 48300 (3.76462 iter/s, 26.5631s/100 iters), loss = 0.0341504
I0818 04:12:24.053472 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0341504 (* 1 = 0.0341504 loss)
I0818 04:12:24.053484 15109 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0818 04:12:50.617884 15109 solver.cpp:357] Iteration 48400 (3.76467 iter/s, 26.5628s/100 iters), loss = 0.0620994
I0818 04:12:50.618150 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0620994 (* 1 = 0.0620994 loss)
I0818 04:12:50.618168 15109 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0818 04:12:59.480914 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:13:17.176494 15109 solver.cpp:514] Iteration 48500, Testing net (#0)
I0818 04:13:42.766968 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:13:42.815485 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.908302
I0818 04:13:42.815544 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.296641 (* 1 = 0.296641 loss)
I0818 04:13:43.002971 15109 solver.cpp:357] Iteration 48500 (1.90898 iter/s, 52.3839s/100 iters), loss = 0.101688
I0818 04:13:43.003038 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101688 (* 1 = 0.101688 loss)
I0818 04:13:43.003051 15109 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0818 04:14:09.816396 15109 solver.cpp:357] Iteration 48600 (3.72971 iter/s, 26.8117s/100 iters), loss = 0.120188
I0818 04:14:09.816476 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120188 (* 1 = 0.120188 loss)
I0818 04:14:09.816489 15109 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0818 04:14:36.399690 15109 solver.cpp:357] Iteration 48700 (3.762 iter/s, 26.5816s/100 iters), loss = 0.0429365
I0818 04:14:36.399842 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0429365 (* 1 = 0.0429365 loss)
I0818 04:14:36.399855 15109 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0818 04:15:02.885670 15109 solver.cpp:357] Iteration 48800 (3.77583 iter/s, 26.4843s/100 iters), loss = 0.0892962
I0818 04:15:02.885752 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0892962 (* 1 = 0.0892962 loss)
I0818 04:15:02.885764 15109 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0818 04:15:09.324169 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:15:29.646410 15109 solver.cpp:357] Iteration 48900 (3.73706 iter/s, 26.759s/100 iters), loss = 0.0404351
I0818 04:15:29.646500 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0404351 (* 1 = 0.0404351 loss)
I0818 04:15:29.646513 15109 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0818 04:15:55.988144 15109 solver.cpp:514] Iteration 49000, Testing net (#0)
I0818 04:16:21.446705 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:16:21.559244 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919103
I0818 04:16:21.559293 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.259738 (* 1 = 0.259738 loss)
I0818 04:16:21.785954 15109 solver.cpp:357] Iteration 49000 (1.91797 iter/s, 52.1383s/100 iters), loss = 0.0352603
I0818 04:16:21.786031 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0352603 (* 1 = 0.0352603 loss)
I0818 04:16:21.786044 15109 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0818 04:16:48.381568 15109 solver.cpp:357] Iteration 49100 (3.76025 iter/s, 26.5939s/100 iters), loss = 0.0781116
I0818 04:16:48.381709 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0781116 (* 1 = 0.0781116 loss)
I0818 04:16:48.381721 15109 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0818 04:17:15.212967 15109 solver.cpp:357] Iteration 49200 (3.72721 iter/s, 26.8297s/100 iters), loss = 0.0469326
I0818 04:17:15.213044 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0469326 (* 1 = 0.0469326 loss)
I0818 04:17:15.213054 15109 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0818 04:17:18.826532 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:17:41.635795 15109 solver.cpp:357] Iteration 49300 (3.78485 iter/s, 26.4211s/100 iters), loss = 0.0867951
I0818 04:17:41.635870 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0867951 (* 1 = 0.0867951 loss)
I0818 04:17:41.635882 15109 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0818 04:18:08.178025 15109 solver.cpp:357] Iteration 49400 (3.76753 iter/s, 26.5426s/100 iters), loss = 0.068595
I0818 04:18:08.178246 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.068595 (* 1 = 0.068595 loss)
I0818 04:18:08.178259 15109 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0818 04:18:34.662545 15109 solver.cpp:514] Iteration 49500, Testing net (#0)
I0818 04:19:00.206598 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:19:00.311765 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.913402
I0818 04:19:00.311813 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.26992 (* 1 = 0.26992 loss)
I0818 04:19:00.427691 15109 solver.cpp:357] Iteration 49500 (1.91393 iter/s, 52.2485s/100 iters), loss = 0.0806512
I0818 04:19:00.427736 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0806512 (* 1 = 0.0806512 loss)
I0818 04:19:00.427745 15109 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0818 04:19:27.249706 15109 solver.cpp:357] Iteration 49600 (3.72822 iter/s, 26.8224s/100 iters), loss = 0.0718129
I0818 04:19:27.249794 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0718129 (* 1 = 0.0718129 loss)
I0818 04:19:27.249809 15109 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0818 04:19:28.508749 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:19:53.818934 15109 solver.cpp:357] Iteration 49700 (3.76399 iter/s, 26.5676s/100 iters), loss = 0.0589751
I0818 04:19:53.819149 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0589751 (* 1 = 0.0589751 loss)
I0818 04:19:53.819164 15109 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0818 04:20:20.449352 15109 solver.cpp:357] Iteration 49800 (3.75534 iter/s, 26.6288s/100 iters), loss = 0.0794018
I0818 04:20:20.449434 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0794018 (* 1 = 0.0794018 loss)
I0818 04:20:20.449445 15109 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0818 04:20:47.227210 15109 solver.cpp:357] Iteration 49900 (3.73466 iter/s, 26.7762s/100 iters), loss = 0.0573673
I0818 04:20:47.227401 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0573673 (* 1 = 0.0573673 loss)
I0818 04:20:47.227414 15109 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0818 04:21:12.516574 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:21:13.504055 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.caffemodel
I0818 04:21:13.516113 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.solverstate
I0818 04:21:13.518993 15109 solver.cpp:514] Iteration 50000, Testing net (#0)
I0818 04:21:38.893534 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:21:39.008419 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918703
I0818 04:21:39.008469 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.251207 (* 1 = 0.251207 loss)
I0818 04:21:39.245134 15109 solver.cpp:357] Iteration 50000 (1.92246 iter/s, 52.0167s/100 iters), loss = 0.0439468
I0818 04:21:39.245206 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0439468 (* 1 = 0.0439468 loss)
I0818 04:21:39.245218 15109 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0818 04:22:05.734688 15109 solver.cpp:357] Iteration 50100 (3.77505 iter/s, 26.4897s/100 iters), loss = 0.0282306
I0818 04:22:05.734766 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0282306 (* 1 = 0.0282306 loss)
I0818 04:22:05.734776 15109 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0818 04:22:32.619063 15109 solver.cpp:357] Iteration 50200 (3.71987 iter/s, 26.8827s/100 iters), loss = 0.0568475
I0818 04:22:32.619206 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0568475 (* 1 = 0.0568475 loss)
I0818 04:22:32.619220 15109 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0818 04:22:59.239341 15109 solver.cpp:357] Iteration 50300 (3.75677 iter/s, 26.6186s/100 iters), loss = 0.0677378
I0818 04:22:59.239425 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0677378 (* 1 = 0.0677378 loss)
I0818 04:22:59.239441 15109 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0818 04:23:22.138298 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:23:25.803473 15109 solver.cpp:357] Iteration 50400 (3.76471 iter/s, 26.5624s/100 iters), loss = 0.108296
I0818 04:23:25.803568 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108296 (* 1 = 0.108296 loss)
I0818 04:23:25.803580 15109 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0818 04:23:52.414477 15109 solver.cpp:514] Iteration 50500, Testing net (#0)
I0818 04:24:17.851325 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:24:17.903178 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.916803
I0818 04:24:17.903237 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.260281 (* 1 = 0.260281 loss)
I0818 04:24:18.125099 15109 solver.cpp:357] Iteration 50500 (1.9113 iter/s, 52.3205s/100 iters), loss = 0.0600496
I0818 04:24:18.125155 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0600496 (* 1 = 0.0600496 loss)
I0818 04:24:18.125169 15109 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0818 04:24:44.922785 15109 solver.cpp:357] Iteration 50600 (3.7319 iter/s, 26.796s/100 iters), loss = 0.0243689
I0818 04:24:44.923133 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0243689 (* 1 = 0.0243689 loss)
I0818 04:24:44.923198 15109 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0818 04:25:11.405433 15109 solver.cpp:357] Iteration 50700 (3.7763 iter/s, 26.481s/100 iters), loss = 0.0312946
I0818 04:25:11.405517 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0312946 (* 1 = 0.0312946 loss)
I0818 04:25:11.405530 15109 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0818 04:25:31.897313 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:25:37.962503 15109 solver.cpp:357] Iteration 50800 (3.76559 iter/s, 26.5563s/100 iters), loss = 0.0445719
I0818 04:25:37.962577 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0445719 (* 1 = 0.0445719 loss)
I0818 04:25:37.962589 15109 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0818 04:26:04.750327 15109 solver.cpp:357] Iteration 50900 (3.73298 iter/s, 26.7882s/100 iters), loss = 0.0340747
I0818 04:26:04.750506 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0340747 (* 1 = 0.0340747 loss)
I0818 04:26:04.750520 15109 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0818 04:26:31.154886 15109 solver.cpp:514] Iteration 51000, Testing net (#0)
I0818 04:26:56.742607 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:26:56.855063 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920103
I0818 04:26:56.855110 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.263381 (* 1 = 0.263381 loss)
I0818 04:26:57.079715 15109 solver.cpp:357] Iteration 51000 (1.91111 iter/s, 52.3257s/100 iters), loss = 0.044219
I0818 04:26:57.079795 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.044219 (* 1 = 0.044219 loss)
I0818 04:26:57.079807 15109 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0818 04:27:23.690593 15109 solver.cpp:357] Iteration 51100 (3.75829 iter/s, 26.6079s/100 iters), loss = 0.030572
I0818 04:27:23.690672 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.030572 (* 1 = 0.030572 loss)
I0818 04:27:23.690685 15109 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0818 04:27:41.627255 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:27:50.374496 15109 solver.cpp:357] Iteration 51200 (3.74799 iter/s, 26.6809s/100 iters), loss = 0.0387442
I0818 04:27:50.374572 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0387442 (* 1 = 0.0387442 loss)
I0818 04:27:50.374583 15109 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0818 04:28:17.151697 15109 solver.cpp:357] Iteration 51300 (3.73463 iter/s, 26.7764s/100 iters), loss = 0.030084
I0818 04:28:17.151911 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.030084 (* 1 = 0.030084 loss)
I0818 04:28:17.151924 15109 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0818 04:28:43.771561 15109 solver.cpp:357] Iteration 51400 (3.757 iter/s, 26.617s/100 iters), loss = 0.0422572
I0818 04:28:43.771642 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0422572 (* 1 = 0.0422572 loss)
I0818 04:28:43.771656 15109 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0818 04:29:10.233549 15109 solver.cpp:514] Iteration 51500, Testing net (#0)
I0818 04:29:35.623062 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:29:35.726675 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919602
I0818 04:29:35.726745 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.259463 (* 1 = 0.259463 loss)
I0818 04:29:35.954475 15109 solver.cpp:357] Iteration 51500 (1.91646 iter/s, 52.1796s/100 iters), loss = 0.0529063
I0818 04:29:35.954535 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0529063 (* 1 = 0.0529063 loss)
I0818 04:29:35.954547 15109 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0818 04:29:51.501771 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:30:02.807534 15109 solver.cpp:357] Iteration 51600 (3.72435 iter/s, 26.8503s/100 iters), loss = 0.0675643
I0818 04:30:02.807612 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0675643 (* 1 = 0.0675643 loss)
I0818 04:30:02.807624 15109 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0818 04:30:29.407825 15109 solver.cpp:357] Iteration 51700 (3.75974 iter/s, 26.5976s/100 iters), loss = 0.0824008
I0818 04:30:29.408030 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0824008 (* 1 = 0.0824008 loss)
I0818 04:30:29.408041 15109 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0818 04:30:56.001963 15109 solver.cpp:357] Iteration 51800 (3.7606 iter/s, 26.5915s/100 iters), loss = 0.0518544
I0818 04:30:56.002041 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0518544 (* 1 = 0.0518544 loss)
I0818 04:30:56.002053 15109 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0818 04:31:22.832496 15109 solver.cpp:357] Iteration 51900 (3.72746 iter/s, 26.8279s/100 iters), loss = 0.0511359
I0818 04:31:22.832643 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0511359 (* 1 = 0.0511359 loss)
I0818 04:31:22.832653 15109 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0818 04:31:35.833077 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:31:49.250953 15109 solver.cpp:514] Iteration 52000, Testing net (#0)
I0818 04:32:14.858368 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:32:14.970860 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919502
I0818 04:32:14.970909 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.253726 (* 1 = 0.253726 loss)
I0818 04:32:15.197881 15109 solver.cpp:357] Iteration 52000 (1.90969 iter/s, 52.3645s/100 iters), loss = 0.0629825
I0818 04:32:15.197959 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0629825 (* 1 = 0.0629825 loss)
I0818 04:32:15.197973 15109 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0818 04:32:41.773705 15109 solver.cpp:357] Iteration 52100 (3.76317 iter/s, 26.5733s/100 iters), loss = 0.0351393
I0818 04:32:41.773793 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0351393 (* 1 = 0.0351393 loss)
I0818 04:32:41.773807 15109 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0818 04:33:08.410138 15109 solver.cpp:357] Iteration 52200 (3.7546 iter/s, 26.634s/100 iters), loss = 0.0175082
I0818 04:33:08.410320 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0175082 (* 1 = 0.0175082 loss)
I0818 04:33:08.410332 15109 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0818 04:33:35.160666 15109 solver.cpp:357] Iteration 52300 (3.73858 iter/s, 26.7481s/100 iters), loss = 0.0608133
I0818 04:33:35.160804 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0608133 (* 1 = 0.0608133 loss)
I0818 04:33:35.160818 15109 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0818 04:33:45.512104 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:34:01.767276 15109 solver.cpp:357] Iteration 52400 (3.75881 iter/s, 26.6042s/100 iters), loss = 0.0391716
I0818 04:34:01.767355 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0391716 (* 1 = 0.0391716 loss)
I0818 04:34:01.767367 15109 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0818 04:34:28.140208 15109 solver.cpp:514] Iteration 52500, Testing net (#0)
I0818 04:34:53.822847 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:34:53.926055 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919702
I0818 04:34:53.926116 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.25791 (* 1 = 0.25791 loss)
I0818 04:34:54.166400 15109 solver.cpp:357] Iteration 52500 (1.90852 iter/s, 52.3966s/100 iters), loss = 0.0610776
I0818 04:34:54.166452 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0610776 (* 1 = 0.0610776 loss)
I0818 04:34:54.166468 15109 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0818 04:35:20.800750 15109 solver.cpp:357] Iteration 52600 (3.75488 iter/s, 26.632s/100 iters), loss = 0.0438651
I0818 04:35:20.800940 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0438651 (* 1 = 0.0438651 loss)
I0818 04:35:20.800954 15109 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0818 04:35:47.646185 15109 solver.cpp:357] Iteration 52700 (3.72534 iter/s, 26.8432s/100 iters), loss = 0.0348627
I0818 04:35:47.646265 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0348627 (* 1 = 0.0348627 loss)
I0818 04:35:47.646279 15109 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0818 04:35:55.630954 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:36:14.282635 15109 solver.cpp:357] Iteration 52800 (3.75458 iter/s, 26.6341s/100 iters), loss = 0.0362296
I0818 04:36:14.282713 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0362296 (* 1 = 0.0362296 loss)
I0818 04:36:14.282727 15109 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0818 04:36:40.910775 15109 solver.cpp:357] Iteration 52900 (3.75575 iter/s, 26.6259s/100 iters), loss = 0.0656748
I0818 04:36:40.910949 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0656748 (* 1 = 0.0656748 loss)
I0818 04:36:40.910962 15109 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0818 04:37:07.553313 15109 solver.cpp:514] Iteration 53000, Testing net (#0)
I0818 04:37:33.101331 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:37:33.198071 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.915002
I0818 04:37:33.198132 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.279848 (* 1 = 0.279848 loss)
I0818 04:37:33.416923 15109 solver.cpp:357] Iteration 53000 (1.90462 iter/s, 52.5039s/100 iters), loss = 0.0394398
I0818 04:37:33.417004 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0394398 (* 1 = 0.0394398 loss)
I0818 04:37:33.417018 15109 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0818 04:38:00.429467 15109 solver.cpp:357] Iteration 53100 (3.70229 iter/s, 27.0103s/100 iters), loss = 0.0460673
I0818 04:38:00.429559 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0460672 (* 1 = 0.0460672 loss)
I0818 04:38:00.429572 15109 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0818 04:38:05.905551 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:38:27.492553 15109 solver.cpp:357] Iteration 53200 (3.69537 iter/s, 27.0609s/100 iters), loss = 0.0381331
I0818 04:38:27.492643 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.038133 (* 1 = 0.038133 loss)
I0818 04:38:27.492657 15109 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0818 04:38:54.529181 15109 solver.cpp:357] Iteration 53300 (3.69898 iter/s, 27.0345s/100 iters), loss = 0.0226309
I0818 04:38:54.529366 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0226309 (* 1 = 0.0226309 loss)
I0818 04:38:54.529377 15109 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0818 04:39:21.566954 15109 solver.cpp:357] Iteration 53400 (3.69873 iter/s, 27.0363s/100 iters), loss = 0.0368604
I0818 04:39:21.567046 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0368604 (* 1 = 0.0368604 loss)
I0818 04:39:21.567060 15109 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0818 04:39:48.289449 15109 solver.cpp:514] Iteration 53500, Testing net (#0)
I0818 04:40:14.494853 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:40:14.598340 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919403
I0818 04:40:14.598408 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.264156 (* 1 = 0.264156 loss)
I0818 04:40:14.823693 15109 solver.cpp:357] Iteration 53500 (1.87777 iter/s, 53.2547s/100 iters), loss = 0.0245481
I0818 04:40:14.823782 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0245481 (* 1 = 0.0245481 loss)
I0818 04:40:14.823796 15109 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0818 04:40:17.852166 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:40:41.869635 15109 solver.cpp:357] Iteration 53600 (3.69755 iter/s, 27.045s/100 iters), loss = 0.0523445
I0818 04:40:41.869756 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0523445 (* 1 = 0.0523445 loss)
I0818 04:40:41.869766 15109 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0818 04:41:08.409008 15109 solver.cpp:357] Iteration 53700 (3.768 iter/s, 26.5393s/100 iters), loss = 0.0191134
I0818 04:41:08.409086 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0191134 (* 1 = 0.0191134 loss)
I0818 04:41:08.409098 15109 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0818 04:41:35.011056 15109 solver.cpp:357] Iteration 53800 (3.75941 iter/s, 26.5999s/100 iters), loss = 0.0344535
I0818 04:41:35.011183 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0344535 (* 1 = 0.0344535 loss)
I0818 04:41:35.011198 15109 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0818 04:42:01.708410 15109 solver.cpp:357] Iteration 53900 (3.74599 iter/s, 26.6952s/100 iters), loss = 0.0377195
I0818 04:42:01.708484 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0377195 (* 1 = 0.0377195 loss)
I0818 04:42:01.708498 15109 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0818 04:42:02.125032 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:42:28.150575 15109 solver.cpp:514] Iteration 54000, Testing net (#0)
I0818 04:42:53.496984 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:42:53.612439 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918102
I0818 04:42:53.612486 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.268474 (* 1 = 0.268474 loss)
I0818 04:42:53.838943 15109 solver.cpp:357] Iteration 54000 (1.91833 iter/s, 52.1286s/100 iters), loss = 0.017977
I0818 04:42:53.839025 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.017977 (* 1 = 0.017977 loss)
I0818 04:42:53.839038 15109 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0818 04:43:20.499356 15109 solver.cpp:357] Iteration 54100 (3.75117 iter/s, 26.6583s/100 iters), loss = 0.0465932
I0818 04:43:20.499526 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0465932 (* 1 = 0.0465932 loss)
I0818 04:43:20.499538 15109 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0818 04:43:47.151340 15109 solver.cpp:357] Iteration 54200 (3.75236 iter/s, 26.6499s/100 iters), loss = 0.0249816
I0818 04:43:47.151432 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0249816 (* 1 = 0.0249816 loss)
I0818 04:43:47.151445 15109 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0818 04:44:11.734040 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:44:13.973893 15109 solver.cpp:357] Iteration 54300 (3.72823 iter/s, 26.8224s/100 iters), loss = 0.0780279
I0818 04:44:13.973983 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0780279 (* 1 = 0.0780279 loss)
I0818 04:44:13.973995 15109 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0818 04:44:40.523984 15109 solver.cpp:357] Iteration 54400 (3.76675 iter/s, 26.5481s/100 iters), loss = 0.0305592
I0818 04:44:40.524056 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0305592 (* 1 = 0.0305592 loss)
I0818 04:44:40.524067 15109 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0818 04:45:06.963081 15109 solver.cpp:514] Iteration 54500, Testing net (#0)
I0818 04:45:32.284742 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:45:32.399611 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920402
I0818 04:45:32.399664 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.267585 (* 1 = 0.267585 loss)
I0818 04:45:32.628178 15109 solver.cpp:357] Iteration 54500 (1.91922 iter/s, 52.1044s/100 iters), loss = 0.0321511
I0818 04:45:32.628262 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0321511 (* 1 = 0.0321511 loss)
I0818 04:45:32.628276 15109 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0818 04:45:59.525372 15109 solver.cpp:357] Iteration 54600 (3.71814 iter/s, 26.8952s/100 iters), loss = 0.0424121
I0818 04:45:59.525532 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0424121 (* 1 = 0.0424121 loss)
I0818 04:45:59.525542 15109 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0818 04:46:21.503996 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:46:26.063984 15109 solver.cpp:357] Iteration 54700 (3.7681 iter/s, 26.5386s/100 iters), loss = 0.022598
I0818 04:46:26.064070 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.022598 (* 1 = 0.022598 loss)
I0818 04:46:26.064083 15109 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0818 04:46:52.683845 15109 solver.cpp:357] Iteration 54800 (3.75688 iter/s, 26.6178s/100 iters), loss = 0.0214475
I0818 04:46:52.684033 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0214475 (* 1 = 0.0214475 loss)
I0818 04:46:52.684047 15109 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0818 04:47:19.423946 15109 solver.cpp:357] Iteration 54900 (3.73998 iter/s, 26.7381s/100 iters), loss = 0.0221777
I0818 04:47:19.424015 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0221777 (* 1 = 0.0221777 loss)
I0818 04:47:19.424026 15109 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0818 04:47:45.894212 15109 solver.cpp:514] Iteration 55000, Testing net (#0)
I0818 04:48:11.478592 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:48:11.589370 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919603
I0818 04:48:11.589421 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265148 (* 1 = 0.265148 loss)
I0818 04:48:11.813632 15109 solver.cpp:357] Iteration 55000 (1.90878 iter/s, 52.3894s/100 iters), loss = 0.025555
I0818 04:48:11.813715 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.025555 (* 1 = 0.025555 loss)
I0818 04:48:11.813729 15109 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0818 04:48:31.546890 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:48:38.481772 15109 solver.cpp:357] Iteration 55100 (3.75007 iter/s, 26.6662s/100 iters), loss = 0.0525104
I0818 04:48:38.481853 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0525105 (* 1 = 0.0525105 loss)
I0818 04:48:38.481864 15109 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0818 04:49:05.115335 15109 solver.cpp:357] Iteration 55200 (3.75494 iter/s, 26.6316s/100 iters), loss = 0.0734072
I0818 04:49:05.115465 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0734072 (* 1 = 0.0734072 loss)
I0818 04:49:05.115476 15109 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0818 04:49:31.908104 15109 solver.cpp:357] Iteration 55300 (3.73262 iter/s, 26.7908s/100 iters), loss = 0.0233279
I0818 04:49:31.908167 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0233279 (* 1 = 0.0233279 loss)
I0818 04:49:31.908179 15109 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0818 04:49:58.693027 15109 solver.cpp:357] Iteration 55400 (3.73343 iter/s, 26.785s/100 iters), loss = 0.0435195
I0818 04:49:58.693274 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0435195 (* 1 = 0.0435195 loss)
I0818 04:49:58.693286 15109 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0818 04:50:15.676270 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:50:24.955441 15109 solver.cpp:514] Iteration 55500, Testing net (#0)
I0818 04:50:50.523046 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:50:50.633222 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920402
I0818 04:50:50.633272 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.264589 (* 1 = 0.264589 loss)
I0818 04:50:50.859788 15109 solver.cpp:357] Iteration 55500 (1.91699 iter/s, 52.165s/100 iters), loss = 0.0295779
I0818 04:50:50.859869 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0295779 (* 1 = 0.0295779 loss)
I0818 04:50:50.859882 15109 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0818 04:51:17.437315 15109 solver.cpp:357] Iteration 55600 (3.76286 iter/s, 26.5755s/100 iters), loss = 0.0186345
I0818 04:51:17.437402 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186345 (* 1 = 0.0186345 loss)
I0818 04:51:17.437413 15109 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0818 04:51:44.302173 15109 solver.cpp:357] Iteration 55700 (3.72261 iter/s, 26.8629s/100 iters), loss = 0.0391813
I0818 04:51:44.302350 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0391813 (* 1 = 0.0391813 loss)
I0818 04:51:44.302359 15109 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0818 04:52:10.868350 15109 solver.cpp:357] Iteration 55800 (3.76446 iter/s, 26.5642s/100 iters), loss = 0.0229453
I0818 04:52:10.868427 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0229453 (* 1 = 0.0229453 loss)
I0818 04:52:10.868438 15109 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0818 04:52:25.596364 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:52:37.442989 15109 solver.cpp:357] Iteration 55900 (3.76326 iter/s, 26.5727s/100 iters), loss = 0.0823333
I0818 04:52:37.443064 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0823333 (* 1 = 0.0823333 loss)
I0818 04:52:37.443076 15109 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0818 04:53:04.112833 15109 solver.cpp:514] Iteration 56000, Testing net (#0)
I0818 04:53:29.401043 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:53:29.475872 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919702
I0818 04:53:29.475927 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271073 (* 1 = 0.271073 loss)
I0818 04:53:29.714993 15109 solver.cpp:357] Iteration 56000 (1.91313 iter/s, 52.2703s/100 iters), loss = 0.0782842
I0818 04:53:29.715040 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0782842 (* 1 = 0.0782842 loss)
I0818 04:53:29.715054 15109 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0818 04:53:56.495967 15109 solver.cpp:357] Iteration 56100 (3.73427 iter/s, 26.779s/100 iters), loss = 0.0131432
I0818 04:53:56.496122 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0131432 (* 1 = 0.0131432 loss)
I0818 04:53:56.496134 15109 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0818 04:54:23.038122 15109 solver.cpp:357] Iteration 56200 (3.76787 iter/s, 26.5402s/100 iters), loss = 0.0258571
I0818 04:54:23.038192 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0258571 (* 1 = 0.0258571 loss)
I0818 04:54:23.038204 15109 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0818 04:54:35.082676 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:54:49.687306 15109 solver.cpp:357] Iteration 56300 (3.75273 iter/s, 26.6472s/100 iters), loss = 0.0307532
I0818 04:54:49.687396 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0307532 (* 1 = 0.0307532 loss)
I0818 04:54:49.687408 15109 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0818 04:55:16.497596 15109 solver.cpp:357] Iteration 56400 (3.73018 iter/s, 26.8083s/100 iters), loss = 0.0332216
I0818 04:55:16.497830 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0332216 (* 1 = 0.0332216 loss)
I0818 04:55:16.497844 15109 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0818 04:55:42.869482 15109 solver.cpp:514] Iteration 56500, Testing net (#0)
I0818 04:56:08.295430 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:56:08.409090 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919102
I0818 04:56:08.409158 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273914 (* 1 = 0.273914 loss)
I0818 04:56:08.637506 15109 solver.cpp:357] Iteration 56500 (1.91798 iter/s, 52.1383s/100 iters), loss = 0.0174199
I0818 04:56:08.637568 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.01742 (* 1 = 0.01742 loss)
I0818 04:56:08.637580 15109 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0818 04:56:35.244961 15109 solver.cpp:357] Iteration 56600 (3.75861 iter/s, 26.6056s/100 iters), loss = 0.0209364
I0818 04:56:35.245033 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0209364 (* 1 = 0.0209364 loss)
I0818 04:56:35.245044 15109 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0818 04:56:44.868424 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:57:01.844756 15109 solver.cpp:357] Iteration 56700 (3.7597 iter/s, 26.5979s/100 iters), loss = 0.0263711
I0818 04:57:01.844825 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0263711 (* 1 = 0.0263711 loss)
I0818 04:57:01.844835 15109 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0818 04:57:28.568835 15109 solver.cpp:357] Iteration 56800 (3.74192 iter/s, 26.7242s/100 iters), loss = 0.0266987
I0818 04:57:28.569005 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0266987 (* 1 = 0.0266987 loss)
I0818 04:57:28.569016 15109 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0818 04:57:55.251701 15109 solver.cpp:357] Iteration 56900 (3.748 iter/s, 26.6809s/100 iters), loss = 0.037536
I0818 04:57:55.251788 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.037536 (* 1 = 0.037536 loss)
I0818 04:57:55.251801 15109 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0818 04:58:21.686877 15109 solver.cpp:514] Iteration 57000, Testing net (#0)
I0818 04:58:47.010648 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:58:47.125597 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.916702
I0818 04:58:47.125646 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.276301 (* 1 = 0.276301 loss)
I0818 04:58:47.349326 15109 solver.cpp:357] Iteration 57000 (1.91953 iter/s, 52.096s/100 iters), loss = 0.00718239
I0818 04:58:47.349409 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0071824 (* 1 = 0.0071824 loss)
I0818 04:58:47.349422 15109 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0818 04:58:54.701088 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 04:59:14.234376 15109 solver.cpp:357] Iteration 57100 (3.71981 iter/s, 26.8831s/100 iters), loss = 0.0370978
I0818 04:59:14.234454 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0370978 (* 1 = 0.0370978 loss)
I0818 04:59:14.234464 15109 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0818 04:59:40.790760 15109 solver.cpp:357] Iteration 57200 (3.76585 iter/s, 26.5544s/100 iters), loss = 0.0535986
I0818 04:59:40.790946 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0535986 (* 1 = 0.0535986 loss)
I0818 04:59:40.790957 15109 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0818 05:00:07.407102 15109 solver.cpp:357] Iteration 57300 (3.75736 iter/s, 26.6144s/100 iters), loss = 0.0265331
I0818 05:00:07.407181 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0265331 (* 1 = 0.0265331 loss)
I0818 05:00:07.407192 15109 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0818 05:00:34.176484 15109 solver.cpp:357] Iteration 57400 (3.73588 iter/s, 26.7675s/100 iters), loss = 0.0402281
I0818 05:00:34.176892 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0402281 (* 1 = 0.0402281 loss)
I0818 05:00:34.176954 15109 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0818 05:00:38.651993 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:01:00.543185 15109 solver.cpp:514] Iteration 57500, Testing net (#0)
I0818 05:01:26.147089 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:01:26.245110 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.921402
I0818 05:01:26.245183 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.269337 (* 1 = 0.269337 loss)
I0818 05:01:26.484766 15109 solver.cpp:357] Iteration 57500 (1.9118 iter/s, 52.3066s/100 iters), loss = 0.0167688
I0818 05:01:26.484822 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0167688 (* 1 = 0.0167688 loss)
I0818 05:01:26.484833 15109 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0818 05:01:53.064297 15109 solver.cpp:357] Iteration 57600 (3.76256 iter/s, 26.5776s/100 iters), loss = 0.0229899
I0818 05:01:53.064373 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0229899 (* 1 = 0.0229899 loss)
I0818 05:01:53.064383 15109 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0818 05:02:19.692375 15109 solver.cpp:357] Iteration 57700 (3.7556 iter/s, 26.6269s/100 iters), loss = 0.0306296
I0818 05:02:19.692534 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0306296 (* 1 = 0.0306296 loss)
I0818 05:02:19.692548 15109 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0818 05:02:46.559758 15109 solver.cpp:357] Iteration 57800 (3.72225 iter/s, 26.8655s/100 iters), loss = 0.0485649
I0818 05:02:46.559844 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0485649 (* 1 = 0.0485649 loss)
I0818 05:02:46.559857 15109 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0818 05:02:48.626639 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:03:13.041967 15109 solver.cpp:357] Iteration 57900 (3.77639 iter/s, 26.4803s/100 iters), loss = 0.0443892
I0818 05:03:13.042143 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0443892 (* 1 = 0.0443892 loss)
I0818 05:03:13.042156 15109 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0818 05:03:39.368443 15109 solver.cpp:514] Iteration 58000, Testing net (#0)
I0818 05:04:04.887253 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:04:04.993743 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920802
I0818 05:04:04.993810 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270165 (* 1 = 0.270165 loss)
I0818 05:04:05.232749 15109 solver.cpp:357] Iteration 58000 (1.91611 iter/s, 52.1891s/100 iters), loss = 0.0187175
I0818 05:04:05.232803 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187175 (* 1 = 0.0187175 loss)
I0818 05:04:05.232815 15109 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0818 05:04:31.957157 15109 solver.cpp:357] Iteration 58100 (3.74217 iter/s, 26.7225s/100 iters), loss = 0.0380353
I0818 05:04:31.957234 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0380353 (* 1 = 0.0380353 loss)
I0818 05:04:31.957245 15109 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0818 05:04:58.575582 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:04:58.805099 15109 solver.cpp:357] Iteration 58200 (3.72495 iter/s, 26.846s/100 iters), loss = 0.0307858
I0818 05:04:58.805188 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0307858 (* 1 = 0.0307858 loss)
I0818 05:04:58.805202 15109 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0818 05:05:25.443733 15109 solver.cpp:357] Iteration 58300 (3.75421 iter/s, 26.6368s/100 iters), loss = 0.0416888
I0818 05:05:25.443819 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0416888 (* 1 = 0.0416888 loss)
I0818 05:05:25.443832 15109 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0818 05:05:51.977365 15109 solver.cpp:357] Iteration 58400 (3.76907 iter/s, 26.5317s/100 iters), loss = 0.0232364
I0818 05:05:51.977587 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0232365 (* 1 = 0.0232365 loss)
I0818 05:05:51.977602 15109 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0818 05:06:18.594877 15109 solver.cpp:514] Iteration 58500, Testing net (#0)
I0818 05:06:44.072499 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:06:44.092140 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918902
I0818 05:06:44.092180 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273718 (* 1 = 0.273718 loss)
I0818 05:06:44.239800 15109 solver.cpp:357] Iteration 58500 (1.91348 iter/s, 52.2608s/100 iters), loss = 0.0346585
I0818 05:06:44.239869 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0346585 (* 1 = 0.0346585 loss)
I0818 05:06:44.239881 15109 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0818 05:07:08.189175 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:07:11.051010 15109 solver.cpp:357] Iteration 58600 (3.72976 iter/s, 26.8114s/100 iters), loss = 0.026825
I0818 05:07:11.051090 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.026825 (* 1 = 0.026825 loss)
I0818 05:07:11.051103 15109 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0818 05:07:37.612293 15109 solver.cpp:357] Iteration 58700 (3.76515 iter/s, 26.5594s/100 iters), loss = 0.0324003
I0818 05:07:37.612481 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0324003 (* 1 = 0.0324003 loss)
I0818 05:07:37.612495 15109 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0818 05:08:04.204401 15109 solver.cpp:357] Iteration 58800 (3.76078 iter/s, 26.5902s/100 iters), loss = 0.0178313
I0818 05:08:04.204483 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0178313 (* 1 = 0.0178313 loss)
I0818 05:08:04.204493 15109 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0818 05:08:31.005324 15109 solver.cpp:357] Iteration 58900 (3.73148 iter/s, 26.799s/100 iters), loss = 0.0310151
I0818 05:08:31.005509 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0310151 (* 1 = 0.0310151 loss)
I0818 05:08:31.005523 15109 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0818 05:08:52.361627 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:08:57.340883 15109 solver.cpp:514] Iteration 59000, Testing net (#0)
I0818 05:09:23.029458 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:09:23.132555 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920502
I0818 05:09:23.132616 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271474 (* 1 = 0.271474 loss)
I0818 05:09:23.366308 15109 solver.cpp:357] Iteration 59000 (1.90988 iter/s, 52.3594s/100 iters), loss = 0.0278274
I0818 05:09:23.366369 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0278274 (* 1 = 0.0278274 loss)
I0818 05:09:23.366381 15109 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0818 05:09:49.950129 15109 solver.cpp:357] Iteration 59100 (3.76196 iter/s, 26.5819s/100 iters), loss = 0.0375864
I0818 05:09:49.950206 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0375864 (* 1 = 0.0375864 loss)
I0818 05:09:49.950217 15109 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0818 05:10:16.570370 15109 solver.cpp:357] Iteration 59200 (3.75681 iter/s, 26.6183s/100 iters), loss = 0.034917
I0818 05:10:16.570510 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.034917 (* 1 = 0.034917 loss)
I0818 05:10:16.570520 15109 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0818 05:10:43.212963 15109 solver.cpp:357] Iteration 59300 (3.75337 iter/s, 26.6427s/100 iters), loss = 0.0190682
I0818 05:10:43.213048 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0190683 (* 1 = 0.0190683 loss)
I0818 05:10:43.213062 15109 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0818 05:11:02.008522 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:11:09.840226 15109 solver.cpp:357] Iteration 59400 (3.75581 iter/s, 26.6254s/100 iters), loss = 0.0459662
I0818 05:11:09.840304 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0459662 (* 1 = 0.0459662 loss)
I0818 05:11:09.840317 15109 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0818 05:11:36.387709 15109 solver.cpp:514] Iteration 59500, Testing net (#0)
I0818 05:12:01.656752 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:12:01.729107 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.917402
I0818 05:12:01.729156 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.274197 (* 1 = 0.274197 loss)
I0818 05:12:01.967000 15109 solver.cpp:357] Iteration 59500 (1.91846 iter/s, 52.1252s/100 iters), loss = 0.0196764
I0818 05:12:01.967061 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0196765 (* 1 = 0.0196765 loss)
I0818 05:12:01.967075 15109 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0818 05:12:28.852718 15109 solver.cpp:357] Iteration 59600 (3.71971 iter/s, 26.8838s/100 iters), loss = 0.0210943
I0818 05:12:28.852883 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0210944 (* 1 = 0.0210944 loss)
I0818 05:12:28.852896 15109 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0818 05:12:55.487244 15109 solver.cpp:357] Iteration 59700 (3.75479 iter/s, 26.6327s/100 iters), loss = 0.0564143
I0818 05:12:55.487321 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0564143 (* 1 = 0.0564143 loss)
I0818 05:12:55.487334 15109 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0818 05:13:11.909108 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:13:22.108049 15109 solver.cpp:357] Iteration 59800 (3.75673 iter/s, 26.6189s/100 iters), loss = 0.0240097
I0818 05:13:22.108140 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0240097 (* 1 = 0.0240097 loss)
I0818 05:13:22.108155 15109 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0818 05:13:48.858263 15109 solver.cpp:357] Iteration 59900 (3.73855 iter/s, 26.7483s/100 iters), loss = 0.0245926
I0818 05:13:48.858392 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0245927 (* 1 = 0.0245927 loss)
I0818 05:13:48.858404 15109 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0818 05:14:15.218724 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.caffemodel
I0818 05:14:15.230772 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.solverstate
I0818 05:14:15.233727 15109 solver.cpp:514] Iteration 60000, Testing net (#0)
I0818 05:14:40.641444 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:14:40.750561 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.917602
I0818 05:14:40.750612 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.285073 (* 1 = 0.285073 loss)
I0818 05:14:40.988808 15109 solver.cpp:357] Iteration 60000 (1.91832 iter/s, 52.1289s/100 iters), loss = 0.0194863
I0818 05:14:40.988885 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0194863 (* 1 = 0.0194863 loss)
I0818 05:14:40.988898 15109 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0818 05:15:07.594373 15109 solver.cpp:357] Iteration 60100 (3.75888 iter/s, 26.6037s/100 iters), loss = 0.0168011
I0818 05:15:07.594460 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0168011 (* 1 = 0.0168011 loss)
I0818 05:15:07.594473 15109 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0818 05:15:21.585852 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:15:34.243804 15109 solver.cpp:357] Iteration 60200 (3.75269 iter/s, 26.6476s/100 iters), loss = 0.025073
I0818 05:15:34.243886 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.025073 (* 1 = 0.025073 loss)
I0818 05:15:34.243897 15109 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0818 05:16:01.114444 15109 solver.cpp:357] Iteration 60300 (3.72179 iter/s, 26.8688s/100 iters), loss = 0.0447136
I0818 05:16:01.114858 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0447136 (* 1 = 0.0447136 loss)
I0818 05:16:01.114923 15109 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0818 05:16:27.713683 15109 solver.cpp:357] Iteration 60400 (3.75977 iter/s, 26.5974s/100 iters), loss = 0.0175414
I0818 05:16:27.713764 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0175415 (* 1 = 0.0175415 loss)
I0818 05:16:27.713778 15109 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0818 05:16:54.069838 15109 solver.cpp:514] Iteration 60500, Testing net (#0)
I0818 05:17:19.408139 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:17:19.522799 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.915602
I0818 05:17:19.522850 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.289144 (* 1 = 0.289144 loss)
I0818 05:17:19.750978 15109 solver.cpp:357] Iteration 60500 (1.92176 iter/s, 52.0357s/100 iters), loss = 0.0161711
I0818 05:17:19.751055 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0161711 (* 1 = 0.0161711 loss)
I0818 05:17:19.751068 15109 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0818 05:17:31.067972 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:17:46.422472 15109 solver.cpp:357] Iteration 60600 (3.74853 iter/s, 26.6771s/100 iters), loss = 0.0221514
I0818 05:17:46.422539 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0221514 (* 1 = 0.0221514 loss)
I0818 05:17:46.422549 15109 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0818 05:18:13.160760 15109 solver.cpp:357] Iteration 60700 (3.73895 iter/s, 26.7455s/100 iters), loss = 0.0648959
I0818 05:18:13.160933 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.064896 (* 1 = 0.064896 loss)
I0818 05:18:13.160944 15109 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0818 05:18:39.665014 15109 solver.cpp:357] Iteration 60800 (3.77223 iter/s, 26.5095s/100 iters), loss = 0.013304
I0818 05:18:39.665091 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.013304 (* 1 = 0.013304 loss)
I0818 05:18:39.665103 15109 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0818 05:19:06.339987 15109 solver.cpp:357] Iteration 60900 (3.74817 iter/s, 26.6797s/100 iters), loss = 0.0274932
I0818 05:19:06.340113 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0274932 (* 1 = 0.0274932 loss)
I0818 05:19:06.340126 15109 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0818 05:19:15.343201 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:19:33.031996 15109 solver.cpp:514] Iteration 61000, Testing net (#0)
I0818 05:19:58.448216 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:19:58.558548 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919703
I0818 05:19:58.558599 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275227 (* 1 = 0.275227 loss)
I0818 05:19:58.754896 15109 solver.cpp:357] Iteration 61000 (1.90743 iter/s, 52.4265s/100 iters), loss = 0.068053
I0818 05:19:58.754951 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.068053 (* 1 = 0.068053 loss)
I0818 05:19:58.754961 15109 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0818 05:20:25.419489 15109 solver.cpp:357] Iteration 61100 (3.74954 iter/s, 26.6699s/100 iters), loss = 0.0216763
I0818 05:20:25.419577 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0216764 (* 1 = 0.0216764 loss)
I0818 05:20:25.419589 15109 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0818 05:20:51.960328 15109 solver.cpp:357] Iteration 61200 (3.76738 iter/s, 26.5437s/100 iters), loss = 0.00859849
I0818 05:20:51.960505 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00859854 (* 1 = 0.00859854 loss)
I0818 05:20:51.960517 15109 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0818 05:21:18.653383 15109 solver.cpp:357] Iteration 61300 (3.74594 iter/s, 26.6955s/100 iters), loss = 0.0376307
I0818 05:21:18.653452 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0376308 (* 1 = 0.0376308 loss)
I0818 05:21:18.653463 15109 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0818 05:21:24.950296 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:21:45.269634 15109 solver.cpp:357] Iteration 61400 (3.75651 iter/s, 26.6205s/100 iters), loss = 0.0369487
I0818 05:21:45.269721 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0369488 (* 1 = 0.0369488 loss)
I0818 05:21:45.269734 15109 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0818 05:22:11.514845 15109 solver.cpp:514] Iteration 61500, Testing net (#0)
I0818 05:22:36.801851 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:22:36.912611 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.917602
I0818 05:22:36.912657 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.284032 (* 1 = 0.284032 loss)
I0818 05:22:37.137719 15109 solver.cpp:357] Iteration 61500 (1.92776 iter/s, 51.8736s/100 iters), loss = 0.0176919
I0818 05:22:37.137802 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.017692 (* 1 = 0.017692 loss)
I0818 05:22:37.137814 15109 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0818 05:23:03.800850 15109 solver.cpp:357] Iteration 61600 (3.7503 iter/s, 26.6645s/100 iters), loss = 0.0220887
I0818 05:23:03.800969 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0220888 (* 1 = 0.0220888 loss)
I0818 05:23:03.800979 15109 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0818 05:23:30.573272 15109 solver.cpp:357] Iteration 61700 (3.73474 iter/s, 26.7756s/100 iters), loss = 0.0323647
I0818 05:23:30.573348 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0323648 (* 1 = 0.0323648 loss)
I0818 05:23:30.573359 15109 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0818 05:23:34.381960 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:23:57.178954 15109 solver.cpp:357] Iteration 61800 (3.75846 iter/s, 26.6066s/100 iters), loss = 0.0449091
I0818 05:23:57.179044 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0449091 (* 1 = 0.0449091 loss)
I0818 05:23:57.179057 15109 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0818 05:24:23.844584 15109 solver.cpp:357] Iteration 61900 (3.75003 iter/s, 26.6664s/100 iters), loss = 0.0204397
I0818 05:24:23.844743 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0204397 (* 1 = 0.0204397 loss)
I0818 05:24:23.844756 15109 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0818 05:24:50.485992 15109 solver.cpp:514] Iteration 62000, Testing net (#0)
I0818 05:25:15.842092 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:25:15.943259 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.919402
I0818 05:25:15.943305 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.281122 (* 1 = 0.281122 loss)
I0818 05:25:16.165832 15109 solver.cpp:357] Iteration 62000 (1.91115 iter/s, 52.3245s/100 iters), loss = 0.0755775
I0818 05:25:16.165877 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0755775 (* 1 = 0.0755775 loss)
I0818 05:25:16.165889 15109 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0818 05:25:42.844877 15109 solver.cpp:357] Iteration 62100 (3.74791 iter/s, 26.6815s/100 iters), loss = 0.0352032
I0818 05:25:42.844956 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0352032 (* 1 = 0.0352032 loss)
I0818 05:25:42.844969 15109 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0818 05:25:44.135902 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:26:09.482990 15109 solver.cpp:357] Iteration 62200 (3.75398 iter/s, 26.6384s/100 iters), loss = 0.0197084
I0818 05:26:09.483135 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0197084 (* 1 = 0.0197084 loss)
I0818 05:26:09.483146 15109 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0818 05:26:36.105641 15109 solver.cpp:357] Iteration 62300 (3.75618 iter/s, 26.6228s/100 iters), loss = 0.0262453
I0818 05:26:36.105716 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0262454 (* 1 = 0.0262454 loss)
I0818 05:26:36.105727 15109 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0818 05:27:02.909548 15109 solver.cpp:357] Iteration 62400 (3.73057 iter/s, 26.8055s/100 iters), loss = 0.0380793
I0818 05:27:02.909801 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0380794 (* 1 = 0.0380794 loss)
I0818 05:27:02.909816 15109 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0818 05:27:28.157287 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:27:29.221674 15109 solver.cpp:514] Iteration 62500, Testing net (#0)
I0818 05:27:54.769454 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:27:54.879966 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922703
I0818 05:27:54.880017 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270732 (* 1 = 0.270732 loss)
I0818 05:27:55.120295 15109 solver.cpp:357] Iteration 62500 (1.91524 iter/s, 52.2128s/100 iters), loss = 0.0228813
I0818 05:27:55.120366 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0228814 (* 1 = 0.0228814 loss)
I0818 05:27:55.120378 15109 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0818 05:28:21.666537 15109 solver.cpp:357] Iteration 62600 (3.76674 iter/s, 26.5482s/100 iters), loss = 0.0151991
I0818 05:28:21.666609 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0151991 (* 1 = 0.0151991 loss)
I0818 05:28:21.666620 15109 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0818 05:28:48.412765 15109 solver.cpp:357] Iteration 62700 (3.73858 iter/s, 26.7481s/100 iters), loss = 0.0423718
I0818 05:28:48.413082 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0423719 (* 1 = 0.0423719 loss)
I0818 05:28:48.413142 15109 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0818 05:29:15.219775 15109 solver.cpp:357] Iteration 62800 (3.73014 iter/s, 26.8086s/100 iters), loss = 0.0567396
I0818 05:29:15.219867 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0567396 (* 1 = 0.0567396 loss)
I0818 05:29:15.219882 15109 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0818 05:29:38.570909 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:29:42.324715 15109 solver.cpp:357] Iteration 62900 (3.68919 iter/s, 27.1063s/100 iters), loss = 0.0527293
I0818 05:29:42.324802 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0527294 (* 1 = 0.0527294 loss)
I0818 05:29:42.324815 15109 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0818 05:30:09.117115 15109 solver.cpp:514] Iteration 63000, Testing net (#0)
I0818 05:30:35.178221 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:30:35.274523 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920302
I0818 05:30:35.274600 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275372 (* 1 = 0.275372 loss)
I0818 05:30:35.500078 15109 solver.cpp:357] Iteration 63000 (1.88052 iter/s, 53.1768s/100 iters), loss = 0.0127347
I0818 05:30:35.500174 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0127347 (* 1 = 0.0127347 loss)
I0818 05:30:35.500186 15109 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0818 05:31:02.514315 15109 solver.cpp:357] Iteration 63100 (3.7018 iter/s, 27.0138s/100 iters), loss = 0.0241692
I0818 05:31:02.514504 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0241693 (* 1 = 0.0241693 loss)
I0818 05:31:02.514518 15109 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0818 05:31:29.600108 15109 solver.cpp:357] Iteration 63200 (3.69203 iter/s, 27.0853s/100 iters), loss = 0.0169002
I0818 05:31:29.600196 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0169002 (* 1 = 0.0169002 loss)
I0818 05:31:29.600208 15109 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0818 05:31:50.550945 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:31:56.739099 15109 solver.cpp:357] Iteration 63300 (3.68462 iter/s, 27.1399s/100 iters), loss = 0.0238709
I0818 05:31:56.739190 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0238709 (* 1 = 0.0238709 loss)
I0818 05:31:56.739203 15109 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0818 05:32:23.648442 15109 solver.cpp:357] Iteration 63400 (3.71625 iter/s, 26.9089s/100 iters), loss = 0.0304311
I0818 05:32:23.648620 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0304312 (* 1 = 0.0304312 loss)
I0818 05:32:23.648632 15109 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0818 05:32:49.937888 15109 solver.cpp:514] Iteration 63500, Testing net (#0)
I0818 05:33:15.349439 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:33:15.442733 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.920203
I0818 05:33:15.442803 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.285054 (* 1 = 0.285054 loss)
I0818 05:33:15.683460 15109 solver.cpp:357] Iteration 63500 (1.92175 iter/s, 52.036s/100 iters), loss = 0.0148296
I0818 05:33:15.683513 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0148297 (* 1 = 0.0148297 loss)
I0818 05:33:15.683526 15109 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0818 05:33:42.478003 15109 solver.cpp:357] Iteration 63600 (3.73218 iter/s, 26.794s/100 iters), loss = 0.0275689
I0818 05:33:42.478070 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0275689 (* 1 = 0.0275689 loss)
I0818 05:33:42.478080 15109 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0818 05:34:00.461048 15115 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:34:09.159054 15109 solver.cpp:357] Iteration 63700 (3.74777 iter/s, 26.6825s/100 iters), loss = 0.0224306
I0818 05:34:09.159140 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0224307 (* 1 = 0.0224307 loss)
I0818 05:34:09.159154 15109 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0818 05:34:35.739827 15109 solver.cpp:357] Iteration 63800 (3.76258 iter/s, 26.5775s/100 iters), loss = 0.0221494
I0818 05:34:35.739964 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0221495 (* 1 = 0.0221495 loss)
I0818 05:34:35.739976 15109 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0818 05:35:02.444686 15109 solver.cpp:357] Iteration 63900 (3.7458 iter/s, 26.6966s/100 iters), loss = 0.0117752
I0818 05:35:02.444753 15109 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0117753 (* 1 = 0.0117753 loss)
I0818 05:35:02.444762 15109 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0818 05:35:28.890600 15109 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.caffemodel
I0818 05:35:28.911414 15109 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.solverstate
I0818 05:35:28.978124 15109 solver.cpp:472] Iteration 64000, loss = 0.0206726
I0818 05:35:28.978184 15109 solver.cpp:514] Iteration 64000, Testing net (#0)
I0818 05:35:54.607941 15135 data_layer.cpp:73] Restarting data prefetching from start.
I0818 05:35:54.712204 15109 solver.cpp:580]     Test net output #0: Accuracy1 = 0.918503
I0818 05:35:54.712262 15109 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.287905 (* 1 = 0.287905 loss)
I0818 05:35:54.712270 15109 solver.cpp:479] Optimization Done.
I0818 05:35:54.712277 15109 caffe.cpp:326] Optimization Done.
