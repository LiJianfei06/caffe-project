WARNING: Logging before InitGoogleLogging() is written to STDERR
I0218 11:49:48.067468 27028 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0218 11:49:48.067596 27028 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0218 11:49:48.067606 27028 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0218 11:49:48.067610 27028 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0218 11:49:48.067613 27028 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0218 11:49:48.067616 27028 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0218 11:49:48.067670 27028 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0218 11:49:48.067857 27028 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0218 11:49:48.069631 27028 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0218 11:49:48.069669 27028 caffe.cpp:269] Using GPUs 0
I0218 11:49:48.127825 27028 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0218 11:49:48.773742 27028 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0218 11:49:48.773785 27028 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0218 11:49:48.858978 27028 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_20.prototxt"
test_net: "./test_ResNet_20.prototxt"
test_iter: 100
test_interval: 1000
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_20"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 1
type: "Nesterov"
I0218 11:49:48.859237 27028 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_20.prototxt
I0218 11:49:48.860189 27028 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_20.prototxt
I0218 11:49:48.860208 27028 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0218 11:49:48.861106 27028 net.cpp:82] Initializing net from parameters: 
name: "ResNet-20"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "data1"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TRAIN
  }
  hdf5_data_param {
    source: "./train_h5.txt"
    batch_size: 128
  }
}
layer {
  name: "slicers"
  type: "Slice"
  bottom: "labels"
  top: "label_1"
  top: "label_2"
  top: "label_3"
  top: "label_4"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    axis: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_1"
  type: "Eltwise"
  bottom: "conv1"
  bottom: "conv2_1_1"
  top: "conv2_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_1"
  top: "conv2_Eltwise_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_2"
  type: "Eltwise"
  bottom: "conv2_Eltwise_1"
  bottom: "conv2_2_1"
  top: "conv2_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_2"
  top: "conv2_Eltwise_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_3"
  type: "Eltwise"
  bottom: "conv2_Eltwise_2"
  bottom: "conv2_3_1"
  top: "conv2_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_3"
  top: "conv2_Eltwise_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_1"
  type: "Eltwise"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_1"
  top: "conv3_Eltwise_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_2"
  type: "Eltwise"
  bottom: "conv3_Eltwise_1"
  bottom: "conv3_2_1"
  top: "conv3_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_2"
  top: "conv3_Eltwise_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_3"
  type: "Eltwise"
  bottom: "conv3_Eltwise_2"
  bottom: "conv3_3_1"
  top: "conv3_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_3"
  top: "conv3_Eltwise_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_1"
  type: "Eltwise"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_1"
  top: "conv4_Eltwise_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_2"
  type: "Eltwise"
  bottom: "conv4_Eltwise_1"
  bottom: "conv4_2_1"
  top: "conv4_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_2"
  top: "conv4_Eltwise_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_3"
  type: "Eltwise"
  bottom: "conv4_Eltwise_2"
  bottom: "conv4_3_1"
  top: "conv4_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_3"
  top: "conv4_Eltwise_3"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "conv4_Eltwise_3"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax1"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label_1"
  top: "Softmax1"
}
layer {
  name: "fc2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax2"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label_2"
  top: "Softmax2"
}
layer {
  name: "fc3"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax3"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label_3"
  top: "Softmax3"
}
layer {
  name: "fc4"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax4"
  type: "SoftmaxWithLoss"
  bottom: "fc4"
  bottom: "label_4"
  top: "Softmax4"
}
I0218 11:49:48.861641 27028 layer_factory.hpp:77] Creating layer data1
I0218 11:49:48.861662 27028 net.cpp:128] Creating Layer data1
I0218 11:49:48.861671 27028 net.cpp:522] data1 -> data
I0218 11:49:48.861697 27028 net.cpp:522] data1 -> labels
I0218 11:49:48.861711 27028 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./train_h5.txt
I0218 11:49:48.861763 27028 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I0218 11:49:48.863165 27028 hdf5.cpp:32] Datatype class: H5T_FLOAT
I0218 11:49:50.103693 27028 net.cpp:172] Setting up data1
I0218 11:49:50.103761 27028 net.cpp:186] Top shape: 128 1 32 72 (294912)
I0218 11:49:50.103768 27028 net.cpp:186] Top shape: 128 4 (512)
I0218 11:49:50.103772 27028 net.cpp:194] Memory required for data: 1181696
I0218 11:49:50.103785 27028 layer_factory.hpp:77] Creating layer slicers
I0218 11:49:50.103806 27028 net.cpp:128] Creating Layer slicers
I0218 11:49:50.103816 27028 net.cpp:558] slicers <- labels
I0218 11:49:50.103839 27028 net.cpp:522] slicers -> label_1
I0218 11:49:50.103857 27028 net.cpp:522] slicers -> label_2
I0218 11:49:50.103863 27028 net.cpp:522] slicers -> label_3
I0218 11:49:50.103871 27028 net.cpp:522] slicers -> label_4
I0218 11:49:50.103927 27028 net.cpp:172] Setting up slicers
I0218 11:49:50.103935 27028 net.cpp:186] Top shape: 128 1 (128)
I0218 11:49:50.103940 27028 net.cpp:186] Top shape: 128 1 (128)
I0218 11:49:50.103943 27028 net.cpp:186] Top shape: 128 1 (128)
I0218 11:49:50.103948 27028 net.cpp:186] Top shape: 128 1 (128)
I0218 11:49:50.103952 27028 net.cpp:194] Memory required for data: 1183744
I0218 11:49:50.103956 27028 layer_factory.hpp:77] Creating layer conv1
I0218 11:49:50.103981 27028 net.cpp:128] Creating Layer conv1
I0218 11:49:50.103984 27028 net.cpp:558] conv1 <- data
I0218 11:49:50.103991 27028 net.cpp:522] conv1 -> conv1
I0218 11:49:51.115525 27028 net.cpp:172] Setting up conv1
I0218 11:49:51.115586 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.115592 27028 net.cpp:194] Memory required for data: 20058112
I0218 11:49:51.115631 27028 layer_factory.hpp:77] Creating layer conv1/bn
I0218 11:49:51.115653 27028 net.cpp:128] Creating Layer conv1/bn
I0218 11:49:51.115659 27028 net.cpp:558] conv1/bn <- conv1
I0218 11:49:51.115669 27028 net.cpp:509] conv1/bn -> conv1 (in-place)
I0218 11:49:51.115902 27028 net.cpp:172] Setting up conv1/bn
I0218 11:49:51.115913 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.115918 27028 net.cpp:194] Memory required for data: 38932480
I0218 11:49:51.115931 27028 layer_factory.hpp:77] Creating layer conv1/scale
I0218 11:49:51.115942 27028 net.cpp:128] Creating Layer conv1/scale
I0218 11:49:51.115945 27028 net.cpp:558] conv1/scale <- conv1
I0218 11:49:51.115952 27028 net.cpp:509] conv1/scale -> conv1 (in-place)
I0218 11:49:51.115998 27028 layer_factory.hpp:77] Creating layer conv1/scale
I0218 11:49:51.116133 27028 net.cpp:172] Setting up conv1/scale
I0218 11:49:51.116144 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.116149 27028 net.cpp:194] Memory required for data: 57806848
I0218 11:49:51.116156 27028 layer_factory.hpp:77] Creating layer conv1/ReLU
I0218 11:49:51.116164 27028 net.cpp:128] Creating Layer conv1/ReLU
I0218 11:49:51.116168 27028 net.cpp:558] conv1/ReLU <- conv1
I0218 11:49:51.116175 27028 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I0218 11:49:51.116405 27028 net.cpp:172] Setting up conv1/ReLU
I0218 11:49:51.116418 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.116423 27028 net.cpp:194] Memory required for data: 76681216
I0218 11:49:51.116427 27028 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I0218 11:49:51.116436 27028 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I0218 11:49:51.116439 27028 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I0218 11:49:51.116446 27028 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I0218 11:49:51.116456 27028 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I0218 11:49:51.116544 27028 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I0218 11:49:51.116552 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.116559 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.116562 27028 net.cpp:194] Memory required for data: 114429952
I0218 11:49:51.116566 27028 layer_factory.hpp:77] Creating layer conv2_1_0
I0218 11:49:51.116580 27028 net.cpp:128] Creating Layer conv2_1_0
I0218 11:49:51.116583 27028 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I0218 11:49:51.116590 27028 net.cpp:522] conv2_1_0 -> conv2_1_0
I0218 11:49:51.122390 27028 net.cpp:172] Setting up conv2_1_0
I0218 11:49:51.122414 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.122421 27028 net.cpp:194] Memory required for data: 133304320
I0218 11:49:51.122431 27028 layer_factory.hpp:77] Creating layer conv2_1_bn0
I0218 11:49:51.122442 27028 net.cpp:128] Creating Layer conv2_1_bn0
I0218 11:49:51.122450 27028 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I0218 11:49:51.122458 27028 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I0218 11:49:51.122668 27028 net.cpp:172] Setting up conv2_1_bn0
I0218 11:49:51.122680 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.122684 27028 net.cpp:194] Memory required for data: 152178688
I0218 11:49:51.122694 27028 layer_factory.hpp:77] Creating layer conv2_1_scale0
I0218 11:49:51.122699 27028 net.cpp:128] Creating Layer conv2_1_scale0
I0218 11:49:51.122704 27028 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I0218 11:49:51.122709 27028 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I0218 11:49:51.122745 27028 layer_factory.hpp:77] Creating layer conv2_1_scale0
I0218 11:49:51.122860 27028 net.cpp:172] Setting up conv2_1_scale0
I0218 11:49:51.122867 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.122871 27028 net.cpp:194] Memory required for data: 171053056
I0218 11:49:51.122879 27028 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I0218 11:49:51.122884 27028 net.cpp:128] Creating Layer conv2_1_ReLU0
I0218 11:49:51.122889 27028 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I0218 11:49:51.122894 27028 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I0218 11:49:51.124456 27028 net.cpp:172] Setting up conv2_1_ReLU0
I0218 11:49:51.124474 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.124477 27028 net.cpp:194] Memory required for data: 189927424
I0218 11:49:51.124482 27028 layer_factory.hpp:77] Creating layer conv2_1_1
I0218 11:49:51.124493 27028 net.cpp:128] Creating Layer conv2_1_1
I0218 11:49:51.124498 27028 net.cpp:558] conv2_1_1 <- conv2_1_0
I0218 11:49:51.124505 27028 net.cpp:522] conv2_1_1 -> conv2_1_1
I0218 11:49:51.131145 27028 net.cpp:172] Setting up conv2_1_1
I0218 11:49:51.131171 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.131175 27028 net.cpp:194] Memory required for data: 208801792
I0218 11:49:51.131184 27028 layer_factory.hpp:77] Creating layer conv2_1bn1
I0218 11:49:51.131194 27028 net.cpp:128] Creating Layer conv2_1bn1
I0218 11:49:51.131199 27028 net.cpp:558] conv2_1bn1 <- conv2_1_1
I0218 11:49:51.131206 27028 net.cpp:509] conv2_1bn1 -> conv2_1_1 (in-place)
I0218 11:49:51.131417 27028 net.cpp:172] Setting up conv2_1bn1
I0218 11:49:51.131424 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.131428 27028 net.cpp:194] Memory required for data: 227676160
I0218 11:49:51.131439 27028 layer_factory.hpp:77] Creating layer conv2_1_scale1
I0218 11:49:51.131448 27028 net.cpp:128] Creating Layer conv2_1_scale1
I0218 11:49:51.131451 27028 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I0218 11:49:51.131456 27028 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I0218 11:49:51.131497 27028 layer_factory.hpp:77] Creating layer conv2_1_scale1
I0218 11:49:51.131615 27028 net.cpp:172] Setting up conv2_1_scale1
I0218 11:49:51.131623 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.131626 27028 net.cpp:194] Memory required for data: 246550528
I0218 11:49:51.131633 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_1
I0218 11:49:51.131659 27028 net.cpp:128] Creating Layer conv2_Eltwise_1
I0218 11:49:51.131664 27028 net.cpp:558] conv2_Eltwise_1 <- conv1_conv1/ReLU_0_split_1
I0218 11:49:51.131669 27028 net.cpp:558] conv2_Eltwise_1 <- conv2_1_1
I0218 11:49:51.131675 27028 net.cpp:522] conv2_Eltwise_1 -> conv2_Eltwise_1
I0218 11:49:51.131703 27028 net.cpp:172] Setting up conv2_Eltwise_1
I0218 11:49:51.131711 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.131713 27028 net.cpp:194] Memory required for data: 265424896
I0218 11:49:51.131717 27028 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I0218 11:49:51.131724 27028 net.cpp:128] Creating Layer conv2_1ReLU_1
I0218 11:49:51.131728 27028 net.cpp:558] conv2_1ReLU_1 <- conv2_Eltwise_1
I0218 11:49:51.131733 27028 net.cpp:509] conv2_1ReLU_1 -> conv2_Eltwise_1 (in-place)
I0218 11:49:51.133257 27028 net.cpp:172] Setting up conv2_1ReLU_1
I0218 11:49:51.133282 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.133286 27028 net.cpp:194] Memory required for data: 284299264
I0218 11:49:51.133293 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.133306 27028 net.cpp:128] Creating Layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.133311 27028 net.cpp:558] conv2_Eltwise_1_conv2_1ReLU_1_0_split <- conv2_Eltwise_1
I0218 11:49:51.133317 27028 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I0218 11:49:51.133327 27028 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I0218 11:49:51.133374 27028 net.cpp:172] Setting up conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.133381 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.133388 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.133390 27028 net.cpp:194] Memory required for data: 322048000
I0218 11:49:51.133394 27028 layer_factory.hpp:77] Creating layer conv2_2_0
I0218 11:49:51.133406 27028 net.cpp:128] Creating Layer conv2_2_0
I0218 11:49:51.133411 27028 net.cpp:558] conv2_2_0 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I0218 11:49:51.133420 27028 net.cpp:522] conv2_2_0 -> conv2_2_0
I0218 11:49:51.139947 27028 net.cpp:172] Setting up conv2_2_0
I0218 11:49:51.139973 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.139977 27028 net.cpp:194] Memory required for data: 340922368
I0218 11:49:51.139986 27028 layer_factory.hpp:77] Creating layer conv2_2_bn0
I0218 11:49:51.139998 27028 net.cpp:128] Creating Layer conv2_2_bn0
I0218 11:49:51.140003 27028 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I0218 11:49:51.140009 27028 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I0218 11:49:51.140238 27028 net.cpp:172] Setting up conv2_2_bn0
I0218 11:49:51.140249 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.140254 27028 net.cpp:194] Memory required for data: 359796736
I0218 11:49:51.140261 27028 layer_factory.hpp:77] Creating layer conv2_2_scale0
I0218 11:49:51.140267 27028 net.cpp:128] Creating Layer conv2_2_scale0
I0218 11:49:51.140271 27028 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I0218 11:49:51.140280 27028 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I0218 11:49:51.140314 27028 layer_factory.hpp:77] Creating layer conv2_2_scale0
I0218 11:49:51.140442 27028 net.cpp:172] Setting up conv2_2_scale0
I0218 11:49:51.140455 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.140458 27028 net.cpp:194] Memory required for data: 378671104
I0218 11:49:51.140465 27028 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I0218 11:49:51.140471 27028 net.cpp:128] Creating Layer conv2_2_ReLU0
I0218 11:49:51.140475 27028 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I0218 11:49:51.140480 27028 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I0218 11:49:51.141999 27028 net.cpp:172] Setting up conv2_2_ReLU0
I0218 11:49:51.142015 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.142020 27028 net.cpp:194] Memory required for data: 397545472
I0218 11:49:51.142040 27028 layer_factory.hpp:77] Creating layer conv2_2_1
I0218 11:49:51.142056 27028 net.cpp:128] Creating Layer conv2_2_1
I0218 11:49:51.142061 27028 net.cpp:558] conv2_2_1 <- conv2_2_0
I0218 11:49:51.142069 27028 net.cpp:522] conv2_2_1 -> conv2_2_1
I0218 11:49:51.148706 27028 net.cpp:172] Setting up conv2_2_1
I0218 11:49:51.148732 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.148736 27028 net.cpp:194] Memory required for data: 416419840
I0218 11:49:51.148746 27028 layer_factory.hpp:77] Creating layer conv2_2bn1
I0218 11:49:51.148756 27028 net.cpp:128] Creating Layer conv2_2bn1
I0218 11:49:51.148761 27028 net.cpp:558] conv2_2bn1 <- conv2_2_1
I0218 11:49:51.148772 27028 net.cpp:509] conv2_2bn1 -> conv2_2_1 (in-place)
I0218 11:49:51.149005 27028 net.cpp:172] Setting up conv2_2bn1
I0218 11:49:51.149017 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.149022 27028 net.cpp:194] Memory required for data: 435294208
I0218 11:49:51.149035 27028 layer_factory.hpp:77] Creating layer conv2_2_scale1
I0218 11:49:51.149042 27028 net.cpp:128] Creating Layer conv2_2_scale1
I0218 11:49:51.149046 27028 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I0218 11:49:51.149051 27028 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I0218 11:49:51.149089 27028 layer_factory.hpp:77] Creating layer conv2_2_scale1
I0218 11:49:51.149219 27028 net.cpp:172] Setting up conv2_2_scale1
I0218 11:49:51.149232 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.149236 27028 net.cpp:194] Memory required for data: 454168576
I0218 11:49:51.149243 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_2
I0218 11:49:51.149250 27028 net.cpp:128] Creating Layer conv2_Eltwise_2
I0218 11:49:51.149253 27028 net.cpp:558] conv2_Eltwise_2 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I0218 11:49:51.149258 27028 net.cpp:558] conv2_Eltwise_2 <- conv2_2_1
I0218 11:49:51.149267 27028 net.cpp:522] conv2_Eltwise_2 -> conv2_Eltwise_2
I0218 11:49:51.149296 27028 net.cpp:172] Setting up conv2_Eltwise_2
I0218 11:49:51.149302 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.149305 27028 net.cpp:194] Memory required for data: 473042944
I0218 11:49:51.149309 27028 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I0218 11:49:51.149317 27028 net.cpp:128] Creating Layer conv2_2ReLU_1
I0218 11:49:51.149320 27028 net.cpp:558] conv2_2ReLU_1 <- conv2_Eltwise_2
I0218 11:49:51.149329 27028 net.cpp:509] conv2_2ReLU_1 -> conv2_Eltwise_2 (in-place)
I0218 11:49:51.150112 27028 net.cpp:172] Setting up conv2_2ReLU_1
I0218 11:49:51.150127 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.150131 27028 net.cpp:194] Memory required for data: 491917312
I0218 11:49:51.150135 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.150146 27028 net.cpp:128] Creating Layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.150149 27028 net.cpp:558] conv2_Eltwise_2_conv2_2ReLU_1_0_split <- conv2_Eltwise_2
I0218 11:49:51.150156 27028 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I0218 11:49:51.150163 27028 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I0218 11:49:51.150211 27028 net.cpp:172] Setting up conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.150218 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.150223 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.150228 27028 net.cpp:194] Memory required for data: 529666048
I0218 11:49:51.150233 27028 layer_factory.hpp:77] Creating layer conv2_3_0
I0218 11:49:51.150244 27028 net.cpp:128] Creating Layer conv2_3_0
I0218 11:49:51.150249 27028 net.cpp:558] conv2_3_0 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I0218 11:49:51.150257 27028 net.cpp:522] conv2_3_0 -> conv2_3_0
I0218 11:49:51.151511 27028 net.cpp:172] Setting up conv2_3_0
I0218 11:49:51.151535 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.151540 27028 net.cpp:194] Memory required for data: 548540416
I0218 11:49:51.151566 27028 layer_factory.hpp:77] Creating layer conv2_3_bn0
I0218 11:49:51.151576 27028 net.cpp:128] Creating Layer conv2_3_bn0
I0218 11:49:51.151582 27028 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I0218 11:49:51.151588 27028 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I0218 11:49:51.151826 27028 net.cpp:172] Setting up conv2_3_bn0
I0218 11:49:51.151839 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.151842 27028 net.cpp:194] Memory required for data: 567414784
I0218 11:49:51.151851 27028 layer_factory.hpp:77] Creating layer conv2_3_scale0
I0218 11:49:51.151859 27028 net.cpp:128] Creating Layer conv2_3_scale0
I0218 11:49:51.151862 27028 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I0218 11:49:51.151867 27028 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I0218 11:49:51.151907 27028 layer_factory.hpp:77] Creating layer conv2_3_scale0
I0218 11:49:51.152041 27028 net.cpp:172] Setting up conv2_3_scale0
I0218 11:49:51.152048 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.152052 27028 net.cpp:194] Memory required for data: 586289152
I0218 11:49:51.152060 27028 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I0218 11:49:51.152068 27028 net.cpp:128] Creating Layer conv2_3_ReLU0
I0218 11:49:51.152072 27028 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I0218 11:49:51.152077 27028 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I0218 11:49:51.152535 27028 net.cpp:172] Setting up conv2_3_ReLU0
I0218 11:49:51.152559 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.152565 27028 net.cpp:194] Memory required for data: 605163520
I0218 11:49:51.152570 27028 layer_factory.hpp:77] Creating layer conv2_3_1
I0218 11:49:51.152582 27028 net.cpp:128] Creating Layer conv2_3_1
I0218 11:49:51.152587 27028 net.cpp:558] conv2_3_1 <- conv2_3_0
I0218 11:49:51.152595 27028 net.cpp:522] conv2_3_1 -> conv2_3_1
I0218 11:49:51.153887 27028 net.cpp:172] Setting up conv2_3_1
I0218 11:49:51.153913 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.153918 27028 net.cpp:194] Memory required for data: 624037888
I0218 11:49:51.153926 27028 layer_factory.hpp:77] Creating layer conv2_3bn1
I0218 11:49:51.153951 27028 net.cpp:128] Creating Layer conv2_3bn1
I0218 11:49:51.153959 27028 net.cpp:558] conv2_3bn1 <- conv2_3_1
I0218 11:49:51.153966 27028 net.cpp:509] conv2_3bn1 -> conv2_3_1 (in-place)
I0218 11:49:51.154201 27028 net.cpp:172] Setting up conv2_3bn1
I0218 11:49:51.154213 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154217 27028 net.cpp:194] Memory required for data: 642912256
I0218 11:49:51.154227 27028 layer_factory.hpp:77] Creating layer conv2_3_scale1
I0218 11:49:51.154233 27028 net.cpp:128] Creating Layer conv2_3_scale1
I0218 11:49:51.154237 27028 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I0218 11:49:51.154248 27028 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I0218 11:49:51.154285 27028 layer_factory.hpp:77] Creating layer conv2_3_scale1
I0218 11:49:51.154417 27028 net.cpp:172] Setting up conv2_3_scale1
I0218 11:49:51.154433 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154436 27028 net.cpp:194] Memory required for data: 661786624
I0218 11:49:51.154443 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_3
I0218 11:49:51.154450 27028 net.cpp:128] Creating Layer conv2_Eltwise_3
I0218 11:49:51.154455 27028 net.cpp:558] conv2_Eltwise_3 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I0218 11:49:51.154460 27028 net.cpp:558] conv2_Eltwise_3 <- conv2_3_1
I0218 11:49:51.154466 27028 net.cpp:522] conv2_Eltwise_3 -> conv2_Eltwise_3
I0218 11:49:51.154494 27028 net.cpp:172] Setting up conv2_Eltwise_3
I0218 11:49:51.154500 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154505 27028 net.cpp:194] Memory required for data: 680660992
I0218 11:49:51.154508 27028 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I0218 11:49:51.154517 27028 net.cpp:128] Creating Layer conv2_3ReLU_1
I0218 11:49:51.154522 27028 net.cpp:558] conv2_3ReLU_1 <- conv2_Eltwise_3
I0218 11:49:51.154527 27028 net.cpp:509] conv2_3ReLU_1 -> conv2_Eltwise_3 (in-place)
I0218 11:49:51.154778 27028 net.cpp:172] Setting up conv2_3ReLU_1
I0218 11:49:51.154788 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154793 27028 net.cpp:194] Memory required for data: 699535360
I0218 11:49:51.154796 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.154805 27028 net.cpp:128] Creating Layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.154810 27028 net.cpp:558] conv2_Eltwise_3_conv2_3ReLU_1_0_split <- conv2_Eltwise_3
I0218 11:49:51.154816 27028 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I0218 11:49:51.154824 27028 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I0218 11:49:51.154870 27028 net.cpp:172] Setting up conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.154877 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154883 27028 net.cpp:186] Top shape: 128 16 32 72 (4718592)
I0218 11:49:51.154887 27028 net.cpp:194] Memory required for data: 737284096
I0218 11:49:51.154891 27028 layer_factory.hpp:77] Creating layer conv3_1_0
I0218 11:49:51.154904 27028 net.cpp:128] Creating Layer conv3_1_0
I0218 11:49:51.154908 27028 net.cpp:558] conv3_1_0 <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I0218 11:49:51.154916 27028 net.cpp:522] conv3_1_0 -> conv3_1_0
I0218 11:49:51.163079 27028 net.cpp:172] Setting up conv3_1_0
I0218 11:49:51.163128 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.163133 27028 net.cpp:194] Memory required for data: 746721280
I0218 11:49:51.163156 27028 layer_factory.hpp:77] Creating layer conv3_1_bn0
I0218 11:49:51.163170 27028 net.cpp:128] Creating Layer conv3_1_bn0
I0218 11:49:51.163177 27028 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I0218 11:49:51.163187 27028 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I0218 11:49:51.163449 27028 net.cpp:172] Setting up conv3_1_bn0
I0218 11:49:51.163458 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.163462 27028 net.cpp:194] Memory required for data: 756158464
I0218 11:49:51.163471 27028 layer_factory.hpp:77] Creating layer conv3_1_scale0
I0218 11:49:51.163480 27028 net.cpp:128] Creating Layer conv3_1_scale0
I0218 11:49:51.163484 27028 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I0218 11:49:51.163492 27028 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I0218 11:49:51.163532 27028 layer_factory.hpp:77] Creating layer conv3_1_scale0
I0218 11:49:51.163676 27028 net.cpp:172] Setting up conv3_1_scale0
I0218 11:49:51.163686 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.163691 27028 net.cpp:194] Memory required for data: 765595648
I0218 11:49:51.163698 27028 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I0218 11:49:51.163707 27028 net.cpp:128] Creating Layer conv3_1_ReLU0
I0218 11:49:51.163710 27028 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I0218 11:49:51.163718 27028 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I0218 11:49:51.163957 27028 net.cpp:172] Setting up conv3_1_ReLU0
I0218 11:49:51.163970 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.163975 27028 net.cpp:194] Memory required for data: 775032832
I0218 11:49:51.163980 27028 layer_factory.hpp:77] Creating layer conv3_1_1
I0218 11:49:51.163995 27028 net.cpp:128] Creating Layer conv3_1_1
I0218 11:49:51.164000 27028 net.cpp:558] conv3_1_1 <- conv3_1_0
I0218 11:49:51.164008 27028 net.cpp:522] conv3_1_1 -> conv3_1_1
I0218 11:49:51.170226 27028 net.cpp:172] Setting up conv3_1_1
I0218 11:49:51.170250 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.170254 27028 net.cpp:194] Memory required for data: 784470016
I0218 11:49:51.170264 27028 layer_factory.hpp:77] Creating layer conv3_1bn1
I0218 11:49:51.170274 27028 net.cpp:128] Creating Layer conv3_1bn1
I0218 11:49:51.170280 27028 net.cpp:558] conv3_1bn1 <- conv3_1_1
I0218 11:49:51.170289 27028 net.cpp:509] conv3_1bn1 -> conv3_1_1 (in-place)
I0218 11:49:51.170536 27028 net.cpp:172] Setting up conv3_1bn1
I0218 11:49:51.170542 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.170567 27028 net.cpp:194] Memory required for data: 793907200
I0218 11:49:51.170578 27028 layer_factory.hpp:77] Creating layer conv3_1_scale1
I0218 11:49:51.170588 27028 net.cpp:128] Creating Layer conv3_1_scale1
I0218 11:49:51.170593 27028 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I0218 11:49:51.170599 27028 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I0218 11:49:51.170639 27028 layer_factory.hpp:77] Creating layer conv3_1_scale1
I0218 11:49:51.170783 27028 net.cpp:172] Setting up conv3_1_scale1
I0218 11:49:51.170796 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.170800 27028 net.cpp:194] Memory required for data: 803344384
I0218 11:49:51.170809 27028 layer_factory.hpp:77] Creating layer conv3_1_down
I0218 11:49:51.170823 27028 net.cpp:128] Creating Layer conv3_1_down
I0218 11:49:51.170830 27028 net.cpp:558] conv3_1_down <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I0218 11:49:51.170836 27028 net.cpp:522] conv3_1_down -> conv3_1_down
I0218 11:49:51.176846 27028 net.cpp:172] Setting up conv3_1_down
I0218 11:49:51.176872 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.176877 27028 net.cpp:194] Memory required for data: 812781568
I0218 11:49:51.176898 27028 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I0218 11:49:51.176908 27028 net.cpp:128] Creating Layer conv3_1_bn_down
I0218 11:49:51.176913 27028 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I0218 11:49:51.176928 27028 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I0218 11:49:51.177165 27028 net.cpp:172] Setting up conv3_1_bn_down
I0218 11:49:51.177178 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.177182 27028 net.cpp:194] Memory required for data: 822218752
I0218 11:49:51.177191 27028 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I0218 11:49:51.177197 27028 net.cpp:128] Creating Layer conv3_1_scale_down
I0218 11:49:51.177201 27028 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I0218 11:49:51.177209 27028 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I0218 11:49:51.177247 27028 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I0218 11:49:51.177383 27028 net.cpp:172] Setting up conv3_1_scale_down
I0218 11:49:51.177390 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.177394 27028 net.cpp:194] Memory required for data: 831655936
I0218 11:49:51.177402 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_1
I0218 11:49:51.177409 27028 net.cpp:128] Creating Layer conv3_Eltwise_1
I0218 11:49:51.177413 27028 net.cpp:558] conv3_Eltwise_1 <- conv3_1_1
I0218 11:49:51.177418 27028 net.cpp:558] conv3_Eltwise_1 <- conv3_1_down
I0218 11:49:51.177426 27028 net.cpp:522] conv3_Eltwise_1 -> conv3_Eltwise_1
I0218 11:49:51.177453 27028 net.cpp:172] Setting up conv3_Eltwise_1
I0218 11:49:51.177464 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.177467 27028 net.cpp:194] Memory required for data: 841093120
I0218 11:49:51.177471 27028 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I0218 11:49:51.177477 27028 net.cpp:128] Creating Layer conv3_1ReLU_1
I0218 11:49:51.177482 27028 net.cpp:558] conv3_1ReLU_1 <- conv3_Eltwise_1
I0218 11:49:51.177487 27028 net.cpp:509] conv3_1ReLU_1 -> conv3_Eltwise_1 (in-place)
I0218 11:49:51.178894 27028 net.cpp:172] Setting up conv3_1ReLU_1
I0218 11:49:51.178913 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.178918 27028 net.cpp:194] Memory required for data: 850530304
I0218 11:49:51.178922 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.178930 27028 net.cpp:128] Creating Layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.178936 27028 net.cpp:558] conv3_Eltwise_1_conv3_1ReLU_1_0_split <- conv3_Eltwise_1
I0218 11:49:51.178942 27028 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I0218 11:49:51.178953 27028 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I0218 11:49:51.179002 27028 net.cpp:172] Setting up conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.179023 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.179029 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.179033 27028 net.cpp:194] Memory required for data: 869404672
I0218 11:49:51.179038 27028 layer_factory.hpp:77] Creating layer conv3_2_0
I0218 11:49:51.179052 27028 net.cpp:128] Creating Layer conv3_2_0
I0218 11:49:51.179056 27028 net.cpp:558] conv3_2_0 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I0218 11:49:51.179064 27028 net.cpp:522] conv3_2_0 -> conv3_2_0
I0218 11:49:51.183007 27028 net.cpp:172] Setting up conv3_2_0
I0218 11:49:51.183032 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.183035 27028 net.cpp:194] Memory required for data: 878841856
I0218 11:49:51.183045 27028 layer_factory.hpp:77] Creating layer conv3_2_bn0
I0218 11:49:51.183055 27028 net.cpp:128] Creating Layer conv3_2_bn0
I0218 11:49:51.183060 27028 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I0218 11:49:51.183069 27028 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I0218 11:49:51.183313 27028 net.cpp:172] Setting up conv3_2_bn0
I0218 11:49:51.183323 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.183327 27028 net.cpp:194] Memory required for data: 888279040
I0218 11:49:51.183336 27028 layer_factory.hpp:77] Creating layer conv3_2_scale0
I0218 11:49:51.183344 27028 net.cpp:128] Creating Layer conv3_2_scale0
I0218 11:49:51.183349 27028 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I0218 11:49:51.183354 27028 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I0218 11:49:51.183393 27028 layer_factory.hpp:77] Creating layer conv3_2_scale0
I0218 11:49:51.183542 27028 net.cpp:172] Setting up conv3_2_scale0
I0218 11:49:51.183548 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.183552 27028 net.cpp:194] Memory required for data: 897716224
I0218 11:49:51.183559 27028 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I0218 11:49:51.183565 27028 net.cpp:128] Creating Layer conv3_2_ReLU0
I0218 11:49:51.183569 27028 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I0218 11:49:51.183578 27028 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I0218 11:49:51.184044 27028 net.cpp:172] Setting up conv3_2_ReLU0
I0218 11:49:51.184062 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.184065 27028 net.cpp:194] Memory required for data: 907153408
I0218 11:49:51.184070 27028 layer_factory.hpp:77] Creating layer conv3_2_1
I0218 11:49:51.184087 27028 net.cpp:128] Creating Layer conv3_2_1
I0218 11:49:51.184092 27028 net.cpp:558] conv3_2_1 <- conv3_2_0
I0218 11:49:51.184103 27028 net.cpp:522] conv3_2_1 -> conv3_2_1
I0218 11:49:51.190366 27028 net.cpp:172] Setting up conv3_2_1
I0218 11:49:51.190393 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.190397 27028 net.cpp:194] Memory required for data: 916590592
I0218 11:49:51.190412 27028 layer_factory.hpp:77] Creating layer conv3_2bn1
I0218 11:49:51.190424 27028 net.cpp:128] Creating Layer conv3_2bn1
I0218 11:49:51.190429 27028 net.cpp:558] conv3_2bn1 <- conv3_2_1
I0218 11:49:51.190443 27028 net.cpp:509] conv3_2bn1 -> conv3_2_1 (in-place)
I0218 11:49:51.190696 27028 net.cpp:172] Setting up conv3_2bn1
I0218 11:49:51.190702 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.190706 27028 net.cpp:194] Memory required for data: 926027776
I0218 11:49:51.190714 27028 layer_factory.hpp:77] Creating layer conv3_2_scale1
I0218 11:49:51.190726 27028 net.cpp:128] Creating Layer conv3_2_scale1
I0218 11:49:51.190729 27028 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I0218 11:49:51.190735 27028 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I0218 11:49:51.190773 27028 layer_factory.hpp:77] Creating layer conv3_2_scale1
I0218 11:49:51.190920 27028 net.cpp:172] Setting up conv3_2_scale1
I0218 11:49:51.190927 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.190932 27028 net.cpp:194] Memory required for data: 935464960
I0218 11:49:51.190937 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_2
I0218 11:49:51.190949 27028 net.cpp:128] Creating Layer conv3_Eltwise_2
I0218 11:49:51.190977 27028 net.cpp:558] conv3_Eltwise_2 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I0218 11:49:51.190982 27028 net.cpp:558] conv3_Eltwise_2 <- conv3_2_1
I0218 11:49:51.190989 27028 net.cpp:522] conv3_Eltwise_2 -> conv3_Eltwise_2
I0218 11:49:51.191025 27028 net.cpp:172] Setting up conv3_Eltwise_2
I0218 11:49:51.191033 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.191037 27028 net.cpp:194] Memory required for data: 944902144
I0218 11:49:51.191041 27028 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I0218 11:49:51.191049 27028 net.cpp:128] Creating Layer conv3_2ReLU_1
I0218 11:49:51.191053 27028 net.cpp:558] conv3_2ReLU_1 <- conv3_Eltwise_2
I0218 11:49:51.191058 27028 net.cpp:509] conv3_2ReLU_1 -> conv3_Eltwise_2 (in-place)
I0218 11:49:51.192404 27028 net.cpp:172] Setting up conv3_2ReLU_1
I0218 11:49:51.192412 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.192416 27028 net.cpp:194] Memory required for data: 954339328
I0218 11:49:51.192421 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.192428 27028 net.cpp:128] Creating Layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.192432 27028 net.cpp:558] conv3_Eltwise_2_conv3_2ReLU_1_0_split <- conv3_Eltwise_2
I0218 11:49:51.192441 27028 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I0218 11:49:51.192450 27028 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I0218 11:49:51.192499 27028 net.cpp:172] Setting up conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.192507 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.192512 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.192517 27028 net.cpp:194] Memory required for data: 973213696
I0218 11:49:51.192520 27028 layer_factory.hpp:77] Creating layer conv3_3_0
I0218 11:49:51.192538 27028 net.cpp:128] Creating Layer conv3_3_0
I0218 11:49:51.192543 27028 net.cpp:558] conv3_3_0 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I0218 11:49:51.192553 27028 net.cpp:522] conv3_3_0 -> conv3_3_0
I0218 11:49:51.199131 27028 net.cpp:172] Setting up conv3_3_0
I0218 11:49:51.199169 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.199174 27028 net.cpp:194] Memory required for data: 982650880
I0218 11:49:51.199189 27028 layer_factory.hpp:77] Creating layer conv3_3_bn0
I0218 11:49:51.199203 27028 net.cpp:128] Creating Layer conv3_3_bn0
I0218 11:49:51.199208 27028 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I0218 11:49:51.199219 27028 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I0218 11:49:51.199479 27028 net.cpp:172] Setting up conv3_3_bn0
I0218 11:49:51.199489 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.199493 27028 net.cpp:194] Memory required for data: 992088064
I0218 11:49:51.199502 27028 layer_factory.hpp:77] Creating layer conv3_3_scale0
I0218 11:49:51.199513 27028 net.cpp:128] Creating Layer conv3_3_scale0
I0218 11:49:51.199518 27028 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I0218 11:49:51.199523 27028 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I0218 11:49:51.199564 27028 layer_factory.hpp:77] Creating layer conv3_3_scale0
I0218 11:49:51.199714 27028 net.cpp:172] Setting up conv3_3_scale0
I0218 11:49:51.199724 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.199728 27028 net.cpp:194] Memory required for data: 1001525248
I0218 11:49:51.199735 27028 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I0218 11:49:51.199745 27028 net.cpp:128] Creating Layer conv3_3_ReLU0
I0218 11:49:51.199749 27028 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I0218 11:49:51.199755 27028 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I0218 11:49:51.201161 27028 net.cpp:172] Setting up conv3_3_ReLU0
I0218 11:49:51.201179 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.201182 27028 net.cpp:194] Memory required for data: 1010962432
I0218 11:49:51.201187 27028 layer_factory.hpp:77] Creating layer conv3_3_1
I0218 11:49:51.201236 27028 net.cpp:128] Creating Layer conv3_3_1
I0218 11:49:51.201241 27028 net.cpp:558] conv3_3_1 <- conv3_3_0
I0218 11:49:51.201248 27028 net.cpp:522] conv3_3_1 -> conv3_3_1
I0218 11:49:51.207851 27028 net.cpp:172] Setting up conv3_3_1
I0218 11:49:51.207880 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.207885 27028 net.cpp:194] Memory required for data: 1020399616
I0218 11:49:51.207898 27028 layer_factory.hpp:77] Creating layer conv3_3bn1
I0218 11:49:51.207906 27028 net.cpp:128] Creating Layer conv3_3bn1
I0218 11:49:51.207911 27028 net.cpp:558] conv3_3bn1 <- conv3_3_1
I0218 11:49:51.207921 27028 net.cpp:509] conv3_3bn1 -> conv3_3_1 (in-place)
I0218 11:49:51.208178 27028 net.cpp:172] Setting up conv3_3bn1
I0218 11:49:51.208190 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.208194 27028 net.cpp:194] Memory required for data: 1029836800
I0218 11:49:51.208204 27028 layer_factory.hpp:77] Creating layer conv3_3_scale1
I0218 11:49:51.208212 27028 net.cpp:128] Creating Layer conv3_3_scale1
I0218 11:49:51.208216 27028 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I0218 11:49:51.208222 27028 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I0218 11:49:51.208259 27028 layer_factory.hpp:77] Creating layer conv3_3_scale1
I0218 11:49:51.208400 27028 net.cpp:172] Setting up conv3_3_scale1
I0218 11:49:51.208411 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.208415 27028 net.cpp:194] Memory required for data: 1039273984
I0218 11:49:51.208423 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_3
I0218 11:49:51.208433 27028 net.cpp:128] Creating Layer conv3_Eltwise_3
I0218 11:49:51.208439 27028 net.cpp:558] conv3_Eltwise_3 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I0218 11:49:51.208444 27028 net.cpp:558] conv3_Eltwise_3 <- conv3_3_1
I0218 11:49:51.208451 27028 net.cpp:522] conv3_Eltwise_3 -> conv3_Eltwise_3
I0218 11:49:51.208478 27028 net.cpp:172] Setting up conv3_Eltwise_3
I0218 11:49:51.208485 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.208489 27028 net.cpp:194] Memory required for data: 1048711168
I0218 11:49:51.208493 27028 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I0218 11:49:51.208500 27028 net.cpp:128] Creating Layer conv3_3ReLU_1
I0218 11:49:51.208504 27028 net.cpp:558] conv3_3ReLU_1 <- conv3_Eltwise_3
I0218 11:49:51.208513 27028 net.cpp:509] conv3_3ReLU_1 -> conv3_Eltwise_3 (in-place)
I0218 11:49:51.209899 27028 net.cpp:172] Setting up conv3_3ReLU_1
I0218 11:49:51.209925 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.209929 27028 net.cpp:194] Memory required for data: 1058148352
I0218 11:49:51.209947 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.210038 27028 net.cpp:128] Creating Layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.210043 27028 net.cpp:558] conv3_Eltwise_3_conv3_3ReLU_1_0_split <- conv3_Eltwise_3
I0218 11:49:51.210053 27028 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I0218 11:49:51.210062 27028 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I0218 11:49:51.210116 27028 net.cpp:172] Setting up conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.210122 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.210129 27028 net.cpp:186] Top shape: 128 32 16 36 (2359296)
I0218 11:49:51.210131 27028 net.cpp:194] Memory required for data: 1077022720
I0218 11:49:51.210135 27028 layer_factory.hpp:77] Creating layer conv4_1_0
I0218 11:49:51.210147 27028 net.cpp:128] Creating Layer conv4_1_0
I0218 11:49:51.210152 27028 net.cpp:558] conv4_1_0 <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I0218 11:49:51.210161 27028 net.cpp:522] conv4_1_0 -> conv4_1_0
I0218 11:49:51.216817 27028 net.cpp:172] Setting up conv4_1_0
I0218 11:49:51.216846 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.216850 27028 net.cpp:194] Memory required for data: 1081741312
I0218 11:49:51.216862 27028 layer_factory.hpp:77] Creating layer conv4_1_bn0
I0218 11:49:51.216897 27028 net.cpp:128] Creating Layer conv4_1_bn0
I0218 11:49:51.216903 27028 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I0218 11:49:51.216912 27028 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I0218 11:49:51.217180 27028 net.cpp:172] Setting up conv4_1_bn0
I0218 11:49:51.217193 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.217197 27028 net.cpp:194] Memory required for data: 1086459904
I0218 11:49:51.217206 27028 layer_factory.hpp:77] Creating layer conv4_1_scale0
I0218 11:49:51.217217 27028 net.cpp:128] Creating Layer conv4_1_scale0
I0218 11:49:51.217221 27028 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I0218 11:49:51.217227 27028 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I0218 11:49:51.217268 27028 layer_factory.hpp:77] Creating layer conv4_1_scale0
I0218 11:49:51.217416 27028 net.cpp:172] Setting up conv4_1_scale0
I0218 11:49:51.217427 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.217432 27028 net.cpp:194] Memory required for data: 1091178496
I0218 11:49:51.217439 27028 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I0218 11:49:51.217445 27028 net.cpp:128] Creating Layer conv4_1_ReLU0
I0218 11:49:51.217449 27028 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I0218 11:49:51.217456 27028 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I0218 11:49:51.218689 27028 net.cpp:172] Setting up conv4_1_ReLU0
I0218 11:49:51.218705 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.218709 27028 net.cpp:194] Memory required for data: 1095897088
I0218 11:49:51.218714 27028 layer_factory.hpp:77] Creating layer conv4_1_1
I0218 11:49:51.218730 27028 net.cpp:128] Creating Layer conv4_1_1
I0218 11:49:51.218735 27028 net.cpp:558] conv4_1_1 <- conv4_1_0
I0218 11:49:51.218746 27028 net.cpp:522] conv4_1_1 -> conv4_1_1
I0218 11:49:51.225600 27028 net.cpp:172] Setting up conv4_1_1
I0218 11:49:51.225627 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.225631 27028 net.cpp:194] Memory required for data: 1100615680
I0218 11:49:51.225641 27028 layer_factory.hpp:77] Creating layer conv4_1bn1
I0218 11:49:51.225652 27028 net.cpp:128] Creating Layer conv4_1bn1
I0218 11:49:51.225657 27028 net.cpp:558] conv4_1bn1 <- conv4_1_1
I0218 11:49:51.225664 27028 net.cpp:509] conv4_1bn1 -> conv4_1_1 (in-place)
I0218 11:49:51.225929 27028 net.cpp:172] Setting up conv4_1bn1
I0218 11:49:51.225951 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.225956 27028 net.cpp:194] Memory required for data: 1105334272
I0218 11:49:51.225965 27028 layer_factory.hpp:77] Creating layer conv4_1_scale1
I0218 11:49:51.225975 27028 net.cpp:128] Creating Layer conv4_1_scale1
I0218 11:49:51.225980 27028 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I0218 11:49:51.225984 27028 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I0218 11:49:51.226029 27028 layer_factory.hpp:77] Creating layer conv4_1_scale1
I0218 11:49:51.226174 27028 net.cpp:172] Setting up conv4_1_scale1
I0218 11:49:51.226182 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.226186 27028 net.cpp:194] Memory required for data: 1110052864
I0218 11:49:51.226193 27028 layer_factory.hpp:77] Creating layer conv4_1_down
I0218 11:49:51.226207 27028 net.cpp:128] Creating Layer conv4_1_down
I0218 11:49:51.226212 27028 net.cpp:558] conv4_1_down <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I0218 11:49:51.226219 27028 net.cpp:522] conv4_1_down -> conv4_1_down
I0218 11:49:51.231983 27028 net.cpp:172] Setting up conv4_1_down
I0218 11:49:51.232012 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.232015 27028 net.cpp:194] Memory required for data: 1114771456
I0218 11:49:51.232028 27028 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I0218 11:49:51.232038 27028 net.cpp:128] Creating Layer conv4_1_bn_down
I0218 11:49:51.232043 27028 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I0218 11:49:51.232054 27028 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I0218 11:49:51.232304 27028 net.cpp:172] Setting up conv4_1_bn_down
I0218 11:49:51.232353 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.232370 27028 net.cpp:194] Memory required for data: 1119490048
I0218 11:49:51.232381 27028 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I0218 11:49:51.232389 27028 net.cpp:128] Creating Layer conv4_1_scale_down
I0218 11:49:51.232393 27028 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I0218 11:49:51.232401 27028 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I0218 11:49:51.232451 27028 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I0218 11:49:51.232601 27028 net.cpp:172] Setting up conv4_1_scale_down
I0218 11:49:51.232609 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.232612 27028 net.cpp:194] Memory required for data: 1124208640
I0218 11:49:51.232620 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_1
I0218 11:49:51.232626 27028 net.cpp:128] Creating Layer conv4_Eltwise_1
I0218 11:49:51.232630 27028 net.cpp:558] conv4_Eltwise_1 <- conv4_1_1
I0218 11:49:51.232635 27028 net.cpp:558] conv4_Eltwise_1 <- conv4_1_down
I0218 11:49:51.232645 27028 net.cpp:522] conv4_Eltwise_1 -> conv4_Eltwise_1
I0218 11:49:51.232672 27028 net.cpp:172] Setting up conv4_Eltwise_1
I0218 11:49:51.232679 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.232683 27028 net.cpp:194] Memory required for data: 1128927232
I0218 11:49:51.232686 27028 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I0218 11:49:51.232692 27028 net.cpp:128] Creating Layer conv4_1ReLU_1
I0218 11:49:51.232697 27028 net.cpp:558] conv4_1ReLU_1 <- conv4_Eltwise_1
I0218 11:49:51.232702 27028 net.cpp:509] conv4_1ReLU_1 -> conv4_Eltwise_1 (in-place)
I0218 11:49:51.234025 27028 net.cpp:172] Setting up conv4_1ReLU_1
I0218 11:49:51.234035 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.234040 27028 net.cpp:194] Memory required for data: 1133645824
I0218 11:49:51.234043 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.234051 27028 net.cpp:128] Creating Layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.234056 27028 net.cpp:558] conv4_Eltwise_1_conv4_1ReLU_1_0_split <- conv4_Eltwise_1
I0218 11:49:51.234064 27028 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I0218 11:49:51.234072 27028 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I0218 11:49:51.234123 27028 net.cpp:172] Setting up conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.234130 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.234135 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.234139 27028 net.cpp:194] Memory required for data: 1143083008
I0218 11:49:51.234143 27028 layer_factory.hpp:77] Creating layer conv4_2_0
I0218 11:49:51.234158 27028 net.cpp:128] Creating Layer conv4_2_0
I0218 11:49:51.234163 27028 net.cpp:558] conv4_2_0 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I0218 11:49:51.234172 27028 net.cpp:522] conv4_2_0 -> conv4_2_0
I0218 11:49:51.240756 27028 net.cpp:172] Setting up conv4_2_0
I0218 11:49:51.240782 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.240785 27028 net.cpp:194] Memory required for data: 1147801600
I0218 11:49:51.240799 27028 layer_factory.hpp:77] Creating layer conv4_2_bn0
I0218 11:49:51.240810 27028 net.cpp:128] Creating Layer conv4_2_bn0
I0218 11:49:51.240815 27028 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I0218 11:49:51.240824 27028 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I0218 11:49:51.241070 27028 net.cpp:172] Setting up conv4_2_bn0
I0218 11:49:51.241080 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.241084 27028 net.cpp:194] Memory required for data: 1152520192
I0218 11:49:51.241092 27028 layer_factory.hpp:77] Creating layer conv4_2_scale0
I0218 11:49:51.241099 27028 net.cpp:128] Creating Layer conv4_2_scale0
I0218 11:49:51.241104 27028 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I0218 11:49:51.241109 27028 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I0218 11:49:51.241153 27028 layer_factory.hpp:77] Creating layer conv4_2_scale0
I0218 11:49:51.241317 27028 net.cpp:172] Setting up conv4_2_scale0
I0218 11:49:51.241328 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.241331 27028 net.cpp:194] Memory required for data: 1157238784
I0218 11:49:51.241338 27028 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I0218 11:49:51.241348 27028 net.cpp:128] Creating Layer conv4_2_ReLU0
I0218 11:49:51.241353 27028 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I0218 11:49:51.241360 27028 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I0218 11:49:51.242836 27028 net.cpp:172] Setting up conv4_2_ReLU0
I0218 11:49:51.242861 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.242866 27028 net.cpp:194] Memory required for data: 1161957376
I0218 11:49:51.242871 27028 layer_factory.hpp:77] Creating layer conv4_2_1
I0218 11:49:51.242892 27028 net.cpp:128] Creating Layer conv4_2_1
I0218 11:49:51.242908 27028 net.cpp:558] conv4_2_1 <- conv4_2_0
I0218 11:49:51.242918 27028 net.cpp:522] conv4_2_1 -> conv4_2_1
I0218 11:49:51.249763 27028 net.cpp:172] Setting up conv4_2_1
I0218 11:49:51.249788 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.249794 27028 net.cpp:194] Memory required for data: 1166675968
I0218 11:49:51.249806 27028 layer_factory.hpp:77] Creating layer conv4_2bn1
I0218 11:49:51.249817 27028 net.cpp:128] Creating Layer conv4_2bn1
I0218 11:49:51.249828 27028 net.cpp:558] conv4_2bn1 <- conv4_2_1
I0218 11:49:51.249837 27028 net.cpp:509] conv4_2bn1 -> conv4_2_1 (in-place)
I0218 11:49:51.250119 27028 net.cpp:172] Setting up conv4_2bn1
I0218 11:49:51.250131 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.250135 27028 net.cpp:194] Memory required for data: 1171394560
I0218 11:49:51.250156 27028 layer_factory.hpp:77] Creating layer conv4_2_scale1
I0218 11:49:51.250169 27028 net.cpp:128] Creating Layer conv4_2_scale1
I0218 11:49:51.250174 27028 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I0218 11:49:51.250180 27028 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I0218 11:49:51.250224 27028 layer_factory.hpp:77] Creating layer conv4_2_scale1
I0218 11:49:51.250370 27028 net.cpp:172] Setting up conv4_2_scale1
I0218 11:49:51.250380 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.250383 27028 net.cpp:194] Memory required for data: 1176113152
I0218 11:49:51.250391 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_2
I0218 11:49:51.250399 27028 net.cpp:128] Creating Layer conv4_Eltwise_2
I0218 11:49:51.250406 27028 net.cpp:558] conv4_Eltwise_2 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I0218 11:49:51.250412 27028 net.cpp:558] conv4_Eltwise_2 <- conv4_2_1
I0218 11:49:51.250422 27028 net.cpp:522] conv4_Eltwise_2 -> conv4_Eltwise_2
I0218 11:49:51.250454 27028 net.cpp:172] Setting up conv4_Eltwise_2
I0218 11:49:51.250461 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.250465 27028 net.cpp:194] Memory required for data: 1180831744
I0218 11:49:51.250469 27028 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I0218 11:49:51.250478 27028 net.cpp:128] Creating Layer conv4_2ReLU_1
I0218 11:49:51.250481 27028 net.cpp:558] conv4_2ReLU_1 <- conv4_Eltwise_2
I0218 11:49:51.250488 27028 net.cpp:509] conv4_2ReLU_1 -> conv4_Eltwise_2 (in-place)
I0218 11:49:51.251616 27028 net.cpp:172] Setting up conv4_2ReLU_1
I0218 11:49:51.251631 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.251634 27028 net.cpp:194] Memory required for data: 1185550336
I0218 11:49:51.251639 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.251651 27028 net.cpp:128] Creating Layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.251655 27028 net.cpp:558] conv4_Eltwise_2_conv4_2ReLU_1_0_split <- conv4_Eltwise_2
I0218 11:49:51.251664 27028 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I0218 11:49:51.251672 27028 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I0218 11:49:51.251724 27028 net.cpp:172] Setting up conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.251749 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.251754 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.251758 27028 net.cpp:194] Memory required for data: 1194987520
I0218 11:49:51.251762 27028 layer_factory.hpp:77] Creating layer conv4_3_0
I0218 11:49:51.251775 27028 net.cpp:128] Creating Layer conv4_3_0
I0218 11:49:51.251780 27028 net.cpp:558] conv4_3_0 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I0218 11:49:51.251786 27028 net.cpp:522] conv4_3_0 -> conv4_3_0
I0218 11:49:51.258342 27028 net.cpp:172] Setting up conv4_3_0
I0218 11:49:51.258368 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.258373 27028 net.cpp:194] Memory required for data: 1199706112
I0218 11:49:51.258381 27028 layer_factory.hpp:77] Creating layer conv4_3_bn0
I0218 11:49:51.258395 27028 net.cpp:128] Creating Layer conv4_3_bn0
I0218 11:49:51.258401 27028 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I0218 11:49:51.258414 27028 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I0218 11:49:51.258666 27028 net.cpp:172] Setting up conv4_3_bn0
I0218 11:49:51.258678 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.258682 27028 net.cpp:194] Memory required for data: 1204424704
I0218 11:49:51.258690 27028 layer_factory.hpp:77] Creating layer conv4_3_scale0
I0218 11:49:51.258697 27028 net.cpp:128] Creating Layer conv4_3_scale0
I0218 11:49:51.258702 27028 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I0218 11:49:51.258709 27028 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I0218 11:49:51.258749 27028 layer_factory.hpp:77] Creating layer conv4_3_scale0
I0218 11:49:51.258893 27028 net.cpp:172] Setting up conv4_3_scale0
I0218 11:49:51.258903 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.258908 27028 net.cpp:194] Memory required for data: 1209143296
I0218 11:49:51.258914 27028 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I0218 11:49:51.258920 27028 net.cpp:128] Creating Layer conv4_3_ReLU0
I0218 11:49:51.258924 27028 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I0218 11:49:51.258931 27028 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I0218 11:49:51.260411 27028 net.cpp:172] Setting up conv4_3_ReLU0
I0218 11:49:51.260428 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.260433 27028 net.cpp:194] Memory required for data: 1213861888
I0218 11:49:51.260438 27028 layer_factory.hpp:77] Creating layer conv4_3_1
I0218 11:49:51.260458 27028 net.cpp:128] Creating Layer conv4_3_1
I0218 11:49:51.260463 27028 net.cpp:558] conv4_3_1 <- conv4_3_0
I0218 11:49:51.260474 27028 net.cpp:522] conv4_3_1 -> conv4_3_1
I0218 11:49:51.266399 27028 net.cpp:172] Setting up conv4_3_1
I0218 11:49:51.266425 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.266429 27028 net.cpp:194] Memory required for data: 1218580480
I0218 11:49:51.266443 27028 layer_factory.hpp:77] Creating layer conv4_3bn1
I0218 11:49:51.266453 27028 net.cpp:128] Creating Layer conv4_3bn1
I0218 11:49:51.266458 27028 net.cpp:558] conv4_3bn1 <- conv4_3_1
I0218 11:49:51.266465 27028 net.cpp:509] conv4_3bn1 -> conv4_3_1 (in-place)
I0218 11:49:51.266724 27028 net.cpp:172] Setting up conv4_3bn1
I0218 11:49:51.266734 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.266738 27028 net.cpp:194] Memory required for data: 1223299072
I0218 11:49:51.266746 27028 layer_factory.hpp:77] Creating layer conv4_3_scale1
I0218 11:49:51.266753 27028 net.cpp:128] Creating Layer conv4_3_scale1
I0218 11:49:51.266757 27028 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I0218 11:49:51.266762 27028 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I0218 11:49:51.266806 27028 layer_factory.hpp:77] Creating layer conv4_3_scale1
I0218 11:49:51.266953 27028 net.cpp:172] Setting up conv4_3_scale1
I0218 11:49:51.266965 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.266979 27028 net.cpp:194] Memory required for data: 1228017664
I0218 11:49:51.266985 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_3
I0218 11:49:51.266993 27028 net.cpp:128] Creating Layer conv4_Eltwise_3
I0218 11:49:51.266996 27028 net.cpp:558] conv4_Eltwise_3 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I0218 11:49:51.267016 27028 net.cpp:558] conv4_Eltwise_3 <- conv4_3_1
I0218 11:49:51.267024 27028 net.cpp:522] conv4_Eltwise_3 -> conv4_Eltwise_3
I0218 11:49:51.267052 27028 net.cpp:172] Setting up conv4_Eltwise_3
I0218 11:49:51.267061 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.267066 27028 net.cpp:194] Memory required for data: 1232736256
I0218 11:49:51.267069 27028 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I0218 11:49:51.267078 27028 net.cpp:128] Creating Layer conv4_3ReLU_1
I0218 11:49:51.267082 27028 net.cpp:558] conv4_3ReLU_1 <- conv4_Eltwise_3
I0218 11:49:51.267087 27028 net.cpp:509] conv4_3ReLU_1 -> conv4_Eltwise_3 (in-place)
I0218 11:49:51.267323 27028 net.cpp:172] Setting up conv4_3ReLU_1
I0218 11:49:51.267335 27028 net.cpp:186] Top shape: 128 64 8 18 (1179648)
I0218 11:49:51.267339 27028 net.cpp:194] Memory required for data: 1237454848
I0218 11:49:51.267343 27028 layer_factory.hpp:77] Creating layer Pooling1
I0218 11:49:51.267354 27028 net.cpp:128] Creating Layer Pooling1
I0218 11:49:51.267361 27028 net.cpp:558] Pooling1 <- conv4_Eltwise_3
I0218 11:49:51.267369 27028 net.cpp:522] Pooling1 -> Pooling1
I0218 11:49:51.267916 27028 net.cpp:172] Setting up Pooling1
I0218 11:49:51.267940 27028 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0218 11:49:51.267943 27028 net.cpp:194] Memory required for data: 1237487616
I0218 11:49:51.267948 27028 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0218 11:49:51.267958 27028 net.cpp:128] Creating Layer Pooling1_Pooling1_0_split
I0218 11:49:51.267963 27028 net.cpp:558] Pooling1_Pooling1_0_split <- Pooling1
I0218 11:49:51.267971 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0218 11:49:51.267982 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0218 11:49:51.267989 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_2
I0218 11:49:51.267997 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_3
I0218 11:49:51.268079 27028 net.cpp:172] Setting up Pooling1_Pooling1_0_split
I0218 11:49:51.268088 27028 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0218 11:49:51.268095 27028 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0218 11:49:51.268100 27028 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0218 11:49:51.268105 27028 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0218 11:49:51.268107 27028 net.cpp:194] Memory required for data: 1237618688
I0218 11:49:51.268111 27028 layer_factory.hpp:77] Creating layer fc1
I0218 11:49:51.268126 27028 net.cpp:128] Creating Layer fc1
I0218 11:49:51.268129 27028 net.cpp:558] fc1 <- Pooling1_Pooling1_0_split_0
I0218 11:49:51.268136 27028 net.cpp:522] fc1 -> fc1
I0218 11:49:51.269531 27028 net.cpp:172] Setting up fc1
I0218 11:49:51.269553 27028 net.cpp:186] Top shape: 128 62 1 1 (7936)
I0218 11:49:51.269557 27028 net.cpp:194] Memory required for data: 1237650432
I0218 11:49:51.269567 27028 layer_factory.hpp:77] Creating layer Softmax1
I0218 11:49:51.269575 27028 net.cpp:128] Creating Layer Softmax1
I0218 11:49:51.269580 27028 net.cpp:558] Softmax1 <- fc1
I0218 11:49:51.269587 27028 net.cpp:558] Softmax1 <- label_1
I0218 11:49:51.269599 27028 net.cpp:522] Softmax1 -> Softmax1
I0218 11:49:51.269613 27028 layer_factory.hpp:77] Creating layer Softmax1
I0218 11:49:51.270009 27028 net.cpp:172] Setting up Softmax1
I0218 11:49:51.270025 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.270030 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.270058 27028 net.cpp:194] Memory required for data: 1237650436
I0218 11:49:51.270062 27028 layer_factory.hpp:77] Creating layer fc2
I0218 11:49:51.270078 27028 net.cpp:128] Creating Layer fc2
I0218 11:49:51.270083 27028 net.cpp:558] fc2 <- Pooling1_Pooling1_0_split_1
I0218 11:49:51.270095 27028 net.cpp:522] fc2 -> fc2
I0218 11:49:51.273207 27028 net.cpp:172] Setting up fc2
I0218 11:49:51.273234 27028 net.cpp:186] Top shape: 128 62 1 1 (7936)
I0218 11:49:51.273238 27028 net.cpp:194] Memory required for data: 1237682180
I0218 11:49:51.273267 27028 layer_factory.hpp:77] Creating layer Softmax2
I0218 11:49:51.273274 27028 net.cpp:128] Creating Layer Softmax2
I0218 11:49:51.273278 27028 net.cpp:558] Softmax2 <- fc2
I0218 11:49:51.273284 27028 net.cpp:558] Softmax2 <- label_2
I0218 11:49:51.273294 27028 net.cpp:522] Softmax2 -> Softmax2
I0218 11:49:51.273303 27028 layer_factory.hpp:77] Creating layer Softmax2
I0218 11:49:51.275398 27028 net.cpp:172] Setting up Softmax2
I0218 11:49:51.275420 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.275424 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.275432 27028 net.cpp:194] Memory required for data: 1237682184
I0218 11:49:51.275439 27028 layer_factory.hpp:77] Creating layer fc3
I0218 11:49:51.275456 27028 net.cpp:128] Creating Layer fc3
I0218 11:49:51.275461 27028 net.cpp:558] fc3 <- Pooling1_Pooling1_0_split_2
I0218 11:49:51.275475 27028 net.cpp:522] fc3 -> fc3
I0218 11:49:51.282050 27028 net.cpp:172] Setting up fc3
I0218 11:49:51.282078 27028 net.cpp:186] Top shape: 128 62 1 1 (7936)
I0218 11:49:51.282083 27028 net.cpp:194] Memory required for data: 1237713928
I0218 11:49:51.282091 27028 layer_factory.hpp:77] Creating layer Softmax3
I0218 11:49:51.282099 27028 net.cpp:128] Creating Layer Softmax3
I0218 11:49:51.282104 27028 net.cpp:558] Softmax3 <- fc3
I0218 11:49:51.282110 27028 net.cpp:558] Softmax3 <- label_3
I0218 11:49:51.282119 27028 net.cpp:522] Softmax3 -> Softmax3
I0218 11:49:51.282137 27028 layer_factory.hpp:77] Creating layer Softmax3
I0218 11:49:51.284236 27028 net.cpp:172] Setting up Softmax3
I0218 11:49:51.284260 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.284265 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.284273 27028 net.cpp:194] Memory required for data: 1237713932
I0218 11:49:51.284281 27028 layer_factory.hpp:77] Creating layer fc4
I0218 11:49:51.284294 27028 net.cpp:128] Creating Layer fc4
I0218 11:49:51.284303 27028 net.cpp:558] fc4 <- Pooling1_Pooling1_0_split_3
I0218 11:49:51.284314 27028 net.cpp:522] fc4 -> fc4
I0218 11:49:51.290779 27028 net.cpp:172] Setting up fc4
I0218 11:49:51.290807 27028 net.cpp:186] Top shape: 128 62 1 1 (7936)
I0218 11:49:51.290812 27028 net.cpp:194] Memory required for data: 1237745676
I0218 11:49:51.290823 27028 layer_factory.hpp:77] Creating layer Softmax4
I0218 11:49:51.290832 27028 net.cpp:128] Creating Layer Softmax4
I0218 11:49:51.290836 27028 net.cpp:558] Softmax4 <- fc4
I0218 11:49:51.290843 27028 net.cpp:558] Softmax4 <- label_4
I0218 11:49:51.290848 27028 net.cpp:522] Softmax4 -> Softmax4
I0218 11:49:51.290858 27028 layer_factory.hpp:77] Creating layer Softmax4
I0218 11:49:51.292944 27028 net.cpp:172] Setting up Softmax4
I0218 11:49:51.292963 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.292968 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.292975 27028 net.cpp:194] Memory required for data: 1237745680
I0218 11:49:51.292985 27028 net.cpp:301] Softmax4 needs backward computation.
I0218 11:49:51.292989 27028 net.cpp:301] fc4 needs backward computation.
I0218 11:49:51.292994 27028 net.cpp:301] Softmax3 needs backward computation.
I0218 11:49:51.292999 27028 net.cpp:301] fc3 needs backward computation.
I0218 11:49:51.293002 27028 net.cpp:301] Softmax2 needs backward computation.
I0218 11:49:51.293009 27028 net.cpp:301] fc2 needs backward computation.
I0218 11:49:51.293023 27028 net.cpp:301] Softmax1 needs backward computation.
I0218 11:49:51.293028 27028 net.cpp:301] fc1 needs backward computation.
I0218 11:49:51.293032 27028 net.cpp:301] Pooling1_Pooling1_0_split needs backward computation.
I0218 11:49:51.293036 27028 net.cpp:301] Pooling1 needs backward computation.
I0218 11:49:51.293040 27028 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I0218 11:49:51.293045 27028 net.cpp:301] conv4_Eltwise_3 needs backward computation.
I0218 11:49:51.293049 27028 net.cpp:301] conv4_3_scale1 needs backward computation.
I0218 11:49:51.293054 27028 net.cpp:301] conv4_3bn1 needs backward computation.
I0218 11:49:51.293058 27028 net.cpp:301] conv4_3_1 needs backward computation.
I0218 11:49:51.293062 27028 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I0218 11:49:51.293081 27028 net.cpp:301] conv4_3_scale0 needs backward computation.
I0218 11:49:51.293087 27028 net.cpp:301] conv4_3_bn0 needs backward computation.
I0218 11:49:51.293090 27028 net.cpp:301] conv4_3_0 needs backward computation.
I0218 11:49:51.293095 27028 net.cpp:301] conv4_Eltwise_2_conv4_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.293100 27028 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I0218 11:49:51.293105 27028 net.cpp:301] conv4_Eltwise_2 needs backward computation.
I0218 11:49:51.293110 27028 net.cpp:301] conv4_2_scale1 needs backward computation.
I0218 11:49:51.293114 27028 net.cpp:301] conv4_2bn1 needs backward computation.
I0218 11:49:51.293118 27028 net.cpp:301] conv4_2_1 needs backward computation.
I0218 11:49:51.293123 27028 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I0218 11:49:51.293128 27028 net.cpp:301] conv4_2_scale0 needs backward computation.
I0218 11:49:51.293131 27028 net.cpp:301] conv4_2_bn0 needs backward computation.
I0218 11:49:51.293135 27028 net.cpp:301] conv4_2_0 needs backward computation.
I0218 11:49:51.293140 27028 net.cpp:301] conv4_Eltwise_1_conv4_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.293144 27028 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I0218 11:49:51.293148 27028 net.cpp:301] conv4_Eltwise_1 needs backward computation.
I0218 11:49:51.293154 27028 net.cpp:301] conv4_1_scale_down needs backward computation.
I0218 11:49:51.293160 27028 net.cpp:301] conv4_1_bn_down needs backward computation.
I0218 11:49:51.293164 27028 net.cpp:301] conv4_1_down needs backward computation.
I0218 11:49:51.293169 27028 net.cpp:301] conv4_1_scale1 needs backward computation.
I0218 11:49:51.293174 27028 net.cpp:301] conv4_1bn1 needs backward computation.
I0218 11:49:51.293179 27028 net.cpp:301] conv4_1_1 needs backward computation.
I0218 11:49:51.293182 27028 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I0218 11:49:51.293187 27028 net.cpp:301] conv4_1_scale0 needs backward computation.
I0218 11:49:51.293191 27028 net.cpp:301] conv4_1_bn0 needs backward computation.
I0218 11:49:51.293195 27028 net.cpp:301] conv4_1_0 needs backward computation.
I0218 11:49:51.293200 27028 net.cpp:301] conv3_Eltwise_3_conv3_3ReLU_1_0_split needs backward computation.
I0218 11:49:51.293208 27028 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I0218 11:49:51.293213 27028 net.cpp:301] conv3_Eltwise_3 needs backward computation.
I0218 11:49:51.293218 27028 net.cpp:301] conv3_3_scale1 needs backward computation.
I0218 11:49:51.293222 27028 net.cpp:301] conv3_3bn1 needs backward computation.
I0218 11:49:51.293226 27028 net.cpp:301] conv3_3_1 needs backward computation.
I0218 11:49:51.293231 27028 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I0218 11:49:51.293236 27028 net.cpp:301] conv3_3_scale0 needs backward computation.
I0218 11:49:51.293241 27028 net.cpp:301] conv3_3_bn0 needs backward computation.
I0218 11:49:51.293244 27028 net.cpp:301] conv3_3_0 needs backward computation.
I0218 11:49:51.293248 27028 net.cpp:301] conv3_Eltwise_2_conv3_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.293262 27028 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I0218 11:49:51.293267 27028 net.cpp:301] conv3_Eltwise_2 needs backward computation.
I0218 11:49:51.293272 27028 net.cpp:301] conv3_2_scale1 needs backward computation.
I0218 11:49:51.293277 27028 net.cpp:301] conv3_2bn1 needs backward computation.
I0218 11:49:51.293280 27028 net.cpp:301] conv3_2_1 needs backward computation.
I0218 11:49:51.293285 27028 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I0218 11:49:51.293289 27028 net.cpp:301] conv3_2_scale0 needs backward computation.
I0218 11:49:51.293293 27028 net.cpp:301] conv3_2_bn0 needs backward computation.
I0218 11:49:51.293298 27028 net.cpp:301] conv3_2_0 needs backward computation.
I0218 11:49:51.293303 27028 net.cpp:301] conv3_Eltwise_1_conv3_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.293308 27028 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I0218 11:49:51.293320 27028 net.cpp:301] conv3_Eltwise_1 needs backward computation.
I0218 11:49:51.293325 27028 net.cpp:301] conv3_1_scale_down needs backward computation.
I0218 11:49:51.293330 27028 net.cpp:301] conv3_1_bn_down needs backward computation.
I0218 11:49:51.293335 27028 net.cpp:301] conv3_1_down needs backward computation.
I0218 11:49:51.293340 27028 net.cpp:301] conv3_1_scale1 needs backward computation.
I0218 11:49:51.293344 27028 net.cpp:301] conv3_1bn1 needs backward computation.
I0218 11:49:51.293349 27028 net.cpp:301] conv3_1_1 needs backward computation.
I0218 11:49:51.293354 27028 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I0218 11:49:51.293357 27028 net.cpp:301] conv3_1_scale0 needs backward computation.
I0218 11:49:51.293361 27028 net.cpp:301] conv3_1_bn0 needs backward computation.
I0218 11:49:51.293365 27028 net.cpp:301] conv3_1_0 needs backward computation.
I0218 11:49:51.293370 27028 net.cpp:301] conv2_Eltwise_3_conv2_3ReLU_1_0_split needs backward computation.
I0218 11:49:51.293375 27028 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I0218 11:49:51.293380 27028 net.cpp:301] conv2_Eltwise_3 needs backward computation.
I0218 11:49:51.293385 27028 net.cpp:301] conv2_3_scale1 needs backward computation.
I0218 11:49:51.293391 27028 net.cpp:301] conv2_3bn1 needs backward computation.
I0218 11:49:51.293396 27028 net.cpp:301] conv2_3_1 needs backward computation.
I0218 11:49:51.293401 27028 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I0218 11:49:51.293404 27028 net.cpp:301] conv2_3_scale0 needs backward computation.
I0218 11:49:51.293409 27028 net.cpp:301] conv2_3_bn0 needs backward computation.
I0218 11:49:51.293413 27028 net.cpp:301] conv2_3_0 needs backward computation.
I0218 11:49:51.293418 27028 net.cpp:301] conv2_Eltwise_2_conv2_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.293423 27028 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I0218 11:49:51.293428 27028 net.cpp:301] conv2_Eltwise_2 needs backward computation.
I0218 11:49:51.293433 27028 net.cpp:301] conv2_2_scale1 needs backward computation.
I0218 11:49:51.293438 27028 net.cpp:301] conv2_2bn1 needs backward computation.
I0218 11:49:51.293442 27028 net.cpp:301] conv2_2_1 needs backward computation.
I0218 11:49:51.293447 27028 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I0218 11:49:51.293452 27028 net.cpp:301] conv2_2_scale0 needs backward computation.
I0218 11:49:51.293457 27028 net.cpp:301] conv2_2_bn0 needs backward computation.
I0218 11:49:51.293460 27028 net.cpp:301] conv2_2_0 needs backward computation.
I0218 11:49:51.293465 27028 net.cpp:301] conv2_Eltwise_1_conv2_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.293471 27028 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I0218 11:49:51.293475 27028 net.cpp:301] conv2_Eltwise_1 needs backward computation.
I0218 11:49:51.293481 27028 net.cpp:301] conv2_1_scale1 needs backward computation.
I0218 11:49:51.293485 27028 net.cpp:301] conv2_1bn1 needs backward computation.
I0218 11:49:51.293490 27028 net.cpp:301] conv2_1_1 needs backward computation.
I0218 11:49:51.293494 27028 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I0218 11:49:51.293499 27028 net.cpp:301] conv2_1_scale0 needs backward computation.
I0218 11:49:51.293503 27028 net.cpp:301] conv2_1_bn0 needs backward computation.
I0218 11:49:51.293509 27028 net.cpp:301] conv2_1_0 needs backward computation.
I0218 11:49:51.293514 27028 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I0218 11:49:51.293519 27028 net.cpp:301] conv1/ReLU needs backward computation.
I0218 11:49:51.293522 27028 net.cpp:301] conv1/scale needs backward computation.
I0218 11:49:51.293527 27028 net.cpp:301] conv1/bn needs backward computation.
I0218 11:49:51.293531 27028 net.cpp:301] conv1 needs backward computation.
I0218 11:49:51.293537 27028 net.cpp:303] slicers does not need backward computation.
I0218 11:49:51.293542 27028 net.cpp:303] data1 does not need backward computation.
I0218 11:49:51.293546 27028 net.cpp:348] This network produces output Softmax1
I0218 11:49:51.293560 27028 net.cpp:348] This network produces output Softmax2
I0218 11:49:51.293575 27028 net.cpp:348] This network produces output Softmax3
I0218 11:49:51.293578 27028 net.cpp:348] This network produces output Softmax4
I0218 11:49:51.293648 27028 net.cpp:363] Network initialization done.
I0218 11:49:51.294895 27028 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_20.prototxt
I0218 11:49:51.294911 27028 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0218 11:49:51.294921 27028 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_20.prototxt
I0218 11:49:51.295675 27028 net.cpp:82] Initializing net from parameters: 
name: "ResNet-20"
state {
  phase: TEST
}
layer {
  name: "data1"
  type: "HDF5Data"
  top: "data"
  top: "labels"
  include {
    phase: TEST
  }
  hdf5_data_param {
    source: "./test_h5.txt"
    batch_size: 10
  }
}
layer {
  name: "slicers"
  type: "Slice"
  bottom: "labels"
  top: "label_1"
  top: "label_2"
  top: "label_3"
  top: "label_4"
  slice_param {
    slice_point: 1
    slice_point: 2
    slice_point: 3
    axis: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv1/bn"
  type: "BatchNorm"
  bottom: "conv1"
  top: "conv1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv1/scale"
  type: "Scale"
  bottom: "conv1"
  top: "conv1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv1/ReLU"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "conv2_1_0"
  type: "Convolution"
  bottom: "conv1"
  top: "conv2_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1_bn0"
  type: "BatchNorm"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale0"
  type: "Scale"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_1_ReLU0"
  type: "ReLU"
  bottom: "conv2_1_0"
  top: "conv2_1_0"
}
layer {
  name: "conv2_1_1"
  type: "Convolution"
  bottom: "conv2_1_0"
  top: "conv2_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_1bn1"
  type: "BatchNorm"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_1_scale1"
  type: "Scale"
  bottom: "conv2_1_1"
  top: "conv2_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_1"
  type: "Eltwise"
  bottom: "conv1"
  bottom: "conv2_1_1"
  top: "conv2_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_1ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_1"
  top: "conv2_Eltwise_1"
}
layer {
  name: "conv2_2_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_1"
  top: "conv2_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2_bn0"
  type: "BatchNorm"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale0"
  type: "Scale"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_2_ReLU0"
  type: "ReLU"
  bottom: "conv2_2_0"
  top: "conv2_2_0"
}
layer {
  name: "conv2_2_1"
  type: "Convolution"
  bottom: "conv2_2_0"
  top: "conv2_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_2bn1"
  type: "BatchNorm"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_2_scale1"
  type: "Scale"
  bottom: "conv2_2_1"
  top: "conv2_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_2"
  type: "Eltwise"
  bottom: "conv2_Eltwise_1"
  bottom: "conv2_2_1"
  top: "conv2_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_2ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_2"
  top: "conv2_Eltwise_2"
}
layer {
  name: "conv2_3_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_2"
  top: "conv2_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3_bn0"
  type: "BatchNorm"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale0"
  type: "Scale"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_3_ReLU0"
  type: "ReLU"
  bottom: "conv2_3_0"
  top: "conv2_3_0"
}
layer {
  name: "conv2_3_1"
  type: "Convolution"
  bottom: "conv2_3_0"
  top: "conv2_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv2_3bn1"
  type: "BatchNorm"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv2_3_scale1"
  type: "Scale"
  bottom: "conv2_3_1"
  top: "conv2_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv2_Eltwise_3"
  type: "Eltwise"
  bottom: "conv2_Eltwise_2"
  bottom: "conv2_3_1"
  top: "conv2_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv2_3ReLU_1"
  type: "ReLU"
  bottom: "conv2_Eltwise_3"
  top: "conv2_Eltwise_3"
}
layer {
  name: "conv3_1_0"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn0"
  type: "BatchNorm"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale0"
  type: "Scale"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_ReLU0"
  type: "ReLU"
  bottom: "conv3_1_0"
  top: "conv3_1_0"
}
layer {
  name: "conv3_1_1"
  type: "Convolution"
  bottom: "conv3_1_0"
  top: "conv3_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1bn1"
  type: "BatchNorm"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale1"
  type: "Scale"
  bottom: "conv3_1_1"
  top: "conv3_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_1_down"
  type: "Convolution"
  bottom: "conv2_Eltwise_3"
  top: "conv3_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_1_bn_down"
  type: "BatchNorm"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_1_scale_down"
  type: "Scale"
  bottom: "conv3_1_down"
  top: "conv3_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_1"
  type: "Eltwise"
  bottom: "conv3_1_1"
  bottom: "conv3_1_down"
  top: "conv3_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_1ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_1"
  top: "conv3_Eltwise_1"
}
layer {
  name: "conv3_2_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_1"
  top: "conv3_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2_bn0"
  type: "BatchNorm"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale0"
  type: "Scale"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_2_ReLU0"
  type: "ReLU"
  bottom: "conv3_2_0"
  top: "conv3_2_0"
}
layer {
  name: "conv3_2_1"
  type: "Convolution"
  bottom: "conv3_2_0"
  top: "conv3_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_2bn1"
  type: "BatchNorm"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_2_scale1"
  type: "Scale"
  bottom: "conv3_2_1"
  top: "conv3_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_2"
  type: "Eltwise"
  bottom: "conv3_Eltwise_1"
  bottom: "conv3_2_1"
  top: "conv3_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_2ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_2"
  top: "conv3_Eltwise_2"
}
layer {
  name: "conv3_3_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_2"
  top: "conv3_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3_bn0"
  type: "BatchNorm"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale0"
  type: "Scale"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_3_ReLU0"
  type: "ReLU"
  bottom: "conv3_3_0"
  top: "conv3_3_0"
}
layer {
  name: "conv3_3_1"
  type: "Convolution"
  bottom: "conv3_3_0"
  top: "conv3_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv3_3bn1"
  type: "BatchNorm"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv3_3_scale1"
  type: "Scale"
  bottom: "conv3_3_1"
  top: "conv3_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv3_Eltwise_3"
  type: "Eltwise"
  bottom: "conv3_Eltwise_2"
  bottom: "conv3_3_1"
  top: "conv3_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv3_3ReLU_1"
  type: "ReLU"
  bottom: "conv3_Eltwise_3"
  top: "conv3_Eltwise_3"
}
layer {
  name: "conv4_1_0"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn0"
  type: "BatchNorm"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale0"
  type: "Scale"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_ReLU0"
  type: "ReLU"
  bottom: "conv4_1_0"
  top: "conv4_1_0"
}
layer {
  name: "conv4_1_1"
  type: "Convolution"
  bottom: "conv4_1_0"
  top: "conv4_1_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1bn1"
  type: "BatchNorm"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale1"
  type: "Scale"
  bottom: "conv4_1_1"
  top: "conv4_1_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_1_down"
  type: "Convolution"
  bottom: "conv3_Eltwise_3"
  top: "conv4_1_down"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_1_bn_down"
  type: "BatchNorm"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_1_scale_down"
  type: "Scale"
  bottom: "conv4_1_down"
  top: "conv4_1_down"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_1"
  type: "Eltwise"
  bottom: "conv4_1_1"
  bottom: "conv4_1_down"
  top: "conv4_Eltwise_1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_1ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_1"
  top: "conv4_Eltwise_1"
}
layer {
  name: "conv4_2_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_1"
  top: "conv4_2_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2_bn0"
  type: "BatchNorm"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale0"
  type: "Scale"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_2_ReLU0"
  type: "ReLU"
  bottom: "conv4_2_0"
  top: "conv4_2_0"
}
layer {
  name: "conv4_2_1"
  type: "Convolution"
  bottom: "conv4_2_0"
  top: "conv4_2_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_2bn1"
  type: "BatchNorm"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_2_scale1"
  type: "Scale"
  bottom: "conv4_2_1"
  top: "conv4_2_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_2"
  type: "Eltwise"
  bottom: "conv4_Eltwise_1"
  bottom: "conv4_2_1"
  top: "conv4_Eltwise_2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_2ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_2"
  top: "conv4_Eltwise_2"
}
layer {
  name: "conv4_3_0"
  type: "Convolution"
  bottom: "conv4_Eltwise_2"
  top: "conv4_3_0"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3_bn0"
  type: "BatchNorm"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale0"
  type: "Scale"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_3_ReLU0"
  type: "ReLU"
  bottom: "conv4_3_0"
  top: "conv4_3_0"
}
layer {
  name: "conv4_3_1"
  type: "Convolution"
  bottom: "conv4_3_0"
  top: "conv4_3_1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "conv4_3bn1"
  type: "BatchNorm"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: true
  }
}
layer {
  name: "conv4_3_scale1"
  type: "Scale"
  bottom: "conv4_3_1"
  top: "conv4_3_1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "conv4_Eltwise_3"
  type: "Eltwise"
  bottom: "conv4_Eltwise_2"
  bottom: "conv4_3_1"
  top: "conv4_Eltwise_3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "conv4_3ReLU_1"
  type: "ReLU"
  bottom: "conv4_Eltwise_3"
  top: "conv4_Eltwise_3"
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "conv4_Eltwise_3"
  top: "Pooling1"
  pooling_param {
    pool: AVE
    global_pooling: true
  }
}
layer {
  name: "fc1"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax1"
  type: "SoftmaxWithLoss"
  bottom: "fc1"
  bottom: "label_1"
  top: "Softmax1"
}
layer {
  name: "prob1"
  type: "Accuracy"
  bottom: "fc1"
  bottom: "label_1"
  top: "prob1"
}
layer {
  name: "fc2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax2"
  type: "SoftmaxWithLoss"
  bottom: "fc2"
  bottom: "label_2"
  top: "Softmax2"
}
layer {
  name: "prob2"
  type: "Accuracy"
  bottom: "fc2"
  bottom: "label_2"
  top: "prob2"
}
layer {
  name: "fc3"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax3"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label_3"
  top: "Softmax3"
}
layer {
  name: "prob3"
  type: "Accuracy"
  bottom: "fc3"
  bottom: "label_3"
  top: "prob3"
}
layer {
  name: "fc4"
  type: "Convolution"
  bottom: "Pooling1"
  top: "fc4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 1
  }
  convolution_param {
    num_output: 62
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "Softmax4"
  type: "SoftmaxWithLoss"
  bottom: "fc4"
  bottom: "label_4"
  top: "Softmax4"
}
layer {
  name: "prob4"
  type: "Accuracy"
  bottom: "fc4"
  bottom: "label_4"
  top: "prob4"
}
I0218 11:49:51.296144 27028 layer_factory.hpp:77] Creating layer data1
I0218 11:49:51.296157 27028 net.cpp:128] Creating Layer data1
I0218 11:49:51.296162 27028 net.cpp:522] data1 -> data
I0218 11:49:51.296181 27028 net.cpp:522] data1 -> labels
I0218 11:49:51.296190 27028 hdf5_data_layer.cpp:80] Loading list of HDF5 filenames from: ./test_h5.txt
I0218 11:49:51.296228 27028 hdf5_data_layer.cpp:94] Number of HDF5 files: 1
I0218 11:49:51.551594 27028 net.cpp:172] Setting up data1
I0218 11:49:51.551661 27028 net.cpp:186] Top shape: 10 1 32 72 (23040)
I0218 11:49:51.551666 27028 net.cpp:186] Top shape: 10 4 (40)
I0218 11:49:51.551671 27028 net.cpp:194] Memory required for data: 92320
I0218 11:49:51.551682 27028 layer_factory.hpp:77] Creating layer slicers
I0218 11:49:51.551702 27028 net.cpp:128] Creating Layer slicers
I0218 11:49:51.551708 27028 net.cpp:558] slicers <- labels
I0218 11:49:51.551721 27028 net.cpp:522] slicers -> label_1
I0218 11:49:51.551738 27028 net.cpp:522] slicers -> label_2
I0218 11:49:51.551744 27028 net.cpp:522] slicers -> label_3
I0218 11:49:51.551753 27028 net.cpp:522] slicers -> label_4
I0218 11:49:51.551829 27028 net.cpp:172] Setting up slicers
I0218 11:49:51.551836 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551841 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551846 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551851 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551853 27028 net.cpp:194] Memory required for data: 92480
I0218 11:49:51.551858 27028 layer_factory.hpp:77] Creating layer label_1_slicers_0_split
I0218 11:49:51.551867 27028 net.cpp:128] Creating Layer label_1_slicers_0_split
I0218 11:49:51.551870 27028 net.cpp:558] label_1_slicers_0_split <- label_1
I0218 11:49:51.551875 27028 net.cpp:522] label_1_slicers_0_split -> label_1_slicers_0_split_0
I0218 11:49:51.551882 27028 net.cpp:522] label_1_slicers_0_split -> label_1_slicers_0_split_1
I0218 11:49:51.551923 27028 net.cpp:172] Setting up label_1_slicers_0_split
I0218 11:49:51.551928 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551934 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.551936 27028 net.cpp:194] Memory required for data: 92560
I0218 11:49:51.551940 27028 layer_factory.hpp:77] Creating layer label_2_slicers_1_split
I0218 11:49:51.551946 27028 net.cpp:128] Creating Layer label_2_slicers_1_split
I0218 11:49:51.551950 27028 net.cpp:558] label_2_slicers_1_split <- label_2
I0218 11:49:51.551955 27028 net.cpp:522] label_2_slicers_1_split -> label_2_slicers_1_split_0
I0218 11:49:51.551964 27028 net.cpp:522] label_2_slicers_1_split -> label_2_slicers_1_split_1
I0218 11:49:51.552003 27028 net.cpp:172] Setting up label_2_slicers_1_split
I0218 11:49:51.552009 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552014 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552018 27028 net.cpp:194] Memory required for data: 92640
I0218 11:49:51.552022 27028 layer_factory.hpp:77] Creating layer label_3_slicers_2_split
I0218 11:49:51.552028 27028 net.cpp:128] Creating Layer label_3_slicers_2_split
I0218 11:49:51.552032 27028 net.cpp:558] label_3_slicers_2_split <- label_3
I0218 11:49:51.552038 27028 net.cpp:522] label_3_slicers_2_split -> label_3_slicers_2_split_0
I0218 11:49:51.552044 27028 net.cpp:522] label_3_slicers_2_split -> label_3_slicers_2_split_1
I0218 11:49:51.552084 27028 net.cpp:172] Setting up label_3_slicers_2_split
I0218 11:49:51.552090 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552094 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552098 27028 net.cpp:194] Memory required for data: 92720
I0218 11:49:51.552101 27028 layer_factory.hpp:77] Creating layer label_4_slicers_3_split
I0218 11:49:51.552107 27028 net.cpp:128] Creating Layer label_4_slicers_3_split
I0218 11:49:51.552111 27028 net.cpp:558] label_4_slicers_3_split <- label_4
I0218 11:49:51.552116 27028 net.cpp:522] label_4_slicers_3_split -> label_4_slicers_3_split_0
I0218 11:49:51.552124 27028 net.cpp:522] label_4_slicers_3_split -> label_4_slicers_3_split_1
I0218 11:49:51.552162 27028 net.cpp:172] Setting up label_4_slicers_3_split
I0218 11:49:51.552168 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552172 27028 net.cpp:186] Top shape: 10 1 (10)
I0218 11:49:51.552220 27028 net.cpp:194] Memory required for data: 92800
I0218 11:49:51.552225 27028 layer_factory.hpp:77] Creating layer conv1
I0218 11:49:51.552244 27028 net.cpp:128] Creating Layer conv1
I0218 11:49:51.552248 27028 net.cpp:558] conv1 <- data
I0218 11:49:51.552254 27028 net.cpp:522] conv1 -> conv1
I0218 11:49:51.556980 27028 net.cpp:172] Setting up conv1
I0218 11:49:51.557008 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.557011 27028 net.cpp:194] Memory required for data: 1567360
I0218 11:49:51.557030 27028 layer_factory.hpp:77] Creating layer conv1/bn
I0218 11:49:51.557040 27028 net.cpp:128] Creating Layer conv1/bn
I0218 11:49:51.557045 27028 net.cpp:558] conv1/bn <- conv1
I0218 11:49:51.557055 27028 net.cpp:509] conv1/bn -> conv1 (in-place)
I0218 11:49:51.557312 27028 net.cpp:172] Setting up conv1/bn
I0218 11:49:51.557324 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.557328 27028 net.cpp:194] Memory required for data: 3041920
I0218 11:49:51.557339 27028 layer_factory.hpp:77] Creating layer conv1/scale
I0218 11:49:51.557348 27028 net.cpp:128] Creating Layer conv1/scale
I0218 11:49:51.557353 27028 net.cpp:558] conv1/scale <- conv1
I0218 11:49:51.557358 27028 net.cpp:509] conv1/scale -> conv1 (in-place)
I0218 11:49:51.557406 27028 layer_factory.hpp:77] Creating layer conv1/scale
I0218 11:49:51.557548 27028 net.cpp:172] Setting up conv1/scale
I0218 11:49:51.557556 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.557560 27028 net.cpp:194] Memory required for data: 4516480
I0218 11:49:51.557567 27028 layer_factory.hpp:77] Creating layer conv1/ReLU
I0218 11:49:51.557574 27028 net.cpp:128] Creating Layer conv1/ReLU
I0218 11:49:51.557579 27028 net.cpp:558] conv1/ReLU <- conv1
I0218 11:49:51.557584 27028 net.cpp:509] conv1/ReLU -> conv1 (in-place)
I0218 11:49:51.559042 27028 net.cpp:172] Setting up conv1/ReLU
I0218 11:49:51.559062 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.559065 27028 net.cpp:194] Memory required for data: 5991040
I0218 11:49:51.559070 27028 layer_factory.hpp:77] Creating layer conv1_conv1/ReLU_0_split
I0218 11:49:51.559078 27028 net.cpp:128] Creating Layer conv1_conv1/ReLU_0_split
I0218 11:49:51.559082 27028 net.cpp:558] conv1_conv1/ReLU_0_split <- conv1
I0218 11:49:51.559089 27028 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_0
I0218 11:49:51.559098 27028 net.cpp:522] conv1_conv1/ReLU_0_split -> conv1_conv1/ReLU_0_split_1
I0218 11:49:51.559151 27028 net.cpp:172] Setting up conv1_conv1/ReLU_0_split
I0218 11:49:51.559159 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.559165 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.559168 27028 net.cpp:194] Memory required for data: 8940160
I0218 11:49:51.559172 27028 layer_factory.hpp:77] Creating layer conv2_1_0
I0218 11:49:51.559183 27028 net.cpp:128] Creating Layer conv2_1_0
I0218 11:49:51.559190 27028 net.cpp:558] conv2_1_0 <- conv1_conv1/ReLU_0_split_0
I0218 11:49:51.559196 27028 net.cpp:522] conv2_1_0 -> conv2_1_0
I0218 11:49:51.565809 27028 net.cpp:172] Setting up conv2_1_0
I0218 11:49:51.565835 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.565840 27028 net.cpp:194] Memory required for data: 10414720
I0218 11:49:51.565852 27028 layer_factory.hpp:77] Creating layer conv2_1_bn0
I0218 11:49:51.565865 27028 net.cpp:128] Creating Layer conv2_1_bn0
I0218 11:49:51.565876 27028 net.cpp:558] conv2_1_bn0 <- conv2_1_0
I0218 11:49:51.565884 27028 net.cpp:509] conv2_1_bn0 -> conv2_1_0 (in-place)
I0218 11:49:51.566162 27028 net.cpp:172] Setting up conv2_1_bn0
I0218 11:49:51.566174 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.566179 27028 net.cpp:194] Memory required for data: 11889280
I0218 11:49:51.566186 27028 layer_factory.hpp:77] Creating layer conv2_1_scale0
I0218 11:49:51.566197 27028 net.cpp:128] Creating Layer conv2_1_scale0
I0218 11:49:51.566206 27028 net.cpp:558] conv2_1_scale0 <- conv2_1_0
I0218 11:49:51.566212 27028 net.cpp:509] conv2_1_scale0 -> conv2_1_0 (in-place)
I0218 11:49:51.566260 27028 layer_factory.hpp:77] Creating layer conv2_1_scale0
I0218 11:49:51.566431 27028 net.cpp:172] Setting up conv2_1_scale0
I0218 11:49:51.566442 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.566447 27028 net.cpp:194] Memory required for data: 13363840
I0218 11:49:51.566454 27028 layer_factory.hpp:77] Creating layer conv2_1_ReLU0
I0218 11:49:51.566463 27028 net.cpp:128] Creating Layer conv2_1_ReLU0
I0218 11:49:51.566468 27028 net.cpp:558] conv2_1_ReLU0 <- conv2_1_0
I0218 11:49:51.566473 27028 net.cpp:509] conv2_1_ReLU0 -> conv2_1_0 (in-place)
I0218 11:49:51.567858 27028 net.cpp:172] Setting up conv2_1_ReLU0
I0218 11:49:51.567883 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.567888 27028 net.cpp:194] Memory required for data: 14838400
I0218 11:49:51.567893 27028 layer_factory.hpp:77] Creating layer conv2_1_1
I0218 11:49:51.567910 27028 net.cpp:128] Creating Layer conv2_1_1
I0218 11:49:51.567916 27028 net.cpp:558] conv2_1_1 <- conv2_1_0
I0218 11:49:51.567926 27028 net.cpp:522] conv2_1_1 -> conv2_1_1
I0218 11:49:51.574604 27028 net.cpp:172] Setting up conv2_1_1
I0218 11:49:51.574631 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.574635 27028 net.cpp:194] Memory required for data: 16312960
I0218 11:49:51.574645 27028 layer_factory.hpp:77] Creating layer conv2_1bn1
I0218 11:49:51.574661 27028 net.cpp:128] Creating Layer conv2_1bn1
I0218 11:49:51.574666 27028 net.cpp:558] conv2_1bn1 <- conv2_1_1
I0218 11:49:51.574676 27028 net.cpp:509] conv2_1bn1 -> conv2_1_1 (in-place)
I0218 11:49:51.574964 27028 net.cpp:172] Setting up conv2_1bn1
I0218 11:49:51.574975 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.574978 27028 net.cpp:194] Memory required for data: 17787520
I0218 11:49:51.574990 27028 layer_factory.hpp:77] Creating layer conv2_1_scale1
I0218 11:49:51.575001 27028 net.cpp:128] Creating Layer conv2_1_scale1
I0218 11:49:51.575006 27028 net.cpp:558] conv2_1_scale1 <- conv2_1_1
I0218 11:49:51.575014 27028 net.cpp:509] conv2_1_scale1 -> conv2_1_1 (in-place)
I0218 11:49:51.575063 27028 layer_factory.hpp:77] Creating layer conv2_1_scale1
I0218 11:49:51.575222 27028 net.cpp:172] Setting up conv2_1_scale1
I0218 11:49:51.575232 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.575237 27028 net.cpp:194] Memory required for data: 19262080
I0218 11:49:51.575244 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_1
I0218 11:49:51.575253 27028 net.cpp:128] Creating Layer conv2_Eltwise_1
I0218 11:49:51.575258 27028 net.cpp:558] conv2_Eltwise_1 <- conv1_conv1/ReLU_0_split_1
I0218 11:49:51.575263 27028 net.cpp:558] conv2_Eltwise_1 <- conv2_1_1
I0218 11:49:51.575271 27028 net.cpp:522] conv2_Eltwise_1 -> conv2_Eltwise_1
I0218 11:49:51.575304 27028 net.cpp:172] Setting up conv2_Eltwise_1
I0218 11:49:51.575314 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.575316 27028 net.cpp:194] Memory required for data: 20736640
I0218 11:49:51.575320 27028 layer_factory.hpp:77] Creating layer conv2_1ReLU_1
I0218 11:49:51.575326 27028 net.cpp:128] Creating Layer conv2_1ReLU_1
I0218 11:49:51.575330 27028 net.cpp:558] conv2_1ReLU_1 <- conv2_Eltwise_1
I0218 11:49:51.575336 27028 net.cpp:509] conv2_1ReLU_1 -> conv2_Eltwise_1 (in-place)
I0218 11:49:51.576625 27028 net.cpp:172] Setting up conv2_1ReLU_1
I0218 11:49:51.576644 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.576649 27028 net.cpp:194] Memory required for data: 22211200
I0218 11:49:51.576653 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.576660 27028 net.cpp:128] Creating Layer conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.576666 27028 net.cpp:558] conv2_Eltwise_1_conv2_1ReLU_1_0_split <- conv2_Eltwise_1
I0218 11:49:51.576676 27028 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I0218 11:49:51.576684 27028 net.cpp:522] conv2_Eltwise_1_conv2_1ReLU_1_0_split -> conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I0218 11:49:51.576737 27028 net.cpp:172] Setting up conv2_Eltwise_1_conv2_1ReLU_1_0_split
I0218 11:49:51.576764 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.576769 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.576773 27028 net.cpp:194] Memory required for data: 25160320
I0218 11:49:51.576777 27028 layer_factory.hpp:77] Creating layer conv2_2_0
I0218 11:49:51.576795 27028 net.cpp:128] Creating Layer conv2_2_0
I0218 11:49:51.576800 27028 net.cpp:558] conv2_2_0 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_0
I0218 11:49:51.576807 27028 net.cpp:522] conv2_2_0 -> conv2_2_0
I0218 11:49:51.583367 27028 net.cpp:172] Setting up conv2_2_0
I0218 11:49:51.583393 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.583397 27028 net.cpp:194] Memory required for data: 26634880
I0218 11:49:51.583406 27028 layer_factory.hpp:77] Creating layer conv2_2_bn0
I0218 11:49:51.583421 27028 net.cpp:128] Creating Layer conv2_2_bn0
I0218 11:49:51.583427 27028 net.cpp:558] conv2_2_bn0 <- conv2_2_0
I0218 11:49:51.583436 27028 net.cpp:509] conv2_2_bn0 -> conv2_2_0 (in-place)
I0218 11:49:51.583709 27028 net.cpp:172] Setting up conv2_2_bn0
I0218 11:49:51.583719 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.583724 27028 net.cpp:194] Memory required for data: 28109440
I0218 11:49:51.583731 27028 layer_factory.hpp:77] Creating layer conv2_2_scale0
I0218 11:49:51.583741 27028 net.cpp:128] Creating Layer conv2_2_scale0
I0218 11:49:51.583746 27028 net.cpp:558] conv2_2_scale0 <- conv2_2_0
I0218 11:49:51.583752 27028 net.cpp:509] conv2_2_scale0 -> conv2_2_0 (in-place)
I0218 11:49:51.583801 27028 layer_factory.hpp:77] Creating layer conv2_2_scale0
I0218 11:49:51.583964 27028 net.cpp:172] Setting up conv2_2_scale0
I0218 11:49:51.583974 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.583978 27028 net.cpp:194] Memory required for data: 29584000
I0218 11:49:51.583986 27028 layer_factory.hpp:77] Creating layer conv2_2_ReLU0
I0218 11:49:51.583992 27028 net.cpp:128] Creating Layer conv2_2_ReLU0
I0218 11:49:51.583997 27028 net.cpp:558] conv2_2_ReLU0 <- conv2_2_0
I0218 11:49:51.584003 27028 net.cpp:509] conv2_2_ReLU0 -> conv2_2_0 (in-place)
I0218 11:49:51.585427 27028 net.cpp:172] Setting up conv2_2_ReLU0
I0218 11:49:51.585445 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.585449 27028 net.cpp:194] Memory required for data: 31058560
I0218 11:49:51.585453 27028 layer_factory.hpp:77] Creating layer conv2_2_1
I0218 11:49:51.585470 27028 net.cpp:128] Creating Layer conv2_2_1
I0218 11:49:51.585475 27028 net.cpp:558] conv2_2_1 <- conv2_2_0
I0218 11:49:51.585486 27028 net.cpp:522] conv2_2_1 -> conv2_2_1
I0218 11:49:51.592434 27028 net.cpp:172] Setting up conv2_2_1
I0218 11:49:51.592460 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.592465 27028 net.cpp:194] Memory required for data: 32533120
I0218 11:49:51.592474 27028 layer_factory.hpp:77] Creating layer conv2_2bn1
I0218 11:49:51.592489 27028 net.cpp:128] Creating Layer conv2_2bn1
I0218 11:49:51.592494 27028 net.cpp:558] conv2_2bn1 <- conv2_2_1
I0218 11:49:51.592501 27028 net.cpp:509] conv2_2bn1 -> conv2_2_1 (in-place)
I0218 11:49:51.592790 27028 net.cpp:172] Setting up conv2_2bn1
I0218 11:49:51.592802 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.592805 27028 net.cpp:194] Memory required for data: 34007680
I0218 11:49:51.592816 27028 layer_factory.hpp:77] Creating layer conv2_2_scale1
I0218 11:49:51.592825 27028 net.cpp:128] Creating Layer conv2_2_scale1
I0218 11:49:51.592830 27028 net.cpp:558] conv2_2_scale1 <- conv2_2_1
I0218 11:49:51.592839 27028 net.cpp:509] conv2_2_scale1 -> conv2_2_1 (in-place)
I0218 11:49:51.592891 27028 layer_factory.hpp:77] Creating layer conv2_2_scale1
I0218 11:49:51.593048 27028 net.cpp:172] Setting up conv2_2_scale1
I0218 11:49:51.593057 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.593061 27028 net.cpp:194] Memory required for data: 35482240
I0218 11:49:51.593068 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_2
I0218 11:49:51.593081 27028 net.cpp:128] Creating Layer conv2_Eltwise_2
I0218 11:49:51.593103 27028 net.cpp:558] conv2_Eltwise_2 <- conv2_Eltwise_1_conv2_1ReLU_1_0_split_1
I0218 11:49:51.593109 27028 net.cpp:558] conv2_Eltwise_2 <- conv2_2_1
I0218 11:49:51.593116 27028 net.cpp:522] conv2_Eltwise_2 -> conv2_Eltwise_2
I0218 11:49:51.593147 27028 net.cpp:172] Setting up conv2_Eltwise_2
I0218 11:49:51.593156 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.593160 27028 net.cpp:194] Memory required for data: 36956800
I0218 11:49:51.593164 27028 layer_factory.hpp:77] Creating layer conv2_2ReLU_1
I0218 11:49:51.593171 27028 net.cpp:128] Creating Layer conv2_2ReLU_1
I0218 11:49:51.593175 27028 net.cpp:558] conv2_2ReLU_1 <- conv2_Eltwise_2
I0218 11:49:51.593183 27028 net.cpp:509] conv2_2ReLU_1 -> conv2_Eltwise_2 (in-place)
I0218 11:49:51.594257 27028 net.cpp:172] Setting up conv2_2ReLU_1
I0218 11:49:51.594276 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.594280 27028 net.cpp:194] Memory required for data: 38431360
I0218 11:49:51.594285 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.594292 27028 net.cpp:128] Creating Layer conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.594296 27028 net.cpp:558] conv2_Eltwise_2_conv2_2ReLU_1_0_split <- conv2_Eltwise_2
I0218 11:49:51.594306 27028 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I0218 11:49:51.594314 27028 net.cpp:522] conv2_Eltwise_2_conv2_2ReLU_1_0_split -> conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I0218 11:49:51.594372 27028 net.cpp:172] Setting up conv2_Eltwise_2_conv2_2ReLU_1_0_split
I0218 11:49:51.594383 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.594388 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.594391 27028 net.cpp:194] Memory required for data: 41380480
I0218 11:49:51.594395 27028 layer_factory.hpp:77] Creating layer conv2_3_0
I0218 11:49:51.594408 27028 net.cpp:128] Creating Layer conv2_3_0
I0218 11:49:51.594413 27028 net.cpp:558] conv2_3_0 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_0
I0218 11:49:51.594424 27028 net.cpp:522] conv2_3_0 -> conv2_3_0
I0218 11:49:51.600952 27028 net.cpp:172] Setting up conv2_3_0
I0218 11:49:51.600978 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.600982 27028 net.cpp:194] Memory required for data: 42855040
I0218 11:49:51.600991 27028 layer_factory.hpp:77] Creating layer conv2_3_bn0
I0218 11:49:51.601007 27028 net.cpp:128] Creating Layer conv2_3_bn0
I0218 11:49:51.601012 27028 net.cpp:558] conv2_3_bn0 <- conv2_3_0
I0218 11:49:51.601029 27028 net.cpp:509] conv2_3_bn0 -> conv2_3_0 (in-place)
I0218 11:49:51.601310 27028 net.cpp:172] Setting up conv2_3_bn0
I0218 11:49:51.601323 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.601327 27028 net.cpp:194] Memory required for data: 44329600
I0218 11:49:51.601336 27028 layer_factory.hpp:77] Creating layer conv2_3_scale0
I0218 11:49:51.601343 27028 net.cpp:128] Creating Layer conv2_3_scale0
I0218 11:49:51.601347 27028 net.cpp:558] conv2_3_scale0 <- conv2_3_0
I0218 11:49:51.601352 27028 net.cpp:509] conv2_3_scale0 -> conv2_3_0 (in-place)
I0218 11:49:51.601402 27028 layer_factory.hpp:77] Creating layer conv2_3_scale0
I0218 11:49:51.601557 27028 net.cpp:172] Setting up conv2_3_scale0
I0218 11:49:51.601565 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.601569 27028 net.cpp:194] Memory required for data: 45804160
I0218 11:49:51.601577 27028 layer_factory.hpp:77] Creating layer conv2_3_ReLU0
I0218 11:49:51.601585 27028 net.cpp:128] Creating Layer conv2_3_ReLU0
I0218 11:49:51.601590 27028 net.cpp:558] conv2_3_ReLU0 <- conv2_3_0
I0218 11:49:51.601595 27028 net.cpp:509] conv2_3_ReLU0 -> conv2_3_0 (in-place)
I0218 11:49:51.603008 27028 net.cpp:172] Setting up conv2_3_ReLU0
I0218 11:49:51.603034 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.603039 27028 net.cpp:194] Memory required for data: 47278720
I0218 11:49:51.603044 27028 layer_factory.hpp:77] Creating layer conv2_3_1
I0218 11:49:51.603062 27028 net.cpp:128] Creating Layer conv2_3_1
I0218 11:49:51.603087 27028 net.cpp:558] conv2_3_1 <- conv2_3_0
I0218 11:49:51.603096 27028 net.cpp:522] conv2_3_1 -> conv2_3_1
I0218 11:49:51.609310 27028 net.cpp:172] Setting up conv2_3_1
I0218 11:49:51.609336 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.609341 27028 net.cpp:194] Memory required for data: 48753280
I0218 11:49:51.609354 27028 layer_factory.hpp:77] Creating layer conv2_3bn1
I0218 11:49:51.609366 27028 net.cpp:128] Creating Layer conv2_3bn1
I0218 11:49:51.609380 27028 net.cpp:558] conv2_3bn1 <- conv2_3_1
I0218 11:49:51.609386 27028 net.cpp:509] conv2_3bn1 -> conv2_3_1 (in-place)
I0218 11:49:51.609668 27028 net.cpp:172] Setting up conv2_3bn1
I0218 11:49:51.609678 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.609681 27028 net.cpp:194] Memory required for data: 50227840
I0218 11:49:51.609689 27028 layer_factory.hpp:77] Creating layer conv2_3_scale1
I0218 11:49:51.609695 27028 net.cpp:128] Creating Layer conv2_3_scale1
I0218 11:49:51.609700 27028 net.cpp:558] conv2_3_scale1 <- conv2_3_1
I0218 11:49:51.609707 27028 net.cpp:509] conv2_3_scale1 -> conv2_3_1 (in-place)
I0218 11:49:51.609756 27028 layer_factory.hpp:77] Creating layer conv2_3_scale1
I0218 11:49:51.609917 27028 net.cpp:172] Setting up conv2_3_scale1
I0218 11:49:51.609927 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.609931 27028 net.cpp:194] Memory required for data: 51702400
I0218 11:49:51.609947 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_3
I0218 11:49:51.609953 27028 net.cpp:128] Creating Layer conv2_Eltwise_3
I0218 11:49:51.609958 27028 net.cpp:558] conv2_Eltwise_3 <- conv2_Eltwise_2_conv2_2ReLU_1_0_split_1
I0218 11:49:51.609963 27028 net.cpp:558] conv2_Eltwise_3 <- conv2_3_1
I0218 11:49:51.609972 27028 net.cpp:522] conv2_Eltwise_3 -> conv2_Eltwise_3
I0218 11:49:51.610002 27028 net.cpp:172] Setting up conv2_Eltwise_3
I0218 11:49:51.610013 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.610016 27028 net.cpp:194] Memory required for data: 53176960
I0218 11:49:51.610021 27028 layer_factory.hpp:77] Creating layer conv2_3ReLU_1
I0218 11:49:51.610028 27028 net.cpp:128] Creating Layer conv2_3ReLU_1
I0218 11:49:51.610031 27028 net.cpp:558] conv2_3ReLU_1 <- conv2_Eltwise_3
I0218 11:49:51.610036 27028 net.cpp:509] conv2_3ReLU_1 -> conv2_Eltwise_3 (in-place)
I0218 11:49:51.610280 27028 net.cpp:172] Setting up conv2_3ReLU_1
I0218 11:49:51.610293 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.610297 27028 net.cpp:194] Memory required for data: 54651520
I0218 11:49:51.610301 27028 layer_factory.hpp:77] Creating layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.610311 27028 net.cpp:128] Creating Layer conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.610316 27028 net.cpp:558] conv2_Eltwise_3_conv2_3ReLU_1_0_split <- conv2_Eltwise_3
I0218 11:49:51.610321 27028 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I0218 11:49:51.610329 27028 net.cpp:522] conv2_Eltwise_3_conv2_3ReLU_1_0_split -> conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I0218 11:49:51.610383 27028 net.cpp:172] Setting up conv2_Eltwise_3_conv2_3ReLU_1_0_split
I0218 11:49:51.610393 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.610399 27028 net.cpp:186] Top shape: 10 16 32 72 (368640)
I0218 11:49:51.610401 27028 net.cpp:194] Memory required for data: 57600640
I0218 11:49:51.610405 27028 layer_factory.hpp:77] Creating layer conv3_1_0
I0218 11:49:51.610419 27028 net.cpp:128] Creating Layer conv3_1_0
I0218 11:49:51.610424 27028 net.cpp:558] conv3_1_0 <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_0
I0218 11:49:51.610431 27028 net.cpp:522] conv3_1_0 -> conv3_1_0
I0218 11:49:51.612088 27028 net.cpp:172] Setting up conv3_1_0
I0218 11:49:51.612114 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.612119 27028 net.cpp:194] Memory required for data: 58337920
I0218 11:49:51.612131 27028 layer_factory.hpp:77] Creating layer conv3_1_bn0
I0218 11:49:51.612143 27028 net.cpp:128] Creating Layer conv3_1_bn0
I0218 11:49:51.612149 27028 net.cpp:558] conv3_1_bn0 <- conv3_1_0
I0218 11:49:51.612175 27028 net.cpp:509] conv3_1_bn0 -> conv3_1_0 (in-place)
I0218 11:49:51.612463 27028 net.cpp:172] Setting up conv3_1_bn0
I0218 11:49:51.612475 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.612479 27028 net.cpp:194] Memory required for data: 59075200
I0218 11:49:51.612488 27028 layer_factory.hpp:77] Creating layer conv3_1_scale0
I0218 11:49:51.612498 27028 net.cpp:128] Creating Layer conv3_1_scale0
I0218 11:49:51.612502 27028 net.cpp:558] conv3_1_scale0 <- conv3_1_0
I0218 11:49:51.612509 27028 net.cpp:509] conv3_1_scale0 -> conv3_1_0 (in-place)
I0218 11:49:51.612557 27028 layer_factory.hpp:77] Creating layer conv3_1_scale0
I0218 11:49:51.612721 27028 net.cpp:172] Setting up conv3_1_scale0
I0218 11:49:51.612731 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.612735 27028 net.cpp:194] Memory required for data: 59812480
I0218 11:49:51.612742 27028 layer_factory.hpp:77] Creating layer conv3_1_ReLU0
I0218 11:49:51.612751 27028 net.cpp:128] Creating Layer conv3_1_ReLU0
I0218 11:49:51.612756 27028 net.cpp:558] conv3_1_ReLU0 <- conv3_1_0
I0218 11:49:51.612761 27028 net.cpp:509] conv3_1_ReLU0 -> conv3_1_0 (in-place)
I0218 11:49:51.613008 27028 net.cpp:172] Setting up conv3_1_ReLU0
I0218 11:49:51.613019 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.613023 27028 net.cpp:194] Memory required for data: 60549760
I0218 11:49:51.613027 27028 layer_factory.hpp:77] Creating layer conv3_1_1
I0218 11:49:51.613040 27028 net.cpp:128] Creating Layer conv3_1_1
I0218 11:49:51.613047 27028 net.cpp:558] conv3_1_1 <- conv3_1_0
I0218 11:49:51.613057 27028 net.cpp:522] conv3_1_1 -> conv3_1_1
I0218 11:49:51.618314 27028 net.cpp:172] Setting up conv3_1_1
I0218 11:49:51.618340 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.618345 27028 net.cpp:194] Memory required for data: 61287040
I0218 11:49:51.618360 27028 layer_factory.hpp:77] Creating layer conv3_1bn1
I0218 11:49:51.618371 27028 net.cpp:128] Creating Layer conv3_1bn1
I0218 11:49:51.618376 27028 net.cpp:558] conv3_1bn1 <- conv3_1_1
I0218 11:49:51.618383 27028 net.cpp:509] conv3_1bn1 -> conv3_1_1 (in-place)
I0218 11:49:51.618667 27028 net.cpp:172] Setting up conv3_1bn1
I0218 11:49:51.618677 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.618681 27028 net.cpp:194] Memory required for data: 62024320
I0218 11:49:51.618690 27028 layer_factory.hpp:77] Creating layer conv3_1_scale1
I0218 11:49:51.618696 27028 net.cpp:128] Creating Layer conv3_1_scale1
I0218 11:49:51.618701 27028 net.cpp:558] conv3_1_scale1 <- conv3_1_1
I0218 11:49:51.618707 27028 net.cpp:509] conv3_1_scale1 -> conv3_1_1 (in-place)
I0218 11:49:51.618762 27028 layer_factory.hpp:77] Creating layer conv3_1_scale1
I0218 11:49:51.618926 27028 net.cpp:172] Setting up conv3_1_scale1
I0218 11:49:51.618935 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.618939 27028 net.cpp:194] Memory required for data: 62761600
I0218 11:49:51.618947 27028 layer_factory.hpp:77] Creating layer conv3_1_down
I0218 11:49:51.618959 27028 net.cpp:128] Creating Layer conv3_1_down
I0218 11:49:51.618968 27028 net.cpp:558] conv3_1_down <- conv2_Eltwise_3_conv2_3ReLU_1_0_split_1
I0218 11:49:51.618978 27028 net.cpp:522] conv3_1_down -> conv3_1_down
I0218 11:49:51.624888 27028 net.cpp:172] Setting up conv3_1_down
I0218 11:49:51.624915 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.624919 27028 net.cpp:194] Memory required for data: 63498880
I0218 11:49:51.624943 27028 layer_factory.hpp:77] Creating layer conv3_1_bn_down
I0218 11:49:51.624955 27028 net.cpp:128] Creating Layer conv3_1_bn_down
I0218 11:49:51.624960 27028 net.cpp:558] conv3_1_bn_down <- conv3_1_down
I0218 11:49:51.624967 27028 net.cpp:509] conv3_1_bn_down -> conv3_1_down (in-place)
I0218 11:49:51.625257 27028 net.cpp:172] Setting up conv3_1_bn_down
I0218 11:49:51.625267 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.625272 27028 net.cpp:194] Memory required for data: 64236160
I0218 11:49:51.625280 27028 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I0218 11:49:51.625308 27028 net.cpp:128] Creating Layer conv3_1_scale_down
I0218 11:49:51.625313 27028 net.cpp:558] conv3_1_scale_down <- conv3_1_down
I0218 11:49:51.625319 27028 net.cpp:509] conv3_1_scale_down -> conv3_1_down (in-place)
I0218 11:49:51.625373 27028 layer_factory.hpp:77] Creating layer conv3_1_scale_down
I0218 11:49:51.625535 27028 net.cpp:172] Setting up conv3_1_scale_down
I0218 11:49:51.625545 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.625550 27028 net.cpp:194] Memory required for data: 64973440
I0218 11:49:51.625557 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_1
I0218 11:49:51.625568 27028 net.cpp:128] Creating Layer conv3_Eltwise_1
I0218 11:49:51.625576 27028 net.cpp:558] conv3_Eltwise_1 <- conv3_1_1
I0218 11:49:51.625581 27028 net.cpp:558] conv3_Eltwise_1 <- conv3_1_down
I0218 11:49:51.625587 27028 net.cpp:522] conv3_Eltwise_1 -> conv3_Eltwise_1
I0218 11:49:51.625618 27028 net.cpp:172] Setting up conv3_Eltwise_1
I0218 11:49:51.625627 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.625632 27028 net.cpp:194] Memory required for data: 65710720
I0218 11:49:51.625636 27028 layer_factory.hpp:77] Creating layer conv3_1ReLU_1
I0218 11:49:51.625643 27028 net.cpp:128] Creating Layer conv3_1ReLU_1
I0218 11:49:51.625648 27028 net.cpp:558] conv3_1ReLU_1 <- conv3_Eltwise_1
I0218 11:49:51.625653 27028 net.cpp:509] conv3_1ReLU_1 -> conv3_Eltwise_1 (in-place)
I0218 11:49:51.626948 27028 net.cpp:172] Setting up conv3_1ReLU_1
I0218 11:49:51.626974 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.626978 27028 net.cpp:194] Memory required for data: 66448000
I0218 11:49:51.626983 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.626996 27028 net.cpp:128] Creating Layer conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.627001 27028 net.cpp:558] conv3_Eltwise_1_conv3_1ReLU_1_0_split <- conv3_Eltwise_1
I0218 11:49:51.627010 27028 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I0218 11:49:51.627019 27028 net.cpp:522] conv3_Eltwise_1_conv3_1ReLU_1_0_split -> conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I0218 11:49:51.627079 27028 net.cpp:172] Setting up conv3_Eltwise_1_conv3_1ReLU_1_0_split
I0218 11:49:51.627089 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.627094 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.627099 27028 net.cpp:194] Memory required for data: 67922560
I0218 11:49:51.627102 27028 layer_factory.hpp:77] Creating layer conv3_2_0
I0218 11:49:51.627115 27028 net.cpp:128] Creating Layer conv3_2_0
I0218 11:49:51.627120 27028 net.cpp:558] conv3_2_0 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_0
I0218 11:49:51.627130 27028 net.cpp:522] conv3_2_0 -> conv3_2_0
I0218 11:49:51.633679 27028 net.cpp:172] Setting up conv3_2_0
I0218 11:49:51.633707 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.633710 27028 net.cpp:194] Memory required for data: 68659840
I0218 11:49:51.633719 27028 layer_factory.hpp:77] Creating layer conv3_2_bn0
I0218 11:49:51.633730 27028 net.cpp:128] Creating Layer conv3_2_bn0
I0218 11:49:51.633736 27028 net.cpp:558] conv3_2_bn0 <- conv3_2_0
I0218 11:49:51.633743 27028 net.cpp:509] conv3_2_bn0 -> conv3_2_0 (in-place)
I0218 11:49:51.634050 27028 net.cpp:172] Setting up conv3_2_bn0
I0218 11:49:51.634063 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.634068 27028 net.cpp:194] Memory required for data: 69397120
I0218 11:49:51.634075 27028 layer_factory.hpp:77] Creating layer conv3_2_scale0
I0218 11:49:51.634083 27028 net.cpp:128] Creating Layer conv3_2_scale0
I0218 11:49:51.634086 27028 net.cpp:558] conv3_2_scale0 <- conv3_2_0
I0218 11:49:51.634093 27028 net.cpp:509] conv3_2_scale0 -> conv3_2_0 (in-place)
I0218 11:49:51.634146 27028 layer_factory.hpp:77] Creating layer conv3_2_scale0
I0218 11:49:51.634306 27028 net.cpp:172] Setting up conv3_2_scale0
I0218 11:49:51.634317 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.634337 27028 net.cpp:194] Memory required for data: 70134400
I0218 11:49:51.634346 27028 layer_factory.hpp:77] Creating layer conv3_2_ReLU0
I0218 11:49:51.634351 27028 net.cpp:128] Creating Layer conv3_2_ReLU0
I0218 11:49:51.634356 27028 net.cpp:558] conv3_2_ReLU0 <- conv3_2_0
I0218 11:49:51.634364 27028 net.cpp:509] conv3_2_ReLU0 -> conv3_2_0 (in-place)
I0218 11:49:51.635694 27028 net.cpp:172] Setting up conv3_2_ReLU0
I0218 11:49:51.635704 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.635709 27028 net.cpp:194] Memory required for data: 70871680
I0218 11:49:51.635712 27028 layer_factory.hpp:77] Creating layer conv3_2_1
I0218 11:49:51.635725 27028 net.cpp:128] Creating Layer conv3_2_1
I0218 11:49:51.635730 27028 net.cpp:558] conv3_2_1 <- conv3_2_0
I0218 11:49:51.635740 27028 net.cpp:522] conv3_2_1 -> conv3_2_1
I0218 11:49:51.641944 27028 net.cpp:172] Setting up conv3_2_1
I0218 11:49:51.641963 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.641966 27028 net.cpp:194] Memory required for data: 71608960
I0218 11:49:51.641975 27028 layer_factory.hpp:77] Creating layer conv3_2bn1
I0218 11:49:51.641983 27028 net.cpp:128] Creating Layer conv3_2bn1
I0218 11:49:51.641988 27028 net.cpp:558] conv3_2bn1 <- conv3_2_1
I0218 11:49:51.641997 27028 net.cpp:509] conv3_2bn1 -> conv3_2_1 (in-place)
I0218 11:49:51.642282 27028 net.cpp:172] Setting up conv3_2bn1
I0218 11:49:51.642289 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642293 27028 net.cpp:194] Memory required for data: 72346240
I0218 11:49:51.642302 27028 layer_factory.hpp:77] Creating layer conv3_2_scale1
I0218 11:49:51.642310 27028 net.cpp:128] Creating Layer conv3_2_scale1
I0218 11:49:51.642314 27028 net.cpp:558] conv3_2_scale1 <- conv3_2_1
I0218 11:49:51.642320 27028 net.cpp:509] conv3_2_scale1 -> conv3_2_1 (in-place)
I0218 11:49:51.642369 27028 layer_factory.hpp:77] Creating layer conv3_2_scale1
I0218 11:49:51.642527 27028 net.cpp:172] Setting up conv3_2_scale1
I0218 11:49:51.642534 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642539 27028 net.cpp:194] Memory required for data: 73083520
I0218 11:49:51.642544 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_2
I0218 11:49:51.642552 27028 net.cpp:128] Creating Layer conv3_Eltwise_2
I0218 11:49:51.642557 27028 net.cpp:558] conv3_Eltwise_2 <- conv3_Eltwise_1_conv3_1ReLU_1_0_split_1
I0218 11:49:51.642562 27028 net.cpp:558] conv3_Eltwise_2 <- conv3_2_1
I0218 11:49:51.642570 27028 net.cpp:522] conv3_Eltwise_2 -> conv3_Eltwise_2
I0218 11:49:51.642597 27028 net.cpp:172] Setting up conv3_Eltwise_2
I0218 11:49:51.642604 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642607 27028 net.cpp:194] Memory required for data: 73820800
I0218 11:49:51.642611 27028 layer_factory.hpp:77] Creating layer conv3_2ReLU_1
I0218 11:49:51.642619 27028 net.cpp:128] Creating Layer conv3_2ReLU_1
I0218 11:49:51.642624 27028 net.cpp:558] conv3_2ReLU_1 <- conv3_Eltwise_2
I0218 11:49:51.642629 27028 net.cpp:509] conv3_2ReLU_1 -> conv3_Eltwise_2 (in-place)
I0218 11:49:51.642871 27028 net.cpp:172] Setting up conv3_2ReLU_1
I0218 11:49:51.642880 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642884 27028 net.cpp:194] Memory required for data: 74558080
I0218 11:49:51.642889 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.642897 27028 net.cpp:128] Creating Layer conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.642901 27028 net.cpp:558] conv3_Eltwise_2_conv3_2ReLU_1_0_split <- conv3_Eltwise_2
I0218 11:49:51.642908 27028 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I0218 11:49:51.642918 27028 net.cpp:522] conv3_Eltwise_2_conv3_2ReLU_1_0_split -> conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I0218 11:49:51.642971 27028 net.cpp:172] Setting up conv3_Eltwise_2_conv3_2ReLU_1_0_split
I0218 11:49:51.642977 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642983 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.642987 27028 net.cpp:194] Memory required for data: 76032640
I0218 11:49:51.643007 27028 layer_factory.hpp:77] Creating layer conv3_3_0
I0218 11:49:51.643031 27028 net.cpp:128] Creating Layer conv3_3_0
I0218 11:49:51.643036 27028 net.cpp:558] conv3_3_0 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_0
I0218 11:49:51.643043 27028 net.cpp:522] conv3_3_0 -> conv3_3_0
I0218 11:49:51.649523 27028 net.cpp:172] Setting up conv3_3_0
I0218 11:49:51.649541 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.649546 27028 net.cpp:194] Memory required for data: 76769920
I0218 11:49:51.649555 27028 layer_factory.hpp:77] Creating layer conv3_3_bn0
I0218 11:49:51.649565 27028 net.cpp:128] Creating Layer conv3_3_bn0
I0218 11:49:51.649570 27028 net.cpp:558] conv3_3_bn0 <- conv3_3_0
I0218 11:49:51.649580 27028 net.cpp:509] conv3_3_bn0 -> conv3_3_0 (in-place)
I0218 11:49:51.649865 27028 net.cpp:172] Setting up conv3_3_bn0
I0218 11:49:51.649873 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.649876 27028 net.cpp:194] Memory required for data: 77507200
I0218 11:49:51.649885 27028 layer_factory.hpp:77] Creating layer conv3_3_scale0
I0218 11:49:51.649891 27028 net.cpp:128] Creating Layer conv3_3_scale0
I0218 11:49:51.649895 27028 net.cpp:558] conv3_3_scale0 <- conv3_3_0
I0218 11:49:51.649901 27028 net.cpp:509] conv3_3_scale0 -> conv3_3_0 (in-place)
I0218 11:49:51.649965 27028 layer_factory.hpp:77] Creating layer conv3_3_scale0
I0218 11:49:51.650128 27028 net.cpp:172] Setting up conv3_3_scale0
I0218 11:49:51.650137 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.650141 27028 net.cpp:194] Memory required for data: 78244480
I0218 11:49:51.650148 27028 layer_factory.hpp:77] Creating layer conv3_3_ReLU0
I0218 11:49:51.650154 27028 net.cpp:128] Creating Layer conv3_3_ReLU0
I0218 11:49:51.650158 27028 net.cpp:558] conv3_3_ReLU0 <- conv3_3_0
I0218 11:49:51.650163 27028 net.cpp:509] conv3_3_ReLU0 -> conv3_3_0 (in-place)
I0218 11:49:51.651602 27028 net.cpp:172] Setting up conv3_3_ReLU0
I0218 11:49:51.651618 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.651623 27028 net.cpp:194] Memory required for data: 78981760
I0218 11:49:51.651628 27028 layer_factory.hpp:77] Creating layer conv3_3_1
I0218 11:49:51.651643 27028 net.cpp:128] Creating Layer conv3_3_1
I0218 11:49:51.651648 27028 net.cpp:558] conv3_3_1 <- conv3_3_0
I0218 11:49:51.651656 27028 net.cpp:522] conv3_3_1 -> conv3_3_1
I0218 11:49:51.658279 27028 net.cpp:172] Setting up conv3_3_1
I0218 11:49:51.658303 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.658308 27028 net.cpp:194] Memory required for data: 79719040
I0218 11:49:51.658316 27028 layer_factory.hpp:77] Creating layer conv3_3bn1
I0218 11:49:51.658329 27028 net.cpp:128] Creating Layer conv3_3bn1
I0218 11:49:51.658334 27028 net.cpp:558] conv3_3bn1 <- conv3_3_1
I0218 11:49:51.658342 27028 net.cpp:509] conv3_3bn1 -> conv3_3_1 (in-place)
I0218 11:49:51.658630 27028 net.cpp:172] Setting up conv3_3bn1
I0218 11:49:51.658638 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.658643 27028 net.cpp:194] Memory required for data: 80456320
I0218 11:49:51.658650 27028 layer_factory.hpp:77] Creating layer conv3_3_scale1
I0218 11:49:51.658659 27028 net.cpp:128] Creating Layer conv3_3_scale1
I0218 11:49:51.658664 27028 net.cpp:558] conv3_3_scale1 <- conv3_3_1
I0218 11:49:51.658669 27028 net.cpp:509] conv3_3_scale1 -> conv3_3_1 (in-place)
I0218 11:49:51.658720 27028 layer_factory.hpp:77] Creating layer conv3_3_scale1
I0218 11:49:51.658885 27028 net.cpp:172] Setting up conv3_3_scale1
I0218 11:49:51.658892 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.658895 27028 net.cpp:194] Memory required for data: 81193600
I0218 11:49:51.658902 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_3
I0218 11:49:51.658911 27028 net.cpp:128] Creating Layer conv3_Eltwise_3
I0218 11:49:51.658916 27028 net.cpp:558] conv3_Eltwise_3 <- conv3_Eltwise_2_conv3_2ReLU_1_0_split_1
I0218 11:49:51.658921 27028 net.cpp:558] conv3_Eltwise_3 <- conv3_3_1
I0218 11:49:51.658927 27028 net.cpp:522] conv3_Eltwise_3 -> conv3_Eltwise_3
I0218 11:49:51.658977 27028 net.cpp:172] Setting up conv3_Eltwise_3
I0218 11:49:51.658984 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.658987 27028 net.cpp:194] Memory required for data: 81930880
I0218 11:49:51.658991 27028 layer_factory.hpp:77] Creating layer conv3_3ReLU_1
I0218 11:49:51.658998 27028 net.cpp:128] Creating Layer conv3_3ReLU_1
I0218 11:49:51.659003 27028 net.cpp:558] conv3_3ReLU_1 <- conv3_Eltwise_3
I0218 11:49:51.659008 27028 net.cpp:509] conv3_3ReLU_1 -> conv3_Eltwise_3 (in-place)
I0218 11:49:51.660347 27028 net.cpp:172] Setting up conv3_3ReLU_1
I0218 11:49:51.660357 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.660362 27028 net.cpp:194] Memory required for data: 82668160
I0218 11:49:51.660365 27028 layer_factory.hpp:77] Creating layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.660372 27028 net.cpp:128] Creating Layer conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.660377 27028 net.cpp:558] conv3_Eltwise_3_conv3_3ReLU_1_0_split <- conv3_Eltwise_3
I0218 11:49:51.660385 27028 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I0218 11:49:51.660393 27028 net.cpp:522] conv3_Eltwise_3_conv3_3ReLU_1_0_split -> conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I0218 11:49:51.660449 27028 net.cpp:172] Setting up conv3_Eltwise_3_conv3_3ReLU_1_0_split
I0218 11:49:51.660455 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.660465 27028 net.cpp:186] Top shape: 10 32 16 36 (184320)
I0218 11:49:51.660467 27028 net.cpp:194] Memory required for data: 84142720
I0218 11:49:51.660471 27028 layer_factory.hpp:77] Creating layer conv4_1_0
I0218 11:49:51.660483 27028 net.cpp:128] Creating Layer conv4_1_0
I0218 11:49:51.660488 27028 net.cpp:558] conv4_1_0 <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_0
I0218 11:49:51.660498 27028 net.cpp:522] conv4_1_0 -> conv4_1_0
I0218 11:49:51.667060 27028 net.cpp:172] Setting up conv4_1_0
I0218 11:49:51.667085 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.667089 27028 net.cpp:194] Memory required for data: 84511360
I0218 11:49:51.667104 27028 layer_factory.hpp:77] Creating layer conv4_1_bn0
I0218 11:49:51.667115 27028 net.cpp:128] Creating Layer conv4_1_bn0
I0218 11:49:51.667124 27028 net.cpp:558] conv4_1_bn0 <- conv4_1_0
I0218 11:49:51.667134 27028 net.cpp:509] conv4_1_bn0 -> conv4_1_0 (in-place)
I0218 11:49:51.667418 27028 net.cpp:172] Setting up conv4_1_bn0
I0218 11:49:51.667428 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.667431 27028 net.cpp:194] Memory required for data: 84880000
I0218 11:49:51.667439 27028 layer_factory.hpp:77] Creating layer conv4_1_scale0
I0218 11:49:51.667449 27028 net.cpp:128] Creating Layer conv4_1_scale0
I0218 11:49:51.667459 27028 net.cpp:558] conv4_1_scale0 <- conv4_1_0
I0218 11:49:51.667464 27028 net.cpp:509] conv4_1_scale0 -> conv4_1_0 (in-place)
I0218 11:49:51.667515 27028 layer_factory.hpp:77] Creating layer conv4_1_scale0
I0218 11:49:51.667685 27028 net.cpp:172] Setting up conv4_1_scale0
I0218 11:49:51.667695 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.667698 27028 net.cpp:194] Memory required for data: 85248640
I0218 11:49:51.667706 27028 layer_factory.hpp:77] Creating layer conv4_1_ReLU0
I0218 11:49:51.667711 27028 net.cpp:128] Creating Layer conv4_1_ReLU0
I0218 11:49:51.667721 27028 net.cpp:558] conv4_1_ReLU0 <- conv4_1_0
I0218 11:49:51.667733 27028 net.cpp:509] conv4_1_ReLU0 -> conv4_1_0 (in-place)
I0218 11:49:51.669066 27028 net.cpp:172] Setting up conv4_1_ReLU0
I0218 11:49:51.669085 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.669090 27028 net.cpp:194] Memory required for data: 85617280
I0218 11:49:51.669093 27028 layer_factory.hpp:77] Creating layer conv4_1_1
I0218 11:49:51.669111 27028 net.cpp:128] Creating Layer conv4_1_1
I0218 11:49:51.669124 27028 net.cpp:558] conv4_1_1 <- conv4_1_0
I0218 11:49:51.669134 27028 net.cpp:522] conv4_1_1 -> conv4_1_1
I0218 11:49:51.675781 27028 net.cpp:172] Setting up conv4_1_1
I0218 11:49:51.675806 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.675829 27028 net.cpp:194] Memory required for data: 85985920
I0218 11:49:51.675839 27028 layer_factory.hpp:77] Creating layer conv4_1bn1
I0218 11:49:51.675854 27028 net.cpp:128] Creating Layer conv4_1bn1
I0218 11:49:51.675863 27028 net.cpp:558] conv4_1bn1 <- conv4_1_1
I0218 11:49:51.675870 27028 net.cpp:509] conv4_1bn1 -> conv4_1_1 (in-place)
I0218 11:49:51.676160 27028 net.cpp:172] Setting up conv4_1bn1
I0218 11:49:51.676170 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.676174 27028 net.cpp:194] Memory required for data: 86354560
I0218 11:49:51.676182 27028 layer_factory.hpp:77] Creating layer conv4_1_scale1
I0218 11:49:51.676189 27028 net.cpp:128] Creating Layer conv4_1_scale1
I0218 11:49:51.676194 27028 net.cpp:558] conv4_1_scale1 <- conv4_1_1
I0218 11:49:51.676200 27028 net.cpp:509] conv4_1_scale1 -> conv4_1_1 (in-place)
I0218 11:49:51.676254 27028 layer_factory.hpp:77] Creating layer conv4_1_scale1
I0218 11:49:51.676424 27028 net.cpp:172] Setting up conv4_1_scale1
I0218 11:49:51.676436 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.676442 27028 net.cpp:194] Memory required for data: 86723200
I0218 11:49:51.676450 27028 layer_factory.hpp:77] Creating layer conv4_1_down
I0218 11:49:51.676461 27028 net.cpp:128] Creating Layer conv4_1_down
I0218 11:49:51.676466 27028 net.cpp:558] conv4_1_down <- conv3_Eltwise_3_conv3_3ReLU_1_0_split_1
I0218 11:49:51.676477 27028 net.cpp:522] conv4_1_down -> conv4_1_down
I0218 11:49:51.682309 27028 net.cpp:172] Setting up conv4_1_down
I0218 11:49:51.682335 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.682340 27028 net.cpp:194] Memory required for data: 87091840
I0218 11:49:51.682349 27028 layer_factory.hpp:77] Creating layer conv4_1_bn_down
I0218 11:49:51.682360 27028 net.cpp:128] Creating Layer conv4_1_bn_down
I0218 11:49:51.682365 27028 net.cpp:558] conv4_1_bn_down <- conv4_1_down
I0218 11:49:51.682373 27028 net.cpp:509] conv4_1_bn_down -> conv4_1_down (in-place)
I0218 11:49:51.682667 27028 net.cpp:172] Setting up conv4_1_bn_down
I0218 11:49:51.682680 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.682684 27028 net.cpp:194] Memory required for data: 87460480
I0218 11:49:51.682693 27028 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I0218 11:49:51.682701 27028 net.cpp:128] Creating Layer conv4_1_scale_down
I0218 11:49:51.682706 27028 net.cpp:558] conv4_1_scale_down <- conv4_1_down
I0218 11:49:51.682713 27028 net.cpp:509] conv4_1_scale_down -> conv4_1_down (in-place)
I0218 11:49:51.682763 27028 layer_factory.hpp:77] Creating layer conv4_1_scale_down
I0218 11:49:51.682929 27028 net.cpp:172] Setting up conv4_1_scale_down
I0218 11:49:51.682940 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.682945 27028 net.cpp:194] Memory required for data: 87829120
I0218 11:49:51.682951 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_1
I0218 11:49:51.682958 27028 net.cpp:128] Creating Layer conv4_Eltwise_1
I0218 11:49:51.682962 27028 net.cpp:558] conv4_Eltwise_1 <- conv4_1_1
I0218 11:49:51.682967 27028 net.cpp:558] conv4_Eltwise_1 <- conv4_1_down
I0218 11:49:51.682976 27028 net.cpp:522] conv4_Eltwise_1 -> conv4_Eltwise_1
I0218 11:49:51.683004 27028 net.cpp:172] Setting up conv4_Eltwise_1
I0218 11:49:51.683012 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.683015 27028 net.cpp:194] Memory required for data: 88197760
I0218 11:49:51.683019 27028 layer_factory.hpp:77] Creating layer conv4_1ReLU_1
I0218 11:49:51.683027 27028 net.cpp:128] Creating Layer conv4_1ReLU_1
I0218 11:49:51.683032 27028 net.cpp:558] conv4_1ReLU_1 <- conv4_Eltwise_1
I0218 11:49:51.683037 27028 net.cpp:509] conv4_1ReLU_1 -> conv4_Eltwise_1 (in-place)
I0218 11:49:51.684353 27028 net.cpp:172] Setting up conv4_1ReLU_1
I0218 11:49:51.684368 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.684375 27028 net.cpp:194] Memory required for data: 88566400
I0218 11:49:51.684378 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.684406 27028 net.cpp:128] Creating Layer conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.684412 27028 net.cpp:558] conv4_Eltwise_1_conv4_1ReLU_1_0_split <- conv4_Eltwise_1
I0218 11:49:51.684422 27028 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I0218 11:49:51.684430 27028 net.cpp:522] conv4_Eltwise_1_conv4_1ReLU_1_0_split -> conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I0218 11:49:51.684485 27028 net.cpp:172] Setting up conv4_Eltwise_1_conv4_1ReLU_1_0_split
I0218 11:49:51.684499 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.684505 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.684509 27028 net.cpp:194] Memory required for data: 89303680
I0218 11:49:51.684514 27028 layer_factory.hpp:77] Creating layer conv4_2_0
I0218 11:49:51.684525 27028 net.cpp:128] Creating Layer conv4_2_0
I0218 11:49:51.684530 27028 net.cpp:558] conv4_2_0 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_0
I0218 11:49:51.684537 27028 net.cpp:522] conv4_2_0 -> conv4_2_0
I0218 11:49:51.691283 27028 net.cpp:172] Setting up conv4_2_0
I0218 11:49:51.691315 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.691320 27028 net.cpp:194] Memory required for data: 89672320
I0218 11:49:51.691334 27028 layer_factory.hpp:77] Creating layer conv4_2_bn0
I0218 11:49:51.691342 27028 net.cpp:128] Creating Layer conv4_2_bn0
I0218 11:49:51.691365 27028 net.cpp:558] conv4_2_bn0 <- conv4_2_0
I0218 11:49:51.691375 27028 net.cpp:509] conv4_2_bn0 -> conv4_2_0 (in-place)
I0218 11:49:51.691673 27028 net.cpp:172] Setting up conv4_2_bn0
I0218 11:49:51.691684 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.691689 27028 net.cpp:194] Memory required for data: 90040960
I0218 11:49:51.691696 27028 layer_factory.hpp:77] Creating layer conv4_2_scale0
I0218 11:49:51.691704 27028 net.cpp:128] Creating Layer conv4_2_scale0
I0218 11:49:51.691715 27028 net.cpp:558] conv4_2_scale0 <- conv4_2_0
I0218 11:49:51.691721 27028 net.cpp:509] conv4_2_scale0 -> conv4_2_0 (in-place)
I0218 11:49:51.691774 27028 layer_factory.hpp:77] Creating layer conv4_2_scale0
I0218 11:49:51.691939 27028 net.cpp:172] Setting up conv4_2_scale0
I0218 11:49:51.691952 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.691956 27028 net.cpp:194] Memory required for data: 90409600
I0218 11:49:51.691964 27028 layer_factory.hpp:77] Creating layer conv4_2_ReLU0
I0218 11:49:51.691972 27028 net.cpp:128] Creating Layer conv4_2_ReLU0
I0218 11:49:51.691977 27028 net.cpp:558] conv4_2_ReLU0 <- conv4_2_0
I0218 11:49:51.691982 27028 net.cpp:509] conv4_2_ReLU0 -> conv4_2_0 (in-place)
I0218 11:49:51.693094 27028 net.cpp:172] Setting up conv4_2_ReLU0
I0218 11:49:51.693120 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.693123 27028 net.cpp:194] Memory required for data: 90778240
I0218 11:49:51.693128 27028 layer_factory.hpp:77] Creating layer conv4_2_1
I0218 11:49:51.693148 27028 net.cpp:128] Creating Layer conv4_2_1
I0218 11:49:51.693153 27028 net.cpp:558] conv4_2_1 <- conv4_2_0
I0218 11:49:51.693161 27028 net.cpp:522] conv4_2_1 -> conv4_2_1
I0218 11:49:51.699796 27028 net.cpp:172] Setting up conv4_2_1
I0218 11:49:51.699822 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.699826 27028 net.cpp:194] Memory required for data: 91146880
I0218 11:49:51.699836 27028 layer_factory.hpp:77] Creating layer conv4_2bn1
I0218 11:49:51.699851 27028 net.cpp:128] Creating Layer conv4_2bn1
I0218 11:49:51.699856 27028 net.cpp:558] conv4_2bn1 <- conv4_2_1
I0218 11:49:51.699867 27028 net.cpp:509] conv4_2bn1 -> conv4_2_1 (in-place)
I0218 11:49:51.700160 27028 net.cpp:172] Setting up conv4_2bn1
I0218 11:49:51.700170 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.700176 27028 net.cpp:194] Memory required for data: 91515520
I0218 11:49:51.700198 27028 layer_factory.hpp:77] Creating layer conv4_2_scale1
I0218 11:49:51.700212 27028 net.cpp:128] Creating Layer conv4_2_scale1
I0218 11:49:51.700217 27028 net.cpp:558] conv4_2_scale1 <- conv4_2_1
I0218 11:49:51.700222 27028 net.cpp:509] conv4_2_scale1 -> conv4_2_1 (in-place)
I0218 11:49:51.700300 27028 layer_factory.hpp:77] Creating layer conv4_2_scale1
I0218 11:49:51.700474 27028 net.cpp:172] Setting up conv4_2_scale1
I0218 11:49:51.700484 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.700490 27028 net.cpp:194] Memory required for data: 91884160
I0218 11:49:51.700497 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_2
I0218 11:49:51.700505 27028 net.cpp:128] Creating Layer conv4_Eltwise_2
I0218 11:49:51.700510 27028 net.cpp:558] conv4_Eltwise_2 <- conv4_Eltwise_1_conv4_1ReLU_1_0_split_1
I0218 11:49:51.700515 27028 net.cpp:558] conv4_Eltwise_2 <- conv4_2_1
I0218 11:49:51.700522 27028 net.cpp:522] conv4_Eltwise_2 -> conv4_Eltwise_2
I0218 11:49:51.700554 27028 net.cpp:172] Setting up conv4_Eltwise_2
I0218 11:49:51.700562 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.700565 27028 net.cpp:194] Memory required for data: 92252800
I0218 11:49:51.700569 27028 layer_factory.hpp:77] Creating layer conv4_2ReLU_1
I0218 11:49:51.700575 27028 net.cpp:128] Creating Layer conv4_2ReLU_1
I0218 11:49:51.700580 27028 net.cpp:558] conv4_2ReLU_1 <- conv4_Eltwise_2
I0218 11:49:51.700587 27028 net.cpp:509] conv4_2ReLU_1 -> conv4_Eltwise_2 (in-place)
I0218 11:49:51.701854 27028 net.cpp:172] Setting up conv4_2ReLU_1
I0218 11:49:51.701872 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.701876 27028 net.cpp:194] Memory required for data: 92621440
I0218 11:49:51.701881 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.701889 27028 net.cpp:128] Creating Layer conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.701894 27028 net.cpp:558] conv4_Eltwise_2_conv4_2ReLU_1_0_split <- conv4_Eltwise_2
I0218 11:49:51.701902 27028 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I0218 11:49:51.701910 27028 net.cpp:522] conv4_Eltwise_2_conv4_2ReLU_1_0_split -> conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I0218 11:49:51.701979 27028 net.cpp:172] Setting up conv4_Eltwise_2_conv4_2ReLU_1_0_split
I0218 11:49:51.701987 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.701992 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.701997 27028 net.cpp:194] Memory required for data: 93358720
I0218 11:49:51.702000 27028 layer_factory.hpp:77] Creating layer conv4_3_0
I0218 11:49:51.702013 27028 net.cpp:128] Creating Layer conv4_3_0
I0218 11:49:51.702018 27028 net.cpp:558] conv4_3_0 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_0
I0218 11:49:51.702026 27028 net.cpp:522] conv4_3_0 -> conv4_3_0
I0218 11:49:51.708832 27028 net.cpp:172] Setting up conv4_3_0
I0218 11:49:51.708858 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.708861 27028 net.cpp:194] Memory required for data: 93727360
I0218 11:49:51.708871 27028 layer_factory.hpp:77] Creating layer conv4_3_bn0
I0218 11:49:51.708883 27028 net.cpp:128] Creating Layer conv4_3_bn0
I0218 11:49:51.708887 27028 net.cpp:558] conv4_3_bn0 <- conv4_3_0
I0218 11:49:51.708894 27028 net.cpp:509] conv4_3_bn0 -> conv4_3_0 (in-place)
I0218 11:49:51.709203 27028 net.cpp:172] Setting up conv4_3_bn0
I0218 11:49:51.709215 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.709219 27028 net.cpp:194] Memory required for data: 94096000
I0218 11:49:51.709228 27028 layer_factory.hpp:77] Creating layer conv4_3_scale0
I0218 11:49:51.709237 27028 net.cpp:128] Creating Layer conv4_3_scale0
I0218 11:49:51.709242 27028 net.cpp:558] conv4_3_scale0 <- conv4_3_0
I0218 11:49:51.709249 27028 net.cpp:509] conv4_3_scale0 -> conv4_3_0 (in-place)
I0218 11:49:51.709300 27028 layer_factory.hpp:77] Creating layer conv4_3_scale0
I0218 11:49:51.709465 27028 net.cpp:172] Setting up conv4_3_scale0
I0218 11:49:51.709476 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.709481 27028 net.cpp:194] Memory required for data: 94464640
I0218 11:49:51.709487 27028 layer_factory.hpp:77] Creating layer conv4_3_ReLU0
I0218 11:49:51.709496 27028 net.cpp:128] Creating Layer conv4_3_ReLU0
I0218 11:49:51.709501 27028 net.cpp:558] conv4_3_ReLU0 <- conv4_3_0
I0218 11:49:51.709524 27028 net.cpp:509] conv4_3_ReLU0 -> conv4_3_0 (in-place)
I0218 11:49:51.710639 27028 net.cpp:172] Setting up conv4_3_ReLU0
I0218 11:49:51.710659 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.710664 27028 net.cpp:194] Memory required for data: 94833280
I0218 11:49:51.710669 27028 layer_factory.hpp:77] Creating layer conv4_3_1
I0218 11:49:51.710688 27028 net.cpp:128] Creating Layer conv4_3_1
I0218 11:49:51.710693 27028 net.cpp:558] conv4_3_1 <- conv4_3_0
I0218 11:49:51.710705 27028 net.cpp:522] conv4_3_1 -> conv4_3_1
I0218 11:49:51.717366 27028 net.cpp:172] Setting up conv4_3_1
I0218 11:49:51.717393 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.717397 27028 net.cpp:194] Memory required for data: 95201920
I0218 11:49:51.717406 27028 layer_factory.hpp:77] Creating layer conv4_3bn1
I0218 11:49:51.717420 27028 net.cpp:128] Creating Layer conv4_3bn1
I0218 11:49:51.717425 27028 net.cpp:558] conv4_3bn1 <- conv4_3_1
I0218 11:49:51.717435 27028 net.cpp:509] conv4_3bn1 -> conv4_3_1 (in-place)
I0218 11:49:51.717730 27028 net.cpp:172] Setting up conv4_3bn1
I0218 11:49:51.717741 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.717744 27028 net.cpp:194] Memory required for data: 95570560
I0218 11:49:51.717752 27028 layer_factory.hpp:77] Creating layer conv4_3_scale1
I0218 11:49:51.717759 27028 net.cpp:128] Creating Layer conv4_3_scale1
I0218 11:49:51.717772 27028 net.cpp:558] conv4_3_scale1 <- conv4_3_1
I0218 11:49:51.717777 27028 net.cpp:509] conv4_3_scale1 -> conv4_3_1 (in-place)
I0218 11:49:51.717833 27028 layer_factory.hpp:77] Creating layer conv4_3_scale1
I0218 11:49:51.718014 27028 net.cpp:172] Setting up conv4_3_scale1
I0218 11:49:51.718027 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.718032 27028 net.cpp:194] Memory required for data: 95939200
I0218 11:49:51.718039 27028 layer_factory.hpp:77] Creating layer conv4_Eltwise_3
I0218 11:49:51.718050 27028 net.cpp:128] Creating Layer conv4_Eltwise_3
I0218 11:49:51.718055 27028 net.cpp:558] conv4_Eltwise_3 <- conv4_Eltwise_2_conv4_2ReLU_1_0_split_1
I0218 11:49:51.718061 27028 net.cpp:558] conv4_Eltwise_3 <- conv4_3_1
I0218 11:49:51.718067 27028 net.cpp:522] conv4_Eltwise_3 -> conv4_Eltwise_3
I0218 11:49:51.718099 27028 net.cpp:172] Setting up conv4_Eltwise_3
I0218 11:49:51.718109 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.718113 27028 net.cpp:194] Memory required for data: 96307840
I0218 11:49:51.718117 27028 layer_factory.hpp:77] Creating layer conv4_3ReLU_1
I0218 11:49:51.718123 27028 net.cpp:128] Creating Layer conv4_3ReLU_1
I0218 11:49:51.718132 27028 net.cpp:558] conv4_3ReLU_1 <- conv4_Eltwise_3
I0218 11:49:51.718137 27028 net.cpp:509] conv4_3ReLU_1 -> conv4_Eltwise_3 (in-place)
I0218 11:49:51.719437 27028 net.cpp:172] Setting up conv4_3ReLU_1
I0218 11:49:51.719462 27028 net.cpp:186] Top shape: 10 64 8 18 (92160)
I0218 11:49:51.719466 27028 net.cpp:194] Memory required for data: 96676480
I0218 11:49:51.719471 27028 layer_factory.hpp:77] Creating layer Pooling1
I0218 11:49:51.719481 27028 net.cpp:128] Creating Layer Pooling1
I0218 11:49:51.719486 27028 net.cpp:558] Pooling1 <- conv4_Eltwise_3
I0218 11:49:51.719496 27028 net.cpp:522] Pooling1 -> Pooling1
I0218 11:49:51.721643 27028 net.cpp:172] Setting up Pooling1
I0218 11:49:51.721660 27028 net.cpp:186] Top shape: 10 64 1 1 (640)
I0218 11:49:51.721664 27028 net.cpp:194] Memory required for data: 96679040
I0218 11:49:51.721669 27028 layer_factory.hpp:77] Creating layer Pooling1_Pooling1_0_split
I0218 11:49:51.721679 27028 net.cpp:128] Creating Layer Pooling1_Pooling1_0_split
I0218 11:49:51.721684 27028 net.cpp:558] Pooling1_Pooling1_0_split <- Pooling1
I0218 11:49:51.721690 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_0
I0218 11:49:51.721700 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_1
I0218 11:49:51.721725 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_2
I0218 11:49:51.721734 27028 net.cpp:522] Pooling1_Pooling1_0_split -> Pooling1_Pooling1_0_split_3
I0218 11:49:51.721843 27028 net.cpp:172] Setting up Pooling1_Pooling1_0_split
I0218 11:49:51.721854 27028 net.cpp:186] Top shape: 10 64 1 1 (640)
I0218 11:49:51.721860 27028 net.cpp:186] Top shape: 10 64 1 1 (640)
I0218 11:49:51.721865 27028 net.cpp:186] Top shape: 10 64 1 1 (640)
I0218 11:49:51.721870 27028 net.cpp:186] Top shape: 10 64 1 1 (640)
I0218 11:49:51.721873 27028 net.cpp:194] Memory required for data: 96689280
I0218 11:49:51.721879 27028 layer_factory.hpp:77] Creating layer fc1
I0218 11:49:51.721892 27028 net.cpp:128] Creating Layer fc1
I0218 11:49:51.721897 27028 net.cpp:558] fc1 <- Pooling1_Pooling1_0_split_0
I0218 11:49:51.721904 27028 net.cpp:522] fc1 -> fc1
I0218 11:49:51.725208 27028 net.cpp:172] Setting up fc1
I0218 11:49:51.725232 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.725236 27028 net.cpp:194] Memory required for data: 96691760
I0218 11:49:51.725245 27028 layer_factory.hpp:77] Creating layer fc1_fc1_0_split
I0218 11:49:51.725255 27028 net.cpp:128] Creating Layer fc1_fc1_0_split
I0218 11:49:51.725261 27028 net.cpp:558] fc1_fc1_0_split <- fc1
I0218 11:49:51.725267 27028 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_0
I0218 11:49:51.725282 27028 net.cpp:522] fc1_fc1_0_split -> fc1_fc1_0_split_1
I0218 11:49:51.725337 27028 net.cpp:172] Setting up fc1_fc1_0_split
I0218 11:49:51.725343 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.725348 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.725352 27028 net.cpp:194] Memory required for data: 96696720
I0218 11:49:51.725355 27028 layer_factory.hpp:77] Creating layer Softmax1
I0218 11:49:51.725366 27028 net.cpp:128] Creating Layer Softmax1
I0218 11:49:51.725370 27028 net.cpp:558] Softmax1 <- fc1_fc1_0_split_0
I0218 11:49:51.725375 27028 net.cpp:558] Softmax1 <- label_1_slicers_0_split_0
I0218 11:49:51.725381 27028 net.cpp:522] Softmax1 -> Softmax1
I0218 11:49:51.725390 27028 layer_factory.hpp:77] Creating layer Softmax1
I0218 11:49:51.725782 27028 net.cpp:172] Setting up Softmax1
I0218 11:49:51.725800 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.725805 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.725821 27028 net.cpp:194] Memory required for data: 96696724
I0218 11:49:51.725826 27028 layer_factory.hpp:77] Creating layer prob1
I0218 11:49:51.725834 27028 net.cpp:128] Creating Layer prob1
I0218 11:49:51.725839 27028 net.cpp:558] prob1 <- fc1_fc1_0_split_1
I0218 11:49:51.725845 27028 net.cpp:558] prob1 <- label_1_slicers_0_split_1
I0218 11:49:51.725852 27028 net.cpp:522] prob1 -> prob1
I0218 11:49:51.725862 27028 net.cpp:172] Setting up prob1
I0218 11:49:51.725867 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.725870 27028 net.cpp:194] Memory required for data: 96696728
I0218 11:49:51.725874 27028 layer_factory.hpp:77] Creating layer fc2
I0218 11:49:51.725888 27028 net.cpp:128] Creating Layer fc2
I0218 11:49:51.725893 27028 net.cpp:558] fc2 <- Pooling1_Pooling1_0_split_1
I0218 11:49:51.725900 27028 net.cpp:522] fc2 -> fc2
I0218 11:49:51.727393 27028 net.cpp:172] Setting up fc2
I0218 11:49:51.727418 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.727422 27028 net.cpp:194] Memory required for data: 96699208
I0218 11:49:51.727432 27028 layer_factory.hpp:77] Creating layer fc2_fc2_0_split
I0218 11:49:51.727443 27028 net.cpp:128] Creating Layer fc2_fc2_0_split
I0218 11:49:51.727448 27028 net.cpp:558] fc2_fc2_0_split <- fc2
I0218 11:49:51.727458 27028 net.cpp:522] fc2_fc2_0_split -> fc2_fc2_0_split_0
I0218 11:49:51.727466 27028 net.cpp:522] fc2_fc2_0_split -> fc2_fc2_0_split_1
I0218 11:49:51.727519 27028 net.cpp:172] Setting up fc2_fc2_0_split
I0218 11:49:51.727530 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.727535 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.727537 27028 net.cpp:194] Memory required for data: 96704168
I0218 11:49:51.727541 27028 layer_factory.hpp:77] Creating layer Softmax2
I0218 11:49:51.727550 27028 net.cpp:128] Creating Layer Softmax2
I0218 11:49:51.727557 27028 net.cpp:558] Softmax2 <- fc2_fc2_0_split_0
I0218 11:49:51.727581 27028 net.cpp:558] Softmax2 <- label_2_slicers_1_split_0
I0218 11:49:51.727588 27028 net.cpp:522] Softmax2 -> Softmax2
I0218 11:49:51.727596 27028 layer_factory.hpp:77] Creating layer Softmax2
I0218 11:49:51.728237 27028 net.cpp:172] Setting up Softmax2
I0218 11:49:51.728258 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.728262 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.728271 27028 net.cpp:194] Memory required for data: 96704172
I0218 11:49:51.728276 27028 layer_factory.hpp:77] Creating layer prob2
I0218 11:49:51.728286 27028 net.cpp:128] Creating Layer prob2
I0218 11:49:51.728291 27028 net.cpp:558] prob2 <- fc2_fc2_0_split_1
I0218 11:49:51.728298 27028 net.cpp:558] prob2 <- label_2_slicers_1_split_1
I0218 11:49:51.728307 27028 net.cpp:522] prob2 -> prob2
I0218 11:49:51.728322 27028 net.cpp:172] Setting up prob2
I0218 11:49:51.728327 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.728332 27028 net.cpp:194] Memory required for data: 96704176
I0218 11:49:51.728335 27028 layer_factory.hpp:77] Creating layer fc3
I0218 11:49:51.728349 27028 net.cpp:128] Creating Layer fc3
I0218 11:49:51.728354 27028 net.cpp:558] fc3 <- Pooling1_Pooling1_0_split_2
I0218 11:49:51.728361 27028 net.cpp:522] fc3 -> fc3
I0218 11:49:51.730065 27028 net.cpp:172] Setting up fc3
I0218 11:49:51.730090 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.730094 27028 net.cpp:194] Memory required for data: 96706656
I0218 11:49:51.730103 27028 layer_factory.hpp:77] Creating layer fc3_fc3_0_split
I0218 11:49:51.730114 27028 net.cpp:128] Creating Layer fc3_fc3_0_split
I0218 11:49:51.730120 27028 net.cpp:558] fc3_fc3_0_split <- fc3
I0218 11:49:51.730130 27028 net.cpp:522] fc3_fc3_0_split -> fc3_fc3_0_split_0
I0218 11:49:51.730139 27028 net.cpp:522] fc3_fc3_0_split -> fc3_fc3_0_split_1
I0218 11:49:51.730197 27028 net.cpp:172] Setting up fc3_fc3_0_split
I0218 11:49:51.730206 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.730211 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.730214 27028 net.cpp:194] Memory required for data: 96711616
I0218 11:49:51.730219 27028 layer_factory.hpp:77] Creating layer Softmax3
I0218 11:49:51.730228 27028 net.cpp:128] Creating Layer Softmax3
I0218 11:49:51.730233 27028 net.cpp:558] Softmax3 <- fc3_fc3_0_split_0
I0218 11:49:51.730238 27028 net.cpp:558] Softmax3 <- label_3_slicers_2_split_0
I0218 11:49:51.730244 27028 net.cpp:522] Softmax3 -> Softmax3
I0218 11:49:51.730252 27028 layer_factory.hpp:77] Creating layer Softmax3
I0218 11:49:51.732080 27028 net.cpp:172] Setting up Softmax3
I0218 11:49:51.732100 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.732103 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.732112 27028 net.cpp:194] Memory required for data: 96711620
I0218 11:49:51.732116 27028 layer_factory.hpp:77] Creating layer prob3
I0218 11:49:51.732129 27028 net.cpp:128] Creating Layer prob3
I0218 11:49:51.732134 27028 net.cpp:558] prob3 <- fc3_fc3_0_split_1
I0218 11:49:51.732141 27028 net.cpp:558] prob3 <- label_3_slicers_2_split_1
I0218 11:49:51.732146 27028 net.cpp:522] prob3 -> prob3
I0218 11:49:51.732157 27028 net.cpp:172] Setting up prob3
I0218 11:49:51.732167 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.732172 27028 net.cpp:194] Memory required for data: 96711624
I0218 11:49:51.732175 27028 layer_factory.hpp:77] Creating layer fc4
I0218 11:49:51.732188 27028 net.cpp:128] Creating Layer fc4
I0218 11:49:51.732193 27028 net.cpp:558] fc4 <- Pooling1_Pooling1_0_split_3
I0218 11:49:51.732200 27028 net.cpp:522] fc4 -> fc4
I0218 11:49:51.738700 27028 net.cpp:172] Setting up fc4
I0218 11:49:51.738725 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.738729 27028 net.cpp:194] Memory required for data: 96714104
I0218 11:49:51.738739 27028 layer_factory.hpp:77] Creating layer fc4_fc4_0_split
I0218 11:49:51.738752 27028 net.cpp:128] Creating Layer fc4_fc4_0_split
I0218 11:49:51.738759 27028 net.cpp:558] fc4_fc4_0_split <- fc4
I0218 11:49:51.738768 27028 net.cpp:522] fc4_fc4_0_split -> fc4_fc4_0_split_0
I0218 11:49:51.738778 27028 net.cpp:522] fc4_fc4_0_split -> fc4_fc4_0_split_1
I0218 11:49:51.738857 27028 net.cpp:172] Setting up fc4_fc4_0_split
I0218 11:49:51.738865 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.738870 27028 net.cpp:186] Top shape: 10 62 1 1 (620)
I0218 11:49:51.738873 27028 net.cpp:194] Memory required for data: 96719064
I0218 11:49:51.738878 27028 layer_factory.hpp:77] Creating layer Softmax4
I0218 11:49:51.738886 27028 net.cpp:128] Creating Layer Softmax4
I0218 11:49:51.738891 27028 net.cpp:558] Softmax4 <- fc4_fc4_0_split_0
I0218 11:49:51.738898 27028 net.cpp:558] Softmax4 <- label_4_slicers_3_split_0
I0218 11:49:51.738903 27028 net.cpp:522] Softmax4 -> Softmax4
I0218 11:49:51.738914 27028 layer_factory.hpp:77] Creating layer Softmax4
I0218 11:49:51.740878 27028 net.cpp:172] Setting up Softmax4
I0218 11:49:51.740897 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.740901 27028 net.cpp:189]     with loss weight 1
I0218 11:49:51.740911 27028 net.cpp:194] Memory required for data: 96719068
I0218 11:49:51.740916 27028 layer_factory.hpp:77] Creating layer prob4
I0218 11:49:51.740923 27028 net.cpp:128] Creating Layer prob4
I0218 11:49:51.740928 27028 net.cpp:558] prob4 <- fc4_fc4_0_split_1
I0218 11:49:51.740933 27028 net.cpp:558] prob4 <- label_4_slicers_3_split_1
I0218 11:49:51.740947 27028 net.cpp:522] prob4 -> prob4
I0218 11:49:51.740954 27028 net.cpp:172] Setting up prob4
I0218 11:49:51.740959 27028 net.cpp:186] Top shape: (1)
I0218 11:49:51.740963 27028 net.cpp:194] Memory required for data: 96719072
I0218 11:49:51.740968 27028 net.cpp:303] prob4 does not need backward computation.
I0218 11:49:51.740973 27028 net.cpp:301] Softmax4 needs backward computation.
I0218 11:49:51.740978 27028 net.cpp:301] fc4_fc4_0_split needs backward computation.
I0218 11:49:51.740983 27028 net.cpp:301] fc4 needs backward computation.
I0218 11:49:51.740986 27028 net.cpp:303] prob3 does not need backward computation.
I0218 11:49:51.740990 27028 net.cpp:301] Softmax3 needs backward computation.
I0218 11:49:51.740996 27028 net.cpp:301] fc3_fc3_0_split needs backward computation.
I0218 11:49:51.741000 27028 net.cpp:301] fc3 needs backward computation.
I0218 11:49:51.741004 27028 net.cpp:303] prob2 does not need backward computation.
I0218 11:49:51.741008 27028 net.cpp:301] Softmax2 needs backward computation.
I0218 11:49:51.741014 27028 net.cpp:301] fc2_fc2_0_split needs backward computation.
I0218 11:49:51.741019 27028 net.cpp:301] fc2 needs backward computation.
I0218 11:49:51.741026 27028 net.cpp:303] prob1 does not need backward computation.
I0218 11:49:51.741031 27028 net.cpp:301] Softmax1 needs backward computation.
I0218 11:49:51.741036 27028 net.cpp:301] fc1_fc1_0_split needs backward computation.
I0218 11:49:51.741041 27028 net.cpp:301] fc1 needs backward computation.
I0218 11:49:51.741045 27028 net.cpp:301] Pooling1_Pooling1_0_split needs backward computation.
I0218 11:49:51.741050 27028 net.cpp:301] Pooling1 needs backward computation.
I0218 11:49:51.741055 27028 net.cpp:301] conv4_3ReLU_1 needs backward computation.
I0218 11:49:51.741060 27028 net.cpp:301] conv4_Eltwise_3 needs backward computation.
I0218 11:49:51.741065 27028 net.cpp:301] conv4_3_scale1 needs backward computation.
I0218 11:49:51.741068 27028 net.cpp:301] conv4_3bn1 needs backward computation.
I0218 11:49:51.741073 27028 net.cpp:301] conv4_3_1 needs backward computation.
I0218 11:49:51.741077 27028 net.cpp:301] conv4_3_ReLU0 needs backward computation.
I0218 11:49:51.741081 27028 net.cpp:301] conv4_3_scale0 needs backward computation.
I0218 11:49:51.741086 27028 net.cpp:301] conv4_3_bn0 needs backward computation.
I0218 11:49:51.741092 27028 net.cpp:301] conv4_3_0 needs backward computation.
I0218 11:49:51.741097 27028 net.cpp:301] conv4_Eltwise_2_conv4_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.741102 27028 net.cpp:301] conv4_2ReLU_1 needs backward computation.
I0218 11:49:51.741106 27028 net.cpp:301] conv4_Eltwise_2 needs backward computation.
I0218 11:49:51.741112 27028 net.cpp:301] conv4_2_scale1 needs backward computation.
I0218 11:49:51.741117 27028 net.cpp:301] conv4_2bn1 needs backward computation.
I0218 11:49:51.741137 27028 net.cpp:301] conv4_2_1 needs backward computation.
I0218 11:49:51.741142 27028 net.cpp:301] conv4_2_ReLU0 needs backward computation.
I0218 11:49:51.741145 27028 net.cpp:301] conv4_2_scale0 needs backward computation.
I0218 11:49:51.741149 27028 net.cpp:301] conv4_2_bn0 needs backward computation.
I0218 11:49:51.741154 27028 net.cpp:301] conv4_2_0 needs backward computation.
I0218 11:49:51.741159 27028 net.cpp:301] conv4_Eltwise_1_conv4_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.741164 27028 net.cpp:301] conv4_1ReLU_1 needs backward computation.
I0218 11:49:51.741168 27028 net.cpp:301] conv4_Eltwise_1 needs backward computation.
I0218 11:49:51.741174 27028 net.cpp:301] conv4_1_scale_down needs backward computation.
I0218 11:49:51.741178 27028 net.cpp:301] conv4_1_bn_down needs backward computation.
I0218 11:49:51.741183 27028 net.cpp:301] conv4_1_down needs backward computation.
I0218 11:49:51.741191 27028 net.cpp:301] conv4_1_scale1 needs backward computation.
I0218 11:49:51.741196 27028 net.cpp:301] conv4_1bn1 needs backward computation.
I0218 11:49:51.741200 27028 net.cpp:301] conv4_1_1 needs backward computation.
I0218 11:49:51.741205 27028 net.cpp:301] conv4_1_ReLU0 needs backward computation.
I0218 11:49:51.741209 27028 net.cpp:301] conv4_1_scale0 needs backward computation.
I0218 11:49:51.741214 27028 net.cpp:301] conv4_1_bn0 needs backward computation.
I0218 11:49:51.741219 27028 net.cpp:301] conv4_1_0 needs backward computation.
I0218 11:49:51.741223 27028 net.cpp:301] conv3_Eltwise_3_conv3_3ReLU_1_0_split needs backward computation.
I0218 11:49:51.741228 27028 net.cpp:301] conv3_3ReLU_1 needs backward computation.
I0218 11:49:51.741232 27028 net.cpp:301] conv3_Eltwise_3 needs backward computation.
I0218 11:49:51.741238 27028 net.cpp:301] conv3_3_scale1 needs backward computation.
I0218 11:49:51.741242 27028 net.cpp:301] conv3_3bn1 needs backward computation.
I0218 11:49:51.741246 27028 net.cpp:301] conv3_3_1 needs backward computation.
I0218 11:49:51.741251 27028 net.cpp:301] conv3_3_ReLU0 needs backward computation.
I0218 11:49:51.741255 27028 net.cpp:301] conv3_3_scale0 needs backward computation.
I0218 11:49:51.741259 27028 net.cpp:301] conv3_3_bn0 needs backward computation.
I0218 11:49:51.741264 27028 net.cpp:301] conv3_3_0 needs backward computation.
I0218 11:49:51.741268 27028 net.cpp:301] conv3_Eltwise_2_conv3_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.741273 27028 net.cpp:301] conv3_2ReLU_1 needs backward computation.
I0218 11:49:51.741277 27028 net.cpp:301] conv3_Eltwise_2 needs backward computation.
I0218 11:49:51.741282 27028 net.cpp:301] conv3_2_scale1 needs backward computation.
I0218 11:49:51.741286 27028 net.cpp:301] conv3_2bn1 needs backward computation.
I0218 11:49:51.741291 27028 net.cpp:301] conv3_2_1 needs backward computation.
I0218 11:49:51.741295 27028 net.cpp:301] conv3_2_ReLU0 needs backward computation.
I0218 11:49:51.741299 27028 net.cpp:301] conv3_2_scale0 needs backward computation.
I0218 11:49:51.741304 27028 net.cpp:301] conv3_2_bn0 needs backward computation.
I0218 11:49:51.741308 27028 net.cpp:301] conv3_2_0 needs backward computation.
I0218 11:49:51.741313 27028 net.cpp:301] conv3_Eltwise_1_conv3_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.741318 27028 net.cpp:301] conv3_1ReLU_1 needs backward computation.
I0218 11:49:51.741323 27028 net.cpp:301] conv3_Eltwise_1 needs backward computation.
I0218 11:49:51.741328 27028 net.cpp:301] conv3_1_scale_down needs backward computation.
I0218 11:49:51.741333 27028 net.cpp:301] conv3_1_bn_down needs backward computation.
I0218 11:49:51.741336 27028 net.cpp:301] conv3_1_down needs backward computation.
I0218 11:49:51.741341 27028 net.cpp:301] conv3_1_scale1 needs backward computation.
I0218 11:49:51.741345 27028 net.cpp:301] conv3_1bn1 needs backward computation.
I0218 11:49:51.741349 27028 net.cpp:301] conv3_1_1 needs backward computation.
I0218 11:49:51.741361 27028 net.cpp:301] conv3_1_ReLU0 needs backward computation.
I0218 11:49:51.741374 27028 net.cpp:301] conv3_1_scale0 needs backward computation.
I0218 11:49:51.741379 27028 net.cpp:301] conv3_1_bn0 needs backward computation.
I0218 11:49:51.741384 27028 net.cpp:301] conv3_1_0 needs backward computation.
I0218 11:49:51.741389 27028 net.cpp:301] conv2_Eltwise_3_conv2_3ReLU_1_0_split needs backward computation.
I0218 11:49:51.741396 27028 net.cpp:301] conv2_3ReLU_1 needs backward computation.
I0218 11:49:51.741401 27028 net.cpp:301] conv2_Eltwise_3 needs backward computation.
I0218 11:49:51.741407 27028 net.cpp:301] conv2_3_scale1 needs backward computation.
I0218 11:49:51.741411 27028 net.cpp:301] conv2_3bn1 needs backward computation.
I0218 11:49:51.741415 27028 net.cpp:301] conv2_3_1 needs backward computation.
I0218 11:49:51.741420 27028 net.cpp:301] conv2_3_ReLU0 needs backward computation.
I0218 11:49:51.741425 27028 net.cpp:301] conv2_3_scale0 needs backward computation.
I0218 11:49:51.741430 27028 net.cpp:301] conv2_3_bn0 needs backward computation.
I0218 11:49:51.741433 27028 net.cpp:301] conv2_3_0 needs backward computation.
I0218 11:49:51.741438 27028 net.cpp:301] conv2_Eltwise_2_conv2_2ReLU_1_0_split needs backward computation.
I0218 11:49:51.741443 27028 net.cpp:301] conv2_2ReLU_1 needs backward computation.
I0218 11:49:51.741447 27028 net.cpp:301] conv2_Eltwise_2 needs backward computation.
I0218 11:49:51.741452 27028 net.cpp:301] conv2_2_scale1 needs backward computation.
I0218 11:49:51.741456 27028 net.cpp:301] conv2_2bn1 needs backward computation.
I0218 11:49:51.741461 27028 net.cpp:301] conv2_2_1 needs backward computation.
I0218 11:49:51.741466 27028 net.cpp:301] conv2_2_ReLU0 needs backward computation.
I0218 11:49:51.741470 27028 net.cpp:301] conv2_2_scale0 needs backward computation.
I0218 11:49:51.741474 27028 net.cpp:301] conv2_2_bn0 needs backward computation.
I0218 11:49:51.741479 27028 net.cpp:301] conv2_2_0 needs backward computation.
I0218 11:49:51.741484 27028 net.cpp:301] conv2_Eltwise_1_conv2_1ReLU_1_0_split needs backward computation.
I0218 11:49:51.741489 27028 net.cpp:301] conv2_1ReLU_1 needs backward computation.
I0218 11:49:51.741493 27028 net.cpp:301] conv2_Eltwise_1 needs backward computation.
I0218 11:49:51.741498 27028 net.cpp:301] conv2_1_scale1 needs backward computation.
I0218 11:49:51.741503 27028 net.cpp:301] conv2_1bn1 needs backward computation.
I0218 11:49:51.741508 27028 net.cpp:301] conv2_1_1 needs backward computation.
I0218 11:49:51.741513 27028 net.cpp:301] conv2_1_ReLU0 needs backward computation.
I0218 11:49:51.741516 27028 net.cpp:301] conv2_1_scale0 needs backward computation.
I0218 11:49:51.741521 27028 net.cpp:301] conv2_1_bn0 needs backward computation.
I0218 11:49:51.741525 27028 net.cpp:301] conv2_1_0 needs backward computation.
I0218 11:49:51.741530 27028 net.cpp:301] conv1_conv1/ReLU_0_split needs backward computation.
I0218 11:49:51.741535 27028 net.cpp:301] conv1/ReLU needs backward computation.
I0218 11:49:51.741540 27028 net.cpp:301] conv1/scale needs backward computation.
I0218 11:49:51.741544 27028 net.cpp:301] conv1/bn needs backward computation.
I0218 11:49:51.741549 27028 net.cpp:301] conv1 needs backward computation.
I0218 11:49:51.741554 27028 net.cpp:303] label_4_slicers_3_split does not need backward computation.
I0218 11:49:51.741559 27028 net.cpp:303] label_3_slicers_2_split does not need backward computation.
I0218 11:49:51.741565 27028 net.cpp:303] label_2_slicers_1_split does not need backward computation.
I0218 11:49:51.741570 27028 net.cpp:303] label_1_slicers_0_split does not need backward computation.
I0218 11:49:51.741578 27028 net.cpp:303] slicers does not need backward computation.
I0218 11:49:51.741583 27028 net.cpp:303] data1 does not need backward computation.
I0218 11:49:51.741588 27028 net.cpp:348] This network produces output Softmax1
I0218 11:49:51.741592 27028 net.cpp:348] This network produces output Softmax2
I0218 11:49:51.741596 27028 net.cpp:348] This network produces output Softmax3
I0218 11:49:51.741601 27028 net.cpp:348] This network produces output Softmax4
I0218 11:49:51.741612 27028 net.cpp:348] This network produces output prob1
I0218 11:49:51.741617 27028 net.cpp:348] This network produces output prob2
I0218 11:49:51.741621 27028 net.cpp:348] This network produces output prob3
I0218 11:49:51.741626 27028 net.cpp:348] This network produces output prob4
I0218 11:49:51.741708 27028 net.cpp:363] Network initialization done.
I0218 11:49:51.742259 27028 solver.cpp:110] Solver scaffolding done.
I0218 11:49:51.750660 27028 caffe.cpp:313] Starting Optimization
I0218 11:49:51.750681 27028 solver.cpp:425] Solving ResNet-20
I0218 11:49:51.750685 27028 solver.cpp:427] Learning Rate Policy: multistep
I0218 11:49:51.753732 27028 solver.cpp:514] Iteration 0, Testing net (#0)
I0218 11:49:54.424898 27028 solver.cpp:580]     Test net output #0: Softmax1 = 4.12714 (* 1 = 4.12714 loss)
I0218 11:49:54.424942 27028 solver.cpp:580]     Test net output #1: Softmax2 = 4.12714 (* 1 = 4.12714 loss)
I0218 11:49:54.424949 27028 solver.cpp:580]     Test net output #2: Softmax3 = 4.12714 (* 1 = 4.12714 loss)
I0218 11:49:54.424958 27028 solver.cpp:580]     Test net output #3: Softmax4 = 4.12714 (* 1 = 4.12714 loss)
I0218 11:49:54.424964 27028 solver.cpp:580]     Test net output #4: prob1 = 0
I0218 11:49:54.424969 27028 solver.cpp:580]     Test net output #5: prob2 = 0
I0218 11:49:54.424975 27028 solver.cpp:580]     Test net output #6: prob3 = 0
I0218 11:49:54.424980 27028 solver.cpp:580]     Test net output #7: prob4 = 0
I0218 11:49:54.825927 27028 solver.cpp:357] Iteration 0 (0 iter/s, 3.07478s/100 iters), loss = 19.7928
I0218 11:49:54.826022 27028 solver.cpp:376]     Train net output #0: Softmax1 = 5.06397 (* 1 = 5.06397 loss)
I0218 11:49:54.826032 27028 solver.cpp:376]     Train net output #1: Softmax2 = 5.0573 (* 1 = 5.0573 loss)
I0218 11:49:54.826041 27028 solver.cpp:376]     Train net output #2: Softmax3 = 4.87136 (* 1 = 4.87136 loss)
I0218 11:49:54.826048 27028 solver.cpp:376]     Train net output #3: Softmax4 = 4.80013 (* 1 = 4.80013 loss)
I0218 11:49:54.826073 27028 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0218 11:50:30.594959 27028 solver.cpp:357] Iteration 100 (2.79566 iter/s, 35.7697s/100 iters), loss = 16.3288
I0218 11:50:30.595129 27028 solver.cpp:376]     Train net output #0: Softmax1 = 4.0537 (* 1 = 4.0537 loss)
I0218 11:50:30.595139 27028 solver.cpp:376]     Train net output #1: Softmax2 = 4.08373 (* 1 = 4.08373 loss)
I0218 11:50:30.595146 27028 solver.cpp:376]     Train net output #2: Softmax3 = 4.10547 (* 1 = 4.10547 loss)
I0218 11:50:30.595155 27028 solver.cpp:376]     Train net output #3: Softmax4 = 4.08594 (* 1 = 4.08594 loss)
I0218 11:50:30.595161 27028 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0218 11:51:06.304971 27028 solver.cpp:357] Iteration 200 (2.80015 iter/s, 35.7123s/100 iters), loss = 16.1704
I0218 11:51:06.305107 27028 solver.cpp:376]     Train net output #0: Softmax1 = 3.99858 (* 1 = 3.99858 loss)
I0218 11:51:06.305117 27028 solver.cpp:376]     Train net output #1: Softmax2 = 4.06962 (* 1 = 4.06962 loss)
I0218 11:51:06.305126 27028 solver.cpp:376]     Train net output #2: Softmax3 = 4.07827 (* 1 = 4.07827 loss)
I0218 11:51:06.305133 27028 solver.cpp:376]     Train net output #3: Softmax4 = 4.02393 (* 1 = 4.02393 loss)
I0218 11:51:06.305140 27028 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0218 11:51:42.023849 27028 solver.cpp:357] Iteration 300 (2.79947 iter/s, 35.721s/100 iters), loss = 15.437
I0218 11:51:42.023963 27028 solver.cpp:376]     Train net output #0: Softmax1 = 3.72758 (* 1 = 3.72758 loss)
I0218 11:51:42.023973 27028 solver.cpp:376]     Train net output #1: Softmax2 = 3.92212 (* 1 = 3.92212 loss)
I0218 11:51:42.023983 27028 solver.cpp:376]     Train net output #2: Softmax3 = 3.88783 (* 1 = 3.88783 loss)
I0218 11:51:42.023989 27028 solver.cpp:376]     Train net output #3: Softmax4 = 3.8995 (* 1 = 3.8995 loss)
I0218 11:51:42.023998 27028 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0218 11:52:10.797531 27028 solver.cpp:357] Iteration 400 (3.4752 iter/s, 28.7753s/100 iters), loss = 12.4943
I0218 11:52:10.797606 27028 solver.cpp:376]     Train net output #0: Softmax1 = 2.59235 (* 1 = 2.59235 loss)
I0218 11:52:10.797616 27028 solver.cpp:376]     Train net output #1: Softmax2 = 3.44893 (* 1 = 3.44893 loss)
I0218 11:52:10.797623 27028 solver.cpp:376]     Train net output #2: Softmax3 = 3.48141 (* 1 = 3.48141 loss)
I0218 11:52:10.797631 27028 solver.cpp:376]     Train net output #3: Softmax4 = 2.97157 (* 1 = 2.97157 loss)
I0218 11:52:10.797638 27028 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0218 11:52:46.497228 27028 solver.cpp:357] Iteration 500 (2.80099 iter/s, 35.7017s/100 iters), loss = 8.21738
I0218 11:52:46.497418 27028 solver.cpp:376]     Train net output #0: Softmax1 = 1.3509 (* 1 = 1.3509 loss)
I0218 11:52:46.497428 27028 solver.cpp:376]     Train net output #1: Softmax2 = 2.80248 (* 1 = 2.80248 loss)
I0218 11:52:46.497437 27028 solver.cpp:376]     Train net output #2: Softmax3 = 2.60134 (* 1 = 2.60134 loss)
I0218 11:52:46.497480 27028 solver.cpp:376]     Train net output #3: Softmax4 = 1.46266 (* 1 = 1.46266 loss)
I0218 11:52:46.497488 27028 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0218 11:53:22.207691 27028 solver.cpp:357] Iteration 600 (2.80017 iter/s, 35.7121s/100 iters), loss = 4.87002
I0218 11:53:22.207813 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.847908 (* 1 = 0.847908 loss)
I0218 11:53:22.207823 27028 solver.cpp:376]     Train net output #1: Softmax2 = 1.41688 (* 1 = 1.41688 loss)
I0218 11:53:22.207830 27028 solver.cpp:376]     Train net output #2: Softmax3 = 1.57276 (* 1 = 1.57276 loss)
I0218 11:53:22.207839 27028 solver.cpp:376]     Train net output #3: Softmax4 = 1.03247 (* 1 = 1.03247 loss)
I0218 11:53:22.207845 27028 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0218 11:53:57.966754 27028 solver.cpp:357] Iteration 700 (2.79636 iter/s, 35.7607s/100 iters), loss = 2.93347
I0218 11:53:57.966923 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.598715 (* 1 = 0.598715 loss)
I0218 11:53:57.966934 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.886741 (* 1 = 0.886741 loss)
I0218 11:53:57.966943 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.893167 (* 1 = 0.893167 loss)
I0218 11:53:57.966950 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.554852 (* 1 = 0.554852 loss)
I0218 11:53:57.966961 27028 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0218 11:54:33.691432 27028 solver.cpp:357] Iteration 800 (2.79921 iter/s, 35.7243s/100 iters), loss = 2.56513
I0218 11:54:33.691599 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.522066 (* 1 = 0.522066 loss)
I0218 11:54:33.691609 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.70656 (* 1 = 0.70656 loss)
I0218 11:54:33.691618 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.743094 (* 1 = 0.743094 loss)
I0218 11:54:33.691627 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.593406 (* 1 = 0.593406 loss)
I0218 11:54:33.691634 27028 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0218 11:55:03.051369 27028 solver.cpp:357] Iteration 900 (3.40609 iter/s, 29.3592s/100 iters), loss = 2.20035
I0218 11:55:03.051443 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.343625 (* 1 = 0.343625 loss)
I0218 11:55:03.051453 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.583972 (* 1 = 0.583972 loss)
I0218 11:55:03.051461 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.763077 (* 1 = 0.763077 loss)
I0218 11:55:03.051470 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.509675 (* 1 = 0.509675 loss)
I0218 11:55:03.051477 27028 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0218 11:55:37.767479 27028 solver.cpp:514] Iteration 1000, Testing net (#0)
I0218 11:55:40.125341 27028 solver.cpp:580]     Test net output #0: Softmax1 = 21.5222 (* 1 = 21.5222 loss)
I0218 11:55:40.125391 27028 solver.cpp:580]     Test net output #1: Softmax2 = 25.883 (* 1 = 25.883 loss)
I0218 11:55:40.125401 27028 solver.cpp:580]     Test net output #2: Softmax3 = 31.4907 (* 1 = 31.4907 loss)
I0218 11:55:40.125409 27028 solver.cpp:580]     Test net output #3: Softmax4 = 39.4732 (* 1 = 39.4732 loss)
I0218 11:55:40.125416 27028 solver.cpp:580]     Test net output #4: prob1 = 0.019
I0218 11:55:40.125423 27028 solver.cpp:580]     Test net output #5: prob2 = 0.024
I0218 11:55:40.125429 27028 solver.cpp:580]     Test net output #6: prob3 = 0.016
I0218 11:55:40.125434 27028 solver.cpp:580]     Test net output #7: prob4 = 0.015
I0218 11:55:40.461251 27028 solver.cpp:357] Iteration 1000 (2.67296 iter/s, 37.4116s/100 iters), loss = 1.74578
I0218 11:55:40.461307 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.409117 (* 1 = 0.409117 loss)
I0218 11:55:40.461315 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.572849 (* 1 = 0.572849 loss)
I0218 11:55:40.461324 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.430687 (* 1 = 0.430687 loss)
I0218 11:55:40.461333 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.333124 (* 1 = 0.333124 loss)
I0218 11:55:40.461340 27028 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0218 11:56:16.255834 27028 solver.cpp:357] Iteration 1100 (2.79359 iter/s, 35.7963s/100 iters), loss = 1.84851
I0218 11:56:16.256063 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.404014 (* 1 = 0.404014 loss)
I0218 11:56:16.256074 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.525638 (* 1 = 0.525638 loss)
I0218 11:56:16.256083 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.586125 (* 1 = 0.586125 loss)
I0218 11:56:16.256091 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.332737 (* 1 = 0.332737 loss)
I0218 11:56:16.256103 27028 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0218 11:56:51.994698 27028 solver.cpp:357] Iteration 1200 (2.79811 iter/s, 35.7384s/100 iters), loss = 1.26381
I0218 11:56:51.994853 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.323161 (* 1 = 0.323161 loss)
I0218 11:56:51.994863 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.277307 (* 1 = 0.277307 loss)
I0218 11:56:51.994873 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.392845 (* 1 = 0.392845 loss)
I0218 11:56:51.994879 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.2705 (* 1 = 0.2705 loss)
I0218 11:56:51.994886 27028 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0218 11:57:27.598938 27028 solver.cpp:357] Iteration 1300 (2.80853 iter/s, 35.6058s/100 iters), loss = 1.07699
I0218 11:57:27.599097 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.299101 (* 1 = 0.299101 loss)
I0218 11:57:27.599107 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.300264 (* 1 = 0.300264 loss)
I0218 11:57:27.599114 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.277368 (* 1 = 0.277368 loss)
I0218 11:57:27.599123 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.200258 (* 1 = 0.200258 loss)
I0218 11:57:27.599133 27028 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0218 11:57:57.417320 27028 solver.cpp:357] Iteration 1400 (3.35349 iter/s, 29.8196s/100 iters), loss = 1.42594
I0218 11:57:57.417392 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.379012 (* 1 = 0.379012 loss)
I0218 11:57:57.417402 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.337897 (* 1 = 0.337897 loss)
I0218 11:57:57.417410 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.416936 (* 1 = 0.416936 loss)
I0218 11:57:57.417418 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.292096 (* 1 = 0.292096 loss)
I0218 11:57:57.417426 27028 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0218 11:58:32.100523 27028 solver.cpp:357] Iteration 1500 (2.88311 iter/s, 34.6848s/100 iters), loss = 0.905076
I0218 11:58:32.100630 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.175421 (* 1 = 0.175421 loss)
I0218 11:58:32.100641 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.282722 (* 1 = 0.282722 loss)
I0218 11:58:32.100648 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.219606 (* 1 = 0.219606 loss)
I0218 11:58:32.100656 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.227327 (* 1 = 0.227327 loss)
I0218 11:58:32.100664 27028 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0218 11:59:07.812165 27028 solver.cpp:357] Iteration 1600 (2.80008 iter/s, 35.7132s/100 iters), loss = 1.0979
I0218 11:59:07.812374 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.225727 (* 1 = 0.225727 loss)
I0218 11:59:07.812453 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.337045 (* 1 = 0.337045 loss)
I0218 11:59:07.812463 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.27835 (* 1 = 0.27835 loss)
I0218 11:59:07.812470 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.256774 (* 1 = 0.256774 loss)
I0218 11:59:07.812479 27028 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0218 11:59:43.527914 27028 solver.cpp:357] Iteration 1700 (2.79977 iter/s, 35.7172s/100 iters), loss = 1.07212
I0218 11:59:43.528044 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.412664 (* 1 = 0.412664 loss)
I0218 11:59:43.528055 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.246883 (* 1 = 0.246883 loss)
I0218 11:59:43.528064 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.288829 (* 1 = 0.288829 loss)
I0218 11:59:43.528072 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.123746 (* 1 = 0.123746 loss)
I0218 11:59:43.528080 27028 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0218 12:00:19.328707 27028 solver.cpp:357] Iteration 1800 (2.79327 iter/s, 35.8003s/100 iters), loss = 0.869962
I0218 12:00:19.328828 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.179516 (* 1 = 0.179516 loss)
I0218 12:00:19.328840 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.198133 (* 1 = 0.198133 loss)
I0218 12:00:19.328848 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.246191 (* 1 = 0.246191 loss)
I0218 12:00:19.328856 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.246122 (* 1 = 0.246122 loss)
I0218 12:00:19.328863 27028 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0218 12:00:50.280447 27028 solver.cpp:357] Iteration 1900 (3.23091 iter/s, 30.951s/100 iters), loss = 0.883461
I0218 12:00:50.280601 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.146769 (* 1 = 0.146769 loss)
I0218 12:00:50.280611 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.324466 (* 1 = 0.324466 loss)
I0218 12:00:50.280619 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.232417 (* 1 = 0.232417 loss)
I0218 12:00:50.280627 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.179809 (* 1 = 0.179809 loss)
I0218 12:00:50.280637 27028 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0218 12:01:23.500578 27028 solver.cpp:514] Iteration 2000, Testing net (#0)
I0218 12:01:25.973028 27028 solver.cpp:580]     Test net output #0: Softmax1 = 3.40508 (* 1 = 3.40508 loss)
I0218 12:01:25.973073 27028 solver.cpp:580]     Test net output #1: Softmax2 = 4.23847 (* 1 = 4.23847 loss)
I0218 12:01:25.973081 27028 solver.cpp:580]     Test net output #2: Softmax3 = 4.92495 (* 1 = 4.92495 loss)
I0218 12:01:25.973090 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.69322 (* 1 = 3.69322 loss)
I0218 12:01:25.973096 27028 solver.cpp:580]     Test net output #4: prob1 = 0.12
I0218 12:01:25.973104 27028 solver.cpp:580]     Test net output #5: prob2 = 0.102
I0218 12:01:25.973109 27028 solver.cpp:580]     Test net output #6: prob3 = 0.069
I0218 12:01:25.973114 27028 solver.cpp:580]     Test net output #7: prob4 = 0.115
I0218 12:01:26.313526 27028 solver.cpp:357] Iteration 2000 (2.77511 iter/s, 36.0346s/100 iters), loss = 1.04568
I0218 12:01:26.313585 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.233426 (* 1 = 0.233426 loss)
I0218 12:01:26.313596 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.390213 (* 1 = 0.390213 loss)
I0218 12:01:26.313603 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.189215 (* 1 = 0.189215 loss)
I0218 12:01:26.313611 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.232824 (* 1 = 0.232824 loss)
I0218 12:01:26.313618 27028 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0218 12:02:02.027242 27028 solver.cpp:357] Iteration 2100 (2.79992 iter/s, 35.7153s/100 iters), loss = 0.831208
I0218 12:02:02.027405 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.141676 (* 1 = 0.141676 loss)
I0218 12:02:02.027416 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.252567 (* 1 = 0.252567 loss)
I0218 12:02:02.027426 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.218387 (* 1 = 0.218387 loss)
I0218 12:02:02.027433 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.218578 (* 1 = 0.218578 loss)
I0218 12:02:02.027441 27028 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0218 12:02:37.743527 27028 solver.cpp:357] Iteration 2200 (2.79972 iter/s, 35.7178s/100 iters), loss = 0.715523
I0218 12:02:37.743690 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.193376 (* 1 = 0.193376 loss)
I0218 12:02:37.743700 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.178075 (* 1 = 0.178075 loss)
I0218 12:02:37.743710 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.162367 (* 1 = 0.162367 loss)
I0218 12:02:37.743716 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.181705 (* 1 = 0.181705 loss)
I0218 12:02:37.743724 27028 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0218 12:03:13.480643 27028 solver.cpp:357] Iteration 2300 (2.79809 iter/s, 35.7386s/100 iters), loss = 0.695032
I0218 12:03:13.480803 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.258838 (* 1 = 0.258838 loss)
I0218 12:03:13.480813 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137654 (* 1 = 0.137654 loss)
I0218 12:03:13.480820 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.140582 (* 1 = 0.140582 loss)
I0218 12:03:13.480829 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.157958 (* 1 = 0.157958 loss)
I0218 12:03:13.480839 27028 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0218 12:03:44.706817 27028 solver.cpp:357] Iteration 2400 (3.20231 iter/s, 31.2275s/100 iters), loss = 0.849591
I0218 12:03:44.706960 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.204786 (* 1 = 0.204786 loss)
I0218 12:03:44.706971 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.209507 (* 1 = 0.209507 loss)
I0218 12:03:44.706979 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.161964 (* 1 = 0.161964 loss)
I0218 12:03:44.706987 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.273334 (* 1 = 0.273334 loss)
I0218 12:03:44.707000 27028 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0218 12:04:18.007453 27028 solver.cpp:357] Iteration 2500 (3.00282 iter/s, 33.3021s/100 iters), loss = 1.05165
I0218 12:04:18.007616 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.232431 (* 1 = 0.232431 loss)
I0218 12:04:18.007627 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.326103 (* 1 = 0.326103 loss)
I0218 12:04:18.007635 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.237534 (* 1 = 0.237534 loss)
I0218 12:04:18.007643 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.25558 (* 1 = 0.25558 loss)
I0218 12:04:18.007650 27028 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0218 12:04:53.726058 27028 solver.cpp:357] Iteration 2600 (2.79954 iter/s, 35.7201s/100 iters), loss = 0.736726
I0218 12:04:53.726223 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.104354 (* 1 = 0.104354 loss)
I0218 12:04:53.726233 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.261862 (* 1 = 0.261862 loss)
I0218 12:04:53.726243 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.156288 (* 1 = 0.156288 loss)
I0218 12:04:53.726250 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.214222 (* 1 = 0.214222 loss)
I0218 12:04:53.726258 27028 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0218 12:05:29.559437 27028 solver.cpp:357] Iteration 2700 (2.79058 iter/s, 35.8349s/100 iters), loss = 1.03439
I0218 12:05:29.559540 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.183804 (* 1 = 0.183804 loss)
I0218 12:05:29.559552 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.296754 (* 1 = 0.296754 loss)
I0218 12:05:29.559561 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.34424 (* 1 = 0.34424 loss)
I0218 12:05:29.559568 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.209593 (* 1 = 0.209593 loss)
I0218 12:05:29.559576 27028 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0218 12:06:05.279532 27028 solver.cpp:357] Iteration 2800 (2.79942 iter/s, 35.7217s/100 iters), loss = 0.622054
I0218 12:06:05.279703 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.165266 (* 1 = 0.165266 loss)
I0218 12:06:05.279714 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.101634 (* 1 = 0.101634 loss)
I0218 12:06:05.279723 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.194771 (* 1 = 0.194771 loss)
I0218 12:06:05.279731 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.160383 (* 1 = 0.160383 loss)
I0218 12:06:05.279738 27028 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0218 12:06:37.610417 27028 solver.cpp:357] Iteration 2900 (3.09289 iter/s, 32.3322s/100 iters), loss = 0.66606
I0218 12:06:37.610585 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.13344 (* 1 = 0.13344 loss)
I0218 12:06:37.610596 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.161339 (* 1 = 0.161339 loss)
I0218 12:06:37.610605 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.199662 (* 1 = 0.199662 loss)
I0218 12:06:37.610613 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.171619 (* 1 = 0.171619 loss)
I0218 12:06:37.610620 27028 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0218 12:07:09.197170 27028 solver.cpp:514] Iteration 3000, Testing net (#0)
I0218 12:07:11.644536 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.00758 (* 1 = 2.00758 loss)
I0218 12:07:11.644584 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.87294 (* 1 = 1.87294 loss)
I0218 12:07:11.644593 27028 solver.cpp:580]     Test net output #2: Softmax3 = 2.21369 (* 1 = 2.21369 loss)
I0218 12:07:11.644604 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.6328 (* 1 = 1.6328 loss)
I0218 12:07:11.644611 27028 solver.cpp:580]     Test net output #4: prob1 = 0.436
I0218 12:07:11.644618 27028 solver.cpp:580]     Test net output #5: prob2 = 0.554
I0218 12:07:11.644624 27028 solver.cpp:580]     Test net output #6: prob3 = 0.381
I0218 12:07:11.644629 27028 solver.cpp:580]     Test net output #7: prob4 = 0.576
I0218 12:07:11.979583 27028 solver.cpp:357] Iteration 3000 (2.90946 iter/s, 34.3706s/100 iters), loss = 1.06889
I0218 12:07:11.979643 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.176281 (* 1 = 0.176281 loss)
I0218 12:07:11.979653 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.283426 (* 1 = 0.283426 loss)
I0218 12:07:11.979661 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.383966 (* 1 = 0.383966 loss)
I0218 12:07:11.979670 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.225219 (* 1 = 0.225219 loss)
I0218 12:07:11.979677 27028 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0218 12:07:47.676471 27028 solver.cpp:357] Iteration 3100 (2.80124 iter/s, 35.6985s/100 iters), loss = 0.84943
I0218 12:07:47.676635 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.175074 (* 1 = 0.175074 loss)
I0218 12:07:47.676645 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.236424 (* 1 = 0.236424 loss)
I0218 12:07:47.676654 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.251531 (* 1 = 0.251531 loss)
I0218 12:07:47.676662 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.186401 (* 1 = 0.186401 loss)
I0218 12:07:47.676669 27028 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0218 12:08:23.387420 27028 solver.cpp:357] Iteration 3200 (2.80014 iter/s, 35.7125s/100 iters), loss = 0.906943
I0218 12:08:23.387534 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.180959 (* 1 = 0.180959 loss)
I0218 12:08:23.387544 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.309746 (* 1 = 0.309746 loss)
I0218 12:08:23.387553 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.26385 (* 1 = 0.26385 loss)
I0218 12:08:23.387562 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.152388 (* 1 = 0.152388 loss)
I0218 12:08:23.387569 27028 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0218 12:08:59.096216 27028 solver.cpp:357] Iteration 3300 (2.80031 iter/s, 35.7103s/100 iters), loss = 0.548451
I0218 12:08:59.096472 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.170496 (* 1 = 0.170496 loss)
I0218 12:08:59.096485 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.117365 (* 1 = 0.117365 loss)
I0218 12:08:59.096493 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.172621 (* 1 = 0.172621 loss)
I0218 12:08:59.096503 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0879691 (* 1 = 0.0879691 loss)
I0218 12:08:59.096509 27028 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0218 12:09:31.852490 27028 solver.cpp:357] Iteration 3400 (3.05273 iter/s, 32.7575s/100 iters), loss = 0.898778
I0218 12:09:31.852607 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0783139 (* 1 = 0.0783139 loss)
I0218 12:09:31.852618 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.176608 (* 1 = 0.176608 loss)
I0218 12:09:31.852627 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.382726 (* 1 = 0.382726 loss)
I0218 12:09:31.852634 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.261131 (* 1 = 0.261131 loss)
I0218 12:09:31.852643 27028 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0218 12:10:03.544520 27028 solver.cpp:357] Iteration 3500 (3.15523 iter/s, 31.6934s/100 iters), loss = 0.598896
I0218 12:10:03.544632 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.150585 (* 1 = 0.150585 loss)
I0218 12:10:03.544642 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.129958 (* 1 = 0.129958 loss)
I0218 12:10:03.544651 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.165231 (* 1 = 0.165231 loss)
I0218 12:10:03.544658 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.153122 (* 1 = 0.153122 loss)
I0218 12:10:03.544665 27028 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0218 12:10:39.246346 27028 solver.cpp:357] Iteration 3600 (2.80101 iter/s, 35.7014s/100 iters), loss = 0.679379
I0218 12:10:39.246459 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.154822 (* 1 = 0.154822 loss)
I0218 12:10:39.246470 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.180776 (* 1 = 0.180776 loss)
I0218 12:10:39.246479 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.174531 (* 1 = 0.174531 loss)
I0218 12:10:39.246487 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.169251 (* 1 = 0.169251 loss)
I0218 12:10:39.246495 27028 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0218 12:11:14.944645 27028 solver.cpp:357] Iteration 3700 (2.80129 iter/s, 35.6979s/100 iters), loss = 0.738207
I0218 12:11:14.944753 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.121003 (* 1 = 0.121003 loss)
I0218 12:11:14.944764 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.166647 (* 1 = 0.166647 loss)
I0218 12:11:14.944773 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.352871 (* 1 = 0.352871 loss)
I0218 12:11:14.944782 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0976864 (* 1 = 0.0976864 loss)
I0218 12:11:14.944788 27028 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0218 12:11:50.555253 27028 solver.cpp:357] Iteration 3800 (2.80818 iter/s, 35.6103s/100 iters), loss = 0.551111
I0218 12:11:50.555378 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.114584 (* 1 = 0.114584 loss)
I0218 12:11:50.555388 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.128249 (* 1 = 0.128249 loss)
I0218 12:11:50.555398 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.158775 (* 1 = 0.158775 loss)
I0218 12:11:50.555405 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.149503 (* 1 = 0.149503 loss)
I0218 12:11:50.555413 27028 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0218 12:12:24.575317 27028 solver.cpp:357] Iteration 3900 (2.93963 iter/s, 34.0178s/100 iters), loss = 0.738471
I0218 12:12:24.575546 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.166888 (* 1 = 0.166888 loss)
I0218 12:12:24.575557 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.133885 (* 1 = 0.133885 loss)
I0218 12:12:24.575567 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.255474 (* 1 = 0.255474 loss)
I0218 12:12:24.575574 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.182225 (* 1 = 0.182225 loss)
I0218 12:12:24.575582 27028 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0218 12:12:54.501142 27028 solver.cpp:514] Iteration 4000, Testing net (#0)
I0218 12:12:56.971344 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.78888 (* 1 = 1.78888 loss)
I0218 12:12:56.971436 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.5159 (* 1 = 1.5159 loss)
I0218 12:12:56.971444 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.74826 (* 1 = 1.74826 loss)
I0218 12:12:56.971457 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.38027 (* 1 = 1.38027 loss)
I0218 12:12:56.971462 27028 solver.cpp:580]     Test net output #4: prob1 = 0.544
I0218 12:12:56.971469 27028 solver.cpp:580]     Test net output #5: prob2 = 0.627
I0218 12:12:56.971475 27028 solver.cpp:580]     Test net output #6: prob3 = 0.58
I0218 12:12:56.971480 27028 solver.cpp:580]     Test net output #7: prob4 = 0.657
I0218 12:12:57.307503 27028 solver.cpp:357] Iteration 4000 (3.05512 iter/s, 32.732s/100 iters), loss = 0.609736
I0218 12:12:57.307559 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.140627 (* 1 = 0.140627 loss)
I0218 12:12:57.307572 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.159759 (* 1 = 0.159759 loss)
I0218 12:12:57.307580 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.178897 (* 1 = 0.178897 loss)
I0218 12:12:57.307588 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.130453 (* 1 = 0.130453 loss)
I0218 12:12:57.307596 27028 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0218 12:13:33.024861 27028 solver.cpp:357] Iteration 4100 (2.79976 iter/s, 35.7174s/100 iters), loss = 0.834666
I0218 12:13:33.025023 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.212949 (* 1 = 0.212949 loss)
I0218 12:13:33.025034 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.204026 (* 1 = 0.204026 loss)
I0218 12:13:33.025043 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.214876 (* 1 = 0.214876 loss)
I0218 12:13:33.025050 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.202815 (* 1 = 0.202815 loss)
I0218 12:13:33.025058 27028 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0218 12:14:08.755729 27028 solver.cpp:357] Iteration 4200 (2.7987 iter/s, 35.7309s/100 iters), loss = 0.566068
I0218 12:14:08.755893 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.133977 (* 1 = 0.133977 loss)
I0218 12:14:08.755903 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.104249 (* 1 = 0.104249 loss)
I0218 12:14:08.755913 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.19355 (* 1 = 0.19355 loss)
I0218 12:14:08.755920 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.134292 (* 1 = 0.134292 loss)
I0218 12:14:08.755928 27028 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0218 12:14:44.467046 27028 solver.cpp:357] Iteration 4300 (2.80022 iter/s, 35.7114s/100 iters), loss = 0.633736
I0218 12:14:44.467152 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.102054 (* 1 = 0.102054 loss)
I0218 12:14:44.467162 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.202247 (* 1 = 0.202247 loss)
I0218 12:14:44.467170 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.207618 (* 1 = 0.207618 loss)
I0218 12:14:44.467178 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.121816 (* 1 = 0.121816 loss)
I0218 12:14:44.467185 27028 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0218 12:15:18.900184 27028 solver.cpp:357] Iteration 4400 (2.90416 iter/s, 34.4334s/100 iters), loss = 0.73372
I0218 12:15:18.900295 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.187882 (* 1 = 0.187882 loss)
I0218 12:15:18.900305 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.123052 (* 1 = 0.123052 loss)
I0218 12:15:18.900315 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.286409 (* 1 = 0.286409 loss)
I0218 12:15:18.900322 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136377 (* 1 = 0.136377 loss)
I0218 12:15:18.900329 27028 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0218 12:15:48.848367 27028 solver.cpp:357] Iteration 4500 (3.33908 iter/s, 29.9484s/100 iters), loss = 0.71875
I0218 12:15:48.848444 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.172693 (* 1 = 0.172693 loss)
I0218 12:15:48.848453 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.166423 (* 1 = 0.166423 loss)
I0218 12:15:48.848462 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.189203 (* 1 = 0.189203 loss)
I0218 12:15:48.848470 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.19043 (* 1 = 0.19043 loss)
I0218 12:15:48.848477 27028 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0218 12:16:24.554705 27028 solver.cpp:357] Iteration 4600 (2.80059 iter/s, 35.7067s/100 iters), loss = 0.477164
I0218 12:16:24.554883 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.109951 (* 1 = 0.109951 loss)
I0218 12:16:24.554893 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.125563 (* 1 = 0.125563 loss)
I0218 12:16:24.554903 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0869301 (* 1 = 0.0869301 loss)
I0218 12:16:24.554911 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.15472 (* 1 = 0.15472 loss)
I0218 12:16:24.554960 27028 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0218 12:17:00.273655 27028 solver.cpp:357] Iteration 4700 (2.79961 iter/s, 35.7193s/100 iters), loss = 0.721529
I0218 12:17:00.273772 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143403 (* 1 = 0.143403 loss)
I0218 12:17:00.273783 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.132805 (* 1 = 0.132805 loss)
I0218 12:17:00.273792 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.235676 (* 1 = 0.235676 loss)
I0218 12:17:00.273799 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.209644 (* 1 = 0.209644 loss)
I0218 12:17:00.273807 27028 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0218 12:17:35.984122 27028 solver.cpp:357] Iteration 4800 (2.80026 iter/s, 35.7109s/100 iters), loss = 0.551774
I0218 12:17:35.984282 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.155492 (* 1 = 0.155492 loss)
I0218 12:17:35.984292 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.165252 (* 1 = 0.165252 loss)
I0218 12:17:35.984299 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.137956 (* 1 = 0.137956 loss)
I0218 12:17:35.984308 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0930741 (* 1 = 0.0930741 loss)
I0218 12:17:35.984315 27028 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0218 12:18:11.725919 27028 solver.cpp:357] Iteration 4900 (2.79781 iter/s, 35.7423s/100 iters), loss = 0.466101
I0218 12:18:11.726008 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.115692 (* 1 = 0.115692 loss)
I0218 12:18:11.726019 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.078776 (* 1 = 0.078776 loss)
I0218 12:18:11.726028 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.179661 (* 1 = 0.179661 loss)
I0218 12:18:11.726037 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0919724 (* 1 = 0.0919724 loss)
I0218 12:18:11.726048 27028 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0218 12:18:40.015626 27028 solver.cpp:514] Iteration 5000, Testing net (#0)
I0218 12:18:42.483397 27028 solver.cpp:580]     Test net output #0: Softmax1 = 8.66588 (* 1 = 8.66588 loss)
I0218 12:18:42.483531 27028 solver.cpp:580]     Test net output #1: Softmax2 = 15.1108 (* 1 = 15.1108 loss)
I0218 12:18:42.483542 27028 solver.cpp:580]     Test net output #2: Softmax3 = 13.3175 (* 1 = 13.3175 loss)
I0218 12:18:42.483551 27028 solver.cpp:580]     Test net output #3: Softmax4 = 10.6949 (* 1 = 10.6949 loss)
I0218 12:18:42.483557 27028 solver.cpp:580]     Test net output #4: prob1 = 0.024
I0218 12:18:42.483564 27028 solver.cpp:580]     Test net output #5: prob2 = 0.02
I0218 12:18:42.483570 27028 solver.cpp:580]     Test net output #6: prob3 = 0.018
I0218 12:18:42.483575 27028 solver.cpp:580]     Test net output #7: prob4 = 0.017
I0218 12:18:42.818084 27028 solver.cpp:357] Iteration 5000 (3.21619 iter/s, 31.0927s/100 iters), loss = 0.675962
I0218 12:18:42.818141 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.144453 (* 1 = 0.144453 loss)
I0218 12:18:42.818151 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.195366 (* 1 = 0.195366 loss)
I0218 12:18:42.818161 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.116973 (* 1 = 0.116973 loss)
I0218 12:18:42.818168 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.21917 (* 1 = 0.21917 loss)
I0218 12:18:42.818176 27028 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0218 12:19:18.524654 27028 solver.cpp:357] Iteration 5100 (2.80055 iter/s, 35.7073s/100 iters), loss = 0.772934
I0218 12:19:18.524873 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.191399 (* 1 = 0.191399 loss)
I0218 12:19:18.524883 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.192652 (* 1 = 0.192652 loss)
I0218 12:19:18.524893 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.227234 (* 1 = 0.227234 loss)
I0218 12:19:18.524900 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.161649 (* 1 = 0.161649 loss)
I0218 12:19:18.524907 27028 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0218 12:19:54.232854 27028 solver.cpp:357] Iteration 5200 (2.80043 iter/s, 35.7088s/100 iters), loss = 0.804766
I0218 12:19:54.232969 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.200947 (* 1 = 0.200947 loss)
I0218 12:19:54.232978 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.162334 (* 1 = 0.162334 loss)
I0218 12:19:54.232987 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.316502 (* 1 = 0.316502 loss)
I0218 12:19:54.232996 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.124983 (* 1 = 0.124983 loss)
I0218 12:19:54.233003 27028 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0218 12:20:29.927325 27028 solver.cpp:357] Iteration 5300 (2.8015 iter/s, 35.6952s/100 iters), loss = 0.541402
I0218 12:20:29.927474 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.113703 (* 1 = 0.113703 loss)
I0218 12:20:29.927484 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108272 (* 1 = 0.108272 loss)
I0218 12:20:29.927492 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.186088 (* 1 = 0.186088 loss)
I0218 12:20:29.927500 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.133339 (* 1 = 0.133339 loss)
I0218 12:20:29.927510 27028 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0218 12:21:05.639166 27028 solver.cpp:357] Iteration 5400 (2.80013 iter/s, 35.7126s/100 iters), loss = 0.494291
I0218 12:21:05.639278 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0761017 (* 1 = 0.0761017 loss)
I0218 12:21:05.639289 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108438 (* 1 = 0.108438 loss)
I0218 12:21:05.639298 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.126477 (* 1 = 0.126477 loss)
I0218 12:21:05.639307 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.183274 (* 1 = 0.183274 loss)
I0218 12:21:05.639313 27028 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0218 12:21:34.314476 27028 solver.cpp:357] Iteration 5500 (3.48725 iter/s, 28.6759s/100 iters), loss = 0.394522
I0218 12:21:34.314553 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.138065 (* 1 = 0.138065 loss)
I0218 12:21:34.314563 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0797666 (* 1 = 0.0797666 loss)
I0218 12:21:34.314572 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0688306 (* 1 = 0.0688306 loss)
I0218 12:21:34.314580 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.10786 (* 1 = 0.10786 loss)
I0218 12:21:34.314587 27028 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0218 12:22:10.017172 27028 solver.cpp:357] Iteration 5600 (2.80084 iter/s, 35.7036s/100 iters), loss = 0.696673
I0218 12:22:10.017395 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.154403 (* 1 = 0.154403 loss)
I0218 12:22:10.017405 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.162173 (* 1 = 0.162173 loss)
I0218 12:22:10.017415 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.233114 (* 1 = 0.233114 loss)
I0218 12:22:10.017422 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.146983 (* 1 = 0.146983 loss)
I0218 12:22:10.017429 27028 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0218 12:22:45.733884 27028 solver.cpp:357] Iteration 5700 (2.79975 iter/s, 35.7175s/100 iters), loss = 0.662577
I0218 12:22:45.734055 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.15842 (* 1 = 0.15842 loss)
I0218 12:22:45.734066 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.152726 (* 1 = 0.152726 loss)
I0218 12:22:45.734074 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.215409 (* 1 = 0.215409 loss)
I0218 12:22:45.734082 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136022 (* 1 = 0.136022 loss)
I0218 12:22:45.734089 27028 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0218 12:23:21.455734 27028 solver.cpp:357] Iteration 5800 (2.79934 iter/s, 35.7227s/100 iters), loss = 0.660908
I0218 12:23:21.455848 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.10447 (* 1 = 0.10447 loss)
I0218 12:23:21.455858 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.239524 (* 1 = 0.239524 loss)
I0218 12:23:21.455868 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.154204 (* 1 = 0.154204 loss)
I0218 12:23:21.455875 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.16271 (* 1 = 0.16271 loss)
I0218 12:23:21.455883 27028 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0218 12:23:57.162849 27028 solver.cpp:357] Iteration 5900 (2.80049 iter/s, 35.708s/100 iters), loss = 0.735335
I0218 12:23:57.162967 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.208706 (* 1 = 0.208706 loss)
I0218 12:23:57.162978 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.175298 (* 1 = 0.175298 loss)
I0218 12:23:57.162987 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.249495 (* 1 = 0.249495 loss)
I0218 12:23:57.162995 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.101836 (* 1 = 0.101836 loss)
I0218 12:23:57.163003 27028 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0218 12:24:25.915697 27028 solver.cpp:514] Iteration 6000, Testing net (#0)
I0218 12:24:27.930913 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.85882 (* 1 = 0.85882 loss)
I0218 12:24:27.931042 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.11048 (* 1 = 1.11048 loss)
I0218 12:24:27.931051 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.01748 (* 1 = 1.01748 loss)
I0218 12:24:27.931061 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.931812 (* 1 = 0.931812 loss)
I0218 12:24:27.931068 27028 solver.cpp:580]     Test net output #4: prob1 = 0.782
I0218 12:24:27.931074 27028 solver.cpp:580]     Test net output #5: prob2 = 0.71
I0218 12:24:27.931079 27028 solver.cpp:580]     Test net output #6: prob3 = 0.742
I0218 12:24:27.931084 27028 solver.cpp:580]     Test net output #7: prob4 = 0.764
I0218 12:24:28.294800 27028 solver.cpp:357] Iteration 6000 (3.21205 iter/s, 31.1328s/100 iters), loss = 0.683807
I0218 12:24:28.294862 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.133132 (* 1 = 0.133132 loss)
I0218 12:24:28.294870 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.188272 (* 1 = 0.188272 loss)
I0218 12:24:28.294879 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.170061 (* 1 = 0.170061 loss)
I0218 12:24:28.294888 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.192342 (* 1 = 0.192342 loss)
I0218 12:24:28.294896 27028 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0218 12:25:03.978724 27028 solver.cpp:357] Iteration 6100 (2.80246 iter/s, 35.6829s/100 iters), loss = 0.635876
I0218 12:25:03.978898 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.171554 (* 1 = 0.171554 loss)
I0218 12:25:03.978907 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.130574 (* 1 = 0.130574 loss)
I0218 12:25:03.978916 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.157809 (* 1 = 0.157809 loss)
I0218 12:25:03.978924 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.175939 (* 1 = 0.175939 loss)
I0218 12:25:03.978931 27028 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0218 12:25:39.736920 27028 solver.cpp:357] Iteration 6200 (2.79649 iter/s, 35.7591s/100 iters), loss = 0.524359
I0218 12:25:39.737078 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0737544 (* 1 = 0.0737544 loss)
I0218 12:25:39.737088 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.189561 (* 1 = 0.189561 loss)
I0218 12:25:39.737097 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.143234 (* 1 = 0.143234 loss)
I0218 12:25:39.737105 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.11781 (* 1 = 0.11781 loss)
I0218 12:25:39.737112 27028 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0218 12:26:15.509352 27028 solver.cpp:357] Iteration 6300 (2.79539 iter/s, 35.7731s/100 iters), loss = 0.442117
I0218 12:26:15.509526 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.112688 (* 1 = 0.112688 loss)
I0218 12:26:15.509536 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.155735 (* 1 = 0.155735 loss)
I0218 12:26:15.509546 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.118425 (* 1 = 0.118425 loss)
I0218 12:26:15.509553 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0552693 (* 1 = 0.0552693 loss)
I0218 12:26:15.509562 27028 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0218 12:26:51.252988 27028 solver.cpp:357] Iteration 6400 (2.79778 iter/s, 35.7427s/100 iters), loss = 0.873263
I0218 12:26:51.253149 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.197449 (* 1 = 0.197449 loss)
I0218 12:26:51.253160 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.182686 (* 1 = 0.182686 loss)
I0218 12:26:51.253170 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.304408 (* 1 = 0.304408 loss)
I0218 12:26:51.253177 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.18872 (* 1 = 0.18872 loss)
I0218 12:26:51.253185 27028 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0218 12:27:20.595254 27028 solver.cpp:357] Iteration 6500 (3.40819 iter/s, 29.3411s/100 iters), loss = 0.669236
I0218 12:27:20.595335 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.18872 (* 1 = 0.18872 loss)
I0218 12:27:20.595346 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.190726 (* 1 = 0.190726 loss)
I0218 12:27:20.595355 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.162089 (* 1 = 0.162089 loss)
I0218 12:27:20.595362 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.127701 (* 1 = 0.127701 loss)
I0218 12:27:20.595371 27028 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0218 12:27:55.577124 27028 solver.cpp:357] Iteration 6600 (2.8587 iter/s, 34.9809s/100 iters), loss = 0.610601
I0218 12:27:55.577251 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.140518 (* 1 = 0.140518 loss)
I0218 12:27:55.577261 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.126737 (* 1 = 0.126737 loss)
I0218 12:27:55.577270 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.206495 (* 1 = 0.206495 loss)
I0218 12:27:55.577278 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136851 (* 1 = 0.136851 loss)
I0218 12:27:55.577286 27028 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0218 12:28:31.292584 27028 solver.cpp:357] Iteration 6700 (2.79998 iter/s, 35.7145s/100 iters), loss = 0.643847
I0218 12:28:31.292755 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.141233 (* 1 = 0.141233 loss)
I0218 12:28:31.292767 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.245181 (* 1 = 0.245181 loss)
I0218 12:28:31.292774 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.14099 (* 1 = 0.14099 loss)
I0218 12:28:31.292783 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.116442 (* 1 = 0.116442 loss)
I0218 12:28:31.292795 27028 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0218 12:29:06.973086 27028 solver.cpp:357] Iteration 6800 (2.80272 iter/s, 35.6796s/100 iters), loss = 0.717455
I0218 12:29:06.973284 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.198951 (* 1 = 0.198951 loss)
I0218 12:29:06.973294 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.224498 (* 1 = 0.224498 loss)
I0218 12:29:06.973302 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.160872 (* 1 = 0.160872 loss)
I0218 12:29:06.973310 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.133134 (* 1 = 0.133134 loss)
I0218 12:29:06.973318 27028 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0218 12:29:42.687253 27028 solver.cpp:357] Iteration 6900 (2.79993 iter/s, 35.7152s/100 iters), loss = 0.637049
I0218 12:29:42.687417 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.158332 (* 1 = 0.158332 loss)
I0218 12:29:42.687427 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.158855 (* 1 = 0.158855 loss)
I0218 12:29:42.687436 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.127077 (* 1 = 0.127077 loss)
I0218 12:29:42.687444 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.192785 (* 1 = 0.192785 loss)
I0218 12:29:42.687451 27028 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0218 12:30:13.088347 27028 solver.cpp:514] Iteration 7000, Testing net (#0)
I0218 12:30:14.624955 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.642492 (* 1 = 0.642492 loss)
I0218 12:30:14.625005 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.954168 (* 1 = 0.954168 loss)
I0218 12:30:14.625012 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.687404 (* 1 = 0.687404 loss)
I0218 12:30:14.625020 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.642316 (* 1 = 0.642316 loss)
I0218 12:30:14.625026 27028 solver.cpp:580]     Test net output #4: prob1 = 0.846
I0218 12:30:14.625032 27028 solver.cpp:580]     Test net output #5: prob2 = 0.752
I0218 12:30:14.625037 27028 solver.cpp:580]     Test net output #6: prob3 = 0.811
I0218 12:30:14.625043 27028 solver.cpp:580]     Test net output #7: prob4 = 0.836
I0218 12:30:14.889076 27028 solver.cpp:357] Iteration 7000 (3.10532 iter/s, 32.2028s/100 iters), loss = 0.524967
I0218 12:30:14.889128 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143251 (* 1 = 0.143251 loss)
I0218 12:30:14.889139 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.186774 (* 1 = 0.186774 loss)
I0218 12:30:14.889148 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.117663 (* 1 = 0.117663 loss)
I0218 12:30:14.889156 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0772788 (* 1 = 0.0772788 loss)
I0218 12:30:14.889165 27028 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0218 12:30:49.462666 27028 solver.cpp:357] Iteration 7100 (2.89228 iter/s, 34.5747s/100 iters), loss = 0.592658
I0218 12:30:49.462739 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.119614 (* 1 = 0.119614 loss)
I0218 12:30:49.462751 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.168274 (* 1 = 0.168274 loss)
I0218 12:30:49.462760 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.191842 (* 1 = 0.191842 loss)
I0218 12:30:49.462769 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.112928 (* 1 = 0.112928 loss)
I0218 12:30:49.462775 27028 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0218 12:31:25.181293 27028 solver.cpp:357] Iteration 7200 (2.79957 iter/s, 35.7198s/100 iters), loss = 0.528506
I0218 12:31:25.181426 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.141617 (* 1 = 0.141617 loss)
I0218 12:31:25.181437 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137703 (* 1 = 0.137703 loss)
I0218 12:31:25.181444 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.150236 (* 1 = 0.150236 loss)
I0218 12:31:25.181453 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0989491 (* 1 = 0.0989491 loss)
I0218 12:31:25.181460 27028 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0218 12:32:00.905025 27028 solver.cpp:357] Iteration 7300 (2.79917 iter/s, 35.7249s/100 iters), loss = 0.608869
I0218 12:32:00.905205 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.146334 (* 1 = 0.146334 loss)
I0218 12:32:00.905215 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.179668 (* 1 = 0.179668 loss)
I0218 12:32:00.905223 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.192285 (* 1 = 0.192285 loss)
I0218 12:32:00.905232 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0905827 (* 1 = 0.0905827 loss)
I0218 12:32:00.905241 27028 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0218 12:32:36.625640 27028 solver.cpp:357] Iteration 7400 (2.79942 iter/s, 35.7217s/100 iters), loss = 0.582706
I0218 12:32:36.625756 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.165551 (* 1 = 0.165551 loss)
I0218 12:32:36.625766 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.152619 (* 1 = 0.152619 loss)
I0218 12:32:36.625775 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0980634 (* 1 = 0.0980634 loss)
I0218 12:32:36.625783 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.166472 (* 1 = 0.166472 loss)
I0218 12:32:36.625790 27028 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0218 12:33:07.741721 27028 solver.cpp:357] Iteration 7500 (3.21367 iter/s, 31.1171s/100 iters), loss = 0.526463
I0218 12:33:07.741865 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.118788 (* 1 = 0.118788 loss)
I0218 12:33:07.741876 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.158971 (* 1 = 0.158971 loss)
I0218 12:33:07.741885 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.114006 (* 1 = 0.114006 loss)
I0218 12:33:07.741892 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.134699 (* 1 = 0.134699 loss)
I0218 12:33:07.741900 27028 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0218 12:33:41.111130 27028 solver.cpp:357] Iteration 7600 (2.99666 iter/s, 33.3705s/100 iters), loss = 0.517239
I0218 12:33:41.111289 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.122174 (* 1 = 0.122174 loss)
I0218 12:33:41.111299 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.113984 (* 1 = 0.113984 loss)
I0218 12:33:41.111308 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.158697 (* 1 = 0.158697 loss)
I0218 12:33:41.111317 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.122385 (* 1 = 0.122385 loss)
I0218 12:33:41.111325 27028 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0218 12:34:16.823748 27028 solver.cpp:357] Iteration 7700 (2.80004 iter/s, 35.7138s/100 iters), loss = 0.50214
I0218 12:34:16.823861 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0918027 (* 1 = 0.0918027 loss)
I0218 12:34:16.823873 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.159619 (* 1 = 0.159619 loss)
I0218 12:34:16.823881 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.139708 (* 1 = 0.139708 loss)
I0218 12:34:16.823889 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.111011 (* 1 = 0.111011 loss)
I0218 12:34:16.823896 27028 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0218 12:34:52.546787 27028 solver.cpp:357] Iteration 7800 (2.79922 iter/s, 35.7242s/100 iters), loss = 0.578731
I0218 12:34:52.546900 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.158567 (* 1 = 0.158567 loss)
I0218 12:34:52.546911 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.143261 (* 1 = 0.143261 loss)
I0218 12:34:52.546919 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.15586 (* 1 = 0.15586 loss)
I0218 12:34:52.546928 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.121043 (* 1 = 0.121043 loss)
I0218 12:34:52.546936 27028 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0218 12:35:28.255931 27028 solver.cpp:357] Iteration 7900 (2.80031 iter/s, 35.7104s/100 iters), loss = 0.53254
I0218 12:35:28.256047 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.141249 (* 1 = 0.141249 loss)
I0218 12:35:28.256057 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.160499 (* 1 = 0.160499 loss)
I0218 12:35:28.256067 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.136487 (* 1 = 0.136487 loss)
I0218 12:35:28.256075 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0943059 (* 1 = 0.0943059 loss)
I0218 12:35:28.256083 27028 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0218 12:36:00.262763 27028 solver.cpp:514] Iteration 8000, Testing net (#0)
I0218 12:36:01.803813 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.71861 (* 1 = 2.71861 loss)
I0218 12:36:01.803860 27028 solver.cpp:580]     Test net output #1: Softmax2 = 3.42219 (* 1 = 3.42219 loss)
I0218 12:36:01.803869 27028 solver.cpp:580]     Test net output #2: Softmax3 = 4.82606 (* 1 = 4.82606 loss)
I0218 12:36:01.803879 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.4681 (* 1 = 3.4681 loss)
I0218 12:36:01.803884 27028 solver.cpp:580]     Test net output #4: prob1 = 0.483
I0218 12:36:01.803891 27028 solver.cpp:580]     Test net output #5: prob2 = 0.378
I0218 12:36:01.803896 27028 solver.cpp:580]     Test net output #6: prob3 = 0.241
I0218 12:36:01.803902 27028 solver.cpp:580]     Test net output #7: prob4 = 0.394
I0218 12:36:02.071981 27028 solver.cpp:357] Iteration 8000 (2.95708 iter/s, 33.8172s/100 iters), loss = 0.572807
I0218 12:36:02.072044 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.102167 (* 1 = 0.102167 loss)
I0218 12:36:02.072055 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.202757 (* 1 = 0.202757 loss)
I0218 12:36:02.072064 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.144048 (* 1 = 0.144048 loss)
I0218 12:36:02.072072 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.123835 (* 1 = 0.123835 loss)
I0218 12:36:02.072080 27028 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0218 12:36:35.179381 27028 solver.cpp:357] Iteration 8100 (3.02037 iter/s, 33.1086s/100 iters), loss = 0.581108
I0218 12:36:35.179549 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.227634 (* 1 = 0.227634 loss)
I0218 12:36:35.179559 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.100498 (* 1 = 0.100498 loss)
I0218 12:36:35.179569 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.102427 (* 1 = 0.102427 loss)
I0218 12:36:35.179576 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.15055 (* 1 = 0.15055 loss)
I0218 12:36:35.179584 27028 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0218 12:37:10.910295 27028 solver.cpp:357] Iteration 8200 (2.7986 iter/s, 35.7321s/100 iters), loss = 0.472373
I0218 12:37:10.910408 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.113272 (* 1 = 0.113272 loss)
I0218 12:37:10.910419 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0879492 (* 1 = 0.0879492 loss)
I0218 12:37:10.910429 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0879311 (* 1 = 0.0879311 loss)
I0218 12:37:10.910436 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.183222 (* 1 = 0.183222 loss)
I0218 12:37:10.910449 27028 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0218 12:37:46.619408 27028 solver.cpp:357] Iteration 8300 (2.80031 iter/s, 35.7104s/100 iters), loss = 0.674647
I0218 12:37:46.619518 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.118271 (* 1 = 0.118271 loss)
I0218 12:37:46.619529 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.206759 (* 1 = 0.206759 loss)
I0218 12:37:46.619537 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.19955 (* 1 = 0.19955 loss)
I0218 12:37:46.619545 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.150067 (* 1 = 0.150067 loss)
I0218 12:37:46.619554 27028 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0218 12:38:22.322893 27028 solver.cpp:357] Iteration 8400 (2.80075 iter/s, 35.7047s/100 iters), loss = 0.495781
I0218 12:38:22.323010 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.126239 (* 1 = 0.126239 loss)
I0218 12:38:22.323020 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.109823 (* 1 = 0.109823 loss)
I0218 12:38:22.323030 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0947158 (* 1 = 0.0947158 loss)
I0218 12:38:22.323038 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.165003 (* 1 = 0.165003 loss)
I0218 12:38:22.323046 27028 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0218 12:38:54.886665 27028 solver.cpp:357] Iteration 8500 (3.07079 iter/s, 32.5649s/100 iters), loss = 0.584939
I0218 12:38:54.886883 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.142543 (* 1 = 0.142543 loss)
I0218 12:38:54.886894 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.156104 (* 1 = 0.156104 loss)
I0218 12:38:54.886904 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.141576 (* 1 = 0.141576 loss)
I0218 12:38:54.886911 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144716 (* 1 = 0.144716 loss)
I0218 12:38:54.886919 27028 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0218 12:39:26.784953 27028 solver.cpp:357] Iteration 8600 (3.13487 iter/s, 31.8993s/100 iters), loss = 0.567873
I0218 12:39:26.785112 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.117513 (* 1 = 0.117513 loss)
I0218 12:39:26.785122 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.191763 (* 1 = 0.191763 loss)
I0218 12:39:26.785131 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.136669 (* 1 = 0.136669 loss)
I0218 12:39:26.785140 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.121928 (* 1 = 0.121928 loss)
I0218 12:39:26.785146 27028 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0218 12:40:02.502451 27028 solver.cpp:357] Iteration 8700 (2.79965 iter/s, 35.7187s/100 iters), loss = 0.448152
I0218 12:40:02.502569 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110679 (* 1 = 0.110679 loss)
I0218 12:40:02.502579 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.103 (* 1 = 0.103 loss)
I0218 12:40:02.502588 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.110704 (* 1 = 0.110704 loss)
I0218 12:40:02.502596 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.123769 (* 1 = 0.123769 loss)
I0218 12:40:02.502604 27028 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0218 12:40:38.220410 27028 solver.cpp:357] Iteration 8800 (2.79961 iter/s, 35.7192s/100 iters), loss = 0.602414
I0218 12:40:38.220528 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139581 (* 1 = 0.139581 loss)
I0218 12:40:38.220538 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.222363 (* 1 = 0.222363 loss)
I0218 12:40:38.220547 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0959405 (* 1 = 0.0959405 loss)
I0218 12:40:38.220556 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144529 (* 1 = 0.144529 loss)
I0218 12:40:38.220563 27028 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0218 12:41:13.928535 27028 solver.cpp:357] Iteration 8900 (2.80038 iter/s, 35.7094s/100 iters), loss = 0.569721
I0218 12:41:13.928680 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.100137 (* 1 = 0.100137 loss)
I0218 12:41:13.928690 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.168343 (* 1 = 0.168343 loss)
I0218 12:41:13.928699 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.146709 (* 1 = 0.146709 loss)
I0218 12:41:13.928706 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.154531 (* 1 = 0.154531 loss)
I0218 12:41:13.928714 27028 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0218 12:41:47.416872 27028 solver.cpp:514] Iteration 9000, Testing net (#0)
I0218 12:41:48.957270 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.24796 (* 1 = 1.24796 loss)
I0218 12:41:48.957320 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.5246 (* 1 = 1.5246 loss)
I0218 12:41:48.957329 27028 solver.cpp:580]     Test net output #2: Softmax3 = 2.08368 (* 1 = 2.08368 loss)
I0218 12:41:48.957339 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.02426 (* 1 = 2.02426 loss)
I0218 12:41:48.957345 27028 solver.cpp:580]     Test net output #4: prob1 = 0.694
I0218 12:41:48.957351 27028 solver.cpp:580]     Test net output #5: prob2 = 0.628
I0218 12:41:48.957356 27028 solver.cpp:580]     Test net output #6: prob3 = 0.538
I0218 12:41:48.957361 27028 solver.cpp:580]     Test net output #7: prob4 = 0.543
I0218 12:41:49.224319 27028 solver.cpp:357] Iteration 9000 (2.8331 iter/s, 35.297s/100 iters), loss = 0.562363
I0218 12:41:49.224372 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0610259 (* 1 = 0.0610259 loss)
I0218 12:41:49.224385 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.179526 (* 1 = 0.179526 loss)
I0218 12:41:49.224393 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.16761 (* 1 = 0.16761 loss)
I0218 12:41:49.224401 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.154201 (* 1 = 0.154201 loss)
I0218 12:41:49.224409 27028 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0218 12:42:20.834061 27028 solver.cpp:357] Iteration 9100 (3.16347 iter/s, 31.6109s/100 iters), loss = 0.627749
I0218 12:42:20.834249 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129019 (* 1 = 0.129019 loss)
I0218 12:42:20.834260 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.163336 (* 1 = 0.163336 loss)
I0218 12:42:20.834270 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0874763 (* 1 = 0.0874763 loss)
I0218 12:42:20.834281 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.247918 (* 1 = 0.247918 loss)
I0218 12:42:20.834288 27028 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0218 12:42:56.547303 27028 solver.cpp:357] Iteration 9200 (2.79999 iter/s, 35.7144s/100 iters), loss = 0.554184
I0218 12:42:56.547446 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.153133 (* 1 = 0.153133 loss)
I0218 12:42:56.547458 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.099599 (* 1 = 0.099599 loss)
I0218 12:42:56.547467 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134605 (* 1 = 0.134605 loss)
I0218 12:42:56.547475 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.166846 (* 1 = 0.166846 loss)
I0218 12:42:56.547482 27028 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0218 12:43:32.246314 27028 solver.cpp:357] Iteration 9300 (2.8011 iter/s, 35.7003s/100 iters), loss = 0.577723
I0218 12:43:32.246601 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.199716 (* 1 = 0.199716 loss)
I0218 12:43:32.246613 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.119743 (* 1 = 0.119743 loss)
I0218 12:43:32.246621 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.176736 (* 1 = 0.176736 loss)
I0218 12:43:32.246629 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0815283 (* 1 = 0.0815283 loss)
I0218 12:43:32.246637 27028 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0218 12:44:07.967725 27028 solver.cpp:357] Iteration 9400 (2.79936 iter/s, 35.7225s/100 iters), loss = 0.504779
I0218 12:44:07.967886 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.046534 (* 1 = 0.046534 loss)
I0218 12:44:07.967897 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.156696 (* 1 = 0.156696 loss)
I0218 12:44:07.967905 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.153456 (* 1 = 0.153456 loss)
I0218 12:44:07.967913 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.148094 (* 1 = 0.148094 loss)
I0218 12:44:07.967921 27028 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0218 12:44:42.071214 27028 solver.cpp:357] Iteration 9500 (2.93259 iter/s, 34.0995s/100 iters), loss = 0.534406
I0218 12:44:42.071372 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.038636 (* 1 = 0.038636 loss)
I0218 12:44:42.071382 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.219196 (* 1 = 0.219196 loss)
I0218 12:44:42.071390 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.197723 (* 1 = 0.197723 loss)
I0218 12:44:42.071398 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.07885 (* 1 = 0.07885 loss)
I0218 12:44:42.071406 27028 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0218 12:45:12.229171 27028 solver.cpp:357] Iteration 9600 (3.31634 iter/s, 30.1537s/100 iters), loss = 0.762147
I0218 12:45:12.229341 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.115619 (* 1 = 0.115619 loss)
I0218 12:45:12.229352 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.278339 (* 1 = 0.278339 loss)
I0218 12:45:12.229362 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.223728 (* 1 = 0.223728 loss)
I0218 12:45:12.229369 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144462 (* 1 = 0.144462 loss)
I0218 12:45:12.229377 27028 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0218 12:45:47.948493 27028 solver.cpp:357] Iteration 9700 (2.80013 iter/s, 35.7126s/100 iters), loss = 0.650131
I0218 12:45:47.948632 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.105525 (* 1 = 0.105525 loss)
I0218 12:45:47.948642 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.260517 (* 1 = 0.260517 loss)
I0218 12:45:47.948652 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0856326 (* 1 = 0.0856326 loss)
I0218 12:45:47.948659 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.198457 (* 1 = 0.198457 loss)
I0218 12:45:47.948668 27028 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0218 12:46:23.687500 27028 solver.cpp:357] Iteration 9800 (2.79856 iter/s, 35.7326s/100 iters), loss = 0.40683
I0218 12:46:23.687608 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0749872 (* 1 = 0.0749872 loss)
I0218 12:46:23.687618 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.132546 (* 1 = 0.132546 loss)
I0218 12:46:23.687628 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0927019 (* 1 = 0.0927019 loss)
I0218 12:46:23.687635 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.106595 (* 1 = 0.106595 loss)
I0218 12:46:23.687642 27028 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0218 12:46:59.472028 27028 solver.cpp:357] Iteration 9900 (2.79482 iter/s, 35.7805s/100 iters), loss = 0.670606
I0218 12:46:59.472152 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.134544 (* 1 = 0.134544 loss)
I0218 12:46:59.472162 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.230029 (* 1 = 0.230029 loss)
I0218 12:46:59.472172 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.181793 (* 1 = 0.181793 loss)
I0218 12:46:59.472180 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.124241 (* 1 = 0.124241 loss)
I0218 12:46:59.472188 27028 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0218 12:47:34.593361 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.caffemodel
I0218 12:47:34.608105 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_10000.solverstate
I0218 12:47:34.611243 27028 solver.cpp:514] Iteration 10000, Testing net (#0)
I0218 12:47:36.125480 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.77854 (* 1 = 2.77854 loss)
I0218 12:47:36.125528 27028 solver.cpp:580]     Test net output #1: Softmax2 = 3.19844 (* 1 = 3.19844 loss)
I0218 12:47:36.125536 27028 solver.cpp:580]     Test net output #2: Softmax3 = 3.55779 (* 1 = 3.55779 loss)
I0218 12:47:36.125545 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.20462 (* 1 = 3.20462 loss)
I0218 12:47:36.125552 27028 solver.cpp:580]     Test net output #4: prob1 = 0.363
I0218 12:47:36.125560 27028 solver.cpp:580]     Test net output #5: prob2 = 0.361
I0218 12:47:36.125564 27028 solver.cpp:580]     Test net output #6: prob3 = 0.326
I0218 12:47:36.125571 27028 solver.cpp:580]     Test net output #7: prob4 = 0.364
I0218 12:47:36.395177 27028 solver.cpp:357] Iteration 10000 (2.70876 iter/s, 36.9173s/100 iters), loss = 0.620478
I0218 12:47:36.395231 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.175647 (* 1 = 0.175647 loss)
I0218 12:47:36.395241 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.166504 (* 1 = 0.166504 loss)
I0218 12:47:36.395248 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.113865 (* 1 = 0.113865 loss)
I0218 12:47:36.395256 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.164462 (* 1 = 0.164462 loss)
I0218 12:47:36.395264 27028 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0218 12:48:06.300899 27028 solver.cpp:357] Iteration 10100 (3.34416 iter/s, 29.9029s/100 iters), loss = 0.528095
I0218 12:48:06.301093 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0997434 (* 1 = 0.0997434 loss)
I0218 12:48:06.301105 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.145633 (* 1 = 0.145633 loss)
I0218 12:48:06.301113 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.186835 (* 1 = 0.186835 loss)
I0218 12:48:06.301121 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0958832 (* 1 = 0.0958832 loss)
I0218 12:48:06.301129 27028 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0218 12:48:41.950459 27028 solver.cpp:357] Iteration 10200 (2.8055 iter/s, 35.6443s/100 iters), loss = 0.662554
I0218 12:48:41.950637 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.223484 (* 1 = 0.223484 loss)
I0218 12:48:41.950649 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.156 (* 1 = 0.156 loss)
I0218 12:48:41.950657 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.126798 (* 1 = 0.126798 loss)
I0218 12:48:41.950665 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.156272 (* 1 = 0.156272 loss)
I0218 12:48:41.950673 27028 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0218 12:49:17.633476 27028 solver.cpp:357] Iteration 10300 (2.80285 iter/s, 35.678s/100 iters), loss = 0.41988
I0218 12:49:17.633615 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0785273 (* 1 = 0.0785273 loss)
I0218 12:49:17.633625 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0886391 (* 1 = 0.0886391 loss)
I0218 12:49:17.633635 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.149198 (* 1 = 0.149198 loss)
I0218 12:49:17.633642 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.103516 (* 1 = 0.103516 loss)
I0218 12:49:17.633651 27028 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0218 12:49:53.338678 27028 solver.cpp:357] Iteration 10400 (2.80109 iter/s, 35.7004s/100 iters), loss = 0.495678
I0218 12:49:53.338795 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.145424 (* 1 = 0.145424 loss)
I0218 12:49:53.338806 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0871149 (* 1 = 0.0871149 loss)
I0218 12:49:53.338815 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.141904 (* 1 = 0.141904 loss)
I0218 12:49:53.338824 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.121236 (* 1 = 0.121236 loss)
I0218 12:49:53.338830 27028 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0218 12:50:29.054750 27028 solver.cpp:357] Iteration 10500 (2.80006 iter/s, 35.7135s/100 iters), loss = 0.716052
I0218 12:50:29.054877 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.167638 (* 1 = 0.167638 loss)
I0218 12:50:29.054888 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.241584 (* 1 = 0.241584 loss)
I0218 12:50:29.054896 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.167978 (* 1 = 0.167978 loss)
I0218 12:50:29.054904 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.138852 (* 1 = 0.138852 loss)
I0218 12:50:29.054911 27028 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0218 12:50:57.831919 27028 solver.cpp:357] Iteration 10600 (3.47522 iter/s, 28.7752s/100 iters), loss = 0.508716
I0218 12:50:57.831995 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0948026 (* 1 = 0.0948026 loss)
I0218 12:50:57.832005 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.119262 (* 1 = 0.119262 loss)
I0218 12:50:57.832015 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.196643 (* 1 = 0.196643 loss)
I0218 12:50:57.832022 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0980086 (* 1 = 0.0980086 loss)
I0218 12:50:57.832029 27028 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0218 12:51:33.550740 27028 solver.cpp:357] Iteration 10700 (2.79982 iter/s, 35.7166s/100 iters), loss = 0.423757
I0218 12:51:33.550851 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.122517 (* 1 = 0.122517 loss)
I0218 12:51:33.550861 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0927698 (* 1 = 0.0927698 loss)
I0218 12:51:33.550870 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.124282 (* 1 = 0.124282 loss)
I0218 12:51:33.550879 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.084188 (* 1 = 0.084188 loss)
I0218 12:51:33.550885 27028 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0218 12:52:09.274528 27028 solver.cpp:357] Iteration 10800 (2.79942 iter/s, 35.7217s/100 iters), loss = 0.446989
I0218 12:52:09.274786 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.130579 (* 1 = 0.130579 loss)
I0218 12:52:09.274798 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0675059 (* 1 = 0.0675059 loss)
I0218 12:52:09.274806 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.175938 (* 1 = 0.175938 loss)
I0218 12:52:09.274814 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0729652 (* 1 = 0.0729652 loss)
I0218 12:52:09.274822 27028 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0218 12:52:44.983748 27028 solver.cpp:357] Iteration 10900 (2.80056 iter/s, 35.7071s/100 iters), loss = 0.390334
I0218 12:52:44.983867 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0547053 (* 1 = 0.0547053 loss)
I0218 12:52:44.983878 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.121788 (* 1 = 0.121788 loss)
I0218 12:52:44.983887 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.120472 (* 1 = 0.120472 loss)
I0218 12:52:44.983894 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0933681 (* 1 = 0.0933681 loss)
I0218 12:52:44.983902 27028 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0218 12:53:20.369477 27028 solver.cpp:514] Iteration 11000, Testing net (#0)
I0218 12:53:22.835160 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.446276 (* 1 = 0.446276 loss)
I0218 12:53:22.835211 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.526236 (* 1 = 0.526236 loss)
I0218 12:53:22.835218 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.587639 (* 1 = 0.587639 loss)
I0218 12:53:22.835227 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.967785 (* 1 = 0.967785 loss)
I0218 12:53:22.835232 27028 solver.cpp:580]     Test net output #4: prob1 = 0.889
I0218 12:53:22.835238 27028 solver.cpp:580]     Test net output #5: prob2 = 0.872
I0218 12:53:22.835243 27028 solver.cpp:580]     Test net output #6: prob3 = 0.843
I0218 12:53:22.835249 27028 solver.cpp:580]     Test net output #7: prob4 = 0.761
I0218 12:53:23.173112 27028 solver.cpp:357] Iteration 11000 (2.61866 iter/s, 38.1875s/100 iters), loss = 0.717883
I0218 12:53:23.173169 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.12336 (* 1 = 0.12336 loss)
I0218 12:53:23.173179 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.158003 (* 1 = 0.158003 loss)
I0218 12:53:23.173188 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.155903 (* 1 = 0.155903 loss)
I0218 12:53:23.173195 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.280618 (* 1 = 0.280618 loss)
I0218 12:53:23.173203 27028 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0218 12:53:51.700170 27028 solver.cpp:357] Iteration 11100 (3.5056 iter/s, 28.5258s/100 iters), loss = 0.633506
I0218 12:53:51.700292 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.128717 (* 1 = 0.128717 loss)
I0218 12:53:51.700302 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.208096 (* 1 = 0.208096 loss)
I0218 12:53:51.700310 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.169041 (* 1 = 0.169041 loss)
I0218 12:53:51.700318 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.127651 (* 1 = 0.127651 loss)
I0218 12:53:51.700325 27028 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0218 12:54:27.499658 27028 solver.cpp:357] Iteration 11200 (2.79361 iter/s, 35.7959s/100 iters), loss = 0.420215
I0218 12:54:27.499810 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0888768 (* 1 = 0.0888768 loss)
I0218 12:54:27.499821 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108283 (* 1 = 0.108283 loss)
I0218 12:54:27.499830 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0931554 (* 1 = 0.0931554 loss)
I0218 12:54:27.499837 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.1299 (* 1 = 0.1299 loss)
I0218 12:54:27.499845 27028 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0218 12:55:03.229339 27028 solver.cpp:357] Iteration 11300 (2.79891 iter/s, 35.7282s/100 iters), loss = 0.511249
I0218 12:55:03.229508 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.164783 (* 1 = 0.164783 loss)
I0218 12:55:03.229519 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0815343 (* 1 = 0.0815343 loss)
I0218 12:55:03.229528 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.107555 (* 1 = 0.107555 loss)
I0218 12:55:03.229537 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.157376 (* 1 = 0.157376 loss)
I0218 12:55:03.229543 27028 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0218 12:55:38.956260 27028 solver.cpp:357] Iteration 11400 (2.79912 iter/s, 35.7256s/100 iters), loss = 0.416709
I0218 12:55:38.956380 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.100936 (* 1 = 0.100936 loss)
I0218 12:55:38.956391 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.111904 (* 1 = 0.111904 loss)
I0218 12:55:38.956399 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.102127 (* 1 = 0.102127 loss)
I0218 12:55:38.956408 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.101742 (* 1 = 0.101742 loss)
I0218 12:55:38.956415 27028 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0218 12:56:14.678270 27028 solver.cpp:357] Iteration 11500 (2.79949 iter/s, 35.7208s/100 iters), loss = 0.445713
I0218 12:56:14.678432 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0779735 (* 1 = 0.0779735 loss)
I0218 12:56:14.678443 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.199524 (* 1 = 0.199524 loss)
I0218 12:56:14.678452 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0689021 (* 1 = 0.0689021 loss)
I0218 12:56:14.678459 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0993129 (* 1 = 0.0993129 loss)
I0218 12:56:14.678467 27028 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0218 12:56:43.677371 27028 solver.cpp:357] Iteration 11600 (3.4485 iter/s, 28.9981s/100 iters), loss = 0.508791
I0218 12:56:43.677445 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.134304 (* 1 = 0.134304 loss)
I0218 12:56:43.677455 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.102841 (* 1 = 0.102841 loss)
I0218 12:56:43.677464 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.147077 (* 1 = 0.147077 loss)
I0218 12:56:43.677471 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.124569 (* 1 = 0.124569 loss)
I0218 12:56:43.677479 27028 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0218 12:57:19.119949 27028 solver.cpp:357] Iteration 11700 (2.82154 iter/s, 35.4416s/100 iters), loss = 0.592409
I0218 12:57:19.120060 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0856954 (* 1 = 0.0856954 loss)
I0218 12:57:19.120070 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.232506 (* 1 = 0.232506 loss)
I0218 12:57:19.120079 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.137544 (* 1 = 0.137544 loss)
I0218 12:57:19.120087 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136664 (* 1 = 0.136664 loss)
I0218 12:57:19.120095 27028 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0218 12:57:54.838593 27028 solver.cpp:357] Iteration 11800 (2.79973 iter/s, 35.7177s/100 iters), loss = 0.678517
I0218 12:57:54.838752 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.215825 (* 1 = 0.215825 loss)
I0218 12:57:54.838762 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.132427 (* 1 = 0.132427 loss)
I0218 12:57:54.838770 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.123128 (* 1 = 0.123128 loss)
I0218 12:57:54.838778 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.207137 (* 1 = 0.207137 loss)
I0218 12:57:54.838785 27028 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0218 12:58:30.567306 27028 solver.cpp:357] Iteration 11900 (2.79894 iter/s, 35.7278s/100 iters), loss = 0.415849
I0218 12:58:30.567487 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0917598 (* 1 = 0.0917598 loss)
I0218 12:58:30.567497 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0729743 (* 1 = 0.0729743 loss)
I0218 12:58:30.567507 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.145118 (* 1 = 0.145118 loss)
I0218 12:58:30.567514 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.105996 (* 1 = 0.105996 loss)
I0218 12:58:30.567523 27028 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0218 12:59:05.949298 27028 solver.cpp:514] Iteration 12000, Testing net (#0)
I0218 12:59:08.418388 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.690562 (* 1 = 0.690562 loss)
I0218 12:59:08.418437 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.43482 (* 1 = 1.43482 loss)
I0218 12:59:08.418444 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.993191 (* 1 = 0.993191 loss)
I0218 12:59:08.418452 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.683734 (* 1 = 0.683734 loss)
I0218 12:59:08.418458 27028 solver.cpp:580]     Test net output #4: prob1 = 0.848
I0218 12:59:08.418464 27028 solver.cpp:580]     Test net output #5: prob2 = 0.623
I0218 12:59:08.418469 27028 solver.cpp:580]     Test net output #6: prob3 = 0.746
I0218 12:59:08.418475 27028 solver.cpp:580]     Test net output #7: prob4 = 0.819
I0218 12:59:08.754875 27028 solver.cpp:357] Iteration 12000 (2.61871 iter/s, 38.1867s/100 iters), loss = 0.499304
I0218 12:59:08.754932 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.117825 (* 1 = 0.117825 loss)
I0218 12:59:08.754943 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0812598 (* 1 = 0.0812598 loss)
I0218 12:59:08.754951 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.132494 (* 1 = 0.132494 loss)
I0218 12:59:08.754959 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.167725 (* 1 = 0.167725 loss)
I0218 12:59:08.754966 27028 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0218 12:59:38.100741 27028 solver.cpp:357] Iteration 12100 (3.4077 iter/s, 29.3453s/100 iters), loss = 0.547105
I0218 12:59:38.100862 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.128772 (* 1 = 0.128772 loss)
I0218 12:59:38.100873 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.150594 (* 1 = 0.150594 loss)
I0218 12:59:38.100881 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.138065 (* 1 = 0.138065 loss)
I0218 12:59:38.100889 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.129673 (* 1 = 0.129673 loss)
I0218 12:59:38.100898 27028 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0218 13:00:13.227599 27028 solver.cpp:357] Iteration 12200 (2.84688 iter/s, 35.1262s/100 iters), loss = 0.574113
I0218 13:00:13.227710 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.203502 (* 1 = 0.203502 loss)
I0218 13:00:13.227722 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.169646 (* 1 = 0.169646 loss)
I0218 13:00:13.227730 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.127575 (* 1 = 0.127575 loss)
I0218 13:00:13.227739 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0733894 (* 1 = 0.0733894 loss)
I0218 13:00:13.227747 27028 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0218 13:00:48.952317 27028 solver.cpp:357] Iteration 12300 (2.79923 iter/s, 35.7241s/100 iters), loss = 0.641913
I0218 13:00:48.952478 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.123294 (* 1 = 0.123294 loss)
I0218 13:00:48.952488 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.169932 (* 1 = 0.169932 loss)
I0218 13:00:48.952497 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.211154 (* 1 = 0.211154 loss)
I0218 13:00:48.952504 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.137532 (* 1 = 0.137532 loss)
I0218 13:00:48.952512 27028 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0218 13:01:24.677017 27028 solver.cpp:357] Iteration 12400 (2.79923 iter/s, 35.7241s/100 iters), loss = 0.448934
I0218 13:01:24.677275 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0587124 (* 1 = 0.0587124 loss)
I0218 13:01:24.677287 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.176968 (* 1 = 0.176968 loss)
I0218 13:01:24.677295 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0815745 (* 1 = 0.0815745 loss)
I0218 13:01:24.677304 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.131679 (* 1 = 0.131679 loss)
I0218 13:01:24.677311 27028 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0218 13:02:00.389822 27028 solver.cpp:357] Iteration 12500 (2.80016 iter/s, 35.7122s/100 iters), loss = 0.44215
I0218 13:02:00.389948 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.126559 (* 1 = 0.126559 loss)
I0218 13:02:00.389959 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.105036 (* 1 = 0.105036 loss)
I0218 13:02:00.389967 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.109258 (* 1 = 0.109258 loss)
I0218 13:02:00.389976 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.101297 (* 1 = 0.101297 loss)
I0218 13:02:00.389983 27028 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0218 13:02:31.008571 27028 solver.cpp:357] Iteration 12600 (3.26602 iter/s, 30.6184s/100 iters), loss = 0.436305
I0218 13:02:31.008726 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0879611 (* 1 = 0.0879611 loss)
I0218 13:02:31.008736 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.124795 (* 1 = 0.124795 loss)
I0218 13:02:31.008745 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.116519 (* 1 = 0.116519 loss)
I0218 13:02:31.008754 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.107029 (* 1 = 0.107029 loss)
I0218 13:02:31.008760 27028 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0218 13:03:04.933413 27028 solver.cpp:357] Iteration 12700 (2.94773 iter/s, 33.9244s/100 iters), loss = 0.611796
I0218 13:03:04.933532 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.106246 (* 1 = 0.106246 loss)
I0218 13:03:04.933543 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.172992 (* 1 = 0.172992 loss)
I0218 13:03:04.933552 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.21433 (* 1 = 0.21433 loss)
I0218 13:03:04.933559 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.118227 (* 1 = 0.118227 loss)
I0218 13:03:04.933567 27028 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0218 13:03:40.646667 27028 solver.cpp:357] Iteration 12800 (2.80011 iter/s, 35.7129s/100 iters), loss = 0.642476
I0218 13:03:40.646826 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.205529 (* 1 = 0.205529 loss)
I0218 13:03:40.646836 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.16714 (* 1 = 0.16714 loss)
I0218 13:03:40.646845 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.152712 (* 1 = 0.152712 loss)
I0218 13:03:40.646853 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.117096 (* 1 = 0.117096 loss)
I0218 13:03:40.646860 27028 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0218 13:04:16.364409 27028 solver.cpp:357] Iteration 12900 (2.79976 iter/s, 35.7174s/100 iters), loss = 0.468171
I0218 13:04:16.364578 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0785359 (* 1 = 0.0785359 loss)
I0218 13:04:16.364588 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.096755 (* 1 = 0.096755 loss)
I0218 13:04:16.364596 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.145726 (* 1 = 0.145726 loss)
I0218 13:04:16.364604 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.147154 (* 1 = 0.147154 loss)
I0218 13:04:16.364611 27028 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0218 13:04:51.749090 27028 solver.cpp:514] Iteration 13000, Testing net (#0)
I0218 13:04:54.218533 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.282698 (* 1 = 0.282698 loss)
I0218 13:04:54.218581 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.501883 (* 1 = 0.501883 loss)
I0218 13:04:54.218590 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.415929 (* 1 = 0.415929 loss)
I0218 13:04:54.218597 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.37228 (* 1 = 0.37228 loss)
I0218 13:04:54.218603 27028 solver.cpp:580]     Test net output #4: prob1 = 0.928
I0218 13:04:54.218610 27028 solver.cpp:580]     Test net output #5: prob2 = 0.888
I0218 13:04:54.218614 27028 solver.cpp:580]     Test net output #6: prob3 = 0.892
I0218 13:04:54.218621 27028 solver.cpp:580]     Test net output #7: prob4 = 0.905
I0218 13:04:54.554034 27028 solver.cpp:357] Iteration 13000 (2.61853 iter/s, 38.1893s/100 iters), loss = 0.707942
I0218 13:04:54.554091 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.167749 (* 1 = 0.167749 loss)
I0218 13:04:54.554101 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.15582 (* 1 = 0.15582 loss)
I0218 13:04:54.554109 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.192791 (* 1 = 0.192791 loss)
I0218 13:04:54.554117 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.191582 (* 1 = 0.191582 loss)
I0218 13:04:54.554126 27028 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0218 13:05:25.407258 27028 solver.cpp:357] Iteration 13100 (3.24117 iter/s, 30.8531s/100 iters), loss = 0.482822
I0218 13:05:25.407420 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0762536 (* 1 = 0.0762536 loss)
I0218 13:05:25.407431 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.160434 (* 1 = 0.160434 loss)
I0218 13:05:25.407482 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.155681 (* 1 = 0.155681 loss)
I0218 13:05:25.407490 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0904532 (* 1 = 0.0904532 loss)
I0218 13:05:25.407500 27028 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0218 13:05:58.917709 27028 solver.cpp:357] Iteration 13200 (2.98417 iter/s, 33.5102s/100 iters), loss = 0.586494
I0218 13:05:58.917876 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.165442 (* 1 = 0.165442 loss)
I0218 13:05:58.917886 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137325 (* 1 = 0.137325 loss)
I0218 13:05:58.917894 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.201987 (* 1 = 0.201987 loss)
I0218 13:05:58.917903 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0817399 (* 1 = 0.0817399 loss)
I0218 13:05:58.917912 27028 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0218 13:06:34.623637 27028 solver.cpp:357] Iteration 13300 (2.80067 iter/s, 35.7057s/100 iters), loss = 0.615449
I0218 13:06:34.623790 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139707 (* 1 = 0.139707 loss)
I0218 13:06:34.623800 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.153742 (* 1 = 0.153742 loss)
I0218 13:06:34.623808 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134777 (* 1 = 0.134777 loss)
I0218 13:06:34.623816 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.187222 (* 1 = 0.187222 loss)
I0218 13:06:34.623823 27028 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0218 13:07:10.337515 27028 solver.cpp:357] Iteration 13400 (2.80005 iter/s, 35.7137s/100 iters), loss = 0.448199
I0218 13:07:10.337633 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.098895 (* 1 = 0.098895 loss)
I0218 13:07:10.337644 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.112878 (* 1 = 0.112878 loss)
I0218 13:07:10.337652 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.115687 (* 1 = 0.115687 loss)
I0218 13:07:10.337661 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.120739 (* 1 = 0.120739 loss)
I0218 13:07:10.337667 27028 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0218 13:07:46.042894 27028 solver.cpp:357] Iteration 13500 (2.80071 iter/s, 35.7053s/100 iters), loss = 0.509234
I0218 13:07:46.043193 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.119298 (* 1 = 0.119298 loss)
I0218 13:07:46.043205 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.146283 (* 1 = 0.146283 loss)
I0218 13:07:46.043213 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.093405 (* 1 = 0.093405 loss)
I0218 13:07:46.043221 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.150248 (* 1 = 0.150248 loss)
I0218 13:07:46.043228 27028 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0218 13:08:18.207361 27028 solver.cpp:357] Iteration 13600 (3.10905 iter/s, 32.1642s/100 iters), loss = 0.608183
I0218 13:08:18.207525 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.171062 (* 1 = 0.171062 loss)
I0218 13:08:18.207536 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.176917 (* 1 = 0.176917 loss)
I0218 13:08:18.207545 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.153789 (* 1 = 0.153789 loss)
I0218 13:08:18.207552 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.106416 (* 1 = 0.106416 loss)
I0218 13:08:18.207561 27028 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0218 13:08:50.465270 27028 solver.cpp:357] Iteration 13700 (3.10003 iter/s, 32.2578s/100 iters), loss = 0.567035
I0218 13:08:50.465404 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.155463 (* 1 = 0.155463 loss)
I0218 13:08:50.465415 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.155313 (* 1 = 0.155313 loss)
I0218 13:08:50.465423 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.119709 (* 1 = 0.119709 loss)
I0218 13:08:50.465431 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.13655 (* 1 = 0.13655 loss)
I0218 13:08:50.465438 27028 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0218 13:09:26.174337 27028 solver.cpp:357] Iteration 13800 (2.80041 iter/s, 35.709s/100 iters), loss = 0.699302
I0218 13:09:26.174495 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.204577 (* 1 = 0.204577 loss)
I0218 13:09:26.174505 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.148473 (* 1 = 0.148473 loss)
I0218 13:09:26.174515 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.214714 (* 1 = 0.214714 loss)
I0218 13:09:26.174522 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.131538 (* 1 = 0.131538 loss)
I0218 13:09:26.174530 27028 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0218 13:10:01.892614 27028 solver.cpp:357] Iteration 13900 (2.79969 iter/s, 35.7182s/100 iters), loss = 0.523937
I0218 13:10:01.892683 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139959 (* 1 = 0.139959 loss)
I0218 13:10:01.892694 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108548 (* 1 = 0.108548 loss)
I0218 13:10:01.892704 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.128414 (* 1 = 0.128414 loss)
I0218 13:10:01.892710 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.147015 (* 1 = 0.147015 loss)
I0218 13:10:01.892717 27028 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0218 13:10:37.274601 27028 solver.cpp:514] Iteration 14000, Testing net (#0)
I0218 13:10:39.741684 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.30524 (* 1 = 1.30524 loss)
I0218 13:10:39.741729 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.5335 (* 1 = 1.5335 loss)
I0218 13:10:39.741736 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.71288 (* 1 = 1.71288 loss)
I0218 13:10:39.741745 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.44808 (* 1 = 1.44808 loss)
I0218 13:10:39.741751 27028 solver.cpp:580]     Test net output #4: prob1 = 0.676
I0218 13:10:39.741758 27028 solver.cpp:580]     Test net output #5: prob2 = 0.614
I0218 13:10:39.741763 27028 solver.cpp:580]     Test net output #6: prob3 = 0.576
I0218 13:10:39.741768 27028 solver.cpp:580]     Test net output #7: prob4 = 0.646
I0218 13:10:40.082944 27028 solver.cpp:357] Iteration 14000 (2.61846 iter/s, 38.1904s/100 iters), loss = 0.552501
I0218 13:10:40.082998 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.163138 (* 1 = 0.163138 loss)
I0218 13:10:40.083009 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.112658 (* 1 = 0.112658 loss)
I0218 13:10:40.083016 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104723 (* 1 = 0.104723 loss)
I0218 13:10:40.083024 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.171982 (* 1 = 0.171982 loss)
I0218 13:10:40.083032 27028 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0218 13:11:12.506268 27028 solver.cpp:357] Iteration 14100 (3.0842 iter/s, 32.4234s/100 iters), loss = 0.520336
I0218 13:11:12.506464 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.113197 (* 1 = 0.113197 loss)
I0218 13:11:12.506474 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.144948 (* 1 = 0.144948 loss)
I0218 13:11:12.506482 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.153273 (* 1 = 0.153273 loss)
I0218 13:11:12.506490 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.108919 (* 1 = 0.108919 loss)
I0218 13:11:12.506500 27028 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0218 13:11:44.547024 27028 solver.cpp:357] Iteration 14200 (3.12122 iter/s, 32.0387s/100 iters), loss = 0.464818
I0218 13:11:44.547152 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.168256 (* 1 = 0.168256 loss)
I0218 13:11:44.547163 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.111059 (* 1 = 0.111059 loss)
I0218 13:11:44.547173 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134162 (* 1 = 0.134162 loss)
I0218 13:11:44.547180 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0513406 (* 1 = 0.0513406 loss)
I0218 13:11:44.547188 27028 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0218 13:12:20.245579 27028 solver.cpp:357] Iteration 14300 (2.80123 iter/s, 35.6986s/100 iters), loss = 0.448353
I0218 13:12:20.245741 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.115946 (* 1 = 0.115946 loss)
I0218 13:12:20.245752 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0628618 (* 1 = 0.0628618 loss)
I0218 13:12:20.245761 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0796421 (* 1 = 0.0796421 loss)
I0218 13:12:20.245769 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.189903 (* 1 = 0.189903 loss)
I0218 13:12:20.245776 27028 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0218 13:12:55.967927 27028 solver.cpp:357] Iteration 14400 (2.79937 iter/s, 35.7224s/100 iters), loss = 0.546365
I0218 13:12:55.968082 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0439068 (* 1 = 0.0439068 loss)
I0218 13:12:55.968092 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.234157 (* 1 = 0.234157 loss)
I0218 13:12:55.968101 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.151851 (* 1 = 0.151851 loss)
I0218 13:12:55.968108 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.11645 (* 1 = 0.11645 loss)
I0218 13:12:55.968120 27028 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0218 13:13:31.692430 27028 solver.cpp:357] Iteration 14500 (2.7992 iter/s, 35.7245s/100 iters), loss = 0.611497
I0218 13:13:31.692544 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.15147 (* 1 = 0.15147 loss)
I0218 13:13:31.692554 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.208533 (* 1 = 0.208533 loss)
I0218 13:13:31.692564 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.091263 (* 1 = 0.091263 loss)
I0218 13:13:31.692571 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.160231 (* 1 = 0.160231 loss)
I0218 13:13:31.692579 27028 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0218 13:14:05.337064 27028 solver.cpp:357] Iteration 14600 (2.97224 iter/s, 33.6447s/100 iters), loss = 0.666355
I0218 13:14:05.337215 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.160139 (* 1 = 0.160139 loss)
I0218 13:14:05.337225 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.201454 (* 1 = 0.201454 loss)
I0218 13:14:05.337234 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.126396 (* 1 = 0.126396 loss)
I0218 13:14:05.337240 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.178367 (* 1 = 0.178367 loss)
I0218 13:14:05.337254 27028 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0218 13:14:36.152096 27028 solver.cpp:357] Iteration 14700 (3.24517 iter/s, 30.815s/100 iters), loss = 0.447251
I0218 13:14:36.152415 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.149735 (* 1 = 0.149735 loss)
I0218 13:14:36.152492 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.118138 (* 1 = 0.118138 loss)
I0218 13:14:36.152534 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.119478 (* 1 = 0.119478 loss)
I0218 13:14:36.152575 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0598997 (* 1 = 0.0598997 loss)
I0218 13:14:36.152609 27028 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0218 13:15:11.886534 27028 solver.cpp:357] Iteration 14800 (2.79859 iter/s, 35.7323s/100 iters), loss = 0.494687
I0218 13:15:11.886788 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.138349 (* 1 = 0.138349 loss)
I0218 13:15:11.886799 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.102288 (* 1 = 0.102288 loss)
I0218 13:15:11.886808 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.125992 (* 1 = 0.125992 loss)
I0218 13:15:11.886816 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.128057 (* 1 = 0.128057 loss)
I0218 13:15:11.886824 27028 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0218 13:15:47.613579 27028 solver.cpp:357] Iteration 14900 (2.79915 iter/s, 35.7251s/100 iters), loss = 0.524686
I0218 13:15:47.613745 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0949714 (* 1 = 0.0949714 loss)
I0218 13:15:47.613756 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.158378 (* 1 = 0.158378 loss)
I0218 13:15:47.613765 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134043 (* 1 = 0.134043 loss)
I0218 13:15:47.613772 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.137294 (* 1 = 0.137294 loss)
I0218 13:15:47.613785 27028 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0218 13:16:23.051259 27028 solver.cpp:514] Iteration 15000, Testing net (#0)
I0218 13:16:25.413100 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.718096 (* 1 = 0.718096 loss)
I0218 13:16:25.413151 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.03401 (* 1 = 1.03401 loss)
I0218 13:16:25.413158 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.22884 (* 1 = 1.22884 loss)
I0218 13:16:25.413168 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.933246 (* 1 = 0.933246 loss)
I0218 13:16:25.413174 27028 solver.cpp:580]     Test net output #4: prob1 = 0.825
I0218 13:16:25.413180 27028 solver.cpp:580]     Test net output #5: prob2 = 0.748
I0218 13:16:25.413185 27028 solver.cpp:580]     Test net output #6: prob3 = 0.692
I0218 13:16:25.413192 27028 solver.cpp:580]     Test net output #7: prob4 = 0.778
I0218 13:16:25.748505 27028 solver.cpp:357] Iteration 15000 (2.6224 iter/s, 38.133s/100 iters), loss = 0.650227
I0218 13:16:25.748560 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.12829 (* 1 = 0.12829 loss)
I0218 13:16:25.748572 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.177018 (* 1 = 0.177018 loss)
I0218 13:16:25.748580 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.185516 (* 1 = 0.185516 loss)
I0218 13:16:25.748589 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.159402 (* 1 = 0.159402 loss)
I0218 13:16:25.748595 27028 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0218 13:16:59.727752 27028 solver.cpp:357] Iteration 15100 (2.94296 iter/s, 33.9794s/100 iters), loss = 0.549303
I0218 13:16:59.727911 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.122675 (* 1 = 0.122675 loss)
I0218 13:16:59.727921 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.145379 (* 1 = 0.145379 loss)
I0218 13:16:59.727929 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.136455 (* 1 = 0.136455 loss)
I0218 13:16:59.727937 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144795 (* 1 = 0.144795 loss)
I0218 13:16:59.727946 27028 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0218 13:17:30.238369 27028 solver.cpp:357] Iteration 15200 (3.27754 iter/s, 30.5107s/100 iters), loss = 0.966286
I0218 13:17:30.238503 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.161333 (* 1 = 0.161333 loss)
I0218 13:17:30.238513 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.298423 (* 1 = 0.298423 loss)
I0218 13:17:30.238523 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.300289 (* 1 = 0.300289 loss)
I0218 13:17:30.238530 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.206241 (* 1 = 0.206241 loss)
I0218 13:17:30.238538 27028 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0218 13:18:05.957278 27028 solver.cpp:357] Iteration 15300 (2.79978 iter/s, 35.717s/100 iters), loss = 0.57197
I0218 13:18:05.957458 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143761 (* 1 = 0.143761 loss)
I0218 13:18:05.957468 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.126436 (* 1 = 0.126436 loss)
I0218 13:18:05.957478 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.158994 (* 1 = 0.158994 loss)
I0218 13:18:05.957485 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.142778 (* 1 = 0.142778 loss)
I0218 13:18:05.957492 27028 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0218 13:18:41.719772 27028 solver.cpp:357] Iteration 15400 (2.79615 iter/s, 35.7635s/100 iters), loss = 0.510277
I0218 13:18:41.719944 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139936 (* 1 = 0.139936 loss)
I0218 13:18:41.719954 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.102297 (* 1 = 0.102297 loss)
I0218 13:18:41.719964 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.142978 (* 1 = 0.142978 loss)
I0218 13:18:41.719971 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.125065 (* 1 = 0.125065 loss)
I0218 13:18:41.719979 27028 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0218 13:19:17.490938 27028 solver.cpp:357] Iteration 15500 (2.79557 iter/s, 35.7709s/100 iters), loss = 0.633201
I0218 13:19:17.491096 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.135161 (* 1 = 0.135161 loss)
I0218 13:19:17.491107 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.177612 (* 1 = 0.177612 loss)
I0218 13:19:17.491116 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.156643 (* 1 = 0.156643 loss)
I0218 13:19:17.491123 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.163786 (* 1 = 0.163786 loss)
I0218 13:19:17.491130 27028 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0218 13:19:52.563453 27028 solver.cpp:357] Iteration 15600 (2.85111 iter/s, 35.0741s/100 iters), loss = 0.548801
I0218 13:19:52.563561 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.152015 (* 1 = 0.152015 loss)
I0218 13:19:52.563573 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.118063 (* 1 = 0.118063 loss)
I0218 13:19:52.563582 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.115586 (* 1 = 0.115586 loss)
I0218 13:19:52.563589 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.163137 (* 1 = 0.163137 loss)
I0218 13:19:52.563597 27028 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0218 13:20:21.874519 27028 solver.cpp:357] Iteration 15700 (3.41153 iter/s, 29.3123s/100 iters), loss = 0.741006
I0218 13:20:21.874594 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.109046 (* 1 = 0.109046 loss)
I0218 13:20:21.874604 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.337344 (* 1 = 0.337344 loss)
I0218 13:20:21.874614 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.171847 (* 1 = 0.171847 loss)
I0218 13:20:21.874620 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.122769 (* 1 = 0.122769 loss)
I0218 13:20:21.874627 27028 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0218 13:20:57.585625 27028 solver.cpp:357] Iteration 15800 (2.80013 iter/s, 35.7127s/100 iters), loss = 0.448977
I0218 13:20:57.585785 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.103957 (* 1 = 0.103957 loss)
I0218 13:20:57.585795 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0650633 (* 1 = 0.0650633 loss)
I0218 13:20:57.585804 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.161255 (* 1 = 0.161255 loss)
I0218 13:20:57.585813 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.118702 (* 1 = 0.118702 loss)
I0218 13:20:57.585822 27028 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0218 13:21:33.309453 27028 solver.cpp:357] Iteration 15900 (2.79914 iter/s, 35.7252s/100 iters), loss = 0.610543
I0218 13:21:33.309670 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.100979 (* 1 = 0.100979 loss)
I0218 13:21:33.309681 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.123274 (* 1 = 0.123274 loss)
I0218 13:21:33.309690 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.202204 (* 1 = 0.202204 loss)
I0218 13:21:33.309697 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.184087 (* 1 = 0.184087 loss)
I0218 13:21:33.309705 27028 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0218 13:22:08.691823 27028 solver.cpp:514] Iteration 16000, Testing net (#0)
I0218 13:22:11.158816 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.18691 (* 1 = 2.18691 loss)
I0218 13:22:11.158861 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.62928 (* 1 = 2.62928 loss)
I0218 13:22:11.158869 27028 solver.cpp:580]     Test net output #2: Softmax3 = 2.95644 (* 1 = 2.95644 loss)
I0218 13:22:11.158880 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.26217 (* 1 = 2.26217 loss)
I0218 13:22:11.158886 27028 solver.cpp:580]     Test net output #4: prob1 = 0.488
I0218 13:22:11.158893 27028 solver.cpp:580]     Test net output #5: prob2 = 0.441
I0218 13:22:11.158900 27028 solver.cpp:580]     Test net output #6: prob3 = 0.453
I0218 13:22:11.158905 27028 solver.cpp:580]     Test net output #7: prob4 = 0.494
I0218 13:22:11.494065 27028 solver.cpp:357] Iteration 16000 (2.61876 iter/s, 38.186s/100 iters), loss = 0.496218
I0218 13:22:11.494118 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.111083 (* 1 = 0.111083 loss)
I0218 13:22:11.494128 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.133463 (* 1 = 0.133463 loss)
I0218 13:22:11.494137 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.109429 (* 1 = 0.109429 loss)
I0218 13:22:11.494143 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.142244 (* 1 = 0.142244 loss)
I0218 13:22:11.494156 27028 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0218 13:22:46.911610 27028 solver.cpp:357] Iteration 16100 (2.82335 iter/s, 35.4189s/100 iters), loss = 0.576083
I0218 13:22:46.911727 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.128603 (* 1 = 0.128603 loss)
I0218 13:22:46.911738 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.182709 (* 1 = 0.182709 loss)
I0218 13:22:46.911746 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.176367 (* 1 = 0.176367 loss)
I0218 13:22:46.911756 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0884035 (* 1 = 0.0884035 loss)
I0218 13:22:46.911763 27028 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0218 13:23:15.798036 27028 solver.cpp:357] Iteration 16200 (3.46171 iter/s, 28.8874s/100 iters), loss = 0.543406
I0218 13:23:15.798110 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0986112 (* 1 = 0.0986112 loss)
I0218 13:23:15.798120 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.131426 (* 1 = 0.131426 loss)
I0218 13:23:15.798130 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.247335 (* 1 = 0.247335 loss)
I0218 13:23:15.798137 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0660344 (* 1 = 0.0660344 loss)
I0218 13:23:15.798144 27028 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0218 13:23:51.515846 27028 solver.cpp:357] Iteration 16300 (2.79963 iter/s, 35.7191s/100 iters), loss = 0.542476
I0218 13:23:51.516008 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.105401 (* 1 = 0.105401 loss)
I0218 13:23:51.516019 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.194805 (* 1 = 0.194805 loss)
I0218 13:23:51.516027 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0993504 (* 1 = 0.0993504 loss)
I0218 13:23:51.516036 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.14292 (* 1 = 0.14292 loss)
I0218 13:23:51.516042 27028 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0218 13:24:27.230283 27028 solver.cpp:357] Iteration 16400 (2.7999 iter/s, 35.7156s/100 iters), loss = 0.595982
I0218 13:24:27.230463 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.187736 (* 1 = 0.187736 loss)
I0218 13:24:27.230473 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.138219 (* 1 = 0.138219 loss)
I0218 13:24:27.230481 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.139582 (* 1 = 0.139582 loss)
I0218 13:24:27.230489 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.130445 (* 1 = 0.130445 loss)
I0218 13:24:27.230497 27028 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0218 13:25:02.947435 27028 solver.cpp:357] Iteration 16500 (2.79969 iter/s, 35.7182s/100 iters), loss = 0.444884
I0218 13:25:02.947571 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.123624 (* 1 = 0.123624 loss)
I0218 13:25:02.947582 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.121602 (* 1 = 0.121602 loss)
I0218 13:25:02.947590 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.106302 (* 1 = 0.106302 loss)
I0218 13:25:02.947598 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0933557 (* 1 = 0.0933557 loss)
I0218 13:25:02.947607 27028 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0218 13:25:38.659456 27028 solver.cpp:357] Iteration 16600 (2.8001 iter/s, 35.7131s/100 iters), loss = 0.759358
I0218 13:25:38.659574 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.112266 (* 1 = 0.112266 loss)
I0218 13:25:38.659584 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.243492 (* 1 = 0.243492 loss)
I0218 13:25:38.659593 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.19713 (* 1 = 0.19713 loss)
I0218 13:25:38.659601 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.20647 (* 1 = 0.20647 loss)
I0218 13:25:38.659608 27028 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0218 13:26:07.388453 27028 solver.cpp:357] Iteration 16700 (3.48071 iter/s, 28.7298s/100 iters), loss = 0.449941
I0218 13:26:07.388533 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0724848 (* 1 = 0.0724848 loss)
I0218 13:26:07.388543 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.120288 (* 1 = 0.120288 loss)
I0218 13:26:07.388552 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.156709 (* 1 = 0.156709 loss)
I0218 13:26:07.388561 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.10046 (* 1 = 0.10046 loss)
I0218 13:26:07.388567 27028 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0218 13:26:43.136878 27028 solver.cpp:357] Iteration 16800 (2.79741 iter/s, 35.7474s/100 iters), loss = 0.654667
I0218 13:26:43.137030 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143677 (* 1 = 0.143677 loss)
I0218 13:26:43.137040 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.156499 (* 1 = 0.156499 loss)
I0218 13:26:43.137049 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.182916 (* 1 = 0.182916 loss)
I0218 13:26:43.137058 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.171576 (* 1 = 0.171576 loss)
I0218 13:26:43.137064 27028 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0218 13:27:18.858593 27028 solver.cpp:357] Iteration 16900 (2.79935 iter/s, 35.7226s/100 iters), loss = 0.507542
I0218 13:27:18.858741 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.133267 (* 1 = 0.133267 loss)
I0218 13:27:18.858750 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.103264 (* 1 = 0.103264 loss)
I0218 13:27:18.858759 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.208275 (* 1 = 0.208275 loss)
I0218 13:27:18.858767 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0627358 (* 1 = 0.0627358 loss)
I0218 13:27:18.858774 27028 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0218 13:27:54.187299 27028 solver.cpp:514] Iteration 17000, Testing net (#0)
I0218 13:27:56.585412 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.27296 (* 1 = 2.27296 loss)
I0218 13:27:56.585458 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.7414 (* 1 = 1.7414 loss)
I0218 13:27:56.585466 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.44507 (* 1 = 1.44507 loss)
I0218 13:27:56.585475 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.15074 (* 1 = 1.15074 loss)
I0218 13:27:56.585481 27028 solver.cpp:580]     Test net output #4: prob1 = 0.481
I0218 13:27:56.585489 27028 solver.cpp:580]     Test net output #5: prob2 = 0.603
I0218 13:27:56.585494 27028 solver.cpp:580]     Test net output #6: prob3 = 0.647
I0218 13:27:56.585500 27028 solver.cpp:580]     Test net output #7: prob4 = 0.685
I0218 13:27:56.919632 27028 solver.cpp:357] Iteration 17000 (2.62729 iter/s, 38.062s/100 iters), loss = 0.596611
I0218 13:27:56.919690 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.15876 (* 1 = 0.15876 loss)
I0218 13:27:56.919701 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.114691 (* 1 = 0.114691 loss)
I0218 13:27:56.919709 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.175393 (* 1 = 0.175393 loss)
I0218 13:27:56.919718 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.147766 (* 1 = 0.147766 loss)
I0218 13:27:56.919724 27028 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0218 13:28:32.649911 27028 solver.cpp:357] Iteration 17100 (2.79867 iter/s, 35.7312s/100 iters), loss = 0.341682
I0218 13:28:32.650151 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110408 (* 1 = 0.110408 loss)
I0218 13:28:32.650162 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0695587 (* 1 = 0.0695587 loss)
I0218 13:28:32.650172 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0907879 (* 1 = 0.0907879 loss)
I0218 13:28:32.650180 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0709273 (* 1 = 0.0709273 loss)
I0218 13:28:32.650187 27028 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0218 13:29:01.389415 27028 solver.cpp:357] Iteration 17200 (3.4797 iter/s, 28.7381s/100 iters), loss = 0.544112
I0218 13:29:01.389472 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0947028 (* 1 = 0.0947028 loss)
I0218 13:29:01.389483 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.102085 (* 1 = 0.102085 loss)
I0218 13:29:01.389492 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.182823 (* 1 = 0.182823 loss)
I0218 13:29:01.389500 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.164501 (* 1 = 0.164501 loss)
I0218 13:29:01.389508 27028 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0218 13:29:37.101630 27028 solver.cpp:357] Iteration 17300 (2.80009 iter/s, 35.7131s/100 iters), loss = 0.366225
I0218 13:29:37.101763 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0866012 (* 1 = 0.0866012 loss)
I0218 13:29:37.101773 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.121218 (* 1 = 0.121218 loss)
I0218 13:29:37.101783 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.109419 (* 1 = 0.109419 loss)
I0218 13:29:37.101790 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0489869 (* 1 = 0.0489869 loss)
I0218 13:29:37.101797 27028 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0218 13:30:12.789857 27028 solver.cpp:357] Iteration 17400 (2.80198 iter/s, 35.689s/100 iters), loss = 0.544028
I0218 13:30:12.790088 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.153012 (* 1 = 0.153012 loss)
I0218 13:30:12.790115 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.106847 (* 1 = 0.106847 loss)
I0218 13:30:12.790135 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.170765 (* 1 = 0.170765 loss)
I0218 13:30:12.790154 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.113404 (* 1 = 0.113404 loss)
I0218 13:30:12.790172 27028 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0218 13:30:48.490305 27028 solver.cpp:357] Iteration 17500 (2.80118 iter/s, 35.6992s/100 iters), loss = 0.544922
I0218 13:30:48.490460 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.117167 (* 1 = 0.117167 loss)
I0218 13:30:48.490473 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.122085 (* 1 = 0.122085 loss)
I0218 13:30:48.490481 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.130467 (* 1 = 0.130467 loss)
I0218 13:30:48.490489 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.175203 (* 1 = 0.175203 loss)
I0218 13:30:48.490497 27028 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0218 13:31:24.213989 27028 solver.cpp:357] Iteration 17600 (2.79936 iter/s, 35.7224s/100 iters), loss = 0.63539
I0218 13:31:24.214166 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143492 (* 1 = 0.143492 loss)
I0218 13:31:24.214177 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.187846 (* 1 = 0.187846 loss)
I0218 13:31:24.214185 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.172687 (* 1 = 0.172687 loss)
I0218 13:31:24.214193 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.131365 (* 1 = 0.131365 loss)
I0218 13:31:24.214200 27028 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0218 13:31:54.130899 27028 solver.cpp:357] Iteration 17700 (3.34253 iter/s, 29.9174s/100 iters), loss = 0.473263
I0218 13:31:54.130970 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.157581 (* 1 = 0.157581 loss)
I0218 13:31:54.130981 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.113801 (* 1 = 0.113801 loss)
I0218 13:31:54.130990 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.122512 (* 1 = 0.122512 loss)
I0218 13:31:54.130998 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0793688 (* 1 = 0.0793688 loss)
I0218 13:31:54.131006 27028 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0218 13:32:28.444067 27028 solver.cpp:357] Iteration 17800 (2.91427 iter/s, 34.3139s/100 iters), loss = 0.41772
I0218 13:32:28.444247 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.105039 (* 1 = 0.105039 loss)
I0218 13:32:28.444258 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0870158 (* 1 = 0.0870158 loss)
I0218 13:32:28.444268 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.116169 (* 1 = 0.116169 loss)
I0218 13:32:28.444275 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.109496 (* 1 = 0.109496 loss)
I0218 13:32:28.444283 27028 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0218 13:33:04.237629 27028 solver.cpp:357] Iteration 17900 (2.7939 iter/s, 35.7922s/100 iters), loss = 0.441136
I0218 13:33:04.237748 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0833068 (* 1 = 0.0833068 loss)
I0218 13:33:04.237758 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.104228 (* 1 = 0.104228 loss)
I0218 13:33:04.237767 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.117498 (* 1 = 0.117498 loss)
I0218 13:33:04.237776 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136104 (* 1 = 0.136104 loss)
I0218 13:33:04.237784 27028 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0218 13:33:39.594785 27028 solver.cpp:514] Iteration 18000, Testing net (#0)
I0218 13:33:41.999532 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.01086 (* 1 = 1.01086 loss)
I0218 13:33:41.999578 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.02331 (* 1 = 1.02331 loss)
I0218 13:33:41.999588 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.32492 (* 1 = 1.32492 loss)
I0218 13:33:41.999594 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.51638 (* 1 = 2.51638 loss)
I0218 13:33:41.999600 27028 solver.cpp:580]     Test net output #4: prob1 = 0.713
I0218 13:33:41.999608 27028 solver.cpp:580]     Test net output #5: prob2 = 0.747
I0218 13:33:41.999614 27028 solver.cpp:580]     Test net output #6: prob3 = 0.657
I0218 13:33:41.999619 27028 solver.cpp:580]     Test net output #7: prob4 = 0.438
I0218 13:33:42.335439 27028 solver.cpp:357] Iteration 18000 (2.62491 iter/s, 38.0965s/100 iters), loss = 0.343962
I0218 13:33:42.335495 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0959111 (* 1 = 0.0959111 loss)
I0218 13:33:42.335505 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0765244 (* 1 = 0.0765244 loss)
I0218 13:33:42.335515 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0692581 (* 1 = 0.0692581 loss)
I0218 13:33:42.335522 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.102268 (* 1 = 0.102268 loss)
I0218 13:33:42.335530 27028 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0218 13:34:18.049468 27028 solver.cpp:357] Iteration 18100 (2.79996 iter/s, 35.7147s/100 iters), loss = 0.471171
I0218 13:34:18.049692 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0719847 (* 1 = 0.0719847 loss)
I0218 13:34:18.049703 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.135119 (* 1 = 0.135119 loss)
I0218 13:34:18.049712 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.157603 (* 1 = 0.157603 loss)
I0218 13:34:18.049721 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.106465 (* 1 = 0.106465 loss)
I0218 13:34:18.049727 27028 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0218 13:34:48.500231 27028 solver.cpp:357] Iteration 18200 (3.28416 iter/s, 30.4492s/100 iters), loss = 0.535961
I0218 13:34:48.500393 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0849671 (* 1 = 0.0849671 loss)
I0218 13:34:48.500403 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.112817 (* 1 = 0.112817 loss)
I0218 13:34:48.500412 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.199346 (* 1 = 0.199346 loss)
I0218 13:34:48.500419 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.13883 (* 1 = 0.13883 loss)
I0218 13:34:48.500427 27028 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0218 13:35:22.554692 27028 solver.cpp:357] Iteration 18300 (2.93642 iter/s, 34.055s/100 iters), loss = 0.592359
I0218 13:35:22.554852 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.108866 (* 1 = 0.108866 loss)
I0218 13:35:22.554863 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.188507 (* 1 = 0.188507 loss)
I0218 13:35:22.554872 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.14054 (* 1 = 0.14054 loss)
I0218 13:35:22.554880 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.154446 (* 1 = 0.154446 loss)
I0218 13:35:22.554888 27028 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0218 13:35:58.286051 27028 solver.cpp:357] Iteration 18400 (2.79862 iter/s, 35.7319s/100 iters), loss = 0.603515
I0218 13:35:58.286160 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.133 (* 1 = 0.133 loss)
I0218 13:35:58.286170 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.158367 (* 1 = 0.158367 loss)
I0218 13:35:58.286180 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.222174 (* 1 = 0.222174 loss)
I0218 13:35:58.286188 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0899733 (* 1 = 0.0899733 loss)
I0218 13:35:58.286195 27028 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0218 13:36:33.990801 27028 solver.cpp:357] Iteration 18500 (2.8007 iter/s, 35.7054s/100 iters), loss = 0.664489
I0218 13:36:33.990917 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.108393 (* 1 = 0.108393 loss)
I0218 13:36:33.990928 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.174241 (* 1 = 0.174241 loss)
I0218 13:36:33.990936 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.163978 (* 1 = 0.163978 loss)
I0218 13:36:33.990944 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.217878 (* 1 = 0.217878 loss)
I0218 13:36:33.990952 27028 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0218 13:37:09.729007 27028 solver.cpp:357] Iteration 18600 (2.79808 iter/s, 35.7388s/100 iters), loss = 0.505207
I0218 13:37:09.729188 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.152376 (* 1 = 0.152376 loss)
I0218 13:37:09.729200 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.087711 (* 1 = 0.087711 loss)
I0218 13:37:09.729209 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.147267 (* 1 = 0.147267 loss)
I0218 13:37:09.729218 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.117853 (* 1 = 0.117853 loss)
I0218 13:37:09.729228 27028 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0218 13:37:41.418905 27028 solver.cpp:357] Iteration 18700 (3.15573 iter/s, 31.6884s/100 iters), loss = 0.446013
I0218 13:37:41.419026 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0661048 (* 1 = 0.0661048 loss)
I0218 13:37:41.419037 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.148047 (* 1 = 0.148047 loss)
I0218 13:37:41.419046 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.11875 (* 1 = 0.11875 loss)
I0218 13:37:41.419054 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.113111 (* 1 = 0.113111 loss)
I0218 13:37:41.419062 27028 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0218 13:38:14.242574 27028 solver.cpp:357] Iteration 18800 (3.04672 iter/s, 32.8222s/100 iters), loss = 0.525901
I0218 13:38:14.242781 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.217579 (* 1 = 0.217579 loss)
I0218 13:38:14.242794 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.114374 (* 1 = 0.114374 loss)
I0218 13:38:14.242802 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.12598 (* 1 = 0.12598 loss)
I0218 13:38:14.242810 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0679677 (* 1 = 0.0679677 loss)
I0218 13:38:14.242818 27028 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0218 13:38:50.034811 27028 solver.cpp:357] Iteration 18900 (2.7939 iter/s, 35.7922s/100 iters), loss = 0.604044
I0218 13:38:50.034931 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.190308 (* 1 = 0.190308 loss)
I0218 13:38:50.034942 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0958562 (* 1 = 0.0958562 loss)
I0218 13:38:50.034951 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.201108 (* 1 = 0.201108 loss)
I0218 13:38:50.034958 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.116771 (* 1 = 0.116771 loss)
I0218 13:38:50.034966 27028 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0218 13:39:25.317162 27028 solver.cpp:514] Iteration 19000, Testing net (#0)
I0218 13:39:27.789160 27028 solver.cpp:580]     Test net output #0: Softmax1 = 5.3197 (* 1 = 5.3197 loss)
I0218 13:39:27.789208 27028 solver.cpp:580]     Test net output #1: Softmax2 = 9.9451 (* 1 = 9.9451 loss)
I0218 13:39:27.789219 27028 solver.cpp:580]     Test net output #2: Softmax3 = 10.5675 (* 1 = 10.5675 loss)
I0218 13:39:27.789227 27028 solver.cpp:580]     Test net output #3: Softmax4 = 11.678 (* 1 = 11.678 loss)
I0218 13:39:27.789234 27028 solver.cpp:580]     Test net output #4: prob1 = 0.148
I0218 13:39:27.789240 27028 solver.cpp:580]     Test net output #5: prob2 = 0.078
I0218 13:39:27.789247 27028 solver.cpp:580]     Test net output #6: prob3 = 0.044
I0218 13:39:27.789252 27028 solver.cpp:580]     Test net output #7: prob4 = 0.02
I0218 13:39:28.125296 27028 solver.cpp:357] Iteration 19000 (2.62529 iter/s, 38.0911s/100 iters), loss = 0.415992
I0218 13:39:28.125351 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129714 (* 1 = 0.129714 loss)
I0218 13:39:28.125361 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.115762 (* 1 = 0.115762 loss)
I0218 13:39:28.125370 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0889086 (* 1 = 0.0889086 loss)
I0218 13:39:28.125377 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0816073 (* 1 = 0.0816073 loss)
I0218 13:39:28.125385 27028 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0218 13:40:03.835804 27028 solver.cpp:357] Iteration 19100 (2.80025 iter/s, 35.7111s/100 iters), loss = 0.547029
I0218 13:40:03.835937 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.107862 (* 1 = 0.107862 loss)
I0218 13:40:03.835947 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.110877 (* 1 = 0.110877 loss)
I0218 13:40:03.835954 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.233504 (* 1 = 0.233504 loss)
I0218 13:40:03.835963 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0947863 (* 1 = 0.0947863 loss)
I0218 13:40:03.835970 27028 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0218 13:40:35.751719 27028 solver.cpp:357] Iteration 19200 (3.13319 iter/s, 31.9164s/100 iters), loss = 0.465607
I0218 13:40:35.751878 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.13151 (* 1 = 0.13151 loss)
I0218 13:40:35.751888 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.121383 (* 1 = 0.121383 loss)
I0218 13:40:35.751896 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0829081 (* 1 = 0.0829081 loss)
I0218 13:40:35.751904 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.129806 (* 1 = 0.129806 loss)
I0218 13:40:35.751912 27028 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0218 13:41:08.314370 27028 solver.cpp:357] Iteration 19300 (3.07096 iter/s, 32.5631s/100 iters), loss = 0.542656
I0218 13:41:08.314595 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.144962 (* 1 = 0.144962 loss)
I0218 13:41:08.314606 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.141172 (* 1 = 0.141172 loss)
I0218 13:41:08.314615 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.131715 (* 1 = 0.131715 loss)
I0218 13:41:08.314625 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.124806 (* 1 = 0.124806 loss)
I0218 13:41:08.314631 27028 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0218 13:41:44.035830 27028 solver.cpp:357] Iteration 19400 (2.7994 iter/s, 35.7219s/100 iters), loss = 0.597071
I0218 13:41:44.035955 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.122607 (* 1 = 0.122607 loss)
I0218 13:41:44.035965 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.118276 (* 1 = 0.118276 loss)
I0218 13:41:44.035974 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.101346 (* 1 = 0.101346 loss)
I0218 13:41:44.035981 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.254842 (* 1 = 0.254842 loss)
I0218 13:41:44.035989 27028 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0218 13:42:19.749039 27028 solver.cpp:357] Iteration 19500 (2.80004 iter/s, 35.7137s/100 iters), loss = 0.478583
I0218 13:42:19.749150 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0822115 (* 1 = 0.0822115 loss)
I0218 13:42:19.749161 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.150949 (* 1 = 0.150949 loss)
I0218 13:42:19.749169 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.154059 (* 1 = 0.154059 loss)
I0218 13:42:19.749177 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0913635 (* 1 = 0.0913635 loss)
I0218 13:42:19.749184 27028 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0218 13:42:55.454069 27028 solver.cpp:357] Iteration 19600 (2.80068 iter/s, 35.7056s/100 iters), loss = 0.572233
I0218 13:42:55.454222 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0985644 (* 1 = 0.0985644 loss)
I0218 13:42:55.454233 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.18888 (* 1 = 0.18888 loss)
I0218 13:42:55.454241 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.150229 (* 1 = 0.150229 loss)
I0218 13:42:55.454249 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.134559 (* 1 = 0.134559 loss)
I0218 13:42:55.454257 27028 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0218 13:43:28.571234 27028 solver.cpp:357] Iteration 19700 (3.01954 iter/s, 33.1176s/100 iters), loss = 0.483937
I0218 13:43:28.571344 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.146157 (* 1 = 0.146157 loss)
I0218 13:43:28.571355 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.141645 (* 1 = 0.141645 loss)
I0218 13:43:28.571364 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.101518 (* 1 = 0.101518 loss)
I0218 13:43:28.571373 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0946175 (* 1 = 0.0946175 loss)
I0218 13:43:28.571382 27028 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0218 13:43:59.670095 27028 solver.cpp:357] Iteration 19800 (3.21551 iter/s, 31.0993s/100 iters), loss = 0.514191
I0218 13:43:59.670260 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129519 (* 1 = 0.129519 loss)
I0218 13:43:59.670270 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137799 (* 1 = 0.137799 loss)
I0218 13:43:59.670279 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.167406 (* 1 = 0.167406 loss)
I0218 13:43:59.670286 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0794672 (* 1 = 0.0794672 loss)
I0218 13:43:59.670295 27028 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0218 13:44:35.384598 27028 solver.cpp:357] Iteration 19900 (2.79995 iter/s, 35.715s/100 iters), loss = 0.451113
I0218 13:44:35.384790 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.123116 (* 1 = 0.123116 loss)
I0218 13:44:35.384801 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.143606 (* 1 = 0.143606 loss)
I0218 13:44:35.384810 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0727529 (* 1 = 0.0727529 loss)
I0218 13:44:35.384819 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.111638 (* 1 = 0.111638 loss)
I0218 13:44:35.384825 27028 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0218 13:45:10.765539 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.caffemodel
I0218 13:45:10.776036 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_20000.solverstate
I0218 13:45:10.779407 27028 solver.cpp:514] Iteration 20000, Testing net (#0)
I0218 13:45:13.233772 27028 solver.cpp:580]     Test net output #0: Softmax1 = 7.31517 (* 1 = 7.31517 loss)
I0218 13:45:13.233824 27028 solver.cpp:580]     Test net output #1: Softmax2 = 8.24355 (* 1 = 8.24355 loss)
I0218 13:45:13.233831 27028 solver.cpp:580]     Test net output #2: Softmax3 = 7.81215 (* 1 = 7.81215 loss)
I0218 13:45:13.233844 27028 solver.cpp:580]     Test net output #3: Softmax4 = 8.93248 (* 1 = 8.93248 loss)
I0218 13:45:13.233850 27028 solver.cpp:580]     Test net output #4: prob1 = 0.124
I0218 13:45:13.233856 27028 solver.cpp:580]     Test net output #5: prob2 = 0.049
I0218 13:45:13.233862 27028 solver.cpp:580]     Test net output #6: prob3 = 0.039
I0218 13:45:13.233867 27028 solver.cpp:580]     Test net output #7: prob4 = 0.022
I0218 13:45:13.568392 27028 solver.cpp:357] Iteration 20000 (2.61888 iter/s, 38.1843s/100 iters), loss = 0.492402
I0218 13:45:13.568445 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0860064 (* 1 = 0.0860064 loss)
I0218 13:45:13.568455 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.165756 (* 1 = 0.165756 loss)
I0218 13:45:13.568464 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.138265 (* 1 = 0.138265 loss)
I0218 13:45:13.568471 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.102374 (* 1 = 0.102374 loss)
I0218 13:45:13.568480 27028 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0218 13:45:49.284178 27028 solver.cpp:357] Iteration 20100 (2.79984 iter/s, 35.7163s/100 iters), loss = 0.547387
I0218 13:45:49.284337 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.135592 (* 1 = 0.135592 loss)
I0218 13:45:49.284346 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137219 (* 1 = 0.137219 loss)
I0218 13:45:49.284354 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.142054 (* 1 = 0.142054 loss)
I0218 13:45:49.284363 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.132521 (* 1 = 0.132521 loss)
I0218 13:45:49.284369 27028 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0218 13:46:22.857031 27028 solver.cpp:357] Iteration 20200 (2.97856 iter/s, 33.5733s/100 iters), loss = 0.447859
I0218 13:46:22.857190 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0858287 (* 1 = 0.0858287 loss)
I0218 13:46:22.857201 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.11482 (* 1 = 0.11482 loss)
I0218 13:46:22.857210 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134383 (* 1 = 0.134383 loss)
I0218 13:46:22.857218 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.112828 (* 1 = 0.112828 loss)
I0218 13:46:22.857225 27028 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0218 13:46:53.750766 27028 solver.cpp:357] Iteration 20300 (3.23687 iter/s, 30.8941s/100 iters), loss = 0.475167
I0218 13:46:53.750882 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139842 (* 1 = 0.139842 loss)
I0218 13:46:53.750892 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137514 (* 1 = 0.137514 loss)
I0218 13:46:53.750902 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0980987 (* 1 = 0.0980987 loss)
I0218 13:46:53.750910 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0997116 (* 1 = 0.0997116 loss)
I0218 13:46:53.750917 27028 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0218 13:47:29.460837 27028 solver.cpp:357] Iteration 20400 (2.80029 iter/s, 35.7106s/100 iters), loss = 0.494554
I0218 13:47:29.461011 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.132561 (* 1 = 0.132561 loss)
I0218 13:47:29.461021 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.141946 (* 1 = 0.141946 loss)
I0218 13:47:29.461031 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.090608 (* 1 = 0.090608 loss)
I0218 13:47:29.461040 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.129439 (* 1 = 0.129439 loss)
I0218 13:47:29.461046 27028 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0218 13:48:05.193547 27028 solver.cpp:357] Iteration 20500 (2.79852 iter/s, 35.7331s/100 iters), loss = 0.607417
I0218 13:48:05.193670 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.101496 (* 1 = 0.101496 loss)
I0218 13:48:05.193680 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.23137 (* 1 = 0.23137 loss)
I0218 13:48:05.193688 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.158856 (* 1 = 0.158856 loss)
I0218 13:48:05.193696 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.115695 (* 1 = 0.115695 loss)
I0218 13:48:05.193703 27028 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0218 13:48:40.927458 27028 solver.cpp:357] Iteration 20600 (2.79843 iter/s, 35.7344s/100 iters), loss = 0.573097
I0218 13:48:40.927619 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.265258 (* 1 = 0.265258 loss)
I0218 13:48:40.927630 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0898868 (* 1 = 0.0898868 loss)
I0218 13:48:40.927639 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.137323 (* 1 = 0.137323 loss)
I0218 13:48:40.927647 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0806298 (* 1 = 0.0806298 loss)
I0218 13:48:40.927659 27028 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0218 13:49:15.733376 27028 solver.cpp:357] Iteration 20700 (2.87304 iter/s, 34.8063s/100 iters), loss = 0.38737
I0218 13:49:15.733484 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.128813 (* 1 = 0.128813 loss)
I0218 13:49:15.733495 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.069684 (* 1 = 0.069684 loss)
I0218 13:49:15.733505 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0607451 (* 1 = 0.0607451 loss)
I0218 13:49:15.733512 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.128129 (* 1 = 0.128129 loss)
I0218 13:49:15.733520 27028 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0218 13:49:45.400439 27028 solver.cpp:357] Iteration 20800 (3.3707 iter/s, 29.6674s/100 iters), loss = 0.618382
I0218 13:49:45.400516 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.176287 (* 1 = 0.176287 loss)
I0218 13:49:45.400526 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.119952 (* 1 = 0.119952 loss)
I0218 13:49:45.400533 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.18951 (* 1 = 0.18951 loss)
I0218 13:49:45.400542 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.132633 (* 1 = 0.132633 loss)
I0218 13:49:45.400548 27028 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0218 13:50:21.140527 27028 solver.cpp:357] Iteration 20900 (2.79794 iter/s, 35.7406s/100 iters), loss = 0.414175
I0218 13:50:21.140633 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.122549 (* 1 = 0.122549 loss)
I0218 13:50:21.140645 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0968707 (* 1 = 0.0968707 loss)
I0218 13:50:21.140653 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.111949 (* 1 = 0.111949 loss)
I0218 13:50:21.140661 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0828062 (* 1 = 0.0828062 loss)
I0218 13:50:21.140668 27028 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0218 13:50:56.572626 27028 solver.cpp:514] Iteration 21000, Testing net (#0)
I0218 13:50:58.953204 27028 solver.cpp:580]     Test net output #0: Softmax1 = 5.61472 (* 1 = 5.61472 loss)
I0218 13:50:58.953255 27028 solver.cpp:580]     Test net output #1: Softmax2 = 12.8687 (* 1 = 12.8687 loss)
I0218 13:50:58.953264 27028 solver.cpp:580]     Test net output #2: Softmax3 = 16.7339 (* 1 = 16.7339 loss)
I0218 13:50:58.953272 27028 solver.cpp:580]     Test net output #3: Softmax4 = 8.75539 (* 1 = 8.75539 loss)
I0218 13:50:58.953279 27028 solver.cpp:580]     Test net output #4: prob1 = 0.201
I0218 13:50:58.953285 27028 solver.cpp:580]     Test net output #5: prob2 = 0.046
I0218 13:50:58.953291 27028 solver.cpp:580]     Test net output #6: prob3 = 0.047
I0218 13:50:58.953297 27028 solver.cpp:580]     Test net output #7: prob4 = 0.026
I0218 13:50:59.290060 27028 solver.cpp:357] Iteration 21000 (2.62123 iter/s, 38.1501s/100 iters), loss = 0.444335
I0218 13:50:59.290114 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.135983 (* 1 = 0.135983 loss)
I0218 13:50:59.290124 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.113591 (* 1 = 0.113591 loss)
I0218 13:50:59.290133 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0820213 (* 1 = 0.0820213 loss)
I0218 13:50:59.290141 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.112739 (* 1 = 0.112739 loss)
I0218 13:50:59.290149 27028 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0218 13:51:35.076604 27028 solver.cpp:357] Iteration 21100 (2.79431 iter/s, 35.7871s/100 iters), loss = 0.477828
I0218 13:51:35.076743 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129437 (* 1 = 0.129437 loss)
I0218 13:51:35.076755 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.138292 (* 1 = 0.138292 loss)
I0218 13:51:35.076763 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0707698 (* 1 = 0.0707698 loss)
I0218 13:51:35.076771 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.139329 (* 1 = 0.139329 loss)
I0218 13:51:35.076779 27028 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0218 13:52:10.097354 27028 solver.cpp:357] Iteration 21200 (2.85558 iter/s, 35.0192s/100 iters), loss = 0.431705
I0218 13:52:10.097468 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.104675 (* 1 = 0.104675 loss)
I0218 13:52:10.097479 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0909176 (* 1 = 0.0909176 loss)
I0218 13:52:10.097488 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0834959 (* 1 = 0.0834959 loss)
I0218 13:52:10.097496 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.152616 (* 1 = 0.152616 loss)
I0218 13:52:10.097503 27028 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0218 13:52:39.348376 27028 solver.cpp:357] Iteration 21300 (3.41863 iter/s, 29.2515s/100 iters), loss = 0.606992
I0218 13:52:39.348465 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.160156 (* 1 = 0.160156 loss)
I0218 13:52:39.348475 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.206022 (* 1 = 0.206022 loss)
I0218 13:52:39.348484 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.105842 (* 1 = 0.105842 loss)
I0218 13:52:39.348492 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.134973 (* 1 = 0.134973 loss)
I0218 13:52:39.348500 27028 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0218 13:53:15.173105 27028 solver.cpp:357] Iteration 21400 (2.79145 iter/s, 35.8237s/100 iters), loss = 0.515433
I0218 13:53:15.173260 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0770659 (* 1 = 0.0770659 loss)
I0218 13:53:15.173271 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.146435 (* 1 = 0.146435 loss)
I0218 13:53:15.173280 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.135527 (* 1 = 0.135527 loss)
I0218 13:53:15.173287 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.156404 (* 1 = 0.156404 loss)
I0218 13:53:15.173295 27028 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0218 13:53:50.907393 27028 solver.cpp:357] Iteration 21500 (2.79836 iter/s, 35.7352s/100 iters), loss = 0.6566
I0218 13:53:50.907519 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.104289 (* 1 = 0.104289 loss)
I0218 13:53:50.907531 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.178356 (* 1 = 0.178356 loss)
I0218 13:53:50.907538 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.208953 (* 1 = 0.208953 loss)
I0218 13:53:50.907546 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.165002 (* 1 = 0.165002 loss)
I0218 13:53:50.907554 27028 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0218 13:54:26.631057 27028 solver.cpp:357] Iteration 21600 (2.7992 iter/s, 35.7246s/100 iters), loss = 0.489939
I0218 13:54:26.631352 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0924412 (* 1 = 0.0924412 loss)
I0218 13:54:26.631363 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.161261 (* 1 = 0.161261 loss)
I0218 13:54:26.631372 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0744699 (* 1 = 0.0744699 loss)
I0218 13:54:26.631379 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.161767 (* 1 = 0.161767 loss)
I0218 13:54:26.631387 27028 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0218 13:55:02.363238 27028 solver.cpp:357] Iteration 21700 (2.79854 iter/s, 35.7329s/100 iters), loss = 0.511352
I0218 13:55:02.363353 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.142582 (* 1 = 0.142582 loss)
I0218 13:55:02.363363 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0807094 (* 1 = 0.0807094 loss)
I0218 13:55:02.363371 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.130725 (* 1 = 0.130725 loss)
I0218 13:55:02.363379 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.157337 (* 1 = 0.157337 loss)
I0218 13:55:02.363386 27028 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0218 13:55:30.962159 27028 solver.cpp:357] Iteration 21800 (3.49656 iter/s, 28.5996s/100 iters), loss = 0.598598
I0218 13:55:30.962236 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.22172 (* 1 = 0.22172 loss)
I0218 13:55:30.962246 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.122824 (* 1 = 0.122824 loss)
I0218 13:55:30.962255 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.170556 (* 1 = 0.170556 loss)
I0218 13:55:30.962263 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0834979 (* 1 = 0.0834979 loss)
I0218 13:55:30.962270 27028 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0218 13:56:06.714443 27028 solver.cpp:357] Iteration 21900 (2.79696 iter/s, 35.7532s/100 iters), loss = 0.550215
I0218 13:56:06.714573 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0641671 (* 1 = 0.0641671 loss)
I0218 13:56:06.714584 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.181493 (* 1 = 0.181493 loss)
I0218 13:56:06.714593 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.163923 (* 1 = 0.163923 loss)
I0218 13:56:06.714601 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.140633 (* 1 = 0.140633 loss)
I0218 13:56:06.714608 27028 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0218 13:56:42.108268 27028 solver.cpp:514] Iteration 22000, Testing net (#0)
I0218 13:56:44.519701 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.78212 (* 1 = 2.78212 loss)
I0218 13:56:44.519748 27028 solver.cpp:580]     Test net output #1: Softmax2 = 3.51711 (* 1 = 3.51711 loss)
I0218 13:56:44.519757 27028 solver.cpp:580]     Test net output #2: Softmax3 = 3.40029 (* 1 = 3.40029 loss)
I0218 13:56:44.519764 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.64069 (* 1 = 2.64069 loss)
I0218 13:56:44.519770 27028 solver.cpp:580]     Test net output #4: prob1 = 0.373
I0218 13:56:44.519778 27028 solver.cpp:580]     Test net output #5: prob2 = 0.3
I0218 13:56:44.519783 27028 solver.cpp:580]     Test net output #6: prob3 = 0.328
I0218 13:56:44.519789 27028 solver.cpp:580]     Test net output #7: prob4 = 0.417
I0218 13:56:44.853888 27028 solver.cpp:357] Iteration 22000 (2.62204 iter/s, 38.1383s/100 iters), loss = 0.466873
I0218 13:56:44.853951 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0632059 (* 1 = 0.0632059 loss)
I0218 13:56:44.853962 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.102411 (* 1 = 0.102411 loss)
I0218 13:56:44.853971 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.217659 (* 1 = 0.217659 loss)
I0218 13:56:44.853979 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0835972 (* 1 = 0.0835972 loss)
I0218 13:56:44.853986 27028 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0218 13:57:20.610406 27028 solver.cpp:357] Iteration 22100 (2.79663 iter/s, 35.7574s/100 iters), loss = 0.818419
I0218 13:57:20.610668 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.118017 (* 1 = 0.118017 loss)
I0218 13:57:20.610680 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.297733 (* 1 = 0.297733 loss)
I0218 13:57:20.610689 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.27409 (* 1 = 0.27409 loss)
I0218 13:57:20.610697 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.128579 (* 1 = 0.128579 loss)
I0218 13:57:20.610705 27028 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0218 13:57:56.306708 27028 solver.cpp:357] Iteration 22200 (2.80151 iter/s, 35.6951s/100 iters), loss = 0.658806
I0218 13:57:56.306869 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0990252 (* 1 = 0.0990252 loss)
I0218 13:57:56.306879 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.23818 (* 1 = 0.23818 loss)
I0218 13:57:56.306888 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.169998 (* 1 = 0.169998 loss)
I0218 13:57:56.306896 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.151604 (* 1 = 0.151604 loss)
I0218 13:57:56.306905 27028 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0218 13:58:24.994817 27028 solver.cpp:357] Iteration 22300 (3.48594 iter/s, 28.6867s/100 iters), loss = 0.333049
I0218 13:58:24.994889 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0398001 (* 1 = 0.0398001 loss)
I0218 13:58:24.994899 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.106655 (* 1 = 0.106655 loss)
I0218 13:58:24.994907 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104904 (* 1 = 0.104904 loss)
I0218 13:58:24.994915 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0816896 (* 1 = 0.0816896 loss)
I0218 13:58:24.994923 27028 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0218 13:59:00.682209 27028 solver.cpp:357] Iteration 22400 (2.80205 iter/s, 35.6882s/100 iters), loss = 0.599898
I0218 13:59:00.682384 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.18319 (* 1 = 0.18319 loss)
I0218 13:59:00.682395 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.162312 (* 1 = 0.162312 loss)
I0218 13:59:00.682404 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.101382 (* 1 = 0.101382 loss)
I0218 13:59:00.682412 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.153014 (* 1 = 0.153014 loss)
I0218 13:59:00.682420 27028 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0218 13:59:36.372954 27028 solver.cpp:357] Iteration 22500 (2.80195 iter/s, 35.6895s/100 iters), loss = 0.598892
I0218 13:59:36.373073 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.184289 (* 1 = 0.184289 loss)
I0218 13:59:36.373083 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.125878 (* 1 = 0.125878 loss)
I0218 13:59:36.373092 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.115854 (* 1 = 0.115854 loss)
I0218 13:59:36.373100 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.172871 (* 1 = 0.172871 loss)
I0218 13:59:36.373107 27028 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0218 14:00:12.083452 27028 solver.cpp:357] Iteration 22600 (2.80024 iter/s, 35.7112s/100 iters), loss = 0.77771
I0218 14:00:12.083570 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.104231 (* 1 = 0.104231 loss)
I0218 14:00:12.083581 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.187931 (* 1 = 0.187931 loss)
I0218 14:00:12.083590 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.386781 (* 1 = 0.386781 loss)
I0218 14:00:12.083598 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0987668 (* 1 = 0.0987668 loss)
I0218 14:00:12.083606 27028 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0218 14:00:47.800194 27028 solver.cpp:357] Iteration 22700 (2.79975 iter/s, 35.7174s/100 iters), loss = 0.63671
I0218 14:00:47.800371 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.184594 (* 1 = 0.184594 loss)
I0218 14:00:47.800381 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.194077 (* 1 = 0.194077 loss)
I0218 14:00:47.800390 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.150134 (* 1 = 0.150134 loss)
I0218 14:00:47.800398 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.107905 (* 1 = 0.107905 loss)
I0218 14:00:47.800405 27028 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0218 14:01:17.355396 27028 solver.cpp:357] Iteration 22800 (3.38344 iter/s, 29.5557s/100 iters), loss = 0.419967
I0218 14:01:17.355468 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0848296 (* 1 = 0.0848296 loss)
I0218 14:01:17.355479 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0776484 (* 1 = 0.0776484 loss)
I0218 14:01:17.355486 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.131091 (* 1 = 0.131091 loss)
I0218 14:01:17.355494 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.126397 (* 1 = 0.126397 loss)
I0218 14:01:17.355502 27028 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0218 14:01:52.130657 27028 solver.cpp:357] Iteration 22900 (2.87555 iter/s, 34.776s/100 iters), loss = 0.444693
I0218 14:01:52.130762 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.134674 (* 1 = 0.134674 loss)
I0218 14:01:52.130774 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0668852 (* 1 = 0.0668852 loss)
I0218 14:01:52.130782 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.114641 (* 1 = 0.114641 loss)
I0218 14:01:52.130790 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.128493 (* 1 = 0.128493 loss)
I0218 14:01:52.130797 27028 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0218 14:02:27.492033 27028 solver.cpp:514] Iteration 23000, Testing net (#0)
I0218 14:02:29.854005 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.25862 (* 1 = 1.25862 loss)
I0218 14:02:29.854053 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.03334 (* 1 = 2.03334 loss)
I0218 14:02:29.854061 27028 solver.cpp:580]     Test net output #2: Softmax3 = 3.63966 (* 1 = 3.63966 loss)
I0218 14:02:29.854070 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.23132 (* 1 = 3.23132 loss)
I0218 14:02:29.854076 27028 solver.cpp:580]     Test net output #4: prob1 = 0.679
I0218 14:02:29.854082 27028 solver.cpp:580]     Test net output #5: prob2 = 0.535
I0218 14:02:29.854089 27028 solver.cpp:580]     Test net output #6: prob3 = 0.282
I0218 14:02:29.854094 27028 solver.cpp:580]     Test net output #7: prob4 = 0.305
I0218 14:02:30.214416 27028 solver.cpp:357] Iteration 23000 (2.62574 iter/s, 38.0845s/100 iters), loss = 0.60002
I0218 14:02:30.214478 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.115697 (* 1 = 0.115697 loss)
I0218 14:02:30.214488 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.138399 (* 1 = 0.138399 loss)
I0218 14:02:30.214496 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.116881 (* 1 = 0.116881 loss)
I0218 14:02:30.214504 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.229042 (* 1 = 0.229042 loss)
I0218 14:02:30.214512 27028 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0218 14:03:05.893184 27028 solver.cpp:357] Iteration 23100 (2.80289 iter/s, 35.6774s/100 iters), loss = 0.413342
I0218 14:03:05.893297 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0938925 (* 1 = 0.0938925 loss)
I0218 14:03:05.893308 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0715924 (* 1 = 0.0715924 loss)
I0218 14:03:05.893316 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.149448 (* 1 = 0.149448 loss)
I0218 14:03:05.893326 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.098409 (* 1 = 0.098409 loss)
I0218 14:03:05.893332 27028 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0218 14:03:41.590673 27028 solver.cpp:357] Iteration 23200 (2.80127 iter/s, 35.6981s/100 iters), loss = 0.565711
I0218 14:03:41.590852 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.159144 (* 1 = 0.159144 loss)
I0218 14:03:41.590862 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.119601 (* 1 = 0.119601 loss)
I0218 14:03:41.590871 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.130631 (* 1 = 0.130631 loss)
I0218 14:03:41.590879 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.156334 (* 1 = 0.156334 loss)
I0218 14:03:41.590888 27028 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0218 14:04:11.647053 27028 solver.cpp:357] Iteration 23300 (3.32703 iter/s, 30.0568s/100 iters), loss = 0.440841
I0218 14:04:11.647212 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0971303 (* 1 = 0.0971303 loss)
I0218 14:04:11.647222 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0775463 (* 1 = 0.0775463 loss)
I0218 14:04:11.647231 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.189849 (* 1 = 0.189849 loss)
I0218 14:04:11.647239 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0763151 (* 1 = 0.0763151 loss)
I0218 14:04:11.647248 27028 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0218 14:04:45.960407 27028 solver.cpp:357] Iteration 23400 (2.91427 iter/s, 34.3139s/100 iters), loss = 0.380626
I0218 14:04:45.960572 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0728665 (* 1 = 0.0728665 loss)
I0218 14:04:45.960582 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.11968 (* 1 = 0.11968 loss)
I0218 14:04:45.960592 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104618 (* 1 = 0.104618 loss)
I0218 14:04:45.960599 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0834621 (* 1 = 0.0834621 loss)
I0218 14:04:45.960608 27028 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0218 14:05:21.671242 27028 solver.cpp:357] Iteration 23500 (2.80023 iter/s, 35.7114s/100 iters), loss = 0.536089
I0218 14:05:21.671355 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.112946 (* 1 = 0.112946 loss)
I0218 14:05:21.671365 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.168777 (* 1 = 0.168777 loss)
I0218 14:05:21.671375 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.109503 (* 1 = 0.109503 loss)
I0218 14:05:21.671382 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144863 (* 1 = 0.144863 loss)
I0218 14:05:21.671389 27028 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0218 14:05:57.378363 27028 solver.cpp:357] Iteration 23600 (2.80051 iter/s, 35.7077s/100 iters), loss = 0.563894
I0218 14:05:57.378523 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.141533 (* 1 = 0.141533 loss)
I0218 14:05:57.378533 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.131207 (* 1 = 0.131207 loss)
I0218 14:05:57.378542 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.193953 (* 1 = 0.193953 loss)
I0218 14:05:57.378551 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0972006 (* 1 = 0.0972006 loss)
I0218 14:05:57.378562 27028 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0218 14:06:33.082231 27028 solver.cpp:357] Iteration 23700 (2.80077 iter/s, 35.7044s/100 iters), loss = 0.451579
I0218 14:06:33.082342 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.124468 (* 1 = 0.124468 loss)
I0218 14:06:33.082353 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.117318 (* 1 = 0.117318 loss)
I0218 14:06:33.082361 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.107818 (* 1 = 0.107818 loss)
I0218 14:06:33.082370 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.101975 (* 1 = 0.101975 loss)
I0218 14:06:33.082376 27028 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0218 14:07:04.366642 27028 solver.cpp:357] Iteration 23800 (3.19643 iter/s, 31.2849s/100 iters), loss = 0.489649
I0218 14:07:04.366735 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.163416 (* 1 = 0.163416 loss)
I0218 14:07:04.366747 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.07978 (* 1 = 0.07978 loss)
I0218 14:07:04.366756 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.10177 (* 1 = 0.10177 loss)
I0218 14:07:04.366765 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.144683 (* 1 = 0.144683 loss)
I0218 14:07:04.366772 27028 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0218 14:07:37.398074 27028 solver.cpp:357] Iteration 23900 (3.02737 iter/s, 33.032s/100 iters), loss = 0.443897
I0218 14:07:37.398298 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.11789 (* 1 = 0.11789 loss)
I0218 14:07:37.398310 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.112102 (* 1 = 0.112102 loss)
I0218 14:07:37.398319 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.101544 (* 1 = 0.101544 loss)
I0218 14:07:37.398327 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.112362 (* 1 = 0.112362 loss)
I0218 14:07:37.398334 27028 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0218 14:08:12.763630 27028 solver.cpp:514] Iteration 24000, Testing net (#0)
I0218 14:08:15.234359 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.645039 (* 1 = 0.645039 loss)
I0218 14:08:15.234403 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.700085 (* 1 = 0.700085 loss)
I0218 14:08:15.234411 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.455275 (* 1 = 0.455275 loss)
I0218 14:08:15.234421 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.576302 (* 1 = 0.576302 loss)
I0218 14:08:15.234426 27028 solver.cpp:580]     Test net output #4: prob1 = 0.847
I0218 14:08:15.234432 27028 solver.cpp:580]     Test net output #5: prob2 = 0.811
I0218 14:08:15.234437 27028 solver.cpp:580]     Test net output #6: prob3 = 0.881001
I0218 14:08:15.234443 27028 solver.cpp:580]     Test net output #7: prob4 = 0.865
I0218 14:08:15.569185 27028 solver.cpp:357] Iteration 24000 (2.61975 iter/s, 38.1717s/100 iters), loss = 0.45243
I0218 14:08:15.569242 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.072133 (* 1 = 0.072133 loss)
I0218 14:08:15.569250 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.142541 (* 1 = 0.142541 loss)
I0218 14:08:15.569259 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.141886 (* 1 = 0.141886 loss)
I0218 14:08:15.569267 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0958701 (* 1 = 0.0958701 loss)
I0218 14:08:15.569275 27028 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0218 14:08:51.272838 27028 solver.cpp:357] Iteration 24100 (2.80078 iter/s, 35.7043s/100 iters), loss = 0.467057
I0218 14:08:51.272989 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.114005 (* 1 = 0.114005 loss)
I0218 14:08:51.273000 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0977938 (* 1 = 0.0977938 loss)
I0218 14:08:51.273007 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.149949 (* 1 = 0.149949 loss)
I0218 14:08:51.273015 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.105309 (* 1 = 0.105309 loss)
I0218 14:08:51.273023 27028 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0218 14:09:26.987365 27028 solver.cpp:357] Iteration 24200 (2.79994 iter/s, 35.7151s/100 iters), loss = 0.56872
I0218 14:09:26.987476 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.150341 (* 1 = 0.150341 loss)
I0218 14:09:26.987489 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.117387 (* 1 = 0.117387 loss)
I0218 14:09:26.987498 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.166604 (* 1 = 0.166604 loss)
I0218 14:09:26.987506 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.134388 (* 1 = 0.134388 loss)
I0218 14:09:26.987514 27028 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0218 14:09:58.689968 27028 solver.cpp:357] Iteration 24300 (3.15427 iter/s, 31.7031s/100 iters), loss = 0.575786
I0218 14:09:58.690073 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.131741 (* 1 = 0.131741 loss)
I0218 14:09:58.690084 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.163514 (* 1 = 0.163514 loss)
I0218 14:09:58.690093 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.144571 (* 1 = 0.144571 loss)
I0218 14:09:58.690100 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.13596 (* 1 = 0.13596 loss)
I0218 14:09:58.690107 27028 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0218 14:10:31.342785 27028 solver.cpp:357] Iteration 24400 (3.06247 iter/s, 32.6533s/100 iters), loss = 0.367547
I0218 14:10:31.343235 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0544371 (* 1 = 0.0544371 loss)
I0218 14:10:31.343307 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.115213 (* 1 = 0.115213 loss)
I0218 14:10:31.343353 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.100586 (* 1 = 0.100586 loss)
I0218 14:10:31.343391 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0973106 (* 1 = 0.0973106 loss)
I0218 14:10:31.343425 27028 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0218 14:11:07.055475 27028 solver.cpp:357] Iteration 24500 (2.8001 iter/s, 35.713s/100 iters), loss = 0.593955
I0218 14:11:07.055600 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.114039 (* 1 = 0.114039 loss)
I0218 14:11:07.055610 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.138204 (* 1 = 0.138204 loss)
I0218 14:11:07.055619 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.206665 (* 1 = 0.206665 loss)
I0218 14:11:07.055626 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.135047 (* 1 = 0.135047 loss)
I0218 14:11:07.055634 27028 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0218 14:11:42.861856 27028 solver.cpp:357] Iteration 24600 (2.79275 iter/s, 35.8069s/100 iters), loss = 0.599717
I0218 14:11:42.861985 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.107905 (* 1 = 0.107905 loss)
I0218 14:11:42.861996 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.148944 (* 1 = 0.148944 loss)
I0218 14:11:42.862004 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.179585 (* 1 = 0.179585 loss)
I0218 14:11:42.862012 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.163284 (* 1 = 0.163284 loss)
I0218 14:11:42.862020 27028 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0218 14:12:18.546602 27028 solver.cpp:357] Iteration 24700 (2.80243 iter/s, 35.6833s/100 iters), loss = 0.477196
I0218 14:12:18.546770 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.197403 (* 1 = 0.197403 loss)
I0218 14:12:18.546782 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0978149 (* 1 = 0.0978149 loss)
I0218 14:12:18.546792 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0863915 (* 1 = 0.0863915 loss)
I0218 14:12:18.546799 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0955862 (* 1 = 0.0955862 loss)
I0218 14:12:18.546810 27028 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0218 14:12:51.545975 27028 solver.cpp:357] Iteration 24800 (3.0305 iter/s, 32.9978s/100 iters), loss = 0.629619
I0218 14:12:51.546128 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.144146 (* 1 = 0.144146 loss)
I0218 14:12:51.546139 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.111185 (* 1 = 0.111185 loss)
I0218 14:12:51.546147 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.190828 (* 1 = 0.190828 loss)
I0218 14:12:51.546155 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.183461 (* 1 = 0.183461 loss)
I0218 14:12:51.546164 27028 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0218 14:13:22.987079 27028 solver.cpp:357] Iteration 24900 (3.18051 iter/s, 31.4415s/100 iters), loss = 0.507559
I0218 14:13:22.987197 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.117202 (* 1 = 0.117202 loss)
I0218 14:13:22.987207 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.208091 (* 1 = 0.208091 loss)
I0218 14:13:22.987216 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0834939 (* 1 = 0.0834939 loss)
I0218 14:13:22.987224 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0987727 (* 1 = 0.0987727 loss)
I0218 14:13:22.987232 27028 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0218 14:13:58.371599 27028 solver.cpp:514] Iteration 25000, Testing net (#0)
I0218 14:14:00.840703 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.898228 (* 1 = 0.898228 loss)
I0218 14:14:00.840751 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.604283 (* 1 = 0.604283 loss)
I0218 14:14:00.840760 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.796544 (* 1 = 0.796544 loss)
I0218 14:14:00.840770 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.74836 (* 1 = 0.74836 loss)
I0218 14:14:00.840776 27028 solver.cpp:580]     Test net output #4: prob1 = 0.799
I0218 14:14:00.840782 27028 solver.cpp:580]     Test net output #5: prob2 = 0.838
I0218 14:14:00.840787 27028 solver.cpp:580]     Test net output #6: prob3 = 0.782
I0218 14:14:00.840793 27028 solver.cpp:580]     Test net output #7: prob4 = 0.828
I0218 14:14:01.176805 27028 solver.cpp:357] Iteration 25000 (2.61846 iter/s, 38.1903s/100 iters), loss = 0.441432
I0218 14:14:01.176869 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.168555 (* 1 = 0.168555 loss)
I0218 14:14:01.176880 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0811973 (* 1 = 0.0811973 loss)
I0218 14:14:01.176888 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.102864 (* 1 = 0.102864 loss)
I0218 14:14:01.176897 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.088815 (* 1 = 0.088815 loss)
I0218 14:14:01.176909 27028 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0218 14:14:36.894115 27028 solver.cpp:357] Iteration 25100 (2.79972 iter/s, 35.7179s/100 iters), loss = 0.573445
I0218 14:14:36.894227 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.115611 (* 1 = 0.115611 loss)
I0218 14:14:36.894237 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.159362 (* 1 = 0.159362 loss)
I0218 14:14:36.894245 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.134566 (* 1 = 0.134566 loss)
I0218 14:14:36.894253 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.163906 (* 1 = 0.163906 loss)
I0218 14:14:36.894259 27028 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0218 14:15:12.598691 27028 solver.cpp:357] Iteration 25200 (2.80072 iter/s, 35.7051s/100 iters), loss = 0.505576
I0218 14:15:12.598850 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0964729 (* 1 = 0.0964729 loss)
I0218 14:15:12.598861 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0675945 (* 1 = 0.0675945 loss)
I0218 14:15:12.598870 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.201994 (* 1 = 0.201994 loss)
I0218 14:15:12.598877 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.139515 (* 1 = 0.139515 loss)
I0218 14:15:12.598884 27028 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0218 14:15:45.897406 27028 solver.cpp:357] Iteration 25300 (3.00308 iter/s, 33.2992s/100 iters), loss = 0.53743
I0218 14:15:45.897554 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.134394 (* 1 = 0.134394 loss)
I0218 14:15:45.897565 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.11322 (* 1 = 0.11322 loss)
I0218 14:15:45.897574 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.179927 (* 1 = 0.179927 loss)
I0218 14:15:45.897583 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.109889 (* 1 = 0.109889 loss)
I0218 14:15:45.897589 27028 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0218 14:16:17.059638 27028 solver.cpp:357] Iteration 25400 (3.20897 iter/s, 31.1627s/100 iters), loss = 0.444899
I0218 14:16:17.059758 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0815422 (* 1 = 0.0815422 loss)
I0218 14:16:17.059768 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0952419 (* 1 = 0.0952419 loss)
I0218 14:16:17.059777 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.153679 (* 1 = 0.153679 loss)
I0218 14:16:17.059785 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.114436 (* 1 = 0.114436 loss)
I0218 14:16:17.059792 27028 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0218 14:16:52.856909 27028 solver.cpp:357] Iteration 25500 (2.79347 iter/s, 35.7978s/100 iters), loss = 0.487069
I0218 14:16:52.857053 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110381 (* 1 = 0.110381 loss)
I0218 14:16:52.857064 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108418 (* 1 = 0.108418 loss)
I0218 14:16:52.857071 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.12171 (* 1 = 0.12171 loss)
I0218 14:16:52.857079 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.14656 (* 1 = 0.14656 loss)
I0218 14:16:52.857087 27028 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0218 14:17:28.491252 27028 solver.cpp:357] Iteration 25600 (2.8064 iter/s, 35.6328s/100 iters), loss = 0.453308
I0218 14:17:28.491431 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0767944 (* 1 = 0.0767944 loss)
I0218 14:17:28.491442 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.091549 (* 1 = 0.091549 loss)
I0218 14:17:28.491451 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.166409 (* 1 = 0.166409 loss)
I0218 14:17:28.491458 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.118555 (* 1 = 0.118555 loss)
I0218 14:17:28.491466 27028 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0218 14:18:04.200338 27028 solver.cpp:357] Iteration 25700 (2.80037 iter/s, 35.7096s/100 iters), loss = 0.512234
I0218 14:18:04.200423 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.102872 (* 1 = 0.102872 loss)
I0218 14:18:04.200435 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0675118 (* 1 = 0.0675118 loss)
I0218 14:18:04.200443 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.204739 (* 1 = 0.204739 loss)
I0218 14:18:04.200451 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.137111 (* 1 = 0.137111 loss)
I0218 14:18:04.200459 27028 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0218 14:18:38.779942 27028 solver.cpp:357] Iteration 25800 (2.89183 iter/s, 34.5802s/100 iters), loss = 0.665166
I0218 14:18:38.780050 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.18774 (* 1 = 0.18774 loss)
I0218 14:18:38.780061 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.175326 (* 1 = 0.175326 loss)
I0218 14:18:38.780071 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.133636 (* 1 = 0.133636 loss)
I0218 14:18:38.780078 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.168465 (* 1 = 0.168465 loss)
I0218 14:18:38.780086 27028 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0218 14:19:08.727596 27028 solver.cpp:357] Iteration 25900 (3.33911 iter/s, 29.9481s/100 iters), loss = 0.455269
I0218 14:19:08.727679 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.112625 (* 1 = 0.112625 loss)
I0218 14:19:08.727689 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.126399 (* 1 = 0.126399 loss)
I0218 14:19:08.727697 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.116174 (* 1 = 0.116174 loss)
I0218 14:19:08.727705 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.100071 (* 1 = 0.100071 loss)
I0218 14:19:08.727713 27028 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0218 14:19:44.113210 27028 solver.cpp:514] Iteration 26000, Testing net (#0)
I0218 14:19:46.517639 27028 solver.cpp:580]     Test net output #0: Softmax1 = 5.52208 (* 1 = 5.52208 loss)
I0218 14:19:46.517686 27028 solver.cpp:580]     Test net output #1: Softmax2 = 11.6011 (* 1 = 11.6011 loss)
I0218 14:19:46.517695 27028 solver.cpp:580]     Test net output #2: Softmax3 = 9.7744 (* 1 = 9.7744 loss)
I0218 14:19:46.517704 27028 solver.cpp:580]     Test net output #3: Softmax4 = 9.44215 (* 1 = 9.44215 loss)
I0218 14:19:46.517710 27028 solver.cpp:580]     Test net output #4: prob1 = 0.123
I0218 14:19:46.517716 27028 solver.cpp:580]     Test net output #5: prob2 = 0.053
I0218 14:19:46.517722 27028 solver.cpp:580]     Test net output #6: prob3 = 0.05
I0218 14:19:46.517729 27028 solver.cpp:580]     Test net output #7: prob4 = 0.028
I0218 14:19:46.852655 27028 solver.cpp:357] Iteration 26000 (2.62304 iter/s, 38.1236s/100 iters), loss = 0.504789
I0218 14:19:46.852710 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129607 (* 1 = 0.129607 loss)
I0218 14:19:46.852720 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.136875 (* 1 = 0.136875 loss)
I0218 14:19:46.852728 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.108513 (* 1 = 0.108513 loss)
I0218 14:19:46.852736 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.129794 (* 1 = 0.129794 loss)
I0218 14:19:46.852744 27028 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0218 14:20:22.617655 27028 solver.cpp:357] Iteration 26100 (2.79598 iter/s, 35.7656s/100 iters), loss = 0.528367
I0218 14:20:22.617898 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.153373 (* 1 = 0.153373 loss)
I0218 14:20:22.617910 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.120929 (* 1 = 0.120929 loss)
I0218 14:20:22.617919 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.175922 (* 1 = 0.175922 loss)
I0218 14:20:22.617928 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0781427 (* 1 = 0.0781427 loss)
I0218 14:20:22.617941 27028 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0218 14:20:58.416306 27028 solver.cpp:357] Iteration 26200 (2.79352 iter/s, 35.7971s/100 iters), loss = 0.407811
I0218 14:20:58.416460 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.136294 (* 1 = 0.136294 loss)
I0218 14:20:58.416472 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0836566 (* 1 = 0.0836566 loss)
I0218 14:20:58.416481 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.1174 (* 1 = 0.1174 loss)
I0218 14:20:58.416488 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0704602 (* 1 = 0.0704602 loss)
I0218 14:20:58.416496 27028 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0218 14:21:33.167718 27028 solver.cpp:357] Iteration 26300 (2.87754 iter/s, 34.7519s/100 iters), loss = 0.493326
I0218 14:21:33.167829 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.145537 (* 1 = 0.145537 loss)
I0218 14:21:33.167841 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.140794 (* 1 = 0.140794 loss)
I0218 14:21:33.167850 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.125711 (* 1 = 0.125711 loss)
I0218 14:21:33.167857 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.081284 (* 1 = 0.081284 loss)
I0218 14:21:33.167865 27028 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0218 14:22:02.898608 27028 solver.cpp:357] Iteration 26400 (3.36346 iter/s, 29.7313s/100 iters), loss = 0.473694
I0218 14:22:02.898684 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.157867 (* 1 = 0.157867 loss)
I0218 14:22:02.898694 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0998199 (* 1 = 0.0998199 loss)
I0218 14:22:02.898703 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.114097 (* 1 = 0.114097 loss)
I0218 14:22:02.898711 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.10191 (* 1 = 0.10191 loss)
I0218 14:22:02.898718 27028 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0218 14:22:38.623451 27028 solver.cpp:357] Iteration 26500 (2.79913 iter/s, 35.7254s/100 iters), loss = 0.537646
I0218 14:22:38.623566 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.16624 (* 1 = 0.16624 loss)
I0218 14:22:38.623576 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.118668 (* 1 = 0.118668 loss)
I0218 14:22:38.623585 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104226 (* 1 = 0.104226 loss)
I0218 14:22:38.623594 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.148512 (* 1 = 0.148512 loss)
I0218 14:22:38.623600 27028 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0218 14:23:14.344890 27028 solver.cpp:357] Iteration 26600 (2.7994 iter/s, 35.722s/100 iters), loss = 0.560391
I0218 14:23:14.345003 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.139167 (* 1 = 0.139167 loss)
I0218 14:23:14.345015 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.159822 (* 1 = 0.159822 loss)
I0218 14:23:14.345022 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.163656 (* 1 = 0.163656 loss)
I0218 14:23:14.345031 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0977463 (* 1 = 0.0977463 loss)
I0218 14:23:14.345038 27028 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0218 14:23:50.071661 27028 solver.cpp:357] Iteration 26700 (2.79898 iter/s, 35.7273s/100 iters), loss = 0.489153
I0218 14:23:50.071882 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.178618 (* 1 = 0.178618 loss)
I0218 14:23:50.071892 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.1288 (* 1 = 0.1288 loss)
I0218 14:23:50.071902 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0968841 (* 1 = 0.0968841 loss)
I0218 14:23:50.071909 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0848505 (* 1 = 0.0848505 loss)
I0218 14:23:50.071916 27028 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0218 14:24:25.790331 27028 solver.cpp:357] Iteration 26800 (2.79962 iter/s, 35.7191s/100 iters), loss = 0.52337
I0218 14:24:25.790453 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0907122 (* 1 = 0.0907122 loss)
I0218 14:24:25.790464 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0994436 (* 1 = 0.0994436 loss)
I0218 14:24:25.790472 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.150775 (* 1 = 0.150775 loss)
I0218 14:24:25.790480 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.182439 (* 1 = 0.182439 loss)
I0218 14:24:25.790488 27028 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0218 14:24:50.911940 27028 solver.cpp:357] Iteration 26900 (3.98059 iter/s, 25.1219s/100 iters), loss = 0.552354
I0218 14:24:50.912011 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0390371 (* 1 = 0.0390371 loss)
I0218 14:24:50.912021 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.242553 (* 1 = 0.242553 loss)
I0218 14:24:50.912030 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.15124 (* 1 = 0.15124 loss)
I0218 14:24:50.912037 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.119524 (* 1 = 0.119524 loss)
I0218 14:24:50.912045 27028 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0218 14:25:10.966228 27028 solver.cpp:514] Iteration 27000, Testing net (#0)
I0218 14:25:11.780616 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.15331 (* 1 = 2.15331 loss)
I0218 14:25:11.780658 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.54191 (* 1 = 2.54191 loss)
I0218 14:25:11.780666 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.70115 (* 1 = 1.70115 loss)
I0218 14:25:11.780678 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.93182 (* 1 = 1.93182 loss)
I0218 14:25:11.780683 27028 solver.cpp:580]     Test net output #4: prob1 = 0.517
I0218 14:25:11.780690 27028 solver.cpp:580]     Test net output #5: prob2 = 0.468
I0218 14:25:11.780695 27028 solver.cpp:580]     Test net output #6: prob3 = 0.577
I0218 14:25:11.780700 27028 solver.cpp:580]     Test net output #7: prob4 = 0.512
I0218 14:25:11.981161 27028 solver.cpp:357] Iteration 27000 (4.74619 iter/s, 21.0695s/100 iters), loss = 0.661313
I0218 14:25:11.981223 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.184966 (* 1 = 0.184966 loss)
I0218 14:25:11.981233 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.218338 (* 1 = 0.218338 loss)
I0218 14:25:11.981242 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.096067 (* 1 = 0.096067 loss)
I0218 14:25:11.981251 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.161942 (* 1 = 0.161942 loss)
I0218 14:25:11.981264 27028 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0218 14:25:32.225867 27028 solver.cpp:357] Iteration 27100 (4.93949 iter/s, 20.245s/100 iters), loss = 0.512859
I0218 14:25:32.225937 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.106564 (* 1 = 0.106564 loss)
I0218 14:25:32.225949 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.176906 (* 1 = 0.176906 loss)
I0218 14:25:32.225960 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.145624 (* 1 = 0.145624 loss)
I0218 14:25:32.225967 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0837649 (* 1 = 0.0837649 loss)
I0218 14:25:32.225977 27028 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0218 14:25:52.512174 27028 solver.cpp:357] Iteration 27200 (4.92936 iter/s, 20.2866s/100 iters), loss = 0.454799
I0218 14:25:52.512346 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.143184 (* 1 = 0.143184 loss)
I0218 14:25:52.512356 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.167032 (* 1 = 0.167032 loss)
I0218 14:25:52.512364 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.102403 (* 1 = 0.102403 loss)
I0218 14:25:52.512372 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0421791 (* 1 = 0.0421791 loss)
I0218 14:25:52.512380 27028 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0218 14:26:12.778796 27028 solver.cpp:357] Iteration 27300 (4.93417 iter/s, 20.2668s/100 iters), loss = 0.526241
I0218 14:26:12.778867 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.147758 (* 1 = 0.147758 loss)
I0218 14:26:12.778877 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.114608 (* 1 = 0.114608 loss)
I0218 14:26:12.778885 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.133115 (* 1 = 0.133115 loss)
I0218 14:26:12.778893 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.13076 (* 1 = 0.13076 loss)
I0218 14:26:12.778901 27028 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0218 14:26:33.048692 27028 solver.cpp:357] Iteration 27400 (4.93335 iter/s, 20.2702s/100 iters), loss = 0.534172
I0218 14:26:33.048856 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.165316 (* 1 = 0.165316 loss)
I0218 14:26:33.048866 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.166788 (* 1 = 0.166788 loss)
I0218 14:26:33.048874 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0719962 (* 1 = 0.0719962 loss)
I0218 14:26:33.048882 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.130072 (* 1 = 0.130072 loss)
I0218 14:26:33.048889 27028 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0218 14:26:53.323089 27028 solver.cpp:357] Iteration 27500 (4.93229 iter/s, 20.2746s/100 iters), loss = 0.71639
I0218 14:26:53.323158 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.162285 (* 1 = 0.162285 loss)
I0218 14:26:53.323168 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.180517 (* 1 = 0.180517 loss)
I0218 14:26:53.323175 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.197244 (* 1 = 0.197244 loss)
I0218 14:26:53.323184 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.176344 (* 1 = 0.176344 loss)
I0218 14:26:53.323190 27028 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0218 14:27:13.600107 27028 solver.cpp:357] Iteration 27600 (4.93163 iter/s, 20.2773s/100 iters), loss = 0.565568
I0218 14:27:13.600244 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0701213 (* 1 = 0.0701213 loss)
I0218 14:27:13.600255 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.172767 (* 1 = 0.172767 loss)
I0218 14:27:13.600262 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.133386 (* 1 = 0.133386 loss)
I0218 14:27:13.600270 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.189295 (* 1 = 0.189295 loss)
I0218 14:27:13.600281 27028 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0218 14:27:33.877207 27028 solver.cpp:357] Iteration 27700 (4.93163 iter/s, 20.2773s/100 iters), loss = 0.638897
I0218 14:27:33.877274 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.116653 (* 1 = 0.116653 loss)
I0218 14:27:33.877285 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.148065 (* 1 = 0.148065 loss)
I0218 14:27:33.877292 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.219394 (* 1 = 0.219394 loss)
I0218 14:27:33.877300 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.154785 (* 1 = 0.154785 loss)
I0218 14:27:33.877307 27028 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0218 14:27:54.188449 27028 solver.cpp:357] Iteration 27800 (4.92332 iter/s, 20.3115s/100 iters), loss = 0.428155
I0218 14:27:54.188602 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.134363 (* 1 = 0.134363 loss)
I0218 14:27:54.188613 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0494559 (* 1 = 0.0494559 loss)
I0218 14:27:54.188621 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.170139 (* 1 = 0.170139 loss)
I0218 14:27:54.188629 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0741974 (* 1 = 0.0741974 loss)
I0218 14:27:54.188637 27028 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0218 14:28:14.498301 27028 solver.cpp:357] Iteration 27900 (4.92368 iter/s, 20.31s/100 iters), loss = 0.582154
I0218 14:28:14.498363 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.17783 (* 1 = 0.17783 loss)
I0218 14:28:14.498373 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.120791 (* 1 = 0.120791 loss)
I0218 14:28:14.498381 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.137517 (* 1 = 0.137517 loss)
I0218 14:28:14.498389 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.146016 (* 1 = 0.146016 loss)
I0218 14:28:14.498397 27028 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0218 14:28:34.603051 27028 solver.cpp:514] Iteration 28000, Testing net (#0)
I0218 14:28:35.421452 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.375871 (* 1 = 0.375871 loss)
I0218 14:28:35.421494 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.496275 (* 1 = 0.496275 loss)
I0218 14:28:35.421502 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.566097 (* 1 = 0.566097 loss)
I0218 14:28:35.421511 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.389652 (* 1 = 0.389652 loss)
I0218 14:28:35.421517 27028 solver.cpp:580]     Test net output #4: prob1 = 0.902
I0218 14:28:35.421524 27028 solver.cpp:580]     Test net output #5: prob2 = 0.881
I0218 14:28:35.421528 27028 solver.cpp:580]     Test net output #6: prob3 = 0.864
I0218 14:28:35.421533 27028 solver.cpp:580]     Test net output #7: prob4 = 0.893
I0218 14:28:35.623435 27028 solver.cpp:357] Iteration 28000 (4.73363 iter/s, 21.1254s/100 iters), loss = 0.538729
I0218 14:28:35.623484 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.123638 (* 1 = 0.123638 loss)
I0218 14:28:35.623497 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.140324 (* 1 = 0.140324 loss)
I0218 14:28:35.623504 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.138494 (* 1 = 0.138494 loss)
I0218 14:28:35.623512 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.136273 (* 1 = 0.136273 loss)
I0218 14:28:35.623520 27028 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0218 14:28:55.941303 27028 solver.cpp:357] Iteration 28100 (4.92171 iter/s, 20.3181s/100 iters), loss = 0.519041
I0218 14:28:55.941370 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.154641 (* 1 = 0.154641 loss)
I0218 14:28:55.941380 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137725 (* 1 = 0.137725 loss)
I0218 14:28:55.941390 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.049671 (* 1 = 0.049671 loss)
I0218 14:28:55.941397 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.177004 (* 1 = 0.177004 loss)
I0218 14:28:55.941406 27028 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0218 14:29:16.262249 27028 solver.cpp:357] Iteration 28200 (4.92097 iter/s, 20.3212s/100 iters), loss = 0.793078
I0218 14:29:16.262399 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.109916 (* 1 = 0.109916 loss)
I0218 14:29:16.262410 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.373757 (* 1 = 0.373757 loss)
I0218 14:29:16.262418 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.193958 (* 1 = 0.193958 loss)
I0218 14:29:16.262426 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.115447 (* 1 = 0.115447 loss)
I0218 14:29:16.262434 27028 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0218 14:29:36.576306 27028 solver.cpp:357] Iteration 28300 (4.92266 iter/s, 20.3142s/100 iters), loss = 0.5225
I0218 14:29:36.576372 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.113477 (* 1 = 0.113477 loss)
I0218 14:29:36.576385 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0945068 (* 1 = 0.0945068 loss)
I0218 14:29:36.576393 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.155254 (* 1 = 0.155254 loss)
I0218 14:29:36.576401 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.159263 (* 1 = 0.159263 loss)
I0218 14:29:36.576408 27028 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0218 14:29:56.891662 27028 solver.cpp:357] Iteration 28400 (4.92232 iter/s, 20.3156s/100 iters), loss = 0.751963
I0218 14:29:56.891832 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.125073 (* 1 = 0.125073 loss)
I0218 14:29:56.891844 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.246854 (* 1 = 0.246854 loss)
I0218 14:29:56.891851 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.226243 (* 1 = 0.226243 loss)
I0218 14:29:56.891860 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.153794 (* 1 = 0.153794 loss)
I0218 14:29:56.891866 27028 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0218 14:30:17.204301 27028 solver.cpp:357] Iteration 28500 (4.923 iter/s, 20.3128s/100 iters), loss = 0.504009
I0218 14:30:17.204370 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0705794 (* 1 = 0.0705794 loss)
I0218 14:30:17.204380 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.15009 (* 1 = 0.15009 loss)
I0218 14:30:17.204387 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.172955 (* 1 = 0.172955 loss)
I0218 14:30:17.204396 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.110384 (* 1 = 0.110384 loss)
I0218 14:30:17.204403 27028 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0218 14:30:37.519552 27028 solver.cpp:357] Iteration 28600 (4.92235 iter/s, 20.3155s/100 iters), loss = 0.468043
I0218 14:30:37.519709 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0886107 (* 1 = 0.0886107 loss)
I0218 14:30:37.519721 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.119102 (* 1 = 0.119102 loss)
I0218 14:30:37.519729 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.13562 (* 1 = 0.13562 loss)
I0218 14:30:37.519737 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.12471 (* 1 = 0.12471 loss)
I0218 14:30:37.519745 27028 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0218 14:30:57.839869 27028 solver.cpp:357] Iteration 28700 (4.92114 iter/s, 20.3205s/100 iters), loss = 0.452748
I0218 14:30:57.839932 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0957672 (* 1 = 0.0957672 loss)
I0218 14:30:57.839942 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0779951 (* 1 = 0.0779951 loss)
I0218 14:30:57.839951 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.223808 (* 1 = 0.223808 loss)
I0218 14:30:57.839958 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0551779 (* 1 = 0.0551779 loss)
I0218 14:30:57.839970 27028 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0218 14:31:18.156152 27028 solver.cpp:357] Iteration 28800 (4.92209 iter/s, 20.3166s/100 iters), loss = 0.406303
I0218 14:31:18.156293 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110427 (* 1 = 0.110427 loss)
I0218 14:31:18.156303 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.104342 (* 1 = 0.104342 loss)
I0218 14:31:18.156312 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104214 (* 1 = 0.104214 loss)
I0218 14:31:18.156321 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0873206 (* 1 = 0.0873206 loss)
I0218 14:31:18.156330 27028 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0218 14:31:38.505370 27028 solver.cpp:357] Iteration 28900 (4.91415 iter/s, 20.3494s/100 iters), loss = 0.603567
I0218 14:31:38.505440 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.130397 (* 1 = 0.130397 loss)
I0218 14:31:38.505450 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.152936 (* 1 = 0.152936 loss)
I0218 14:31:38.505460 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.172114 (* 1 = 0.172114 loss)
I0218 14:31:38.505466 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.148121 (* 1 = 0.148121 loss)
I0218 14:31:38.505475 27028 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0218 14:31:58.611361 27028 solver.cpp:514] Iteration 29000, Testing net (#0)
I0218 14:31:59.447806 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.99984 (* 1 = 1.99984 loss)
I0218 14:31:59.447849 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.43407 (* 1 = 2.43407 loss)
I0218 14:31:59.447856 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.70225 (* 1 = 1.70225 loss)
I0218 14:31:59.447863 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.23865 (* 1 = 1.23865 loss)
I0218 14:31:59.447870 27028 solver.cpp:580]     Test net output #4: prob1 = 0.529
I0218 14:31:59.447875 27028 solver.cpp:580]     Test net output #5: prob2 = 0.487
I0218 14:31:59.447881 27028 solver.cpp:580]     Test net output #6: prob3 = 0.607
I0218 14:31:59.447886 27028 solver.cpp:580]     Test net output #7: prob4 = 0.689
I0218 14:31:59.649439 27028 solver.cpp:357] Iteration 29000 (4.72939 iter/s, 21.1444s/100 iters), loss = 0.285683
I0218 14:31:59.649489 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0662357 (* 1 = 0.0662357 loss)
I0218 14:31:59.649499 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0634384 (* 1 = 0.0634384 loss)
I0218 14:31:59.649508 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0895329 (* 1 = 0.0895329 loss)
I0218 14:31:59.649515 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0664765 (* 1 = 0.0664765 loss)
I0218 14:31:59.649524 27028 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0218 14:32:19.964475 27028 solver.cpp:357] Iteration 29100 (4.92239 iter/s, 20.3153s/100 iters), loss = 0.643195
I0218 14:32:19.964542 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.109079 (* 1 = 0.109079 loss)
I0218 14:32:19.964552 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.198523 (* 1 = 0.198523 loss)
I0218 14:32:19.964560 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.196707 (* 1 = 0.196707 loss)
I0218 14:32:19.964567 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.138887 (* 1 = 0.138887 loss)
I0218 14:32:19.964576 27028 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0218 14:32:40.287725 27028 solver.cpp:357] Iteration 29200 (4.92041 iter/s, 20.3235s/100 iters), loss = 0.394447
I0218 14:32:40.287890 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.121133 (* 1 = 0.121133 loss)
I0218 14:32:40.287901 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.114449 (* 1 = 0.114449 loss)
I0218 14:32:40.287910 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0861015 (* 1 = 0.0861015 loss)
I0218 14:32:40.287917 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0727638 (* 1 = 0.0727638 loss)
I0218 14:32:40.287925 27028 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0218 14:33:00.599411 27028 solver.cpp:357] Iteration 29300 (4.92323 iter/s, 20.3119s/100 iters), loss = 0.581793
I0218 14:33:00.599478 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.182339 (* 1 = 0.182339 loss)
I0218 14:33:00.599488 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.184415 (* 1 = 0.184415 loss)
I0218 14:33:00.599498 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.124554 (* 1 = 0.124554 loss)
I0218 14:33:00.599506 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0904848 (* 1 = 0.0904848 loss)
I0218 14:33:00.599512 27028 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0218 14:33:20.891610 27028 solver.cpp:357] Iteration 29400 (4.92794 iter/s, 20.2925s/100 iters), loss = 0.556038
I0218 14:33:20.891755 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.101506 (* 1 = 0.101506 loss)
I0218 14:33:20.891767 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.12884 (* 1 = 0.12884 loss)
I0218 14:33:20.891775 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.256552 (* 1 = 0.256552 loss)
I0218 14:33:20.891783 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0691409 (* 1 = 0.0691409 loss)
I0218 14:33:20.891790 27028 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0218 14:33:41.185762 27028 solver.cpp:357] Iteration 29500 (4.92748 iter/s, 20.2943s/100 iters), loss = 0.563287
I0218 14:33:41.185830 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.120942 (* 1 = 0.120942 loss)
I0218 14:33:41.185842 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.118984 (* 1 = 0.118984 loss)
I0218 14:33:41.185849 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.143816 (* 1 = 0.143816 loss)
I0218 14:33:41.185858 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.179546 (* 1 = 0.179546 loss)
I0218 14:33:41.185864 27028 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0218 14:34:01.468559 27028 solver.cpp:357] Iteration 29600 (4.93022 iter/s, 20.2831s/100 iters), loss = 0.382057
I0218 14:34:01.468755 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129877 (* 1 = 0.129877 loss)
I0218 14:34:01.468766 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0713918 (* 1 = 0.0713918 loss)
I0218 14:34:01.468775 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0690064 (* 1 = 0.0690064 loss)
I0218 14:34:01.468783 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.111782 (* 1 = 0.111782 loss)
I0218 14:34:01.468791 27028 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0218 14:34:21.749263 27028 solver.cpp:357] Iteration 29700 (4.93076 iter/s, 20.2809s/100 iters), loss = 0.459635
I0218 14:34:21.749330 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.063089 (* 1 = 0.063089 loss)
I0218 14:34:21.749341 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.122742 (* 1 = 0.122742 loss)
I0218 14:34:21.749349 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.143043 (* 1 = 0.143043 loss)
I0218 14:34:21.749356 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.130761 (* 1 = 0.130761 loss)
I0218 14:34:21.749363 27028 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0218 14:34:42.033519 27028 solver.cpp:357] Iteration 29800 (4.92987 iter/s, 20.2845s/100 iters), loss = 0.403702
I0218 14:34:42.033634 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0957654 (* 1 = 0.0957654 loss)
I0218 14:34:42.033645 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.152913 (* 1 = 0.152913 loss)
I0218 14:34:42.033653 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.100389 (* 1 = 0.100389 loss)
I0218 14:34:42.033661 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0546354 (* 1 = 0.0546354 loss)
I0218 14:34:42.033668 27028 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0218 14:35:02.327216 27028 solver.cpp:357] Iteration 29900 (4.92758 iter/s, 20.2939s/100 iters), loss = 0.455136
I0218 14:35:02.327278 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.117982 (* 1 = 0.117982 loss)
I0218 14:35:02.327288 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.104256 (* 1 = 0.104256 loss)
I0218 14:35:02.327297 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.139602 (* 1 = 0.139602 loss)
I0218 14:35:02.327306 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0932956 (* 1 = 0.0932956 loss)
I0218 14:35:02.327312 27028 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0218 14:35:22.423408 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.caffemodel
I0218 14:35:22.431954 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_30000.solverstate
I0218 14:35:22.435073 27028 solver.cpp:514] Iteration 30000, Testing net (#0)
I0218 14:35:23.253506 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.08452 (* 1 = 1.08452 loss)
I0218 14:35:23.253547 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.0795 (* 1 = 1.0795 loss)
I0218 14:35:23.253556 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.749417 (* 1 = 0.749417 loss)
I0218 14:35:23.253566 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.579407 (* 1 = 0.579407 loss)
I0218 14:35:23.253571 27028 solver.cpp:580]     Test net output #4: prob1 = 0.733
I0218 14:35:23.253577 27028 solver.cpp:580]     Test net output #5: prob2 = 0.721
I0218 14:35:23.253582 27028 solver.cpp:580]     Test net output #6: prob3 = 0.804
I0218 14:35:23.253588 27028 solver.cpp:580]     Test net output #7: prob4 = 0.842
I0218 14:35:23.454895 27028 solver.cpp:357] Iteration 30000 (4.73306 iter/s, 21.128s/100 iters), loss = 0.51491
I0218 14:35:23.454941 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0860422 (* 1 = 0.0860422 loss)
I0218 14:35:23.454952 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.143401 (* 1 = 0.143401 loss)
I0218 14:35:23.454960 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0995689 (* 1 = 0.0995689 loss)
I0218 14:35:23.454968 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.185897 (* 1 = 0.185897 loss)
I0218 14:35:23.454977 27028 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0218 14:35:46.757453 27028 solver.cpp:357] Iteration 30100 (4.29131 iter/s, 23.3029s/100 iters), loss = 0.537483
I0218 14:35:46.757527 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.129359 (* 1 = 0.129359 loss)
I0218 14:35:46.757537 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137186 (* 1 = 0.137186 loss)
I0218 14:35:46.757545 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.135688 (* 1 = 0.135688 loss)
I0218 14:35:46.757553 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.13525 (* 1 = 0.13525 loss)
I0218 14:35:46.757561 27028 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0218 14:36:19.041983 27028 solver.cpp:357] Iteration 30200 (3.09761 iter/s, 32.2829s/100 iters), loss = 0.480311
I0218 14:36:19.042206 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.175954 (* 1 = 0.175954 loss)
I0218 14:36:19.042217 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0951642 (* 1 = 0.0951642 loss)
I0218 14:36:19.042225 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.139538 (* 1 = 0.139538 loss)
I0218 14:36:19.042233 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0696538 (* 1 = 0.0696538 loss)
I0218 14:36:19.042240 27028 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0218 14:36:54.864444 27028 solver.cpp:357] Iteration 30300 (2.79152 iter/s, 35.8228s/100 iters), loss = 0.383812
I0218 14:36:54.864622 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0983669 (* 1 = 0.0983669 loss)
I0218 14:36:54.864634 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0408932 (* 1 = 0.0408932 loss)
I0218 14:36:54.864642 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.157474 (* 1 = 0.157474 loss)
I0218 14:36:54.864650 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0870779 (* 1 = 0.0870779 loss)
I0218 14:36:54.864660 27028 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0218 14:37:30.732544 27028 solver.cpp:357] Iteration 30400 (2.78811 iter/s, 35.8666s/100 iters), loss = 0.453975
I0218 14:37:30.732673 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0750268 (* 1 = 0.0750268 loss)
I0218 14:37:30.732684 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.124411 (* 1 = 0.124411 loss)
I0218 14:37:30.732692 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.102537 (* 1 = 0.102537 loss)
I0218 14:37:30.732700 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.152001 (* 1 = 0.152001 loss)
I0218 14:37:30.732707 27028 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0218 14:38:06.505913 27028 solver.cpp:357] Iteration 30500 (2.79534 iter/s, 35.7738s/100 iters), loss = 0.387423
I0218 14:38:06.506050 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.118867 (* 1 = 0.118867 loss)
I0218 14:38:06.506062 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0864379 (* 1 = 0.0864379 loss)
I0218 14:38:06.506070 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0883893 (* 1 = 0.0883893 loss)
I0218 14:38:06.506078 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0937296 (* 1 = 0.0937296 loss)
I0218 14:38:06.506085 27028 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0218 14:38:40.253638 27028 solver.cpp:357] Iteration 30600 (2.96312 iter/s, 33.7482s/100 iters), loss = 0.488939
I0218 14:38:40.253799 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.128812 (* 1 = 0.128812 loss)
I0218 14:38:40.253810 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.120357 (* 1 = 0.120357 loss)
I0218 14:38:40.253818 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.129672 (* 1 = 0.129672 loss)
I0218 14:38:40.253826 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.110099 (* 1 = 0.110099 loss)
I0218 14:38:40.253834 27028 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0218 14:39:11.630829 27028 solver.cpp:357] Iteration 30700 (3.18699 iter/s, 31.3776s/100 iters), loss = 0.496031
I0218 14:39:11.631211 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0779943 (* 1 = 0.0779943 loss)
I0218 14:39:11.631281 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.133854 (* 1 = 0.133854 loss)
I0218 14:39:11.631321 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.168122 (* 1 = 0.168122 loss)
I0218 14:39:11.631361 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.116061 (* 1 = 0.116061 loss)
I0218 14:39:11.631394 27028 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0218 14:39:47.521558 27028 solver.cpp:357] Iteration 30800 (2.78621 iter/s, 35.891s/100 iters), loss = 0.507222
I0218 14:39:47.521895 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110794 (* 1 = 0.110794 loss)
I0218 14:39:47.521997 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.160478 (* 1 = 0.160478 loss)
I0218 14:39:47.522039 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.104863 (* 1 = 0.104863 loss)
I0218 14:39:47.522078 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.131087 (* 1 = 0.131087 loss)
I0218 14:39:47.522111 27028 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0218 14:40:23.299178 27028 solver.cpp:357] Iteration 30900 (2.79517 iter/s, 35.776s/100 iters), loss = 0.675364
I0218 14:40:23.299476 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.130688 (* 1 = 0.130688 loss)
I0218 14:40:23.299546 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.137527 (* 1 = 0.137527 loss)
I0218 14:40:23.299585 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.286853 (* 1 = 0.286853 loss)
I0218 14:40:23.299624 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.120297 (* 1 = 0.120297 loss)
I0218 14:40:23.299656 27028 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0218 14:40:58.732497 27028 solver.cpp:514] Iteration 31000, Testing net (#0)
I0218 14:41:01.207484 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.10424 (* 1 = 2.10424 loss)
I0218 14:41:01.207531 27028 solver.cpp:580]     Test net output #1: Softmax2 = 4.40229 (* 1 = 4.40229 loss)
I0218 14:41:01.207540 27028 solver.cpp:580]     Test net output #2: Softmax3 = 5.33948 (* 1 = 5.33948 loss)
I0218 14:41:01.207548 27028 solver.cpp:580]     Test net output #3: Softmax4 = 5.60881 (* 1 = 5.60881 loss)
I0218 14:41:01.207554 27028 solver.cpp:580]     Test net output #4: prob1 = 0.527
I0218 14:41:01.207561 27028 solver.cpp:580]     Test net output #5: prob2 = 0.35
I0218 14:41:01.207567 27028 solver.cpp:580]     Test net output #6: prob3 = 0.158
I0218 14:41:01.207572 27028 solver.cpp:580]     Test net output #7: prob4 = 0.133
I0218 14:41:01.543238 27028 solver.cpp:357] Iteration 31000 (2.61476 iter/s, 38.2445s/100 iters), loss = 0.591916
I0218 14:41:01.543295 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.149965 (* 1 = 0.149965 loss)
I0218 14:41:01.543305 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.171963 (* 1 = 0.171963 loss)
I0218 14:41:01.543314 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0932597 (* 1 = 0.0932597 loss)
I0218 14:41:01.543323 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.176727 (* 1 = 0.176727 loss)
I0218 14:41:01.543329 27028 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0218 14:41:35.290491 27028 solver.cpp:357] Iteration 31100 (2.96316 iter/s, 33.7478s/100 iters), loss = 0.598507
I0218 14:41:35.290643 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.183635 (* 1 = 0.183635 loss)
I0218 14:41:35.290655 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0904019 (* 1 = 0.0904019 loss)
I0218 14:41:35.290664 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.194002 (* 1 = 0.194002 loss)
I0218 14:41:35.290671 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.130467 (* 1 = 0.130467 loss)
I0218 14:41:35.290678 27028 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0218 14:42:06.660154 27028 solver.cpp:357] Iteration 31200 (3.18776 iter/s, 31.37s/100 iters), loss = 0.468831
I0218 14:42:06.660411 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0660983 (* 1 = 0.0660983 loss)
I0218 14:42:06.660434 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.149524 (* 1 = 0.149524 loss)
I0218 14:42:06.660454 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.147997 (* 1 = 0.147997 loss)
I0218 14:42:06.660481 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.105213 (* 1 = 0.105213 loss)
I0218 14:42:06.660517 27028 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0218 14:42:42.425806 27028 solver.cpp:357] Iteration 31300 (2.79595 iter/s, 35.766s/100 iters), loss = 0.409184
I0218 14:42:42.426141 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0850344 (* 1 = 0.0850344 loss)
I0218 14:42:42.426208 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.108067 (* 1 = 0.108067 loss)
I0218 14:42:42.426242 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.178593 (* 1 = 0.178593 loss)
I0218 14:42:42.426275 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0374903 (* 1 = 0.0374903 loss)
I0218 14:42:42.426304 27028 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0218 14:43:18.285835 27028 solver.cpp:357] Iteration 31400 (2.78859 iter/s, 35.8604s/100 iters), loss = 0.50191
I0218 14:43:18.285985 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.111386 (* 1 = 0.111386 loss)
I0218 14:43:18.286001 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.096654 (* 1 = 0.096654 loss)
I0218 14:43:18.286010 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.170179 (* 1 = 0.170179 loss)
I0218 14:43:18.286018 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.123691 (* 1 = 0.123691 loss)
I0218 14:43:18.286026 27028 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0218 14:43:54.080848 27028 solver.cpp:357] Iteration 31500 (2.79381 iter/s, 35.7934s/100 iters), loss = 0.431269
I0218 14:43:54.080935 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.15082 (* 1 = 0.15082 loss)
I0218 14:43:54.080947 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0853544 (* 1 = 0.0853544 loss)
I0218 14:43:54.080956 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.122826 (* 1 = 0.122826 loss)
I0218 14:43:54.080965 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0722695 (* 1 = 0.0722695 loss)
I0218 14:43:54.080971 27028 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0218 14:44:28.720336 27028 solver.cpp:357] Iteration 31600 (2.88684 iter/s, 34.64s/100 iters), loss = 0.476939
I0218 14:44:28.720453 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.106198 (* 1 = 0.106198 loss)
I0218 14:44:28.720463 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0886548 (* 1 = 0.0886548 loss)
I0218 14:44:28.720471 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.169183 (* 1 = 0.169183 loss)
I0218 14:44:28.720479 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.112904 (* 1 = 0.112904 loss)
I0218 14:44:28.720487 27028 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0218 14:44:59.125368 27028 solver.cpp:357] Iteration 31700 (3.28889 iter/s, 30.4054s/100 iters), loss = 0.353585
I0218 14:44:59.125677 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0607475 (* 1 = 0.0607475 loss)
I0218 14:44:59.125742 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.113673 (* 1 = 0.113673 loss)
I0218 14:44:59.125778 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0597008 (* 1 = 0.0597008 loss)
I0218 14:44:59.125811 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.119463 (* 1 = 0.119463 loss)
I0218 14:44:59.125840 27028 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0218 14:45:34.894819 27028 solver.cpp:357] Iteration 31800 (2.79565 iter/s, 35.7698s/100 iters), loss = 0.647354
I0218 14:45:34.895244 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.230998 (* 1 = 0.230998 loss)
I0218 14:45:34.895321 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.190733 (* 1 = 0.190733 loss)
I0218 14:45:34.895366 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.129564 (* 1 = 0.129564 loss)
I0218 14:45:34.895408 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.09606 (* 1 = 0.09606 loss)
I0218 14:45:34.895443 27028 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0218 14:46:10.692100 27028 solver.cpp:357] Iteration 31900 (2.79363 iter/s, 35.7957s/100 iters), loss = 0.545454
I0218 14:46:10.692246 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.105378 (* 1 = 0.105378 loss)
I0218 14:46:10.692256 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.141028 (* 1 = 0.141028 loss)
I0218 14:46:10.692265 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.10652 (* 1 = 0.10652 loss)
I0218 14:46:10.692273 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.192528 (* 1 = 0.192528 loss)
I0218 14:46:10.692281 27028 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0218 14:46:46.165169 27028 solver.cpp:514] Iteration 32000, Testing net (#0)
I0218 14:46:48.556805 27028 solver.cpp:580]     Test net output #0: Softmax1 = 4.77915 (* 1 = 4.77915 loss)
I0218 14:46:48.556857 27028 solver.cpp:580]     Test net output #1: Softmax2 = 10.7509 (* 1 = 10.7509 loss)
I0218 14:46:48.556866 27028 solver.cpp:580]     Test net output #2: Softmax3 = 8.18605 (* 1 = 8.18605 loss)
I0218 14:46:48.556875 27028 solver.cpp:580]     Test net output #3: Softmax4 = 11.5817 (* 1 = 11.5817 loss)
I0218 14:46:48.556881 27028 solver.cpp:580]     Test net output #4: prob1 = 0.245
I0218 14:46:48.556887 27028 solver.cpp:580]     Test net output #5: prob2 = 0.084
I0218 14:46:48.556893 27028 solver.cpp:580]     Test net output #6: prob3 = 0.052
I0218 14:46:48.556900 27028 solver.cpp:580]     Test net output #7: prob4 = 0.039
I0218 14:46:48.894532 27028 solver.cpp:357] Iteration 32000 (2.61774 iter/s, 38.2009s/100 iters), loss = 0.518086
I0218 14:46:48.894584 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.110632 (* 1 = 0.110632 loss)
I0218 14:46:48.894594 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.171079 (* 1 = 0.171079 loss)
I0218 14:46:48.894603 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.113438 (* 1 = 0.113438 loss)
I0218 14:46:48.894615 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.122937 (* 1 = 0.122937 loss)
I0218 14:46:48.894621 27028 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0218 14:46:48.894626 27028 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0218 14:47:23.661980 27028 solver.cpp:357] Iteration 32100 (2.87621 iter/s, 34.768s/100 iters), loss = 0.29376
I0218 14:47:23.662092 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0829829 (* 1 = 0.0829829 loss)
I0218 14:47:23.662103 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.070131 (* 1 = 0.070131 loss)
I0218 14:47:23.662112 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0569567 (* 1 = 0.0569567 loss)
I0218 14:47:23.662120 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0836896 (* 1 = 0.0836896 loss)
I0218 14:47:23.662128 27028 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0218 14:47:53.947960 27028 solver.cpp:357] Iteration 32200 (3.30181 iter/s, 30.2864s/100 iters), loss = 0.297848
I0218 14:47:53.948251 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0766446 (* 1 = 0.0766446 loss)
I0218 14:47:53.948318 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0636939 (* 1 = 0.0636939 loss)
I0218 14:47:53.948354 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0646623 (* 1 = 0.0646623 loss)
I0218 14:47:53.948386 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0928468 (* 1 = 0.0928468 loss)
I0218 14:47:53.948413 27028 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0218 14:48:29.733180 27028 solver.cpp:357] Iteration 32300 (2.79442 iter/s, 35.7856s/100 iters), loss = 0.221292
I0218 14:48:29.733379 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0503393 (* 1 = 0.0503393 loss)
I0218 14:48:29.733391 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0265417 (* 1 = 0.0265417 loss)
I0218 14:48:29.733398 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0850588 (* 1 = 0.0850588 loss)
I0218 14:48:29.733407 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0593526 (* 1 = 0.0593526 loss)
I0218 14:48:29.733413 27028 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0218 14:49:05.504446 27028 solver.cpp:357] Iteration 32400 (2.79551 iter/s, 35.7717s/100 iters), loss = 0.249381
I0218 14:49:05.504762 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0878184 (* 1 = 0.0878184 loss)
I0218 14:49:05.504835 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0647502 (* 1 = 0.0647502 loss)
I0218 14:49:05.504878 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0504909 (* 1 = 0.0504909 loss)
I0218 14:49:05.504917 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0463214 (* 1 = 0.0463214 loss)
I0218 14:49:05.504952 27028 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0218 14:49:41.306736 27028 solver.cpp:357] Iteration 32500 (2.79309 iter/s, 35.8027s/100 iters), loss = 0.179524
I0218 14:49:41.306887 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0233046 (* 1 = 0.0233046 loss)
I0218 14:49:41.306897 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0452686 (* 1 = 0.0452686 loss)
I0218 14:49:41.306906 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0860165 (* 1 = 0.0860165 loss)
I0218 14:49:41.306913 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0249345 (* 1 = 0.0249345 loss)
I0218 14:49:41.306921 27028 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0218 14:50:17.012008 27028 solver.cpp:357] Iteration 32600 (2.80067 iter/s, 35.7057s/100 iters), loss = 0.234194
I0218 14:50:17.012115 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0667491 (* 1 = 0.0667491 loss)
I0218 14:50:17.012125 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0250883 (* 1 = 0.0250883 loss)
I0218 14:50:17.012133 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.101344 (* 1 = 0.101344 loss)
I0218 14:50:17.012141 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.041013 (* 1 = 0.041013 loss)
I0218 14:50:17.012149 27028 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0218 14:50:46.209329 27028 solver.cpp:357] Iteration 32700 (3.42493 iter/s, 29.1977s/100 iters), loss = 0.143549
I0218 14:50:46.209404 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0470196 (* 1 = 0.0470196 loss)
I0218 14:50:46.209414 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.020411 (* 1 = 0.020411 loss)
I0218 14:50:46.209424 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.03034 (* 1 = 0.03034 loss)
I0218 14:50:46.209431 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0457781 (* 1 = 0.0457781 loss)
I0218 14:50:46.209439 27028 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0218 14:51:21.959394 27028 solver.cpp:357] Iteration 32800 (2.79716 iter/s, 35.7506s/100 iters), loss = 0.209818
I0218 14:51:21.959561 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0264248 (* 1 = 0.0264248 loss)
I0218 14:51:21.959571 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0827884 (* 1 = 0.0827884 loss)
I0218 14:51:21.959580 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0623942 (* 1 = 0.0623942 loss)
I0218 14:51:21.959589 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0382109 (* 1 = 0.0382109 loss)
I0218 14:51:21.959595 27028 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0218 14:51:57.745450 27028 solver.cpp:357] Iteration 32900 (2.79435 iter/s, 35.7865s/100 iters), loss = 0.206615
I0218 14:51:57.745901 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0541326 (* 1 = 0.0541326 loss)
I0218 14:51:57.745999 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0553681 (* 1 = 0.0553681 loss)
I0218 14:51:57.746037 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0512104 (* 1 = 0.0512104 loss)
I0218 14:51:57.746068 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0459042 (* 1 = 0.0459042 loss)
I0218 14:51:57.746096 27028 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0218 14:52:33.254874 27028 solver.cpp:514] Iteration 33000, Testing net (#0)
I0218 14:52:35.659376 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.312765 (* 1 = 0.312765 loss)
I0218 14:52:35.659425 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.220291 (* 1 = 0.220291 loss)
I0218 14:52:35.659435 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.309748 (* 1 = 0.309748 loss)
I0218 14:52:35.659445 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.348128 (* 1 = 0.348128 loss)
I0218 14:52:35.659451 27028 solver.cpp:580]     Test net output #4: prob1 = 0.952
I0218 14:52:35.659456 27028 solver.cpp:580]     Test net output #5: prob2 = 0.951
I0218 14:52:35.659461 27028 solver.cpp:580]     Test net output #6: prob3 = 0.932
I0218 14:52:35.659467 27028 solver.cpp:580]     Test net output #7: prob4 = 0.927
I0218 14:52:35.996378 27028 solver.cpp:357] Iteration 33000 (2.6143 iter/s, 38.2512s/100 iters), loss = 0.169932
I0218 14:52:35.996431 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0362809 (* 1 = 0.0362809 loss)
I0218 14:52:35.996441 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0550437 (* 1 = 0.0550437 loss)
I0218 14:52:35.996449 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0480647 (* 1 = 0.0480647 loss)
I0218 14:52:35.996457 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0305423 (* 1 = 0.0305423 loss)
I0218 14:52:35.996465 27028 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0218 14:53:11.852330 27028 solver.cpp:357] Iteration 33100 (2.78889 iter/s, 35.8565s/100 iters), loss = 0.132418
I0218 14:53:11.852515 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0631181 (* 1 = 0.0631181 loss)
I0218 14:53:11.852526 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0277747 (* 1 = 0.0277747 loss)
I0218 14:53:11.852535 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0153711 (* 1 = 0.0153711 loss)
I0218 14:53:11.852543 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0261544 (* 1 = 0.0261544 loss)
I0218 14:53:11.852551 27028 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0218 14:53:40.894145 27028 solver.cpp:357] Iteration 33200 (3.44351 iter/s, 29.0401s/100 iters), loss = 0.15374
I0218 14:53:40.894217 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0695938 (* 1 = 0.0695938 loss)
I0218 14:53:40.894227 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0242541 (* 1 = 0.0242541 loss)
I0218 14:53:40.894235 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0230537 (* 1 = 0.0230537 loss)
I0218 14:53:40.894243 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0368386 (* 1 = 0.0368386 loss)
I0218 14:53:40.894250 27028 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0218 14:54:16.654533 27028 solver.cpp:357] Iteration 33300 (2.79635 iter/s, 35.7609s/100 iters), loss = 0.119647
I0218 14:54:16.654845 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0445142 (* 1 = 0.0445142 loss)
I0218 14:54:16.654917 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.020632 (* 1 = 0.020632 loss)
I0218 14:54:16.654959 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.021727 (* 1 = 0.021727 loss)
I0218 14:54:16.654999 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0327734 (* 1 = 0.0327734 loss)
I0218 14:54:16.655032 27028 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0218 14:54:52.433013 27028 solver.cpp:357] Iteration 33400 (2.79495 iter/s, 35.7789s/100 iters), loss = 0.125644
I0218 14:54:52.433174 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0310041 (* 1 = 0.0310041 loss)
I0218 14:54:52.433184 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0157201 (* 1 = 0.0157201 loss)
I0218 14:54:52.433193 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0488278 (* 1 = 0.0488278 loss)
I0218 14:54:52.433200 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0300925 (* 1 = 0.0300925 loss)
I0218 14:54:52.433212 27028 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0218 14:55:28.191471 27028 solver.cpp:357] Iteration 33500 (2.79651 iter/s, 35.7589s/100 iters), loss = 0.116643
I0218 14:55:28.191661 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.028962 (* 1 = 0.028962 loss)
I0218 14:55:28.191673 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0269684 (* 1 = 0.0269684 loss)
I0218 14:55:28.191684 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0275908 (* 1 = 0.0275908 loss)
I0218 14:55:28.191692 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0331213 (* 1 = 0.0331213 loss)
I0218 14:55:28.191700 27028 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0218 14:56:04.045711 27028 solver.cpp:357] Iteration 33600 (2.78904 iter/s, 35.8547s/100 iters), loss = 0.176589
I0218 14:56:04.045882 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0537592 (* 1 = 0.0537592 loss)
I0218 14:56:04.045893 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0328813 (* 1 = 0.0328813 loss)
I0218 14:56:04.045903 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0325095 (* 1 = 0.0325095 loss)
I0218 14:56:04.045912 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0574386 (* 1 = 0.0574386 loss)
I0218 14:56:04.045918 27028 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0218 14:56:33.258803 27028 solver.cpp:357] Iteration 33700 (3.42331 iter/s, 29.2115s/100 iters), loss = 0.106799
I0218 14:56:33.258878 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0248181 (* 1 = 0.0248181 loss)
I0218 14:56:33.258889 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0236617 (* 1 = 0.0236617 loss)
I0218 14:56:33.258898 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0246335 (* 1 = 0.0246335 loss)
I0218 14:56:33.258906 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0336859 (* 1 = 0.0336859 loss)
I0218 14:56:33.258913 27028 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0218 14:57:09.030704 27028 solver.cpp:357] Iteration 33800 (2.79545 iter/s, 35.7724s/100 iters), loss = 0.152614
I0218 14:57:09.030869 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0488287 (* 1 = 0.0488287 loss)
I0218 14:57:09.030879 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0560885 (* 1 = 0.0560885 loss)
I0218 14:57:09.030887 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0145768 (* 1 = 0.0145768 loss)
I0218 14:57:09.030895 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0331203 (* 1 = 0.0331203 loss)
I0218 14:57:09.030902 27028 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0218 14:57:44.783361 27028 solver.cpp:357] Iteration 33900 (2.79696 iter/s, 35.7531s/100 iters), loss = 0.13227
I0218 14:57:44.783524 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0310287 (* 1 = 0.0310287 loss)
I0218 14:57:44.783535 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0478353 (* 1 = 0.0478353 loss)
I0218 14:57:44.783543 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.028116 (* 1 = 0.028116 loss)
I0218 14:57:44.783551 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0252897 (* 1 = 0.0252897 loss)
I0218 14:57:44.783558 27028 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0218 14:58:20.213264 27028 solver.cpp:514] Iteration 34000, Testing net (#0)
I0218 14:58:22.690657 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.917183 (* 1 = 0.917183 loss)
I0218 14:58:22.690707 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.867445 (* 1 = 0.867445 loss)
I0218 14:58:22.690716 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.917415 (* 1 = 0.917415 loss)
I0218 14:58:22.690723 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.30018 (* 1 = 1.30018 loss)
I0218 14:58:22.690729 27028 solver.cpp:580]     Test net output #4: prob1 = 0.825
I0218 14:58:22.690735 27028 solver.cpp:580]     Test net output #5: prob2 = 0.819
I0218 14:58:22.690740 27028 solver.cpp:580]     Test net output #6: prob3 = 0.813
I0218 14:58:22.690747 27028 solver.cpp:580]     Test net output #7: prob4 = 0.774
I0218 14:58:23.026772 27028 solver.cpp:357] Iteration 34000 (2.61479 iter/s, 38.2439s/100 iters), loss = 0.158392
I0218 14:58:23.026831 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0205582 (* 1 = 0.0205582 loss)
I0218 14:58:23.026841 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0395688 (* 1 = 0.0395688 loss)
I0218 14:58:23.026850 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0231867 (* 1 = 0.0231867 loss)
I0218 14:58:23.026857 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0750778 (* 1 = 0.0750778 loss)
I0218 14:58:23.026865 27028 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0218 14:58:58.797396 27028 solver.cpp:357] Iteration 34100 (2.79555 iter/s, 35.7712s/100 iters), loss = 0.142024
I0218 14:58:58.797631 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0306205 (* 1 = 0.0306205 loss)
I0218 14:58:58.797643 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0286603 (* 1 = 0.0286603 loss)
I0218 14:58:58.797652 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0409335 (* 1 = 0.0409335 loss)
I0218 14:58:58.797660 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0418097 (* 1 = 0.0418097 loss)
I0218 14:58:58.797667 27028 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0218 14:59:28.074447 27028 solver.cpp:357] Iteration 34200 (3.41561 iter/s, 29.2773s/100 iters), loss = 0.124131
I0218 14:59:28.074525 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0460665 (* 1 = 0.0460665 loss)
I0218 14:59:28.074535 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0247457 (* 1 = 0.0247457 loss)
I0218 14:59:28.074544 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0156513 (* 1 = 0.0156513 loss)
I0218 14:59:28.074553 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.037668 (* 1 = 0.037668 loss)
I0218 14:59:28.074559 27028 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0218 15:00:03.873848 27028 solver.cpp:357] Iteration 34300 (2.79346 iter/s, 35.7979s/100 iters), loss = 0.217731
I0218 15:00:03.874009 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.118318 (* 1 = 0.118318 loss)
I0218 15:00:03.874019 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0494632 (* 1 = 0.0494632 loss)
I0218 15:00:03.874029 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0224161 (* 1 = 0.0224161 loss)
I0218 15:00:03.874037 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0275335 (* 1 = 0.0275335 loss)
I0218 15:00:03.874044 27028 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0218 15:00:39.648037 27028 solver.cpp:357] Iteration 34400 (2.79528 iter/s, 35.7746s/100 iters), loss = 0.14221
I0218 15:00:39.648156 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0242673 (* 1 = 0.0242673 loss)
I0218 15:00:39.648166 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0401181 (* 1 = 0.0401181 loss)
I0218 15:00:39.648175 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0610425 (* 1 = 0.0610425 loss)
I0218 15:00:39.648183 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0167825 (* 1 = 0.0167825 loss)
I0218 15:00:39.648190 27028 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0218 15:01:15.411559 27028 solver.cpp:357] Iteration 34500 (2.79588 iter/s, 35.7669s/100 iters), loss = 0.10133
I0218 15:01:15.411675 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0128671 (* 1 = 0.0128671 loss)
I0218 15:01:15.411685 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0169136 (* 1 = 0.0169136 loss)
I0218 15:01:15.411695 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0437626 (* 1 = 0.0437626 loss)
I0218 15:01:15.411702 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0277867 (* 1 = 0.0277867 loss)
I0218 15:01:15.411710 27028 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0218 15:01:51.178678 27028 solver.cpp:357] Iteration 34600 (2.79554 iter/s, 35.7712s/100 iters), loss = 0.148564
I0218 15:01:51.178905 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0287814 (* 1 = 0.0287814 loss)
I0218 15:01:51.178915 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0618551 (* 1 = 0.0618551 loss)
I0218 15:01:51.178925 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0324822 (* 1 = 0.0324822 loss)
I0218 15:01:51.178932 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0254458 (* 1 = 0.0254458 loss)
I0218 15:01:51.178941 27028 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0218 15:02:21.077826 27028 solver.cpp:357] Iteration 34700 (3.34423 iter/s, 29.9023s/100 iters), loss = 0.178936
I0218 15:02:21.077893 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0466267 (* 1 = 0.0466267 loss)
I0218 15:02:21.077903 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0535756 (* 1 = 0.0535756 loss)
I0218 15:02:21.077913 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0210298 (* 1 = 0.0210298 loss)
I0218 15:02:21.077920 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0577041 (* 1 = 0.0577041 loss)
I0218 15:02:21.077929 27028 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0218 15:02:56.199790 27028 solver.cpp:357] Iteration 34800 (2.84692 iter/s, 35.1257s/100 iters), loss = 0.0834717
I0218 15:02:56.199923 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0171101 (* 1 = 0.0171101 loss)
I0218 15:02:56.199932 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0167271 (* 1 = 0.0167271 loss)
I0218 15:02:56.199941 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.018507 (* 1 = 0.018507 loss)
I0218 15:02:56.199949 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0311275 (* 1 = 0.0311275 loss)
I0218 15:02:56.199957 27028 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0218 15:03:32.002825 27028 solver.cpp:357] Iteration 34900 (2.79294 iter/s, 35.8045s/100 iters), loss = 0.0959076
I0218 15:03:32.002980 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0205509 (* 1 = 0.0205509 loss)
I0218 15:03:32.002990 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0140954 (* 1 = 0.0140954 loss)
I0218 15:03:32.003000 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0179315 (* 1 = 0.0179315 loss)
I0218 15:03:32.003007 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0433298 (* 1 = 0.0433298 loss)
I0218 15:03:32.003015 27028 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0218 15:04:07.434312 27028 solver.cpp:514] Iteration 35000, Testing net (#0)
I0218 15:04:09.800449 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.46356 (* 1 = 1.46356 loss)
I0218 15:04:09.800496 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.56194 (* 1 = 1.56194 loss)
I0218 15:04:09.800504 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.64758 (* 1 = 1.64758 loss)
I0218 15:04:09.800515 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.10889 (* 1 = 2.10889 loss)
I0218 15:04:09.800521 27028 solver.cpp:580]     Test net output #4: prob1 = 0.736
I0218 15:04:09.800529 27028 solver.cpp:580]     Test net output #5: prob2 = 0.678
I0218 15:04:09.800534 27028 solver.cpp:580]     Test net output #6: prob3 = 0.657
I0218 15:04:09.800539 27028 solver.cpp:580]     Test net output #7: prob4 = 0.615
I0218 15:04:10.140940 27028 solver.cpp:357] Iteration 35000 (2.6218 iter/s, 38.1417s/100 iters), loss = 0.0965924
I0218 15:04:10.141000 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0211967 (* 1 = 0.0211967 loss)
I0218 15:04:10.141010 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0145681 (* 1 = 0.0145681 loss)
I0218 15:04:10.141019 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0318739 (* 1 = 0.0318739 loss)
I0218 15:04:10.141027 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0289538 (* 1 = 0.0289538 loss)
I0218 15:04:10.141036 27028 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0218 15:04:45.929172 27028 solver.cpp:357] Iteration 35100 (2.79412 iter/s, 35.7894s/100 iters), loss = 0.111881
I0218 15:04:45.929345 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.029776 (* 1 = 0.029776 loss)
I0218 15:04:45.929356 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0202277 (* 1 = 0.0202277 loss)
I0218 15:04:45.929365 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0173455 (* 1 = 0.0173455 loss)
I0218 15:04:45.929411 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0445313 (* 1 = 0.0445313 loss)
I0218 15:04:45.929420 27028 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0218 15:05:15.921960 27028 solver.cpp:357] Iteration 35200 (3.33408 iter/s, 29.9933s/100 iters), loss = 0.0957634
I0218 15:05:15.922037 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0148049 (* 1 = 0.0148049 loss)
I0218 15:05:15.922047 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0283363 (* 1 = 0.0283363 loss)
I0218 15:05:15.922055 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0200204 (* 1 = 0.0200204 loss)
I0218 15:05:15.922063 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0326019 (* 1 = 0.0326019 loss)
I0218 15:05:15.922071 27028 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0218 15:05:50.809932 27028 solver.cpp:357] Iteration 35300 (2.86608 iter/s, 34.8909s/100 iters), loss = 0.0882069
I0218 15:05:50.810056 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0197005 (* 1 = 0.0197005 loss)
I0218 15:05:50.810067 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0144029 (* 1 = 0.0144029 loss)
I0218 15:05:50.810076 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0199666 (* 1 = 0.0199666 loss)
I0218 15:05:50.810084 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0341369 (* 1 = 0.0341369 loss)
I0218 15:05:50.810091 27028 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0218 15:06:26.581040 27028 solver.cpp:357] Iteration 35400 (2.79533 iter/s, 35.774s/100 iters), loss = 0.0901375
I0218 15:06:26.581195 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0137004 (* 1 = 0.0137004 loss)
I0218 15:06:26.581205 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0246578 (* 1 = 0.0246578 loss)
I0218 15:06:26.581214 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0225836 (* 1 = 0.0225836 loss)
I0218 15:06:26.581223 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0291958 (* 1 = 0.0291958 loss)
I0218 15:06:26.581230 27028 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0218 15:07:02.368198 27028 solver.cpp:357] Iteration 35500 (2.79409 iter/s, 35.7899s/100 iters), loss = 0.103027
I0218 15:07:02.368326 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0332627 (* 1 = 0.0332627 loss)
I0218 15:07:02.368337 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.024207 (* 1 = 0.024207 loss)
I0218 15:07:02.368346 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0178182 (* 1 = 0.0178182 loss)
I0218 15:07:02.368355 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0277387 (* 1 = 0.0277387 loss)
I0218 15:07:02.368362 27028 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0218 15:07:38.143595 27028 solver.cpp:357] Iteration 35600 (2.79517 iter/s, 35.776s/100 iters), loss = 0.104611
I0218 15:07:38.143718 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.029227 (* 1 = 0.029227 loss)
I0218 15:07:38.143729 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0163695 (* 1 = 0.0163695 loss)
I0218 15:07:38.143738 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0288576 (* 1 = 0.0288576 loss)
I0218 15:07:38.143746 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0301566 (* 1 = 0.0301566 loss)
I0218 15:07:38.143754 27028 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0218 15:08:09.151175 27028 solver.cpp:357] Iteration 35700 (3.225 iter/s, 31.0077s/100 iters), loss = 0.092171
I0218 15:08:09.151327 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.031308 (* 1 = 0.031308 loss)
I0218 15:08:09.151338 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0213156 (* 1 = 0.0213156 loss)
I0218 15:08:09.151347 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0175543 (* 1 = 0.0175543 loss)
I0218 15:08:09.151355 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0219931 (* 1 = 0.0219931 loss)
I0218 15:08:09.151363 27028 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0218 15:08:43.050307 27028 solver.cpp:357] Iteration 35800 (2.94973 iter/s, 33.9014s/100 iters), loss = 0.0899397
I0218 15:08:43.050490 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0168986 (* 1 = 0.0168986 loss)
I0218 15:08:43.050503 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0179622 (* 1 = 0.0179622 loss)
I0218 15:08:43.050513 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.041951 (* 1 = 0.041951 loss)
I0218 15:08:43.050561 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0131279 (* 1 = 0.0131279 loss)
I0218 15:08:43.050570 27028 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0218 15:09:18.836578 27028 solver.cpp:357] Iteration 35900 (2.79419 iter/s, 35.7886s/100 iters), loss = 0.0905986
I0218 15:09:18.836709 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0194818 (* 1 = 0.0194818 loss)
I0218 15:09:18.836719 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0173429 (* 1 = 0.0173429 loss)
I0218 15:09:18.836728 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0260252 (* 1 = 0.0260252 loss)
I0218 15:09:18.836736 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0277486 (* 1 = 0.0277486 loss)
I0218 15:09:18.836745 27028 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0218 15:09:54.319427 27028 solver.cpp:514] Iteration 36000, Testing net (#0)
I0218 15:09:56.713608 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.17549 (* 1 = 1.17549 loss)
I0218 15:09:56.713657 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.31614 (* 1 = 1.31614 loss)
I0218 15:09:56.713666 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.14085 (* 1 = 1.14085 loss)
I0218 15:09:56.713676 27028 solver.cpp:580]     Test net output #3: Softmax4 = 1.66975 (* 1 = 1.66975 loss)
I0218 15:09:56.713683 27028 solver.cpp:580]     Test net output #4: prob1 = 0.783
I0218 15:09:56.713690 27028 solver.cpp:580]     Test net output #5: prob2 = 0.755
I0218 15:09:56.713696 27028 solver.cpp:580]     Test net output #6: prob3 = 0.757
I0218 15:09:56.713701 27028 solver.cpp:580]     Test net output #7: prob4 = 0.685
I0218 15:09:57.050209 27028 solver.cpp:357] Iteration 36000 (2.61684 iter/s, 38.2141s/100 iters), loss = 0.0757226
I0218 15:09:57.050258 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0216458 (* 1 = 0.0216458 loss)
I0218 15:09:57.050268 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.018461 (* 1 = 0.018461 loss)
I0218 15:09:57.050277 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0161351 (* 1 = 0.0161351 loss)
I0218 15:09:57.050284 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0194808 (* 1 = 0.0194808 loss)
I0218 15:09:57.050297 27028 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0218 15:10:32.841348 27028 solver.cpp:357] Iteration 36100 (2.79381 iter/s, 35.7934s/100 iters), loss = 0.0885185
I0218 15:10:32.841533 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0208332 (* 1 = 0.0208332 loss)
I0218 15:10:32.841544 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0171593 (* 1 = 0.0171593 loss)
I0218 15:10:32.841552 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0298331 (* 1 = 0.0298331 loss)
I0218 15:10:32.841560 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0206929 (* 1 = 0.0206929 loss)
I0218 15:10:32.841568 27028 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0218 15:11:04.108249 27028 solver.cpp:357] Iteration 36200 (3.19826 iter/s, 31.267s/100 iters), loss = 0.105332
I0218 15:11:04.108350 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0454496 (* 1 = 0.0454496 loss)
I0218 15:11:04.108361 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0167685 (* 1 = 0.0167685 loss)
I0218 15:11:04.108369 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0201165 (* 1 = 0.0201165 loss)
I0218 15:11:04.108377 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0229978 (* 1 = 0.0229978 loss)
I0218 15:11:04.108384 27028 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0218 15:11:37.939368 27028 solver.cpp:357] Iteration 36300 (2.95569 iter/s, 33.8331s/100 iters), loss = 0.0954115
I0218 15:11:37.939599 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0192596 (* 1 = 0.0192596 loss)
I0218 15:11:37.939610 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.025354 (* 1 = 0.025354 loss)
I0218 15:11:37.939620 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0192591 (* 1 = 0.0192591 loss)
I0218 15:11:37.939627 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0315389 (* 1 = 0.0315389 loss)
I0218 15:11:37.939635 27028 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0218 15:12:13.698153 27028 solver.cpp:357] Iteration 36400 (2.79637 iter/s, 35.7607s/100 iters), loss = 0.103425
I0218 15:12:13.698273 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0335991 (* 1 = 0.0335991 loss)
I0218 15:12:13.698283 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0148457 (* 1 = 0.0148457 loss)
I0218 15:12:13.698292 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.025763 (* 1 = 0.025763 loss)
I0218 15:12:13.698300 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.029217 (* 1 = 0.029217 loss)
I0218 15:12:13.698307 27028 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0218 15:12:49.461719 27028 solver.cpp:357] Iteration 36500 (2.79599 iter/s, 35.7655s/100 iters), loss = 0.0707427
I0218 15:12:49.461887 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0129682 (* 1 = 0.0129682 loss)
I0218 15:12:49.461899 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0254842 (* 1 = 0.0254842 loss)
I0218 15:12:49.461907 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0158245 (* 1 = 0.0158245 loss)
I0218 15:12:49.461915 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0164659 (* 1 = 0.0164659 loss)
I0218 15:12:49.461923 27028 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0218 15:13:25.294428 27028 solver.cpp:357] Iteration 36600 (2.79076 iter/s, 35.8325s/100 iters), loss = 0.081347
I0218 15:13:25.294576 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0154231 (* 1 = 0.0154231 loss)
I0218 15:13:25.294587 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0194765 (* 1 = 0.0194765 loss)
I0218 15:13:25.294596 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0307227 (* 1 = 0.0307227 loss)
I0218 15:13:25.294605 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0157248 (* 1 = 0.0157248 loss)
I0218 15:13:25.294616 27028 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0218 15:13:57.428045 27028 solver.cpp:357] Iteration 36700 (3.11205 iter/s, 32.1332s/100 iters), loss = 0.0671257
I0218 15:13:57.428194 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.018079 (* 1 = 0.018079 loss)
I0218 15:13:57.428205 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0158303 (* 1 = 0.0158303 loss)
I0218 15:13:57.428212 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0178415 (* 1 = 0.0178415 loss)
I0218 15:13:57.428220 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.015375 (* 1 = 0.015375 loss)
I0218 15:13:57.428232 27028 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0218 15:14:30.288130 27028 solver.cpp:357] Iteration 36800 (3.04306 iter/s, 32.8617s/100 iters), loss = 0.100943
I0218 15:14:30.288296 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0226434 (* 1 = 0.0226434 loss)
I0218 15:14:30.288306 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0157564 (* 1 = 0.0157564 loss)
I0218 15:14:30.288316 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0214688 (* 1 = 0.0214688 loss)
I0218 15:14:30.288323 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.041074 (* 1 = 0.041074 loss)
I0218 15:14:30.288331 27028 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0218 15:15:06.049301 27028 solver.cpp:357] Iteration 36900 (2.7962 iter/s, 35.7629s/100 iters), loss = 0.0657923
I0218 15:15:06.049482 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.012991 (* 1 = 0.012991 loss)
I0218 15:15:06.049494 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0174777 (* 1 = 0.0174777 loss)
I0218 15:15:06.049502 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0174091 (* 1 = 0.0174091 loss)
I0218 15:15:06.049510 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0179145 (* 1 = 0.0179145 loss)
I0218 15:15:06.049517 27028 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0218 15:15:41.520391 27028 solver.cpp:514] Iteration 37000, Testing net (#0)
I0218 15:15:43.945072 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.648918 (* 1 = 0.648918 loss)
I0218 15:15:43.945122 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.598637 (* 1 = 0.598637 loss)
I0218 15:15:43.945132 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.552166 (* 1 = 0.552166 loss)
I0218 15:15:43.945140 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.675321 (* 1 = 0.675321 loss)
I0218 15:15:43.945147 27028 solver.cpp:580]     Test net output #4: prob1 = 0.875
I0218 15:15:43.945152 27028 solver.cpp:580]     Test net output #5: prob2 = 0.875
I0218 15:15:43.945158 27028 solver.cpp:580]     Test net output #6: prob3 = 0.870001
I0218 15:15:43.945163 27028 solver.cpp:580]     Test net output #7: prob4 = 0.879
I0218 15:15:44.281388 27028 solver.cpp:357] Iteration 37000 (2.61548 iter/s, 38.2339s/100 iters), loss = 0.0748545
I0218 15:15:44.281445 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0177215 (* 1 = 0.0177215 loss)
I0218 15:15:44.281455 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0176053 (* 1 = 0.0176053 loss)
I0218 15:15:44.281464 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0170047 (* 1 = 0.0170047 loss)
I0218 15:15:44.281472 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0225229 (* 1 = 0.0225229 loss)
I0218 15:15:44.281479 27028 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0218 15:16:20.161803 27028 solver.cpp:357] Iteration 37100 (2.7869 iter/s, 35.8821s/100 iters), loss = 0.0897071
I0218 15:16:20.161913 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0175916 (* 1 = 0.0175916 loss)
I0218 15:16:20.161923 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0184907 (* 1 = 0.0184907 loss)
I0218 15:16:20.161932 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0159881 (* 1 = 0.0159881 loss)
I0218 15:16:20.161944 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0376367 (* 1 = 0.0376367 loss)
I0218 15:16:20.161952 27028 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0218 15:16:52.350070 27028 solver.cpp:357] Iteration 37200 (3.10658 iter/s, 32.1897s/100 iters), loss = 0.0870473
I0218 15:16:52.350178 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0159544 (* 1 = 0.0159544 loss)
I0218 15:16:52.350188 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0147131 (* 1 = 0.0147131 loss)
I0218 15:16:52.350196 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0329103 (* 1 = 0.0329103 loss)
I0218 15:16:52.350204 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0234695 (* 1 = 0.0234695 loss)
I0218 15:16:52.350211 27028 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0218 15:17:25.134102 27028 solver.cpp:357] Iteration 37300 (3.05013 iter/s, 32.7855s/100 iters), loss = 0.0719753
I0218 15:17:25.134233 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0137245 (* 1 = 0.0137245 loss)
I0218 15:17:25.134244 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0207105 (* 1 = 0.0207105 loss)
I0218 15:17:25.134253 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0218935 (* 1 = 0.0218935 loss)
I0218 15:17:25.134261 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0156468 (* 1 = 0.0156468 loss)
I0218 15:17:25.134268 27028 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0218 15:18:01.001713 27028 solver.cpp:357] Iteration 37400 (2.78807 iter/s, 35.8671s/100 iters), loss = 0.0942901
I0218 15:18:01.001909 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.037796 (* 1 = 0.037796 loss)
I0218 15:18:01.001919 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0262361 (* 1 = 0.0262361 loss)
I0218 15:18:01.001929 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0120699 (* 1 = 0.0120699 loss)
I0218 15:18:01.001946 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.018188 (* 1 = 0.018188 loss)
I0218 15:18:01.001955 27028 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0218 15:18:36.754894 27028 solver.cpp:357] Iteration 37500 (2.79684 iter/s, 35.7546s/100 iters), loss = 0.0858182
I0218 15:18:36.755012 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.035195 (* 1 = 0.035195 loss)
I0218 15:18:36.755022 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0189873 (* 1 = 0.0189873 loss)
I0218 15:18:36.755030 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0122931 (* 1 = 0.0122931 loss)
I0218 15:18:36.755038 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0193428 (* 1 = 0.0193428 loss)
I0218 15:18:36.755045 27028 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0218 15:19:12.511135 27028 solver.cpp:357] Iteration 37600 (2.7966 iter/s, 35.7577s/100 iters), loss = 0.100507
I0218 15:19:12.511253 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0145241 (* 1 = 0.0145241 loss)
I0218 15:19:12.511263 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.027253 (* 1 = 0.027253 loss)
I0218 15:19:12.511272 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0273717 (* 1 = 0.0273717 loss)
I0218 15:19:12.511281 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0313581 (* 1 = 0.0313581 loss)
I0218 15:19:12.511287 27028 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0218 15:19:45.631577 27028 solver.cpp:357] Iteration 37700 (3.01916 iter/s, 33.1218s/100 iters), loss = 0.0819718
I0218 15:19:45.631727 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.024188 (* 1 = 0.024188 loss)
I0218 15:19:45.631737 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0138127 (* 1 = 0.0138127 loss)
I0218 15:19:45.631747 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0283436 (* 1 = 0.0283436 loss)
I0218 15:19:45.631754 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0156276 (* 1 = 0.0156276 loss)
I0218 15:19:45.631767 27028 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0218 15:20:17.464946 27028 solver.cpp:357] Iteration 37800 (3.14123 iter/s, 31.8346s/100 iters), loss = 0.0730224
I0218 15:20:17.465103 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0283559 (* 1 = 0.0283559 loss)
I0218 15:20:17.465114 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0173297 (* 1 = 0.0173297 loss)
I0218 15:20:17.465122 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0134864 (* 1 = 0.0134864 loss)
I0218 15:20:17.465131 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0138504 (* 1 = 0.0138504 loss)
I0218 15:20:17.465142 27028 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0218 15:20:53.233939 27028 solver.cpp:357] Iteration 37900 (2.79561 iter/s, 35.7704s/100 iters), loss = 0.0843139
I0218 15:20:53.234110 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0253367 (* 1 = 0.0253367 loss)
I0218 15:20:53.234122 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.014936 (* 1 = 0.014936 loss)
I0218 15:20:53.234129 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0153916 (* 1 = 0.0153916 loss)
I0218 15:20:53.234138 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0286496 (* 1 = 0.0286496 loss)
I0218 15:20:53.234144 27028 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0218 15:21:28.656522 27028 solver.cpp:514] Iteration 38000, Testing net (#0)
I0218 15:21:31.128182 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.3576 (* 1 = 1.3576 loss)
I0218 15:21:31.128226 27028 solver.cpp:580]     Test net output #1: Softmax2 = 1.84534 (* 1 = 1.84534 loss)
I0218 15:21:31.128233 27028 solver.cpp:580]     Test net output #2: Softmax3 = 1.61953 (* 1 = 1.61953 loss)
I0218 15:21:31.128244 27028 solver.cpp:580]     Test net output #3: Softmax4 = 2.08559 (* 1 = 2.08559 loss)
I0218 15:21:31.128250 27028 solver.cpp:580]     Test net output #4: prob1 = 0.733
I0218 15:21:31.128257 27028 solver.cpp:580]     Test net output #5: prob2 = 0.676
I0218 15:21:31.128262 27028 solver.cpp:580]     Test net output #6: prob3 = 0.668
I0218 15:21:31.128268 27028 solver.cpp:580]     Test net output #7: prob4 = 0.649
I0218 15:21:31.463376 27028 solver.cpp:357] Iteration 38000 (2.61568 iter/s, 38.2309s/100 iters), loss = 0.103765
I0218 15:21:31.463428 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0175833 (* 1 = 0.0175833 loss)
I0218 15:21:31.463438 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0143343 (* 1 = 0.0143343 loss)
I0218 15:21:31.463448 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0391091 (* 1 = 0.0391091 loss)
I0218 15:21:31.463455 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0327381 (* 1 = 0.0327381 loss)
I0218 15:21:31.463462 27028 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0218 15:22:07.221717 27028 solver.cpp:357] Iteration 38100 (2.79644 iter/s, 35.7598s/100 iters), loss = 0.0694426
I0218 15:22:07.221951 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0169618 (* 1 = 0.0169618 loss)
I0218 15:22:07.221962 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0152555 (* 1 = 0.0152555 loss)
I0218 15:22:07.221971 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0176641 (* 1 = 0.0176641 loss)
I0218 15:22:07.221979 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0195612 (* 1 = 0.0195612 loss)
I0218 15:22:07.221987 27028 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0218 15:22:40.469128 27028 solver.cpp:357] Iteration 38200 (3.00765 iter/s, 33.2486s/100 iters), loss = 0.0751361
I0218 15:22:40.469278 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0162956 (* 1 = 0.0162956 loss)
I0218 15:22:40.469290 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0215384 (* 1 = 0.0215384 loss)
I0218 15:22:40.469297 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.020773 (* 1 = 0.020773 loss)
I0218 15:22:40.469305 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0165291 (* 1 = 0.0165291 loss)
I0218 15:22:40.469316 27028 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0218 15:23:12.091106 27028 solver.cpp:357] Iteration 38300 (3.16224 iter/s, 31.6231s/100 iters), loss = 0.0869701
I0218 15:23:12.091240 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0300208 (* 1 = 0.0300208 loss)
I0218 15:23:12.091251 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0183244 (* 1 = 0.0183244 loss)
I0218 15:23:12.091260 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0209613 (* 1 = 0.0209613 loss)
I0218 15:23:12.091269 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0176635 (* 1 = 0.0176635 loss)
I0218 15:23:12.091276 27028 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0218 15:23:47.874589 27028 solver.cpp:357] Iteration 38400 (2.79464 iter/s, 35.7828s/100 iters), loss = 0.078464
I0218 15:23:47.874699 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0201126 (* 1 = 0.0201126 loss)
I0218 15:23:47.874709 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0178671 (* 1 = 0.0178671 loss)
I0218 15:23:47.874718 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0199223 (* 1 = 0.0199223 loss)
I0218 15:23:47.874727 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0205621 (* 1 = 0.0205621 loss)
I0218 15:23:47.874734 27028 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0218 15:24:23.657191 27028 solver.cpp:357] Iteration 38500 (2.79455 iter/s, 35.7839s/100 iters), loss = 0.0667505
I0218 15:24:23.657320 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0161375 (* 1 = 0.0161375 loss)
I0218 15:24:23.657330 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0165824 (* 1 = 0.0165824 loss)
I0218 15:24:23.657339 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0124775 (* 1 = 0.0124775 loss)
I0218 15:24:23.657347 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0215532 (* 1 = 0.0215532 loss)
I0218 15:24:23.657356 27028 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0218 15:24:59.434696 27028 solver.cpp:357] Iteration 38600 (2.79511 iter/s, 35.7768s/100 iters), loss = 0.0953111
I0218 15:24:59.434882 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0329983 (* 1 = 0.0329983 loss)
I0218 15:24:59.434893 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0186195 (* 1 = 0.0186195 loss)
I0218 15:24:59.434945 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0248556 (* 1 = 0.0248556 loss)
I0218 15:24:59.434955 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0188377 (* 1 = 0.0188377 loss)
I0218 15:24:59.434963 27028 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0218 15:25:33.717608 27028 solver.cpp:357] Iteration 38700 (2.91697 iter/s, 34.2821s/100 iters), loss = 0.0939065
I0218 15:25:33.717778 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0243349 (* 1 = 0.0243349 loss)
I0218 15:25:33.717804 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0154809 (* 1 = 0.0154809 loss)
I0218 15:25:33.717824 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0305205 (* 1 = 0.0305205 loss)
I0218 15:25:33.717844 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0235702 (* 1 = 0.0235702 loss)
I0218 15:25:33.717862 27028 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0218 15:26:04.582311 27028 solver.cpp:357] Iteration 38800 (3.23984 iter/s, 30.8657s/100 iters), loss = 0.0689904
I0218 15:26:04.582414 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0229111 (* 1 = 0.0229111 loss)
I0218 15:26:04.582425 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0143764 (* 1 = 0.0143764 loss)
I0218 15:26:04.582434 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0193438 (* 1 = 0.0193438 loss)
I0218 15:26:04.582442 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0123591 (* 1 = 0.0123591 loss)
I0218 15:26:04.582450 27028 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0218 15:26:40.377387 27028 solver.cpp:357] Iteration 38900 (2.79358 iter/s, 35.7964s/100 iters), loss = 0.0897619
I0218 15:26:40.377542 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0265245 (* 1 = 0.0265245 loss)
I0218 15:26:40.377553 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.022041 (* 1 = 0.022041 loss)
I0218 15:26:40.377562 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0143154 (* 1 = 0.0143154 loss)
I0218 15:26:40.377569 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.026881 (* 1 = 0.026881 loss)
I0218 15:26:40.377581 27028 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0218 15:27:15.806782 27028 solver.cpp:514] Iteration 39000, Testing net (#0)
I0218 15:27:18.231189 27028 solver.cpp:580]     Test net output #0: Softmax1 = 1.78497 (* 1 = 1.78497 loss)
I0218 15:27:18.231240 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.84043 (* 1 = 2.84043 loss)
I0218 15:27:18.231248 27028 solver.cpp:580]     Test net output #2: Softmax3 = 2.36214 (* 1 = 2.36214 loss)
I0218 15:27:18.231257 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.25543 (* 1 = 3.25543 loss)
I0218 15:27:18.231264 27028 solver.cpp:580]     Test net output #4: prob1 = 0.656
I0218 15:27:18.231271 27028 solver.cpp:580]     Test net output #5: prob2 = 0.565
I0218 15:27:18.231276 27028 solver.cpp:580]     Test net output #6: prob3 = 0.539
I0218 15:27:18.231282 27028 solver.cpp:580]     Test net output #7: prob4 = 0.474
I0218 15:27:18.610169 27028 solver.cpp:357] Iteration 39000 (2.61547 iter/s, 38.2341s/100 iters), loss = 0.101318
I0218 15:27:18.610227 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.039398 (* 1 = 0.039398 loss)
I0218 15:27:18.610237 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0164481 (* 1 = 0.0164481 loss)
I0218 15:27:18.610245 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0169818 (* 1 = 0.0169818 loss)
I0218 15:27:18.610253 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.02849 (* 1 = 0.02849 loss)
I0218 15:27:18.610266 27028 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0218 15:27:54.260545 27028 solver.cpp:357] Iteration 39100 (2.80508 iter/s, 35.6496s/100 iters), loss = 0.0953769
I0218 15:27:54.260867 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0353488 (* 1 = 0.0353488 loss)
I0218 15:27:54.260879 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0173544 (* 1 = 0.0173544 loss)
I0218 15:27:54.260887 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0242804 (* 1 = 0.0242804 loss)
I0218 15:27:54.260895 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0183933 (* 1 = 0.0183933 loss)
I0218 15:27:54.260902 27028 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0218 15:28:28.671738 27028 solver.cpp:357] Iteration 39200 (2.90595 iter/s, 34.4122s/100 iters), loss = 0.0587386
I0218 15:28:28.671854 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0150817 (* 1 = 0.0150817 loss)
I0218 15:28:28.671864 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0134932 (* 1 = 0.0134932 loss)
I0218 15:28:28.671874 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0146043 (* 1 = 0.0146043 loss)
I0218 15:28:28.671881 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0155594 (* 1 = 0.0155594 loss)
I0218 15:28:28.671888 27028 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0218 15:28:59.326239 27028 solver.cpp:357] Iteration 39300 (3.26205 iter/s, 30.6555s/100 iters), loss = 0.175413
I0218 15:28:59.326408 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0224397 (* 1 = 0.0224397 loss)
I0218 15:28:59.326418 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0150313 (* 1 = 0.0150313 loss)
I0218 15:28:59.326427 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0582185 (* 1 = 0.0582185 loss)
I0218 15:28:59.326436 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0797232 (* 1 = 0.0797232 loss)
I0218 15:28:59.326442 27028 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0218 15:29:35.128756 27028 solver.cpp:357] Iteration 39400 (2.79316 iter/s, 35.8017s/100 iters), loss = 0.0924494
I0218 15:29:35.128922 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0224175 (* 1 = 0.0224175 loss)
I0218 15:29:35.128933 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0385339 (* 1 = 0.0385339 loss)
I0218 15:29:35.128942 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0140013 (* 1 = 0.0140013 loss)
I0218 15:29:35.128950 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0174967 (* 1 = 0.0174967 loss)
I0218 15:29:35.128957 27028 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0218 15:30:10.932571 27028 solver.cpp:357] Iteration 39500 (2.79291 iter/s, 35.805s/100 iters), loss = 0.0913751
I0218 15:30:10.932731 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0206434 (* 1 = 0.0206434 loss)
I0218 15:30:10.932742 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0372134 (* 1 = 0.0372134 loss)
I0218 15:30:10.932750 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.016405 (* 1 = 0.016405 loss)
I0218 15:30:10.932760 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0171134 (* 1 = 0.0171134 loss)
I0218 15:30:10.932770 27028 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0218 15:30:46.697249 27028 solver.cpp:357] Iteration 39600 (2.79596 iter/s, 35.7659s/100 iters), loss = 0.0794566
I0218 15:30:46.697407 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0154246 (* 1 = 0.0154246 loss)
I0218 15:30:46.697417 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0206621 (* 1 = 0.0206621 loss)
I0218 15:30:46.697427 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0200321 (* 1 = 0.0200321 loss)
I0218 15:30:46.697433 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0233378 (* 1 = 0.0233378 loss)
I0218 15:30:46.697443 27028 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0218 15:31:22.096454 27028 solver.cpp:357] Iteration 39700 (2.82483 iter/s, 35.4004s/100 iters), loss = 0.0921762
I0218 15:31:22.096624 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0324615 (* 1 = 0.0324615 loss)
I0218 15:31:22.096635 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0327507 (* 1 = 0.0327507 loss)
I0218 15:31:22.096644 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.014803 (* 1 = 0.014803 loss)
I0218 15:31:22.096652 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0121611 (* 1 = 0.0121611 loss)
I0218 15:31:22.096660 27028 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0218 15:31:51.901850 27028 solver.cpp:357] Iteration 39800 (3.35499 iter/s, 29.8063s/100 iters), loss = 0.136363
I0218 15:31:51.901927 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0283536 (* 1 = 0.0283536 loss)
I0218 15:31:51.901949 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0357498 (* 1 = 0.0357498 loss)
I0218 15:31:51.901958 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0360279 (* 1 = 0.0360279 loss)
I0218 15:31:51.901967 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0362317 (* 1 = 0.0362317 loss)
I0218 15:31:51.901973 27028 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0218 15:32:27.660379 27028 solver.cpp:357] Iteration 39900 (2.79644 iter/s, 35.7598s/100 iters), loss = 0.0826716
I0218 15:32:27.660545 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0185295 (* 1 = 0.0185295 loss)
I0218 15:32:27.660557 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0310524 (* 1 = 0.0310524 loss)
I0218 15:32:27.660564 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0142883 (* 1 = 0.0142883 loss)
I0218 15:32:27.660573 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0188015 (* 1 = 0.0188015 loss)
I0218 15:32:27.660583 27028 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0218 15:33:03.071393 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.caffemodel
I0218 15:33:03.081568 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_40000.solverstate
I0218 15:33:03.084857 27028 solver.cpp:514] Iteration 40000, Testing net (#0)
I0218 15:33:05.547250 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.3806 (* 1 = 0.3806 loss)
I0218 15:33:05.547297 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.353833 (* 1 = 0.353833 loss)
I0218 15:33:05.547305 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.387498 (* 1 = 0.387498 loss)
I0218 15:33:05.547313 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.402096 (* 1 = 0.402096 loss)
I0218 15:33:05.547319 27028 solver.cpp:580]     Test net output #4: prob1 = 0.918
I0218 15:33:05.547325 27028 solver.cpp:580]     Test net output #5: prob2 = 0.924
I0218 15:33:05.547332 27028 solver.cpp:580]     Test net output #6: prob3 = 0.91
I0218 15:33:05.547336 27028 solver.cpp:580]     Test net output #7: prob4 = 0.914
I0218 15:33:05.882992 27028 solver.cpp:357] Iteration 40000 (2.61617 iter/s, 38.2239s/100 iters), loss = 0.0956408
I0218 15:33:05.883051 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0158772 (* 1 = 0.0158772 loss)
I0218 15:33:05.883061 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0228343 (* 1 = 0.0228343 loss)
I0218 15:33:05.883070 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0266377 (* 1 = 0.0266377 loss)
I0218 15:33:05.883078 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0302916 (* 1 = 0.0302916 loss)
I0218 15:33:05.883086 27028 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0218 15:33:41.704551 27028 solver.cpp:357] Iteration 40100 (2.79152 iter/s, 35.8228s/100 iters), loss = 0.0867907
I0218 15:33:41.704718 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0187676 (* 1 = 0.0187676 loss)
I0218 15:33:41.704730 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0191508 (* 1 = 0.0191508 loss)
I0218 15:33:41.704738 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.019672 (* 1 = 0.019672 loss)
I0218 15:33:41.704746 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0292002 (* 1 = 0.0292002 loss)
I0218 15:33:41.704753 27028 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0218 15:34:17.046124 27028 solver.cpp:357] Iteration 40200 (2.8296 iter/s, 35.3407s/100 iters), loss = 0.0863337
I0218 15:34:17.046411 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.026713 (* 1 = 0.026713 loss)
I0218 15:34:17.046422 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0247747 (* 1 = 0.0247747 loss)
I0218 15:34:17.046432 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.021553 (* 1 = 0.021553 loss)
I0218 15:34:17.046439 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0132929 (* 1 = 0.0132929 loss)
I0218 15:34:17.046450 27028 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0218 15:34:46.684831 27028 solver.cpp:357] Iteration 40300 (3.37388 iter/s, 29.6395s/100 iters), loss = 0.101612
I0218 15:34:46.684907 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0210156 (* 1 = 0.0210156 loss)
I0218 15:34:46.684918 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0153237 (* 1 = 0.0153237 loss)
I0218 15:34:46.684926 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.03868 (* 1 = 0.03868 loss)
I0218 15:34:46.684934 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0265929 (* 1 = 0.0265929 loss)
I0218 15:34:46.684942 27028 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0218 15:35:22.465979 27028 solver.cpp:357] Iteration 40400 (2.79481 iter/s, 35.7806s/100 iters), loss = 0.120744
I0218 15:35:22.466148 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0172465 (* 1 = 0.0172465 loss)
I0218 15:35:22.466159 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0220697 (* 1 = 0.0220697 loss)
I0218 15:35:22.466167 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0538099 (* 1 = 0.0538099 loss)
I0218 15:35:22.466176 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0276179 (* 1 = 0.0276179 loss)
I0218 15:35:22.466182 27028 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0218 15:35:58.270972 27028 solver.cpp:357] Iteration 40500 (2.793 iter/s, 35.8038s/100 iters), loss = 0.0907506
I0218 15:35:58.271136 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.01752 (* 1 = 0.01752 loss)
I0218 15:35:58.271147 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0195716 (* 1 = 0.0195716 loss)
I0218 15:35:58.271157 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0257053 (* 1 = 0.0257053 loss)
I0218 15:35:58.271164 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0279536 (* 1 = 0.0279536 loss)
I0218 15:35:58.271175 27028 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0218 15:36:34.050644 27028 solver.cpp:357] Iteration 40600 (2.79512 iter/s, 35.7766s/100 iters), loss = 0.120532
I0218 15:36:34.050825 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0362521 (* 1 = 0.0362521 loss)
I0218 15:36:34.050835 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0184969 (* 1 = 0.0184969 loss)
I0218 15:36:34.050844 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0265745 (* 1 = 0.0265745 loss)
I0218 15:36:34.050853 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0392087 (* 1 = 0.0392087 loss)
I0218 15:36:34.050860 27028 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0218 15:37:09.906841 27028 solver.cpp:357] Iteration 40700 (2.78914 iter/s, 35.8533s/100 iters), loss = 0.0924821
I0218 15:37:09.906996 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0259897 (* 1 = 0.0259897 loss)
I0218 15:37:09.907006 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0279089 (* 1 = 0.0279089 loss)
I0218 15:37:09.907016 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0253746 (* 1 = 0.0253746 loss)
I0218 15:37:09.907023 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0132089 (* 1 = 0.0132089 loss)
I0218 15:37:09.907030 27028 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0218 15:37:38.907873 27028 solver.cpp:357] Iteration 40800 (3.44824 iter/s, 29.0003s/100 iters), loss = 0.0685055
I0218 15:37:38.907949 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0130725 (* 1 = 0.0130725 loss)
I0218 15:37:38.907960 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0126774 (* 1 = 0.0126774 loss)
I0218 15:37:38.907968 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0290136 (* 1 = 0.0290136 loss)
I0218 15:37:38.907976 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.013742 (* 1 = 0.013742 loss)
I0218 15:37:38.907984 27028 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0218 15:38:14.813640 27028 solver.cpp:357] Iteration 40900 (2.78512 iter/s, 35.9051s/100 iters), loss = 0.109272
I0218 15:38:14.813810 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0160604 (* 1 = 0.0160604 loss)
I0218 15:38:14.813822 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0140739 (* 1 = 0.0140739 loss)
I0218 15:38:14.813830 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0519315 (* 1 = 0.0519315 loss)
I0218 15:38:14.813838 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0272065 (* 1 = 0.0272065 loss)
I0218 15:38:14.813845 27028 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0218 15:38:50.255358 27028 solver.cpp:514] Iteration 41000, Testing net (#0)
I0218 15:38:52.683045 27028 solver.cpp:580]     Test net output #0: Softmax1 = 4.28166 (* 1 = 4.28166 loss)
I0218 15:38:52.683099 27028 solver.cpp:580]     Test net output #1: Softmax2 = 8.29963 (* 1 = 8.29963 loss)
I0218 15:38:52.683109 27028 solver.cpp:580]     Test net output #2: Softmax3 = 5.87257 (* 1 = 5.87257 loss)
I0218 15:38:52.683116 27028 solver.cpp:580]     Test net output #3: Softmax4 = 7.17639 (* 1 = 7.17639 loss)
I0218 15:38:52.683122 27028 solver.cpp:580]     Test net output #4: prob1 = 0.288
I0218 15:38:52.683128 27028 solver.cpp:580]     Test net output #5: prob2 = 0.127
I0218 15:38:52.683133 27028 solver.cpp:580]     Test net output #6: prob3 = 0.097
I0218 15:38:52.683140 27028 solver.cpp:580]     Test net output #7: prob4 = 0.052
I0218 15:38:53.058252 27028 solver.cpp:357] Iteration 41000 (2.61479 iter/s, 38.244s/100 iters), loss = 0.116522
I0218 15:38:53.058310 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0405912 (* 1 = 0.0405912 loss)
I0218 15:38:53.058320 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0182888 (* 1 = 0.0182888 loss)
I0218 15:38:53.058328 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.020878 (* 1 = 0.020878 loss)
I0218 15:38:53.058336 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0367639 (* 1 = 0.0367639 loss)
I0218 15:38:53.058349 27028 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0218 15:39:28.838192 27028 solver.cpp:357] Iteration 41100 (2.79506 iter/s, 35.7775s/100 iters), loss = 0.0786148
I0218 15:39:28.838317 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0231607 (* 1 = 0.0231607 loss)
I0218 15:39:28.838328 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0192775 (* 1 = 0.0192775 loss)
I0218 15:39:28.838336 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0223389 (* 1 = 0.0223389 loss)
I0218 15:39:28.838344 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0138377 (* 1 = 0.0138377 loss)
I0218 15:39:28.838351 27028 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0218 15:40:04.620249 27028 solver.cpp:357] Iteration 41200 (2.79473 iter/s, 35.7817s/100 iters), loss = 0.119275
I0218 15:40:04.620362 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0293472 (* 1 = 0.0293472 loss)
I0218 15:40:04.620373 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.023536 (* 1 = 0.023536 loss)
I0218 15:40:04.620381 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0535312 (* 1 = 0.0535312 loss)
I0218 15:40:04.620389 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0128602 (* 1 = 0.0128602 loss)
I0218 15:40:04.620396 27028 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0218 15:40:33.922868 27028 solver.cpp:357] Iteration 41300 (3.4127 iter/s, 29.3023s/100 iters), loss = 0.0722074
I0218 15:40:33.922941 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0170719 (* 1 = 0.0170719 loss)
I0218 15:40:33.922951 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0143115 (* 1 = 0.0143115 loss)
I0218 15:40:33.922960 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0155069 (* 1 = 0.0155069 loss)
I0218 15:40:33.922968 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0253172 (* 1 = 0.0253172 loss)
I0218 15:40:33.922976 27028 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0218 15:41:09.696843 27028 solver.cpp:357] Iteration 41400 (2.79534 iter/s, 35.7738s/100 iters), loss = 0.0902484
I0218 15:41:09.697023 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0227191 (* 1 = 0.0227191 loss)
I0218 15:41:09.697034 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.018089 (* 1 = 0.018089 loss)
I0218 15:41:09.697042 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0167559 (* 1 = 0.0167559 loss)
I0218 15:41:09.697051 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0326844 (* 1 = 0.0326844 loss)
I0218 15:41:09.697058 27028 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0218 15:41:45.458001 27028 solver.cpp:357] Iteration 41500 (2.79635 iter/s, 35.7609s/100 iters), loss = 0.0867851
I0218 15:41:45.458163 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0210904 (* 1 = 0.0210904 loss)
I0218 15:41:45.458173 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0288183 (* 1 = 0.0288183 loss)
I0218 15:41:45.458181 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0203586 (* 1 = 0.0203586 loss)
I0218 15:41:45.458189 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0165178 (* 1 = 0.0165178 loss)
I0218 15:41:45.458197 27028 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0218 15:42:21.224287 27028 solver.cpp:357] Iteration 41600 (2.79594 iter/s, 35.7661s/100 iters), loss = 0.156266
I0218 15:42:21.224403 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0241806 (* 1 = 0.0241806 loss)
I0218 15:42:21.224414 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0515105 (* 1 = 0.0515105 loss)
I0218 15:42:21.224422 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0309513 (* 1 = 0.0309513 loss)
I0218 15:42:21.224431 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0496235 (* 1 = 0.0496235 loss)
I0218 15:42:21.224437 27028 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0218 15:42:57.082041 27028 solver.cpp:357] Iteration 41700 (2.7888 iter/s, 35.8577s/100 iters), loss = 0.113563
I0218 15:42:57.082163 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0321178 (* 1 = 0.0321178 loss)
I0218 15:42:57.082175 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0188201 (* 1 = 0.0188201 loss)
I0218 15:42:57.082183 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0313685 (* 1 = 0.0313685 loss)
I0218 15:42:57.082191 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0312568 (* 1 = 0.0312568 loss)
I0218 15:42:57.082199 27028 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0218 15:43:26.402715 27028 solver.cpp:357] Iteration 41800 (3.4108 iter/s, 29.3186s/100 iters), loss = 0.0904043
I0218 15:43:26.402792 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0234603 (* 1 = 0.0234603 loss)
I0218 15:43:26.402802 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0207199 (* 1 = 0.0207199 loss)
I0218 15:43:26.402810 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0295864 (* 1 = 0.0295864 loss)
I0218 15:43:26.402818 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0166377 (* 1 = 0.0166377 loss)
I0218 15:43:26.402827 27028 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0218 15:44:02.022568 27028 solver.cpp:357] Iteration 41900 (2.80741 iter/s, 35.62s/100 iters), loss = 0.0709782
I0218 15:44:02.022660 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.016757 (* 1 = 0.016757 loss)
I0218 15:44:02.022670 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.020824 (* 1 = 0.020824 loss)
I0218 15:44:02.022678 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0143822 (* 1 = 0.0143822 loss)
I0218 15:44:02.022686 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0190149 (* 1 = 0.0190149 loss)
I0218 15:44:02.022693 27028 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0218 15:44:37.447029 27028 solver.cpp:514] Iteration 42000, Testing net (#0)
I0218 15:44:39.921694 27028 solver.cpp:580]     Test net output #0: Softmax1 = 2.20254 (* 1 = 2.20254 loss)
I0218 15:44:39.921738 27028 solver.cpp:580]     Test net output #1: Softmax2 = 2.65685 (* 1 = 2.65685 loss)
I0218 15:44:39.921746 27028 solver.cpp:580]     Test net output #2: Softmax3 = 3.01455 (* 1 = 3.01455 loss)
I0218 15:44:39.921754 27028 solver.cpp:580]     Test net output #3: Softmax4 = 3.42853 (* 1 = 3.42853 loss)
I0218 15:44:39.921761 27028 solver.cpp:580]     Test net output #4: prob1 = 0.598
I0218 15:44:39.921767 27028 solver.cpp:580]     Test net output #5: prob2 = 0.567
I0218 15:44:39.921772 27028 solver.cpp:580]     Test net output #6: prob3 = 0.445
I0218 15:44:39.921778 27028 solver.cpp:580]     Test net output #7: prob4 = 0.458
I0218 15:44:40.261639 27028 solver.cpp:357] Iteration 42000 (2.61511 iter/s, 38.2392s/100 iters), loss = 0.086347
I0218 15:44:40.261696 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0206855 (* 1 = 0.0206855 loss)
I0218 15:44:40.261705 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0219063 (* 1 = 0.0219063 loss)
I0218 15:44:40.261714 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0148892 (* 1 = 0.0148892 loss)
I0218 15:44:40.261723 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.028866 (* 1 = 0.028866 loss)
I0218 15:44:40.261729 27028 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0218 15:45:16.040571 27028 solver.cpp:357] Iteration 42100 (2.79492 iter/s, 35.7792s/100 iters), loss = 0.0856469
I0218 15:45:16.040738 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0236717 (* 1 = 0.0236717 loss)
I0218 15:45:16.040750 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0176122 (* 1 = 0.0176122 loss)
I0218 15:45:16.040757 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0141558 (* 1 = 0.0141558 loss)
I0218 15:45:16.040766 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0302072 (* 1 = 0.0302072 loss)
I0218 15:45:16.040777 27028 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0218 15:45:51.792603 27028 solver.cpp:357] Iteration 42200 (2.79703 iter/s, 35.7522s/100 iters), loss = 0.110606
I0218 15:45:51.792762 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0186461 (* 1 = 0.0186461 loss)
I0218 15:45:51.792771 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0198385 (* 1 = 0.0198385 loss)
I0218 15:45:51.792780 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.032089 (* 1 = 0.032089 loss)
I0218 15:45:51.792788 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.040032 (* 1 = 0.040032 loss)
I0218 15:45:51.792796 27028 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0218 15:46:21.297857 27028 solver.cpp:357] Iteration 42300 (3.38921 iter/s, 29.5054s/100 iters), loss = 0.0917098
I0218 15:46:21.297927 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0275739 (* 1 = 0.0275739 loss)
I0218 15:46:21.297945 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0326967 (* 1 = 0.0326967 loss)
I0218 15:46:21.297955 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0187472 (* 1 = 0.0187472 loss)
I0218 15:46:21.297962 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0126919 (* 1 = 0.0126919 loss)
I0218 15:46:21.297969 27028 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0218 15:46:56.862205 27028 solver.cpp:357] Iteration 42400 (2.81178 iter/s, 35.5647s/100 iters), loss = 0.0767773
I0218 15:46:56.862356 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0172644 (* 1 = 0.0172644 loss)
I0218 15:46:56.862366 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0200489 (* 1 = 0.0200489 loss)
I0218 15:46:56.862375 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0215797 (* 1 = 0.0215797 loss)
I0218 15:46:56.862382 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0178843 (* 1 = 0.0178843 loss)
I0218 15:46:56.862390 27028 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0218 15:47:32.626684 27028 solver.cpp:357] Iteration 42500 (2.79605 iter/s, 35.7648s/100 iters), loss = 0.0755737
I0218 15:47:32.626904 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0189582 (* 1 = 0.0189582 loss)
I0218 15:47:32.626915 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0193159 (* 1 = 0.0193159 loss)
I0218 15:47:32.626924 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0156095 (* 1 = 0.0156095 loss)
I0218 15:47:32.626932 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0216901 (* 1 = 0.0216901 loss)
I0218 15:47:32.626940 27028 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0218 15:48:08.390830 27028 solver.cpp:357] Iteration 42600 (2.79608 iter/s, 35.7644s/100 iters), loss = 0.107752
I0218 15:48:08.391002 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0175481 (* 1 = 0.0175481 loss)
I0218 15:48:08.391013 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0226999 (* 1 = 0.0226999 loss)
I0218 15:48:08.391022 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0353461 (* 1 = 0.0353461 loss)
I0218 15:48:08.391031 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.032158 (* 1 = 0.032158 loss)
I0218 15:48:08.391037 27028 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0218 15:48:44.183879 27028 solver.cpp:357] Iteration 42700 (2.79381 iter/s, 35.7934s/100 iters), loss = 0.0964561
I0218 15:48:44.184057 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0415373 (* 1 = 0.0415373 loss)
I0218 15:48:44.184068 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0205077 (* 1 = 0.0205077 loss)
I0218 15:48:44.184077 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0144087 (* 1 = 0.0144087 loss)
I0218 15:48:44.184084 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0200025 (* 1 = 0.0200025 loss)
I0218 15:48:44.184096 27028 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0218 15:49:14.594244 27028 solver.cpp:357] Iteration 42800 (3.28854 iter/s, 30.4087s/100 iters), loss = 0.0901174
I0218 15:49:14.594362 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0177695 (* 1 = 0.0177695 loss)
I0218 15:49:14.594377 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0212031 (* 1 = 0.0212031 loss)
I0218 15:49:14.594384 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0160723 (* 1 = 0.0160723 loss)
I0218 15:49:14.594393 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0350725 (* 1 = 0.0350725 loss)
I0218 15:49:14.594400 27028 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0218 15:49:49.178879 27028 solver.cpp:357] Iteration 42900 (2.89142 iter/s, 34.5851s/100 iters), loss = 0.0880352
I0218 15:49:49.179039 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0195138 (* 1 = 0.0195138 loss)
I0218 15:49:49.179049 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0214569 (* 1 = 0.0214569 loss)
I0218 15:49:49.179059 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0174465 (* 1 = 0.0174465 loss)
I0218 15:49:49.179065 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0296179 (* 1 = 0.0296179 loss)
I0218 15:49:49.179074 27028 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0218 15:50:24.610182 27028 solver.cpp:514] Iteration 43000, Testing net (#0)
I0218 15:50:27.021701 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.475511 (* 1 = 0.475511 loss)
I0218 15:50:27.021755 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.470486 (* 1 = 0.470486 loss)
I0218 15:50:27.021764 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.490652 (* 1 = 0.490652 loss)
I0218 15:50:27.021771 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.596235 (* 1 = 0.596235 loss)
I0218 15:50:27.021778 27028 solver.cpp:580]     Test net output #4: prob1 = 0.892
I0218 15:50:27.021785 27028 solver.cpp:580]     Test net output #5: prob2 = 0.886
I0218 15:50:27.021790 27028 solver.cpp:580]     Test net output #6: prob3 = 0.89
I0218 15:50:27.021795 27028 solver.cpp:580]     Test net output #7: prob4 = 0.882001
I0218 15:50:27.403406 27028 solver.cpp:357] Iteration 43000 (2.61609 iter/s, 38.225s/100 iters), loss = 0.0814024
I0218 15:50:27.403466 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0213415 (* 1 = 0.0213415 loss)
I0218 15:50:27.403477 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0149881 (* 1 = 0.0149881 loss)
I0218 15:50:27.403486 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0226847 (* 1 = 0.0226847 loss)
I0218 15:50:27.403493 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.022388 (* 1 = 0.022388 loss)
I0218 15:50:27.403502 27028 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0218 15:51:03.184955 27028 solver.cpp:357] Iteration 43100 (2.79486 iter/s, 35.78s/100 iters), loss = 0.12685
I0218 15:51:03.185125 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0311608 (* 1 = 0.0311608 loss)
I0218 15:51:03.185135 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0329579 (* 1 = 0.0329579 loss)
I0218 15:51:03.185144 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0147432 (* 1 = 0.0147432 loss)
I0218 15:51:03.185153 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.047988 (* 1 = 0.047988 loss)
I0218 15:51:03.185159 27028 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0218 15:51:38.953512 27028 solver.cpp:357] Iteration 43200 (2.79571 iter/s, 35.769s/100 iters), loss = 0.0777728
I0218 15:51:38.953632 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0155025 (* 1 = 0.0155025 loss)
I0218 15:51:38.953644 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0155375 (* 1 = 0.0155375 loss)
I0218 15:51:38.953652 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0305187 (* 1 = 0.0305187 loss)
I0218 15:51:38.953660 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0162141 (* 1 = 0.0162141 loss)
I0218 15:51:38.953666 27028 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0218 15:52:09.423753 27028 solver.cpp:357] Iteration 43300 (3.28184 iter/s, 30.4707s/100 iters), loss = 0.0816118
I0218 15:52:09.423921 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0169669 (* 1 = 0.0169669 loss)
I0218 15:52:09.423933 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0254974 (* 1 = 0.0254974 loss)
I0218 15:52:09.423940 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.017062 (* 1 = 0.017062 loss)
I0218 15:52:09.423949 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0220856 (* 1 = 0.0220856 loss)
I0218 15:52:09.423956 27028 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0218 15:52:43.861133 27028 solver.cpp:357] Iteration 43400 (2.90378 iter/s, 34.4379s/100 iters), loss = 0.101611
I0218 15:52:43.861248 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0158374 (* 1 = 0.0158374 loss)
I0218 15:52:43.861258 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0330393 (* 1 = 0.0330393 loss)
I0218 15:52:43.861266 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.033266 (* 1 = 0.033266 loss)
I0218 15:52:43.861274 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0194682 (* 1 = 0.0194682 loss)
I0218 15:52:43.861282 27028 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0218 15:53:19.616154 27028 solver.cpp:357] Iteration 43500 (2.79676 iter/s, 35.7556s/100 iters), loss = 0.115001
I0218 15:53:19.616266 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0239606 (* 1 = 0.0239606 loss)
I0218 15:53:19.616276 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0223763 (* 1 = 0.0223763 loss)
I0218 15:53:19.616286 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0167671 (* 1 = 0.0167671 loss)
I0218 15:53:19.616294 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0518974 (* 1 = 0.0518974 loss)
I0218 15:53:19.616302 27028 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0218 15:53:55.376492 27028 solver.cpp:357] Iteration 43600 (2.79635 iter/s, 35.7609s/100 iters), loss = 0.095375
I0218 15:53:55.376670 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0134507 (* 1 = 0.0134507 loss)
I0218 15:53:55.376682 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0175556 (* 1 = 0.0175556 loss)
I0218 15:53:55.376690 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0211887 (* 1 = 0.0211887 loss)
I0218 15:53:55.376698 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0431799 (* 1 = 0.0431799 loss)
I0218 15:53:55.376705 27028 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0218 15:54:31.133657 27028 solver.cpp:357] Iteration 43700 (2.7966 iter/s, 35.7577s/100 iters), loss = 0.15674
I0218 15:54:31.133775 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0432834 (* 1 = 0.0432834 loss)
I0218 15:54:31.133786 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0349182 (* 1 = 0.0349182 loss)
I0218 15:54:31.133795 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.03833 (* 1 = 0.03833 loss)
I0218 15:54:31.133803 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0402082 (* 1 = 0.0402082 loss)
I0218 15:54:31.133810 27028 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0218 15:55:02.611738 27028 solver.cpp:357] Iteration 43800 (3.17676 iter/s, 31.4786s/100 iters), loss = 0.0842046
I0218 15:55:02.611840 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0158202 (* 1 = 0.0158202 loss)
I0218 15:55:02.611850 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0215934 (* 1 = 0.0215934 loss)
I0218 15:55:02.611860 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0239405 (* 1 = 0.0239405 loss)
I0218 15:55:02.611867 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0228505 (* 1 = 0.0228505 loss)
I0218 15:55:02.611876 27028 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0218 15:55:36.005112 27028 solver.cpp:357] Iteration 43900 (2.99455 iter/s, 33.394s/100 iters), loss = 0.141082
I0218 15:55:36.005228 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0467625 (* 1 = 0.0467625 loss)
I0218 15:55:36.005239 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0382515 (* 1 = 0.0382515 loss)
I0218 15:55:36.005247 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0418593 (* 1 = 0.0418593 loss)
I0218 15:55:36.005255 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0142083 (* 1 = 0.0142083 loss)
I0218 15:55:36.005262 27028 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0218 15:56:11.423451 27028 solver.cpp:514] Iteration 44000, Testing net (#0)
I0218 15:56:13.897419 27028 solver.cpp:580]     Test net output #0: Softmax1 = 3.92393 (* 1 = 3.92393 loss)
I0218 15:56:13.897469 27028 solver.cpp:580]     Test net output #1: Softmax2 = 5.78895 (* 1 = 5.78895 loss)
I0218 15:56:13.897477 27028 solver.cpp:580]     Test net output #2: Softmax3 = 5.13208 (* 1 = 5.13208 loss)
I0218 15:56:13.897485 27028 solver.cpp:580]     Test net output #3: Softmax4 = 6.67957 (* 1 = 6.67957 loss)
I0218 15:56:13.897491 27028 solver.cpp:580]     Test net output #4: prob1 = 0.43
I0218 15:56:13.897500 27028 solver.cpp:580]     Test net output #5: prob2 = 0.303
I0218 15:56:13.897505 27028 solver.cpp:580]     Test net output #6: prob3 = 0.185
I0218 15:56:13.897509 27028 solver.cpp:580]     Test net output #7: prob4 = 0.141
I0218 15:56:14.233233 27028 solver.cpp:357] Iteration 44000 (2.61583 iter/s, 38.2288s/100 iters), loss = 0.0980465
I0218 15:56:14.233294 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0158612 (* 1 = 0.0158612 loss)
I0218 15:56:14.233304 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0351715 (* 1 = 0.0351715 loss)
I0218 15:56:14.233311 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0231808 (* 1 = 0.0231808 loss)
I0218 15:56:14.233319 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.023833 (* 1 = 0.023833 loss)
I0218 15:56:14.233327 27028 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0218 15:56:50.011339 27028 solver.cpp:357] Iteration 44100 (2.79495 iter/s, 35.7788s/100 iters), loss = 0.0706467
I0218 15:56:50.011569 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0133962 (* 1 = 0.0133962 loss)
I0218 15:56:50.011579 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0174386 (* 1 = 0.0174386 loss)
I0218 15:56:50.011587 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0250509 (* 1 = 0.0250509 loss)
I0218 15:56:50.011595 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.014761 (* 1 = 0.014761 loss)
I0218 15:56:50.011603 27028 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0218 15:57:25.779718 27028 solver.cpp:357] Iteration 44200 (2.79572 iter/s, 35.769s/100 iters), loss = 0.0804109
I0218 15:57:25.779839 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0136266 (* 1 = 0.0136266 loss)
I0218 15:57:25.779850 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0210959 (* 1 = 0.0210959 loss)
I0218 15:57:25.779860 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0226913 (* 1 = 0.0226913 loss)
I0218 15:57:25.779867 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0229971 (* 1 = 0.0229971 loss)
I0218 15:57:25.779875 27028 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0218 15:57:57.464385 27028 solver.cpp:357] Iteration 44300 (3.15604 iter/s, 31.6853s/100 iters), loss = 0.113903
I0218 15:57:57.464586 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0242525 (* 1 = 0.0242525 loss)
I0218 15:57:57.464601 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0250918 (* 1 = 0.0250918 loss)
I0218 15:57:57.464614 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0448619 (* 1 = 0.0448619 loss)
I0218 15:57:57.464622 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0196965 (* 1 = 0.0196965 loss)
I0218 15:57:57.464630 27028 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0218 15:58:30.828258 27028 solver.cpp:357] Iteration 44400 (2.9972 iter/s, 33.3644s/100 iters), loss = 0.0616095
I0218 15:58:30.828415 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0148336 (* 1 = 0.0148336 loss)
I0218 15:58:30.828426 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0165404 (* 1 = 0.0165404 loss)
I0218 15:58:30.828434 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0164553 (* 1 = 0.0164553 loss)
I0218 15:58:30.828442 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0137802 (* 1 = 0.0137802 loss)
I0218 15:58:30.828454 27028 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0218 15:59:06.593421 27028 solver.cpp:357] Iteration 44500 (2.79596 iter/s, 35.7658s/100 iters), loss = 0.0836657
I0218 15:59:06.593578 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0137713 (* 1 = 0.0137713 loss)
I0218 15:59:06.593590 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0274342 (* 1 = 0.0274342 loss)
I0218 15:59:06.593598 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0181529 (* 1 = 0.0181529 loss)
I0218 15:59:06.593606 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0243073 (* 1 = 0.0243073 loss)
I0218 15:59:06.593613 27028 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0218 15:59:42.350462 27028 solver.cpp:357] Iteration 44600 (2.7966 iter/s, 35.7577s/100 iters), loss = 0.0888902
I0218 15:59:42.350626 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0247546 (* 1 = 0.0247546 loss)
I0218 15:59:42.350637 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0245875 (* 1 = 0.0245875 loss)
I0218 15:59:42.350646 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0207521 (* 1 = 0.0207521 loss)
I0218 15:59:42.350653 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0187961 (* 1 = 0.0187961 loss)
I0218 15:59:42.350661 27028 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0218 16:00:18.113867 27028 solver.cpp:357] Iteration 44700 (2.7961 iter/s, 35.7641s/100 iters), loss = 0.0640332
I0218 16:00:18.114032 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0173632 (* 1 = 0.0173632 loss)
I0218 16:00:18.114043 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0158708 (* 1 = 0.0158708 loss)
I0218 16:00:18.114051 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0151484 (* 1 = 0.0151484 loss)
I0218 16:00:18.114059 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0156507 (* 1 = 0.0156507 loss)
I0218 16:00:18.114066 27028 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0218 16:00:50.776844 27028 solver.cpp:357] Iteration 44800 (3.06151 iter/s, 32.6636s/100 iters), loss = 0.109655
I0218 16:00:50.777052 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0378177 (* 1 = 0.0378177 loss)
I0218 16:00:50.777063 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.021273 (* 1 = 0.021273 loss)
I0218 16:00:50.777072 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0216743 (* 1 = 0.0216743 loss)
I0218 16:00:50.777081 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0288906 (* 1 = 0.0288906 loss)
I0218 16:00:50.777088 27028 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0218 16:01:22.927009 27028 solver.cpp:357] Iteration 44900 (3.11035 iter/s, 32.1507s/100 iters), loss = 0.117245
I0218 16:01:22.927171 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0221071 (* 1 = 0.0221071 loss)
I0218 16:01:22.927182 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0588448 (* 1 = 0.0588448 loss)
I0218 16:01:22.927191 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0229211 (* 1 = 0.0229211 loss)
I0218 16:01:22.927199 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0133722 (* 1 = 0.0133722 loss)
I0218 16:01:22.927206 27028 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0218 16:01:58.359455 27028 solver.cpp:514] Iteration 45000, Testing net (#0)
I0218 16:02:00.837255 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.188728 (* 1 = 0.188728 loss)
I0218 16:02:00.837298 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.219364 (* 1 = 0.219364 loss)
I0218 16:02:00.837307 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.24016 (* 1 = 0.24016 loss)
I0218 16:02:00.837316 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.303213 (* 1 = 0.303213 loss)
I0218 16:02:00.837323 27028 solver.cpp:580]     Test net output #4: prob1 = 0.958
I0218 16:02:00.837328 27028 solver.cpp:580]     Test net output #5: prob2 = 0.944
I0218 16:02:00.837334 27028 solver.cpp:580]     Test net output #6: prob3 = 0.944
I0218 16:02:00.837339 27028 solver.cpp:580]     Test net output #7: prob4 = 0.937
I0218 16:02:01.177124 27028 solver.cpp:357] Iteration 45000 (2.61432 iter/s, 38.2509s/100 iters), loss = 0.112591
I0218 16:02:01.177186 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0190724 (* 1 = 0.0190724 loss)
I0218 16:02:01.177197 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0196078 (* 1 = 0.0196078 loss)
I0218 16:02:01.177206 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0291828 (* 1 = 0.0291828 loss)
I0218 16:02:01.177213 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0447281 (* 1 = 0.0447281 loss)
I0218 16:02:01.177222 27028 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0218 16:02:36.957355 27028 solver.cpp:357] Iteration 45100 (2.79478 iter/s, 35.781s/100 iters), loss = 0.139068
I0218 16:02:36.957459 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0525239 (* 1 = 0.0525239 loss)
I0218 16:02:36.957469 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0149597 (* 1 = 0.0149597 loss)
I0218 16:02:36.957479 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0383978 (* 1 = 0.0383978 loss)
I0218 16:02:36.957486 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0331865 (* 1 = 0.0331865 loss)
I0218 16:02:36.957494 27028 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0218 16:03:12.734073 27028 solver.cpp:357] Iteration 45200 (2.79505 iter/s, 35.7775s/100 iters), loss = 0.252196
I0218 16:03:12.734231 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0759939 (* 1 = 0.0759939 loss)
I0218 16:03:12.734242 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0189823 (* 1 = 0.0189823 loss)
I0218 16:03:12.734251 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0712883 (* 1 = 0.0712883 loss)
I0218 16:03:12.734258 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0859312 (* 1 = 0.0859312 loss)
I0218 16:03:12.734266 27028 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0218 16:03:45.591325 27028 solver.cpp:357] Iteration 45300 (3.04341 iter/s, 32.8579s/100 iters), loss = 0.128727
I0218 16:03:45.591496 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0152521 (* 1 = 0.0152521 loss)
I0218 16:03:45.591507 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0629217 (* 1 = 0.0629217 loss)
I0218 16:03:45.591516 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0281071 (* 1 = 0.0281071 loss)
I0218 16:03:45.591523 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0224466 (* 1 = 0.0224466 loss)
I0218 16:03:45.591531 27028 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0218 16:04:17.741940 27028 solver.cpp:357] Iteration 45400 (3.1103 iter/s, 32.1512s/100 iters), loss = 0.137212
I0218 16:04:17.742115 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0492872 (* 1 = 0.0492872 loss)
I0218 16:04:17.742125 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0304556 (* 1 = 0.0304556 loss)
I0218 16:04:17.742135 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0403677 (* 1 = 0.0403677 loss)
I0218 16:04:17.742142 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0171014 (* 1 = 0.0171014 loss)
I0218 16:04:17.742151 27028 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0218 16:04:53.463325 27028 solver.cpp:357] Iteration 45500 (2.79954 iter/s, 35.7201s/100 iters), loss = 0.138239
I0218 16:04:53.463485 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0317306 (* 1 = 0.0317306 loss)
I0218 16:04:53.463495 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0390007 (* 1 = 0.0390007 loss)
I0218 16:04:53.463503 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0316843 (* 1 = 0.0316843 loss)
I0218 16:04:53.463511 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0358234 (* 1 = 0.0358234 loss)
I0218 16:04:53.463521 27028 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0218 16:05:29.224211 27028 solver.cpp:357] Iteration 45600 (2.79629 iter/s, 35.7616s/100 iters), loss = 0.0831861
I0218 16:05:29.224354 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0291044 (* 1 = 0.0291044 loss)
I0218 16:05:29.224365 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.016979 (* 1 = 0.016979 loss)
I0218 16:05:29.224375 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0144253 (* 1 = 0.0144253 loss)
I0218 16:05:29.224382 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0226775 (* 1 = 0.0226775 loss)
I0218 16:05:29.224390 27028 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0218 16:06:04.982983 27028 solver.cpp:357] Iteration 45700 (2.79646 iter/s, 35.7595s/100 iters), loss = 0.105246
I0218 16:06:04.983144 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0399167 (* 1 = 0.0399167 loss)
I0218 16:06:04.983155 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0240183 (* 1 = 0.0240183 loss)
I0218 16:06:04.983163 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0145366 (* 1 = 0.0145366 loss)
I0218 16:06:04.983171 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0267742 (* 1 = 0.0267742 loss)
I0218 16:06:04.983180 27028 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0218 16:06:38.885479 27028 solver.cpp:357] Iteration 45800 (2.94957 iter/s, 33.9032s/100 iters), loss = 0.0807556
I0218 16:06:38.885628 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0263003 (* 1 = 0.0263003 loss)
I0218 16:06:38.885638 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0160279 (* 1 = 0.0160279 loss)
I0218 16:06:38.885648 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0191211 (* 1 = 0.0191211 loss)
I0218 16:06:38.885655 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0193063 (* 1 = 0.0193063 loss)
I0218 16:06:38.885666 27028 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0218 16:07:09.859645 27028 solver.cpp:357] Iteration 45900 (3.22843 iter/s, 30.9748s/100 iters), loss = 0.119457
I0218 16:07:09.859869 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0471812 (* 1 = 0.0471812 loss)
I0218 16:07:09.859879 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0156509 (* 1 = 0.0156509 loss)
I0218 16:07:09.859889 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0285242 (* 1 = 0.0285242 loss)
I0218 16:07:09.859896 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0281009 (* 1 = 0.0281009 loss)
I0218 16:07:09.859903 27028 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0218 16:07:45.287925 27028 solver.cpp:514] Iteration 46000, Testing net (#0)
I0218 16:07:47.699920 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.22106 (* 1 = 0.22106 loss)
I0218 16:07:47.699973 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.185554 (* 1 = 0.185554 loss)
I0218 16:07:47.699981 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.168888 (* 1 = 0.168888 loss)
I0218 16:07:47.699992 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.201373 (* 1 = 0.201373 loss)
I0218 16:07:47.699998 27028 solver.cpp:580]     Test net output #4: prob1 = 0.958
I0218 16:07:47.700004 27028 solver.cpp:580]     Test net output #5: prob2 = 0.949
I0218 16:07:47.700009 27028 solver.cpp:580]     Test net output #6: prob3 = 0.959
I0218 16:07:47.700014 27028 solver.cpp:580]     Test net output #7: prob4 = 0.95
I0218 16:07:48.078532 27028 solver.cpp:357] Iteration 46000 (2.61646 iter/s, 38.2197s/100 iters), loss = 0.0913131
I0218 16:07:48.078594 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.028876 (* 1 = 0.028876 loss)
I0218 16:07:48.078605 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0196541 (* 1 = 0.0196541 loss)
I0218 16:07:48.078613 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0247545 (* 1 = 0.0247545 loss)
I0218 16:07:48.078621 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0180286 (* 1 = 0.0180286 loss)
I0218 16:07:48.078629 27028 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0218 16:08:23.869290 27028 solver.cpp:357] Iteration 46100 (2.79411 iter/s, 35.7895s/100 iters), loss = 0.0658414
I0218 16:08:23.869439 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0157025 (* 1 = 0.0157025 loss)
I0218 16:08:23.869451 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0165011 (* 1 = 0.0165011 loss)
I0218 16:08:23.869458 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0143646 (* 1 = 0.0143646 loss)
I0218 16:08:23.869467 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0192732 (* 1 = 0.0192732 loss)
I0218 16:08:23.869473 27028 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0218 16:08:59.540107 27028 solver.cpp:357] Iteration 46200 (2.80335 iter/s, 35.6716s/100 iters), loss = 0.063852
I0218 16:08:59.540225 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0141567 (* 1 = 0.0141567 loss)
I0218 16:08:59.540236 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0163775 (* 1 = 0.0163775 loss)
I0218 16:08:59.540246 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0177221 (* 1 = 0.0177221 loss)
I0218 16:08:59.540253 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0155957 (* 1 = 0.0155957 loss)
I0218 16:08:59.540261 27028 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0218 16:09:33.628058 27028 solver.cpp:357] Iteration 46300 (2.93377 iter/s, 34.0858s/100 iters), loss = 0.122152
I0218 16:09:33.628206 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0188839 (* 1 = 0.0188839 loss)
I0218 16:09:33.628217 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0434127 (* 1 = 0.0434127 loss)
I0218 16:09:33.628226 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0179187 (* 1 = 0.0179187 loss)
I0218 16:09:33.628233 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0419371 (* 1 = 0.0419371 loss)
I0218 16:09:33.628247 27028 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0218 16:10:04.528672 27028 solver.cpp:357] Iteration 46400 (3.23621 iter/s, 30.9004s/100 iters), loss = 0.128148
I0218 16:10:04.528847 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0419627 (* 1 = 0.0419627 loss)
I0218 16:10:04.528858 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0387644 (* 1 = 0.0387644 loss)
I0218 16:10:04.528867 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0301928 (* 1 = 0.0301928 loss)
I0218 16:10:04.528875 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0172279 (* 1 = 0.0172279 loss)
I0218 16:10:04.528882 27028 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0218 16:10:40.282768 27028 solver.cpp:357] Iteration 46500 (2.7969 iter/s, 35.7539s/100 iters), loss = 0.0754598
I0218 16:10:40.282943 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0126868 (* 1 = 0.0126868 loss)
I0218 16:10:40.282953 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0240233 (* 1 = 0.0240233 loss)
I0218 16:10:40.282961 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.014436 (* 1 = 0.014436 loss)
I0218 16:10:40.282969 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0243137 (* 1 = 0.0243137 loss)
I0218 16:10:40.282977 27028 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0218 16:11:16.047186 27028 solver.cpp:357] Iteration 46600 (2.79609 iter/s, 35.7642s/100 iters), loss = 0.167149
I0218 16:11:16.047305 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0521367 (* 1 = 0.0521367 loss)
I0218 16:11:16.047315 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0581516 (* 1 = 0.0581516 loss)
I0218 16:11:16.047324 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0182721 (* 1 = 0.0182721 loss)
I0218 16:11:16.047333 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0385885 (* 1 = 0.0385885 loss)
I0218 16:11:16.047339 27028 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0218 16:11:51.803123 27028 solver.cpp:357] Iteration 46700 (2.79674 iter/s, 35.7559s/100 iters), loss = 0.0890762
I0218 16:11:51.803284 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0391294 (* 1 = 0.0391294 loss)
I0218 16:11:51.803294 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0168926 (* 1 = 0.0168926 loss)
I0218 16:11:51.803303 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.015118 (* 1 = 0.015118 loss)
I0218 16:11:51.803311 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0179361 (* 1 = 0.0179361 loss)
I0218 16:11:51.803321 27028 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0218 16:12:26.872786 27028 solver.cpp:357] Iteration 46800 (2.85147 iter/s, 35.0696s/100 iters), loss = 0.107233
I0218 16:12:26.872897 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0579703 (* 1 = 0.0579703 loss)
I0218 16:12:26.872908 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0163843 (* 1 = 0.0163843 loss)
I0218 16:12:26.872917 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0161353 (* 1 = 0.0161353 loss)
I0218 16:12:26.872925 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0167436 (* 1 = 0.0167436 loss)
I0218 16:12:26.872932 27028 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0218 16:12:56.718585 27028 solver.cpp:357] Iteration 46900 (3.35056 iter/s, 29.8458s/100 iters), loss = 0.108903
I0218 16:12:56.718659 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.01533 (* 1 = 0.01533 loss)
I0218 16:12:56.718670 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0353043 (* 1 = 0.0353043 loss)
I0218 16:12:56.718678 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0202459 (* 1 = 0.0202459 loss)
I0218 16:12:56.718686 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0380232 (* 1 = 0.0380232 loss)
I0218 16:12:56.718693 27028 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0218 16:13:32.147238 27028 solver.cpp:514] Iteration 47000, Testing net (#0)
I0218 16:13:34.621459 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.384932 (* 1 = 0.384932 loss)
I0218 16:13:34.621505 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.524287 (* 1 = 0.524287 loss)
I0218 16:13:34.621512 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.848632 (* 1 = 0.848632 loss)
I0218 16:13:34.621520 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.818142 (* 1 = 0.818142 loss)
I0218 16:13:34.621526 27028 solver.cpp:580]     Test net output #4: prob1 = 0.911
I0218 16:13:34.621532 27028 solver.cpp:580]     Test net output #5: prob2 = 0.87
I0218 16:13:34.621537 27028 solver.cpp:580]     Test net output #6: prob3 = 0.814
I0218 16:13:34.621542 27028 solver.cpp:580]     Test net output #7: prob4 = 0.843
I0218 16:13:34.958119 27028 solver.cpp:357] Iteration 47000 (2.61509 iter/s, 38.2397s/100 iters), loss = 0.0976735
I0218 16:13:34.958176 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0184202 (* 1 = 0.0184202 loss)
I0218 16:13:34.958186 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0153876 (* 1 = 0.0153876 loss)
I0218 16:13:34.958194 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0340381 (* 1 = 0.0340381 loss)
I0218 16:13:34.958202 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0298277 (* 1 = 0.0298277 loss)
I0218 16:13:34.958209 27028 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0218 16:14:10.729933 27028 solver.cpp:357] Iteration 47100 (2.79548 iter/s, 35.772s/100 iters), loss = 0.123877
I0218 16:14:10.730173 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0287562 (* 1 = 0.0287562 loss)
I0218 16:14:10.730185 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0242221 (* 1 = 0.0242221 loss)
I0218 16:14:10.730192 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0245745 (* 1 = 0.0245745 loss)
I0218 16:14:10.730201 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0463237 (* 1 = 0.0463237 loss)
I0218 16:14:10.730208 27028 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0218 16:14:46.503615 27028 solver.cpp:357] Iteration 47200 (2.79535 iter/s, 35.7737s/100 iters), loss = 0.157326
I0218 16:14:46.503778 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0154319 (* 1 = 0.0154319 loss)
I0218 16:14:46.503789 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0425733 (* 1 = 0.0425733 loss)
I0218 16:14:46.503798 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0288363 (* 1 = 0.0288363 loss)
I0218 16:14:46.503806 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0704848 (* 1 = 0.0704848 loss)
I0218 16:14:46.503814 27028 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0218 16:15:21.766423 27028 solver.cpp:357] Iteration 47300 (2.83584 iter/s, 35.2629s/100 iters), loss = 0.0924197
I0218 16:15:21.766541 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0249201 (* 1 = 0.0249201 loss)
I0218 16:15:21.766551 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0205822 (* 1 = 0.0205822 loss)
I0218 16:15:21.766559 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0224511 (* 1 = 0.0224511 loss)
I0218 16:15:21.766567 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0244663 (* 1 = 0.0244663 loss)
I0218 16:15:21.766575 27028 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0218 16:15:51.578277 27028 solver.cpp:357] Iteration 47400 (3.35435 iter/s, 29.812s/100 iters), loss = 0.113322
I0218 16:15:51.578352 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0263249 (* 1 = 0.0263249 loss)
I0218 16:15:51.578362 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0264214 (* 1 = 0.0264214 loss)
I0218 16:15:51.578372 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.035372 (* 1 = 0.035372 loss)
I0218 16:15:51.578379 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0252043 (* 1 = 0.0252043 loss)
I0218 16:15:51.578387 27028 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0218 16:16:27.331219 27028 solver.cpp:357] Iteration 47500 (2.79695 iter/s, 35.7532s/100 iters), loss = 0.0813337
I0218 16:16:27.331377 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0288089 (* 1 = 0.0288089 loss)
I0218 16:16:27.331387 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0164117 (* 1 = 0.0164117 loss)
I0218 16:16:27.331395 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0198382 (* 1 = 0.0198382 loss)
I0218 16:16:27.331403 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0162748 (* 1 = 0.0162748 loss)
I0218 16:16:27.331414 27028 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0218 16:17:03.136291 27028 solver.cpp:357] Iteration 47600 (2.79288 iter/s, 35.8053s/100 iters), loss = 0.0977148
I0218 16:17:03.136646 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.031556 (* 1 = 0.031556 loss)
I0218 16:17:03.136672 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0194562 (* 1 = 0.0194562 loss)
I0218 16:17:03.136695 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0270471 (* 1 = 0.0270471 loss)
I0218 16:17:03.136718 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0196555 (* 1 = 0.0196555 loss)
I0218 16:17:03.136737 27028 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0218 16:17:38.994037 27028 solver.cpp:357] Iteration 47700 (2.78882 iter/s, 35.8574s/100 iters), loss = 0.123768
I0218 16:17:38.994349 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0193195 (* 1 = 0.0193195 loss)
I0218 16:17:38.994375 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0492351 (* 1 = 0.0492351 loss)
I0218 16:17:38.994398 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.024015 (* 1 = 0.024015 loss)
I0218 16:17:38.994421 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0311981 (* 1 = 0.0311981 loss)
I0218 16:17:38.994441 27028 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0218 16:18:14.744566 27028 solver.cpp:357] Iteration 47800 (2.79715 iter/s, 35.7507s/100 iters), loss = 0.0818779
I0218 16:18:14.744804 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0317411 (* 1 = 0.0317411 loss)
I0218 16:18:14.744832 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0139763 (* 1 = 0.0139763 loss)
I0218 16:18:14.744853 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0196392 (* 1 = 0.0196392 loss)
I0218 16:18:14.744865 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0165214 (* 1 = 0.0165214 loss)
I0218 16:18:14.744874 27028 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0218 16:18:43.771234 27028 solver.cpp:357] Iteration 47900 (3.44532 iter/s, 29.0249s/100 iters), loss = 0.089008
I0218 16:18:43.771317 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0168713 (* 1 = 0.0168713 loss)
I0218 16:18:43.771328 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0276261 (* 1 = 0.0276261 loss)
I0218 16:18:43.771337 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0204025 (* 1 = 0.0204025 loss)
I0218 16:18:43.771345 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0241081 (* 1 = 0.0241081 loss)
I0218 16:18:43.771353 27028 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0218 16:19:19.195116 27028 solver.cpp:514] Iteration 48000, Testing net (#0)
I0218 16:19:21.669890 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.203957 (* 1 = 0.203957 loss)
I0218 16:19:21.669947 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.231962 (* 1 = 0.231962 loss)
I0218 16:19:21.669957 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.284436 (* 1 = 0.284436 loss)
I0218 16:19:21.669965 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.214011 (* 1 = 0.214011 loss)
I0218 16:19:21.669970 27028 solver.cpp:580]     Test net output #4: prob1 = 0.958
I0218 16:19:21.669979 27028 solver.cpp:580]     Test net output #5: prob2 = 0.952
I0218 16:19:21.669986 27028 solver.cpp:580]     Test net output #6: prob3 = 0.937
I0218 16:19:21.669991 27028 solver.cpp:580]     Test net output #7: prob4 = 0.952
I0218 16:19:22.006960 27028 solver.cpp:357] Iteration 48000 (2.61547 iter/s, 38.2341s/100 iters), loss = 0.131272
I0218 16:19:22.007019 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0636422 (* 1 = 0.0636422 loss)
I0218 16:19:22.007030 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0226203 (* 1 = 0.0226203 loss)
I0218 16:19:22.007038 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0228379 (* 1 = 0.0228379 loss)
I0218 16:19:22.007045 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0221716 (* 1 = 0.0221716 loss)
I0218 16:19:22.007052 27028 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0218 16:19:22.007057 27028 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0218 16:19:57.767994 27028 solver.cpp:357] Iteration 48100 (2.79631 iter/s, 35.7615s/100 iters), loss = 0.0925125
I0218 16:19:57.768225 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0321211 (* 1 = 0.0321211 loss)
I0218 16:19:57.768236 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0148011 (* 1 = 0.0148011 loss)
I0218 16:19:57.768244 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0301457 (* 1 = 0.0301457 loss)
I0218 16:19:57.768252 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0154446 (* 1 = 0.0154446 loss)
I0218 16:19:57.768260 27028 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0218 16:20:33.546448 27028 solver.cpp:357] Iteration 48200 (2.79496 iter/s, 35.7787s/100 iters), loss = 0.0557621
I0218 16:20:33.546645 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.01094 (* 1 = 0.01094 loss)
I0218 16:20:33.546656 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00992953 (* 1 = 0.00992953 loss)
I0218 16:20:33.546665 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.017942 (* 1 = 0.017942 loss)
I0218 16:20:33.546674 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0169506 (* 1 = 0.0169506 loss)
I0218 16:20:33.546682 27028 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0218 16:21:09.318075 27028 solver.cpp:357] Iteration 48300 (2.79564 iter/s, 35.77s/100 iters), loss = 0.0654514
I0218 16:21:09.318217 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0120285 (* 1 = 0.0120285 loss)
I0218 16:21:09.318228 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0232503 (* 1 = 0.0232503 loss)
I0218 16:21:09.318238 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0127204 (* 1 = 0.0127204 loss)
I0218 16:21:09.318245 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0174522 (* 1 = 0.0174522 loss)
I0218 16:21:09.318253 27028 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0218 16:21:38.470271 27028 solver.cpp:357] Iteration 48400 (3.43047 iter/s, 29.1505s/100 iters), loss = 0.0483643
I0218 16:21:38.470350 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104145 (* 1 = 0.0104145 loss)
I0218 16:21:38.470360 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0127576 (* 1 = 0.0127576 loss)
I0218 16:21:38.470369 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0135711 (* 1 = 0.0135711 loss)
I0218 16:21:38.470377 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0116211 (* 1 = 0.0116211 loss)
I0218 16:21:38.470386 27028 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0218 16:22:14.315387 27028 solver.cpp:357] Iteration 48500 (2.7899 iter/s, 35.8435s/100 iters), loss = 0.0543024
I0218 16:22:14.315506 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0143323 (* 1 = 0.0143323 loss)
I0218 16:22:14.315517 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0122631 (* 1 = 0.0122631 loss)
I0218 16:22:14.315526 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0122246 (* 1 = 0.0122246 loss)
I0218 16:22:14.315534 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0154824 (* 1 = 0.0154824 loss)
I0218 16:22:14.315541 27028 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0218 16:22:50.070175 27028 solver.cpp:357] Iteration 48600 (2.79679 iter/s, 35.7552s/100 iters), loss = 0.050902
I0218 16:22:50.070289 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0116503 (* 1 = 0.0116503 loss)
I0218 16:22:50.070300 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106093 (* 1 = 0.0106093 loss)
I0218 16:22:50.070309 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0171228 (* 1 = 0.0171228 loss)
I0218 16:22:50.070317 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0115197 (* 1 = 0.0115197 loss)
I0218 16:22:50.070324 27028 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0218 16:23:25.842813 27028 solver.cpp:357] Iteration 48700 (2.7954 iter/s, 35.7731s/100 iters), loss = 0.0531186
I0218 16:23:25.843031 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106299 (* 1 = 0.0106299 loss)
I0218 16:23:25.843042 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0121023 (* 1 = 0.0121023 loss)
I0218 16:23:25.843050 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0153764 (* 1 = 0.0153764 loss)
I0218 16:23:25.843058 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.01501 (* 1 = 0.01501 loss)
I0218 16:23:25.843070 27028 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0218 16:24:01.708271 27028 solver.cpp:357] Iteration 48800 (2.78817 iter/s, 35.8658s/100 iters), loss = 0.0499004
I0218 16:24:01.708379 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0122169 (* 1 = 0.0122169 loss)
I0218 16:24:01.708389 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0112593 (* 1 = 0.0112593 loss)
I0218 16:24:01.708398 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0138957 (* 1 = 0.0138957 loss)
I0218 16:24:01.708406 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0125285 (* 1 = 0.0125285 loss)
I0218 16:24:01.708413 27028 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0218 16:24:30.812078 27028 solver.cpp:357] Iteration 48900 (3.43617 iter/s, 29.1022s/100 iters), loss = 0.0484527
I0218 16:24:30.812151 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.011685 (* 1 = 0.011685 loss)
I0218 16:24:30.812161 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0123189 (* 1 = 0.0123189 loss)
I0218 16:24:30.812170 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0131438 (* 1 = 0.0131438 loss)
I0218 16:24:30.812178 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.011305 (* 1 = 0.011305 loss)
I0218 16:24:30.812186 27028 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0218 16:25:06.145519 27028 solver.cpp:514] Iteration 49000, Testing net (#0)
I0218 16:25:08.513204 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.110197 (* 1 = 0.110197 loss)
I0218 16:25:08.513254 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.17094 (* 1 = 0.17094 loss)
I0218 16:25:08.513263 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.160922 (* 1 = 0.160922 loss)
I0218 16:25:08.513272 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.131275 (* 1 = 0.131275 loss)
I0218 16:25:08.513278 27028 solver.cpp:580]     Test net output #4: prob1 = 0.975
I0218 16:25:08.513284 27028 solver.cpp:580]     Test net output #5: prob2 = 0.957
I0218 16:25:08.513289 27028 solver.cpp:580]     Test net output #6: prob3 = 0.957
I0218 16:25:08.513295 27028 solver.cpp:580]     Test net output #7: prob4 = 0.969
I0218 16:25:08.850172 27028 solver.cpp:357] Iteration 49000 (2.6289 iter/s, 38.0387s/100 iters), loss = 0.047717
I0218 16:25:08.850224 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00901041 (* 1 = 0.00901041 loss)
I0218 16:25:08.850234 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0148305 (* 1 = 0.0148305 loss)
I0218 16:25:08.850242 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.012809 (* 1 = 0.012809 loss)
I0218 16:25:08.850255 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0110671 (* 1 = 0.0110671 loss)
I0218 16:25:08.850262 27028 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0218 16:25:44.646798 27028 solver.cpp:357] Iteration 49100 (2.79351 iter/s, 35.7972s/100 iters), loss = 0.0440947
I0218 16:25:44.646965 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102372 (* 1 = 0.0102372 loss)
I0218 16:25:44.646976 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0111342 (* 1 = 0.0111342 loss)
I0218 16:25:44.646986 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.012686 (* 1 = 0.012686 loss)
I0218 16:25:44.646993 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100372 (* 1 = 0.0100372 loss)
I0218 16:25:44.647006 27028 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0218 16:26:20.408262 27028 solver.cpp:357] Iteration 49200 (2.79642 iter/s, 35.7599s/100 iters), loss = 0.0446236
I0218 16:26:20.408439 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109781 (* 1 = 0.0109781 loss)
I0218 16:26:20.408450 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0121724 (* 1 = 0.0121724 loss)
I0218 16:26:20.408459 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0121721 (* 1 = 0.0121721 loss)
I0218 16:26:20.408468 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00930103 (* 1 = 0.00930103 loss)
I0218 16:26:20.408475 27028 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0218 16:26:56.171586 27028 solver.cpp:357] Iteration 49300 (2.79614 iter/s, 35.7636s/100 iters), loss = 0.0461717
I0218 16:26:56.171752 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0118597 (* 1 = 0.0118597 loss)
I0218 16:26:56.171763 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010293 (* 1 = 0.010293 loss)
I0218 16:26:56.171772 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0129494 (* 1 = 0.0129494 loss)
I0218 16:26:56.171780 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0110696 (* 1 = 0.0110696 loss)
I0218 16:26:56.171787 27028 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0218 16:27:25.645062 27028 solver.cpp:357] Iteration 49400 (3.39284 iter/s, 29.4739s/100 iters), loss = 0.0451806
I0218 16:27:25.645134 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0119986 (* 1 = 0.0119986 loss)
I0218 16:27:25.645144 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0111268 (* 1 = 0.0111268 loss)
I0218 16:27:25.645153 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0123628 (* 1 = 0.0123628 loss)
I0218 16:27:25.645161 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00969236 (* 1 = 0.00969236 loss)
I0218 16:27:25.645170 27028 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0218 16:28:01.122176 27028 solver.cpp:357] Iteration 49500 (2.81867 iter/s, 35.4777s/100 iters), loss = 0.0456246
I0218 16:28:01.122287 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103781 (* 1 = 0.0103781 loss)
I0218 16:28:01.122298 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107608 (* 1 = 0.0107608 loss)
I0218 16:28:01.122308 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0138236 (* 1 = 0.0138236 loss)
I0218 16:28:01.122315 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106621 (* 1 = 0.0106621 loss)
I0218 16:28:01.122323 27028 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0218 16:28:36.880501 27028 solver.cpp:357] Iteration 49600 (2.79651 iter/s, 35.7589s/100 iters), loss = 0.0513012
I0218 16:28:36.880658 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104386 (* 1 = 0.0104386 loss)
I0218 16:28:36.880668 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011696 (* 1 = 0.011696 loss)
I0218 16:28:36.880677 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0125235 (* 1 = 0.0125235 loss)
I0218 16:28:36.880686 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0166431 (* 1 = 0.0166431 loss)
I0218 16:28:36.880697 27028 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0218 16:29:12.651052 27028 solver.cpp:357] Iteration 49700 (2.79555 iter/s, 35.7711s/100 iters), loss = 0.0470985
I0218 16:29:12.651165 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00965382 (* 1 = 0.00965382 loss)
I0218 16:29:12.651175 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0124158 (* 1 = 0.0124158 loss)
I0218 16:29:12.651183 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0132298 (* 1 = 0.0132298 loss)
I0218 16:29:12.651191 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0117991 (* 1 = 0.0117991 loss)
I0218 16:29:12.651198 27028 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0218 16:29:48.424532 27028 solver.cpp:357] Iteration 49800 (2.79532 iter/s, 35.7741s/100 iters), loss = 0.043197
I0218 16:29:48.424651 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00910607 (* 1 = 0.00910607 loss)
I0218 16:29:48.424662 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0139021 (* 1 = 0.0139021 loss)
I0218 16:29:48.424670 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.010499 (* 1 = 0.010499 loss)
I0218 16:29:48.424679 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00968977 (* 1 = 0.00968977 loss)
I0218 16:29:48.424686 27028 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0218 16:30:18.946985 27028 solver.cpp:357] Iteration 49900 (3.27623 iter/s, 30.5229s/100 iters), loss = 0.0430852
I0218 16:30:18.947196 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109418 (* 1 = 0.0109418 loss)
I0218 16:30:18.947207 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011663 (* 1 = 0.011663 loss)
I0218 16:30:18.947216 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00892343 (* 1 = 0.00892343 loss)
I0218 16:30:18.947224 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.011557 (* 1 = 0.011557 loss)
I0218 16:30:18.947232 27028 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0218 16:30:53.052680 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.caffemodel
I0218 16:30:53.063045 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_50000.solverstate
I0218 16:30:53.066325 27028 solver.cpp:514] Iteration 50000, Testing net (#0)
I0218 16:30:55.526473 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0742277 (* 1 = 0.0742277 loss)
I0218 16:30:55.526522 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.111893 (* 1 = 0.111893 loss)
I0218 16:30:55.526531 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0966089 (* 1 = 0.0966089 loss)
I0218 16:30:55.526540 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.117767 (* 1 = 0.117767 loss)
I0218 16:30:55.526546 27028 solver.cpp:580]     Test net output #4: prob1 = 0.979
I0218 16:30:55.526552 27028 solver.cpp:580]     Test net output #5: prob2 = 0.967
I0218 16:30:55.526557 27028 solver.cpp:580]     Test net output #6: prob3 = 0.976
I0218 16:30:55.526562 27028 solver.cpp:580]     Test net output #7: prob4 = 0.969
I0218 16:30:55.862776 27028 solver.cpp:357] Iteration 50000 (2.70883 iter/s, 36.9163s/100 iters), loss = 0.0422866
I0218 16:30:55.862833 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0123941 (* 1 = 0.0123941 loss)
I0218 16:30:55.862843 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0102623 (* 1 = 0.0102623 loss)
I0218 16:30:55.862852 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00929204 (* 1 = 0.00929204 loss)
I0218 16:30:55.862860 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103382 (* 1 = 0.0103382 loss)
I0218 16:30:55.862867 27028 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0218 16:31:31.633421 27028 solver.cpp:357] Iteration 50100 (2.79554 iter/s, 35.7713s/100 iters), loss = 0.047062
I0218 16:31:31.633589 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0111689 (* 1 = 0.0111689 loss)
I0218 16:31:31.633599 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.012348 (* 1 = 0.012348 loss)
I0218 16:31:31.633608 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0125414 (* 1 = 0.0125414 loss)
I0218 16:31:31.633616 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0110037 (* 1 = 0.0110037 loss)
I0218 16:31:31.633623 27028 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0218 16:32:07.407956 27028 solver.cpp:357] Iteration 50200 (2.79524 iter/s, 35.7751s/100 iters), loss = 0.0443306
I0218 16:32:07.408130 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106728 (* 1 = 0.0106728 loss)
I0218 16:32:07.408141 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0112555 (* 1 = 0.0112555 loss)
I0218 16:32:07.408149 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0117175 (* 1 = 0.0117175 loss)
I0218 16:32:07.408157 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106848 (* 1 = 0.0106848 loss)
I0218 16:32:07.408170 27028 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0218 16:32:43.178897 27028 solver.cpp:357] Iteration 50300 (2.79568 iter/s, 35.7695s/100 iters), loss = 0.0417515
I0218 16:32:43.179139 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00939789 (* 1 = 0.00939789 loss)
I0218 16:32:43.179150 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106058 (* 1 = 0.0106058 loss)
I0218 16:32:43.179160 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0108241 (* 1 = 0.0108241 loss)
I0218 16:32:43.179168 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0109238 (* 1 = 0.0109238 loss)
I0218 16:32:43.179177 27028 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0218 16:33:13.892398 27028 solver.cpp:357] Iteration 50400 (3.25606 iter/s, 30.712s/100 iters), loss = 0.0423686
I0218 16:33:13.892560 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107202 (* 1 = 0.0107202 loss)
I0218 16:33:13.892570 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0105424 (* 1 = 0.0105424 loss)
I0218 16:33:13.892578 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0107749 (* 1 = 0.0107749 loss)
I0218 16:33:13.892586 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103311 (* 1 = 0.0103311 loss)
I0218 16:33:13.892594 27028 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0218 16:33:48.231932 27028 solver.cpp:357] Iteration 50500 (2.91205 iter/s, 34.3401s/100 iters), loss = 0.0413556
I0218 16:33:48.232107 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00968263 (* 1 = 0.00968263 loss)
I0218 16:33:48.232118 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011306 (* 1 = 0.011306 loss)
I0218 16:33:48.232127 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0106657 (* 1 = 0.0106657 loss)
I0218 16:33:48.232136 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00970121 (* 1 = 0.00970121 loss)
I0218 16:33:48.232143 27028 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0218 16:34:24.020886 27028 solver.cpp:357] Iteration 50600 (2.79427 iter/s, 35.7876s/100 iters), loss = 0.0421207
I0218 16:34:24.021010 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00961297 (* 1 = 0.00961297 loss)
I0218 16:34:24.021021 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00917687 (* 1 = 0.00917687 loss)
I0218 16:34:24.021030 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0116378 (* 1 = 0.0116378 loss)
I0218 16:34:24.021039 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.011693 (* 1 = 0.011693 loss)
I0218 16:34:24.021046 27028 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0218 16:34:59.840827 27028 solver.cpp:357] Iteration 50700 (2.79185 iter/s, 35.8185s/100 iters), loss = 0.0450789
I0218 16:34:59.841042 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0114452 (* 1 = 0.0114452 loss)
I0218 16:34:59.841058 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0100726 (* 1 = 0.0100726 loss)
I0218 16:34:59.841073 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.010561 (* 1 = 0.010561 loss)
I0218 16:34:59.841081 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.013 (* 1 = 0.013 loss)
I0218 16:34:59.841089 27028 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0218 16:35:35.564729 27028 solver.cpp:357] Iteration 50800 (2.79936 iter/s, 35.7225s/100 iters), loss = 0.0548464
I0218 16:35:35.564849 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0192912 (* 1 = 0.0192912 loss)
I0218 16:35:35.564860 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106255 (* 1 = 0.0106255 loss)
I0218 16:35:35.564869 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0139145 (* 1 = 0.0139145 loss)
I0218 16:35:35.564877 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0110151 (* 1 = 0.0110151 loss)
I0218 16:35:35.564885 27028 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0218 16:36:07.229737 27028 solver.cpp:357] Iteration 50900 (3.15821 iter/s, 31.6635s/100 iters), loss = 0.0443788
I0218 16:36:07.229897 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0121183 (* 1 = 0.0121183 loss)
I0218 16:36:07.229907 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107299 (* 1 = 0.0107299 loss)
I0218 16:36:07.229916 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112962 (* 1 = 0.0112962 loss)
I0218 16:36:07.229923 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102344 (* 1 = 0.0102344 loss)
I0218 16:36:07.229931 27028 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0218 16:36:40.363533 27028 solver.cpp:514] Iteration 51000, Testing net (#0)
I0218 16:36:42.762034 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.100608 (* 1 = 0.100608 loss)
I0218 16:36:42.762087 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.0689372 (* 1 = 0.0689372 loss)
I0218 16:36:42.762095 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.113152 (* 1 = 0.113152 loss)
I0218 16:36:42.762104 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.103214 (* 1 = 0.103214 loss)
I0218 16:36:42.762110 27028 solver.cpp:580]     Test net output #4: prob1 = 0.972
I0218 16:36:42.762116 27028 solver.cpp:580]     Test net output #5: prob2 = 0.981
I0218 16:36:42.762121 27028 solver.cpp:580]     Test net output #6: prob3 = 0.967
I0218 16:36:42.762127 27028 solver.cpp:580]     Test net output #7: prob4 = 0.97
I0218 16:36:43.139773 27028 solver.cpp:357] Iteration 51000 (2.78469 iter/s, 35.9106s/100 iters), loss = 0.0454774
I0218 16:36:43.139833 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0129084 (* 1 = 0.0129084 loss)
I0218 16:36:43.139843 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.012702 (* 1 = 0.012702 loss)
I0218 16:36:43.139853 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00949597 (* 1 = 0.00949597 loss)
I0218 16:36:43.139860 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103711 (* 1 = 0.0103711 loss)
I0218 16:36:43.139869 27028 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0218 16:37:18.951469 27028 solver.cpp:357] Iteration 51100 (2.79249 iter/s, 35.8103s/100 iters), loss = 0.0441731
I0218 16:37:18.951622 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010299 (* 1 = 0.010299 loss)
I0218 16:37:18.951633 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107912 (* 1 = 0.0107912 loss)
I0218 16:37:18.951642 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0127788 (* 1 = 0.0127788 loss)
I0218 16:37:18.951649 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103041 (* 1 = 0.0103041 loss)
I0218 16:37:18.951660 27028 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0218 16:37:54.729830 27028 solver.cpp:357] Iteration 51200 (2.79494 iter/s, 35.779s/100 iters), loss = 0.0440724
I0218 16:37:54.730012 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107198 (* 1 = 0.0107198 loss)
I0218 16:37:54.730023 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010479 (* 1 = 0.010479 loss)
I0218 16:37:54.730032 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0125848 (* 1 = 0.0125848 loss)
I0218 16:37:54.730039 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102888 (* 1 = 0.0102888 loss)
I0218 16:37:54.730047 27028 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0218 16:38:30.502081 27028 solver.cpp:357] Iteration 51300 (2.79542 iter/s, 35.7728s/100 iters), loss = 0.042097
I0218 16:38:30.502197 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0127048 (* 1 = 0.0127048 loss)
I0218 16:38:30.502208 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00989193 (* 1 = 0.00989193 loss)
I0218 16:38:30.502218 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0101676 (* 1 = 0.0101676 loss)
I0218 16:38:30.502225 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00933265 (* 1 = 0.00933265 loss)
I0218 16:38:30.502233 27028 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0218 16:39:02.081547 27028 solver.cpp:357] Iteration 51400 (3.16656 iter/s, 31.58s/100 iters), loss = 0.0468183
I0218 16:39:02.081650 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0129325 (* 1 = 0.0129325 loss)
I0218 16:39:02.081660 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0119113 (* 1 = 0.0119113 loss)
I0218 16:39:02.081670 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102033 (* 1 = 0.0102033 loss)
I0218 16:39:02.081677 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0117712 (* 1 = 0.0117712 loss)
I0218 16:39:02.081686 27028 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0218 16:39:35.436461 27028 solver.cpp:357] Iteration 51500 (2.99801 iter/s, 33.3555s/100 iters), loss = 0.042106
I0218 16:39:35.436638 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0105029 (* 1 = 0.0105029 loss)
I0218 16:39:35.436650 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00961532 (* 1 = 0.00961532 loss)
I0218 16:39:35.436658 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0113722 (* 1 = 0.0113722 loss)
I0218 16:39:35.436666 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106156 (* 1 = 0.0106156 loss)
I0218 16:39:35.436674 27028 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0218 16:40:11.209179 27028 solver.cpp:357] Iteration 51600 (2.79538 iter/s, 35.7733s/100 iters), loss = 0.0443302
I0218 16:40:11.209337 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104253 (* 1 = 0.0104253 loss)
I0218 16:40:11.209348 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0111075 (* 1 = 0.0111075 loss)
I0218 16:40:11.209357 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0133971 (* 1 = 0.0133971 loss)
I0218 16:40:11.209364 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0094003 (* 1 = 0.0094003 loss)
I0218 16:40:11.209373 27028 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0218 16:40:46.975901 27028 solver.cpp:357] Iteration 51700 (2.79585 iter/s, 35.7673s/100 iters), loss = 0.0407835
I0218 16:40:46.976016 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109083 (* 1 = 0.0109083 loss)
I0218 16:40:46.976025 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00941014 (* 1 = 0.00941014 loss)
I0218 16:40:46.976034 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0107833 (* 1 = 0.0107833 loss)
I0218 16:40:46.976042 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00968175 (* 1 = 0.00968175 loss)
I0218 16:40:46.976050 27028 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0218 16:41:22.752645 27028 solver.cpp:357] Iteration 51800 (2.79506 iter/s, 35.7774s/100 iters), loss = 0.048764
I0218 16:41:22.752760 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00977924 (* 1 = 0.00977924 loss)
I0218 16:41:22.752770 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0100954 (* 1 = 0.0100954 loss)
I0218 16:41:22.752779 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0111475 (* 1 = 0.0111475 loss)
I0218 16:41:22.752787 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0177419 (* 1 = 0.0177419 loss)
I0218 16:41:22.752794 27028 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0218 16:41:55.452896 27028 solver.cpp:357] Iteration 51900 (3.05803 iter/s, 32.7008s/100 iters), loss = 0.0417251
I0218 16:41:55.453012 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00867345 (* 1 = 0.00867345 loss)
I0218 16:41:55.453023 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0127948 (* 1 = 0.0127948 loss)
I0218 16:41:55.453032 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102097 (* 1 = 0.0102097 loss)
I0218 16:41:55.453040 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100471 (* 1 = 0.0100471 loss)
I0218 16:41:55.453048 27028 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0218 16:42:27.475833 27028 solver.cpp:514] Iteration 52000, Testing net (#0)
I0218 16:42:29.882570 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0788084 (* 1 = 0.0788084 loss)
I0218 16:42:29.882617 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.0768078 (* 1 = 0.0768078 loss)
I0218 16:42:29.882625 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0875287 (* 1 = 0.0875287 loss)
I0218 16:42:29.882633 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0666899 (* 1 = 0.0666899 loss)
I0218 16:42:29.882640 27028 solver.cpp:580]     Test net output #4: prob1 = 0.979
I0218 16:42:29.882647 27028 solver.cpp:580]     Test net output #5: prob2 = 0.98
I0218 16:42:29.882652 27028 solver.cpp:580]     Test net output #6: prob3 = 0.979
I0218 16:42:29.882656 27028 solver.cpp:580]     Test net output #7: prob4 = 0.982
I0218 16:42:30.218852 27028 solver.cpp:357] Iteration 52000 (2.87633 iter/s, 34.7666s/100 iters), loss = 0.0413673
I0218 16:42:30.218906 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0116895 (* 1 = 0.0116895 loss)
I0218 16:42:30.218917 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0105973 (* 1 = 0.0105973 loss)
I0218 16:42:30.218926 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00983361 (* 1 = 0.00983361 loss)
I0218 16:42:30.218935 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00924692 (* 1 = 0.00924692 loss)
I0218 16:42:30.218942 27028 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0218 16:43:06.004297 27028 solver.cpp:357] Iteration 52100 (2.79438 iter/s, 35.7862s/100 iters), loss = 0.0511737
I0218 16:43:06.004590 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103729 (* 1 = 0.0103729 loss)
I0218 16:43:06.004601 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108115 (* 1 = 0.0108115 loss)
I0218 16:43:06.004611 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0170022 (* 1 = 0.0170022 loss)
I0218 16:43:06.004619 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0129872 (* 1 = 0.0129872 loss)
I0218 16:43:06.004628 27028 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0218 16:43:41.794839 27028 solver.cpp:357] Iteration 52200 (2.79435 iter/s, 35.7864s/100 iters), loss = 0.047784
I0218 16:43:41.795020 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.013288 (* 1 = 0.013288 loss)
I0218 16:43:41.795032 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0120743 (* 1 = 0.0120743 loss)
I0218 16:43:41.795040 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0128663 (* 1 = 0.0128663 loss)
I0218 16:43:41.795049 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00955539 (* 1 = 0.00955539 loss)
I0218 16:43:41.795060 27028 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0218 16:44:17.649756 27028 solver.cpp:357] Iteration 52300 (2.78937 iter/s, 35.8504s/100 iters), loss = 0.0476069
I0218 16:44:17.649866 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0133429 (* 1 = 0.0133429 loss)
I0218 16:44:17.649876 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0117376 (* 1 = 0.0117376 loss)
I0218 16:44:17.649885 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0125413 (* 1 = 0.0125413 loss)
I0218 16:44:17.649894 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00998513 (* 1 = 0.00998513 loss)
I0218 16:44:17.649901 27028 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0218 16:44:50.401072 27028 solver.cpp:357] Iteration 52400 (3.05351 iter/s, 32.7491s/100 iters), loss = 0.04166
I0218 16:44:50.401230 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102678 (* 1 = 0.0102678 loss)
I0218 16:44:50.401242 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104434 (* 1 = 0.0104434 loss)
I0218 16:44:50.401249 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0103351 (* 1 = 0.0103351 loss)
I0218 16:44:50.401257 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106137 (* 1 = 0.0106137 loss)
I0218 16:44:50.401269 27028 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0218 16:45:22.628265 27028 solver.cpp:357] Iteration 52500 (3.10317 iter/s, 32.2252s/100 iters), loss = 0.0452734
I0218 16:45:22.628422 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0108632 (* 1 = 0.0108632 loss)
I0218 16:45:22.628432 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.012089 (* 1 = 0.012089 loss)
I0218 16:45:22.628442 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109817 (* 1 = 0.0109817 loss)
I0218 16:45:22.628449 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0113394 (* 1 = 0.0113394 loss)
I0218 16:45:22.628458 27028 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0218 16:45:58.416558 27028 solver.cpp:357] Iteration 52600 (2.79437 iter/s, 35.7862s/100 iters), loss = 0.0405746
I0218 16:45:58.416890 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00983233 (* 1 = 0.00983233 loss)
I0218 16:45:58.416901 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00984057 (* 1 = 0.00984057 loss)
I0218 16:45:58.416910 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00984734 (* 1 = 0.00984734 loss)
I0218 16:45:58.416919 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0110543 (* 1 = 0.0110543 loss)
I0218 16:45:58.416927 27028 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0218 16:46:34.199316 27028 solver.cpp:357] Iteration 52700 (2.79495 iter/s, 35.7789s/100 iters), loss = 0.0479389
I0218 16:46:34.199625 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00956773 (* 1 = 0.00956773 loss)
I0218 16:46:34.199636 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0146596 (* 1 = 0.0146596 loss)
I0218 16:46:34.199645 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0135786 (* 1 = 0.0135786 loss)
I0218 16:46:34.199652 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.010133 (* 1 = 0.010133 loss)
I0218 16:46:34.199661 27028 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0218 16:47:10.050691 27028 solver.cpp:357] Iteration 52800 (2.78959 iter/s, 35.8476s/100 iters), loss = 0.0430903
I0218 16:47:10.050843 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0110814 (* 1 = 0.0110814 loss)
I0218 16:47:10.050853 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0113977 (* 1 = 0.0113977 loss)
I0218 16:47:10.050861 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109403 (* 1 = 0.0109403 loss)
I0218 16:47:10.050869 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00967096 (* 1 = 0.00967096 loss)
I0218 16:47:10.050880 27028 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0218 16:47:43.672536 27028 solver.cpp:357] Iteration 52900 (2.97439 iter/s, 33.6203s/100 iters), loss = 0.042815
I0218 16:47:43.672684 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103884 (* 1 = 0.0103884 loss)
I0218 16:47:43.672694 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0101018 (* 1 = 0.0101018 loss)
I0218 16:47:43.672703 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102823 (* 1 = 0.0102823 loss)
I0218 16:47:43.672710 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0120425 (* 1 = 0.0120425 loss)
I0218 16:47:43.672724 27028 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0218 16:48:14.544833 27028 solver.cpp:514] Iteration 53000, Testing net (#0)
I0218 16:48:16.931460 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.13683 (* 1 = 0.13683 loss)
I0218 16:48:16.931510 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.121696 (* 1 = 0.121696 loss)
I0218 16:48:16.931519 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.144502 (* 1 = 0.144502 loss)
I0218 16:48:16.931527 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.077902 (* 1 = 0.077902 loss)
I0218 16:48:16.931535 27028 solver.cpp:580]     Test net output #4: prob1 = 0.962
I0218 16:48:16.931540 27028 solver.cpp:580]     Test net output #5: prob2 = 0.971
I0218 16:48:16.931545 27028 solver.cpp:580]     Test net output #6: prob3 = 0.969
I0218 16:48:16.931550 27028 solver.cpp:580]     Test net output #7: prob4 = 0.978
I0218 16:48:17.268852 27028 solver.cpp:357] Iteration 53000 (2.97664 iter/s, 33.5949s/100 iters), loss = 0.0483676
I0218 16:48:17.268908 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00993485 (* 1 = 0.00993485 loss)
I0218 16:48:17.268918 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0127878 (* 1 = 0.0127878 loss)
I0218 16:48:17.268925 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112845 (* 1 = 0.0112845 loss)
I0218 16:48:17.268934 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0143605 (* 1 = 0.0143605 loss)
I0218 16:48:17.268941 27028 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0218 16:48:53.057618 27028 solver.cpp:357] Iteration 53100 (2.79427 iter/s, 35.7875s/100 iters), loss = 0.0454044
I0218 16:48:53.057967 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00877766 (* 1 = 0.00877766 loss)
I0218 16:48:53.057981 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010779 (* 1 = 0.010779 loss)
I0218 16:48:53.057989 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0153624 (* 1 = 0.0153624 loss)
I0218 16:48:53.057996 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0104854 (* 1 = 0.0104854 loss)
I0218 16:48:53.058004 27028 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0218 16:49:28.806087 27028 solver.cpp:357] Iteration 53200 (2.79758 iter/s, 35.7452s/100 iters), loss = 0.042473
I0218 16:49:28.806224 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010811 (* 1 = 0.010811 loss)
I0218 16:49:28.806236 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0120956 (* 1 = 0.0120956 loss)
I0218 16:49:28.806244 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0118074 (* 1 = 0.0118074 loss)
I0218 16:49:28.806253 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00775896 (* 1 = 0.00775896 loss)
I0218 16:49:28.806262 27028 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0218 16:50:04.610710 27028 solver.cpp:357] Iteration 53300 (2.79319 iter/s, 35.8014s/100 iters), loss = 0.0388324
I0218 16:50:04.610857 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103351 (* 1 = 0.0103351 loss)
I0218 16:50:04.610868 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00833634 (* 1 = 0.00833634 loss)
I0218 16:50:04.610877 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00996522 (* 1 = 0.00996522 loss)
I0218 16:50:04.610885 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101958 (* 1 = 0.0101958 loss)
I0218 16:50:04.610893 27028 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0218 16:50:38.625732 27028 solver.cpp:357] Iteration 53400 (2.93997 iter/s, 34.014s/100 iters), loss = 0.0410275
I0218 16:50:38.625890 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00899348 (* 1 = 0.00899348 loss)
I0218 16:50:38.625901 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00994643 (* 1 = 0.00994643 loss)
I0218 16:50:38.625910 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0106616 (* 1 = 0.0106616 loss)
I0218 16:50:38.625917 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.011426 (* 1 = 0.011426 loss)
I0218 16:50:38.625931 27028 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0218 16:51:09.835315 27028 solver.cpp:357] Iteration 53500 (3.20424 iter/s, 31.2087s/100 iters), loss = 0.0454855
I0218 16:51:09.835465 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0113107 (* 1 = 0.0113107 loss)
I0218 16:51:09.835475 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011897 (* 1 = 0.011897 loss)
I0218 16:51:09.835484 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0121087 (* 1 = 0.0121087 loss)
I0218 16:51:09.835492 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101691 (* 1 = 0.0101691 loss)
I0218 16:51:09.835500 27028 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0218 16:51:45.602499 27028 solver.cpp:357] Iteration 53600 (2.79593 iter/s, 35.7663s/100 iters), loss = 0.0416028
I0218 16:51:45.602656 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010409 (* 1 = 0.010409 loss)
I0218 16:51:45.602668 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104347 (* 1 = 0.0104347 loss)
I0218 16:51:45.602676 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0105769 (* 1 = 0.0105769 loss)
I0218 16:51:45.602684 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101822 (* 1 = 0.0101822 loss)
I0218 16:51:45.602695 27028 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0218 16:52:21.371944 27028 solver.cpp:357] Iteration 53700 (2.79575 iter/s, 35.7686s/100 iters), loss = 0.0433173
I0218 16:52:21.372058 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109539 (* 1 = 0.0109539 loss)
I0218 16:52:21.372068 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0103083 (* 1 = 0.0103083 loss)
I0218 16:52:21.372077 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0123583 (* 1 = 0.0123583 loss)
I0218 16:52:21.372086 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00969676 (* 1 = 0.00969676 loss)
I0218 16:52:21.372093 27028 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0218 16:52:57.156046 27028 solver.cpp:357] Iteration 53800 (2.79459 iter/s, 35.7834s/100 iters), loss = 0.0381273
I0218 16:52:57.156220 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00945613 (* 1 = 0.00945613 loss)
I0218 16:52:57.156231 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00993158 (* 1 = 0.00993158 loss)
I0218 16:52:57.156240 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00933582 (* 1 = 0.00933582 loss)
I0218 16:52:57.156249 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00940379 (* 1 = 0.00940379 loss)
I0218 16:52:57.156256 27028 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0218 16:53:31.996022 27028 solver.cpp:357] Iteration 53900 (2.87033 iter/s, 34.8393s/100 iters), loss = 0.0457996
I0218 16:53:31.996140 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0117595 (* 1 = 0.0117595 loss)
I0218 16:53:31.996150 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0112594 (* 1 = 0.0112594 loss)
I0218 16:53:31.996160 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0105889 (* 1 = 0.0105889 loss)
I0218 16:53:31.996167 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0121918 (* 1 = 0.0121918 loss)
I0218 16:53:31.996176 27028 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0218 16:54:01.920912 27028 solver.cpp:514] Iteration 54000, Testing net (#0)
I0218 16:54:04.398356 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.116905 (* 1 = 0.116905 loss)
I0218 16:54:04.398443 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.109472 (* 1 = 0.109472 loss)
I0218 16:54:04.398453 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0983577 (* 1 = 0.0983577 loss)
I0218 16:54:04.398461 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0848367 (* 1 = 0.0848367 loss)
I0218 16:54:04.398468 27028 solver.cpp:580]     Test net output #4: prob1 = 0.974
I0218 16:54:04.398473 27028 solver.cpp:580]     Test net output #5: prob2 = 0.971
I0218 16:54:04.398479 27028 solver.cpp:580]     Test net output #6: prob3 = 0.978
I0218 16:54:04.398484 27028 solver.cpp:580]     Test net output #7: prob4 = 0.979
I0218 16:54:04.734220 27028 solver.cpp:357] Iteration 54000 (3.05459 iter/s, 32.7376s/100 iters), loss = 0.0556234
I0218 16:54:04.734274 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00889651 (* 1 = 0.00889651 loss)
I0218 16:54:04.734284 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0256492 (* 1 = 0.0256492 loss)
I0218 16:54:04.734293 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0103971 (* 1 = 0.0103971 loss)
I0218 16:54:04.734302 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106805 (* 1 = 0.0106805 loss)
I0218 16:54:04.734308 27028 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0218 16:54:40.492882 27028 solver.cpp:357] Iteration 54100 (2.79656 iter/s, 35.7582s/100 iters), loss = 0.04318
I0218 16:54:40.493037 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106235 (* 1 = 0.0106235 loss)
I0218 16:54:40.493049 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108268 (* 1 = 0.0108268 loss)
I0218 16:54:40.493057 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0116433 (* 1 = 0.0116433 loss)
I0218 16:54:40.493065 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100864 (* 1 = 0.0100864 loss)
I0218 16:54:40.493077 27028 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0218 16:55:16.246788 27028 solver.cpp:357] Iteration 54200 (2.79694 iter/s, 35.7534s/100 iters), loss = 0.0401632
I0218 16:55:16.246901 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00906184 (* 1 = 0.00906184 loss)
I0218 16:55:16.246912 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00865156 (* 1 = 0.00865156 loss)
I0218 16:55:16.246922 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0118608 (* 1 = 0.0118608 loss)
I0218 16:55:16.246929 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.010589 (* 1 = 0.010589 loss)
I0218 16:55:16.246937 27028 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0218 16:55:52.016300 27028 solver.cpp:357] Iteration 54300 (2.79571 iter/s, 35.7691s/100 iters), loss = 0.041611
I0218 16:55:52.016521 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00975987 (* 1 = 0.00975987 loss)
I0218 16:55:52.016533 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104695 (* 1 = 0.0104695 loss)
I0218 16:55:52.016542 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0117494 (* 1 = 0.0117494 loss)
I0218 16:55:52.016551 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00963224 (* 1 = 0.00963224 loss)
I0218 16:55:52.016557 27028 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0218 16:56:26.901998 27028 solver.cpp:357] Iteration 54400 (2.86654 iter/s, 34.8852s/100 iters), loss = 0.045143
I0218 16:56:26.902115 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.01172 (* 1 = 0.01172 loss)
I0218 16:56:26.902127 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0118938 (* 1 = 0.0118938 loss)
I0218 16:56:26.902134 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0113962 (* 1 = 0.0113962 loss)
I0218 16:56:26.902143 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.010133 (* 1 = 0.010133 loss)
I0218 16:56:26.902150 27028 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0218 16:56:57.033835 27028 solver.cpp:357] Iteration 54500 (3.31878 iter/s, 30.1315s/100 iters), loss = 0.0396641
I0218 16:56:57.034006 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0114779 (* 1 = 0.0114779 loss)
I0218 16:56:57.034018 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0098946 (* 1 = 0.0098946 loss)
I0218 16:56:57.034026 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00934384 (* 1 = 0.00934384 loss)
I0218 16:56:57.034034 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0089478 (* 1 = 0.0089478 loss)
I0218 16:56:57.034041 27028 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0218 16:57:32.799412 27028 solver.cpp:357] Iteration 54600 (2.79601 iter/s, 35.7652s/100 iters), loss = 0.0409019
I0218 16:57:32.799577 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102802 (* 1 = 0.0102802 loss)
I0218 16:57:32.799588 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00887457 (* 1 = 0.00887457 loss)
I0218 16:57:32.799597 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00996889 (* 1 = 0.00996889 loss)
I0218 16:57:32.799605 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0117782 (* 1 = 0.0117782 loss)
I0218 16:57:32.799612 27028 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0218 16:58:08.567188 27028 solver.cpp:357] Iteration 54700 (2.79584 iter/s, 35.7675s/100 iters), loss = 0.0421748
I0218 16:58:08.567344 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.011299 (* 1 = 0.011299 loss)
I0218 16:58:08.567355 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00908951 (* 1 = 0.00908951 loss)
I0218 16:58:08.567364 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0120366 (* 1 = 0.0120366 loss)
I0218 16:58:08.567373 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0097497 (* 1 = 0.0097497 loss)
I0218 16:58:08.567384 27028 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0218 16:58:44.325486 27028 solver.cpp:357] Iteration 54800 (2.79658 iter/s, 35.758s/100 iters), loss = 0.045444
I0218 16:58:44.325603 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107382 (* 1 = 0.0107382 loss)
I0218 16:58:44.325613 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.012916 (* 1 = 0.012916 loss)
I0218 16:58:44.325621 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0111597 (* 1 = 0.0111597 loss)
I0218 16:58:44.325629 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106301 (* 1 = 0.0106301 loss)
I0218 16:58:44.325637 27028 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0218 16:59:20.101291 27028 solver.cpp:357] Iteration 54900 (2.7952 iter/s, 35.7756s/100 iters), loss = 0.0420528
I0218 16:59:20.101467 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0110827 (* 1 = 0.0110827 loss)
I0218 16:59:20.101477 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0101272 (* 1 = 0.0101272 loss)
I0218 16:59:20.101486 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.011646 (* 1 = 0.011646 loss)
I0218 16:59:20.101495 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00919686 (* 1 = 0.00919686 loss)
I0218 16:59:20.101502 27028 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0218 16:59:49.031738 27028 solver.cpp:514] Iteration 55000, Testing net (#0)
I0218 16:59:51.428107 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0703204 (* 1 = 0.0703204 loss)
I0218 16:59:51.428247 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.11665 (* 1 = 0.11665 loss)
I0218 16:59:51.428256 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.13766 (* 1 = 0.13766 loss)
I0218 16:59:51.428264 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0802253 (* 1 = 0.0802253 loss)
I0218 16:59:51.428272 27028 solver.cpp:580]     Test net output #4: prob1 = 0.975
I0218 16:59:51.428277 27028 solver.cpp:580]     Test net output #5: prob2 = 0.97
I0218 16:59:51.428282 27028 solver.cpp:580]     Test net output #6: prob3 = 0.97
I0218 16:59:51.428287 27028 solver.cpp:580]     Test net output #7: prob4 = 0.978
I0218 16:59:51.765312 27028 solver.cpp:357] Iteration 55000 (3.15818 iter/s, 31.6638s/100 iters), loss = 0.0386565
I0218 16:59:51.765372 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00932195 (* 1 = 0.00932195 loss)
I0218 16:59:51.765383 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0094484 (* 1 = 0.0094484 loss)
I0218 16:59:51.765391 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00942305 (* 1 = 0.00942305 loss)
I0218 16:59:51.765399 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0104631 (* 1 = 0.0104631 loss)
I0218 16:59:51.765408 27028 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0218 17:00:27.535449 27028 solver.cpp:357] Iteration 55100 (2.79564 iter/s, 35.77s/100 iters), loss = 0.0414033
I0218 17:00:27.535616 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00921736 (* 1 = 0.00921736 loss)
I0218 17:00:27.535626 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0096202 (* 1 = 0.0096202 loss)
I0218 17:00:27.535635 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112356 (* 1 = 0.0112356 loss)
I0218 17:00:27.535643 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0113302 (* 1 = 0.0113302 loss)
I0218 17:00:27.535651 27028 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0218 17:01:03.291508 27028 solver.cpp:357] Iteration 55200 (2.79674 iter/s, 35.7559s/100 iters), loss = 0.0427509
I0218 17:01:03.291651 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0127708 (* 1 = 0.0127708 loss)
I0218 17:01:03.291661 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108728 (* 1 = 0.0108728 loss)
I0218 17:01:03.291669 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00975405 (* 1 = 0.00975405 loss)
I0218 17:01:03.291677 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00935315 (* 1 = 0.00935315 loss)
I0218 17:01:03.291685 27028 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0218 17:01:39.056614 27028 solver.cpp:357] Iteration 55300 (2.79603 iter/s, 35.765s/100 iters), loss = 0.0414798
I0218 17:01:39.056764 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010534 (* 1 = 0.010534 loss)
I0218 17:01:39.056776 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0103171 (* 1 = 0.0103171 loss)
I0218 17:01:39.056784 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0111343 (* 1 = 0.0111343 loss)
I0218 17:01:39.056792 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00949439 (* 1 = 0.00949439 loss)
I0218 17:01:39.056799 27028 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0218 17:02:14.808292 27028 solver.cpp:357] Iteration 55400 (2.79708 iter/s, 35.7516s/100 iters), loss = 0.0409645
I0218 17:02:14.808583 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00819833 (* 1 = 0.00819833 loss)
I0218 17:02:14.808594 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00872256 (* 1 = 0.00872256 loss)
I0218 17:02:14.808604 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00991662 (* 1 = 0.00991662 loss)
I0218 17:02:14.808611 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0141269 (* 1 = 0.0141269 loss)
I0218 17:02:14.808619 27028 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0218 17:02:43.977870 27028 solver.cpp:357] Iteration 55500 (3.42826 iter/s, 29.1693s/100 iters), loss = 0.0383638
I0218 17:02:43.977947 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0100013 (* 1 = 0.0100013 loss)
I0218 17:02:43.977958 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00928057 (* 1 = 0.00928057 loss)
I0218 17:02:43.977967 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0111104 (* 1 = 0.0111104 loss)
I0218 17:02:43.977975 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00797153 (* 1 = 0.00797153 loss)
I0218 17:02:43.977982 27028 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0218 17:03:19.753283 27028 solver.cpp:357] Iteration 55600 (2.79521 iter/s, 35.7754s/100 iters), loss = 0.0406962
I0218 17:03:19.753465 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101835 (* 1 = 0.0101835 loss)
I0218 17:03:19.753477 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010154 (* 1 = 0.010154 loss)
I0218 17:03:19.753486 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0103462 (* 1 = 0.0103462 loss)
I0218 17:03:19.753494 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100125 (* 1 = 0.0100125 loss)
I0218 17:03:19.753502 27028 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0218 17:03:55.529232 27028 solver.cpp:357] Iteration 55700 (2.79533 iter/s, 35.7739s/100 iters), loss = 0.0442698
I0218 17:03:55.529412 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106582 (* 1 = 0.0106582 loss)
I0218 17:03:55.529422 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107704 (* 1 = 0.0107704 loss)
I0218 17:03:55.529431 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0122448 (* 1 = 0.0122448 loss)
I0218 17:03:55.529440 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0105964 (* 1 = 0.0105964 loss)
I0218 17:03:55.529448 27028 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0218 17:04:31.316563 27028 solver.cpp:357] Iteration 55800 (2.79444 iter/s, 35.7853s/100 iters), loss = 0.0417406
I0218 17:04:31.316694 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00895692 (* 1 = 0.00895692 loss)
I0218 17:04:31.316705 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0118675 (* 1 = 0.0118675 loss)
I0218 17:04:31.316715 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.01118 (* 1 = 0.01118 loss)
I0218 17:04:31.316722 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00973614 (* 1 = 0.00973614 loss)
I0218 17:04:31.316730 27028 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0218 17:05:07.092576 27028 solver.cpp:357] Iteration 55900 (2.79532 iter/s, 35.7741s/100 iters), loss = 0.0414312
I0218 17:05:07.092730 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0097526 (* 1 = 0.0097526 loss)
I0218 17:05:07.092741 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0105368 (* 1 = 0.0105368 loss)
I0218 17:05:07.092749 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115679 (* 1 = 0.0115679 loss)
I0218 17:05:07.092758 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00957394 (* 1 = 0.00957394 loss)
I0218 17:05:07.092767 27028 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0218 17:05:35.934620 27028 solver.cpp:514] Iteration 56000, Testing net (#0)
I0218 17:05:38.407882 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.123459 (* 1 = 0.123459 loss)
I0218 17:05:38.408033 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.088182 (* 1 = 0.088182 loss)
I0218 17:05:38.408043 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0952855 (* 1 = 0.0952855 loss)
I0218 17:05:38.408051 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.113645 (* 1 = 0.113645 loss)
I0218 17:05:38.408058 27028 solver.cpp:580]     Test net output #4: prob1 = 0.972
I0218 17:05:38.408063 27028 solver.cpp:580]     Test net output #5: prob2 = 0.974
I0218 17:05:38.408069 27028 solver.cpp:580]     Test net output #6: prob3 = 0.98
I0218 17:05:38.408074 27028 solver.cpp:580]     Test net output #7: prob4 = 0.972
I0218 17:05:38.744643 27028 solver.cpp:357] Iteration 56000 (3.15955 iter/s, 31.6501s/100 iters), loss = 0.0403091
I0218 17:05:38.744702 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00973216 (* 1 = 0.00973216 loss)
I0218 17:05:38.744712 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107695 (* 1 = 0.0107695 loss)
I0218 17:05:38.744721 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00954384 (* 1 = 0.00954384 loss)
I0218 17:05:38.744729 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102636 (* 1 = 0.0102636 loss)
I0218 17:05:38.744736 27028 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0218 17:06:14.507025 27028 solver.cpp:357] Iteration 56100 (2.79623 iter/s, 35.7625s/100 iters), loss = 0.0396612
I0218 17:06:14.507148 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00932485 (* 1 = 0.00932485 loss)
I0218 17:06:14.507158 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00936131 (* 1 = 0.00936131 loss)
I0218 17:06:14.507167 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112281 (* 1 = 0.0112281 loss)
I0218 17:06:14.507175 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00974693 (* 1 = 0.00974693 loss)
I0218 17:06:14.507184 27028 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0218 17:06:50.272415 27028 solver.cpp:357] Iteration 56200 (2.79599 iter/s, 35.7655s/100 iters), loss = 0.0395853
I0218 17:06:50.272534 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0084149 (* 1 = 0.0084149 loss)
I0218 17:06:50.272544 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0124445 (* 1 = 0.0124445 loss)
I0218 17:06:50.272553 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104035 (* 1 = 0.0104035 loss)
I0218 17:06:50.272562 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00832242 (* 1 = 0.00832242 loss)
I0218 17:06:50.272568 27028 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0218 17:07:26.042019 27028 solver.cpp:357] Iteration 56300 (2.79566 iter/s, 35.7697s/100 iters), loss = 0.043754
I0218 17:07:26.042181 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101342 (* 1 = 0.0101342 loss)
I0218 17:07:26.042191 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0121647 (* 1 = 0.0121647 loss)
I0218 17:07:26.042201 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0118991 (* 1 = 0.0118991 loss)
I0218 17:07:26.042209 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00955595 (* 1 = 0.00955595 loss)
I0218 17:07:26.042217 27028 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0218 17:08:01.814116 27028 solver.cpp:357] Iteration 56400 (2.79547 iter/s, 35.7722s/100 iters), loss = 0.0425738
I0218 17:08:01.814255 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101763 (* 1 = 0.0101763 loss)
I0218 17:08:01.814266 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00977196 (* 1 = 0.00977196 loss)
I0218 17:08:01.814275 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0131786 (* 1 = 0.0131786 loss)
I0218 17:08:01.814283 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00944688 (* 1 = 0.00944688 loss)
I0218 17:08:01.814292 27028 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0218 17:08:31.127894 27028 solver.cpp:357] Iteration 56500 (3.41136 iter/s, 29.3138s/100 iters), loss = 0.0379556
I0218 17:08:31.127962 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00971502 (* 1 = 0.00971502 loss)
I0218 17:08:31.127972 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010952 (* 1 = 0.010952 loss)
I0218 17:08:31.127981 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00855432 (* 1 = 0.00855432 loss)
I0218 17:08:31.127990 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00873425 (* 1 = 0.00873425 loss)
I0218 17:08:31.127996 27028 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0218 17:09:06.880885 27028 solver.cpp:357] Iteration 56600 (2.79696 iter/s, 35.7532s/100 iters), loss = 0.0414658
I0218 17:09:06.881223 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00918908 (* 1 = 0.00918908 loss)
I0218 17:09:06.881235 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0101952 (* 1 = 0.0101952 loss)
I0218 17:09:06.881244 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115999 (* 1 = 0.0115999 loss)
I0218 17:09:06.881251 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0104816 (* 1 = 0.0104816 loss)
I0218 17:09:06.881259 27028 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0218 17:09:42.687702 27028 solver.cpp:357] Iteration 56700 (2.79277 iter/s, 35.8067s/100 iters), loss = 0.043519
I0218 17:09:42.688010 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101673 (* 1 = 0.0101673 loss)
I0218 17:09:42.688022 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0109728 (* 1 = 0.0109728 loss)
I0218 17:09:42.688031 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104135 (* 1 = 0.0104135 loss)
I0218 17:09:42.688040 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0119654 (* 1 = 0.0119654 loss)
I0218 17:09:42.688047 27028 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0218 17:10:18.506978 27028 solver.cpp:357] Iteration 56800 (2.79194 iter/s, 35.8174s/100 iters), loss = 0.044199
I0218 17:10:18.507143 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102595 (* 1 = 0.0102595 loss)
I0218 17:10:18.507154 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106866 (* 1 = 0.0106866 loss)
I0218 17:10:18.507164 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0130326 (* 1 = 0.0130326 loss)
I0218 17:10:18.507171 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102203 (* 1 = 0.0102203 loss)
I0218 17:10:18.507179 27028 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0218 17:10:54.310158 27028 solver.cpp:357] Iteration 56900 (2.79319 iter/s, 35.8013s/100 iters), loss = 0.0408491
I0218 17:10:54.310307 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00944592 (* 1 = 0.00944592 loss)
I0218 17:10:54.310318 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00956216 (* 1 = 0.00956216 loss)
I0218 17:10:54.310328 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0108944 (* 1 = 0.0108944 loss)
I0218 17:10:54.310335 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0109466 (* 1 = 0.0109466 loss)
I0218 17:10:54.310343 27028 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0218 17:11:24.018539 27028 solver.cpp:514] Iteration 57000, Testing net (#0)
I0218 17:11:25.562891 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0773432 (* 1 = 0.0773432 loss)
I0218 17:11:25.562963 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.101755 (* 1 = 0.101755 loss)
I0218 17:11:25.562973 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.120341 (* 1 = 0.120341 loss)
I0218 17:11:25.562984 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.102658 (* 1 = 0.102658 loss)
I0218 17:11:25.562990 27028 solver.cpp:580]     Test net output #4: prob1 = 0.981
I0218 17:11:25.562996 27028 solver.cpp:580]     Test net output #5: prob2 = 0.973
I0218 17:11:25.563001 27028 solver.cpp:580]     Test net output #6: prob3 = 0.97
I0218 17:11:25.563007 27028 solver.cpp:580]     Test net output #7: prob4 = 0.978
I0218 17:11:25.899642 27028 solver.cpp:357] Iteration 57000 (3.1656 iter/s, 31.5896s/100 iters), loss = 0.0420721
I0218 17:11:25.899686 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010329 (* 1 = 0.010329 loss)
I0218 17:11:25.899698 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104117 (* 1 = 0.0104117 loss)
I0218 17:11:25.899705 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104228 (* 1 = 0.0104228 loss)
I0218 17:11:25.899713 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0109087 (* 1 = 0.0109087 loss)
I0218 17:11:25.899721 27028 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0218 17:12:01.676206 27028 solver.cpp:357] Iteration 57100 (2.79511 iter/s, 35.7768s/100 iters), loss = 0.0425803
I0218 17:12:01.676566 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00926307 (* 1 = 0.00926307 loss)
I0218 17:12:01.676578 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0105175 (* 1 = 0.0105175 loss)
I0218 17:12:01.676586 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0125618 (* 1 = 0.0125618 loss)
I0218 17:12:01.676594 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102379 (* 1 = 0.0102379 loss)
I0218 17:12:01.676601 27028 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0218 17:12:37.434859 27028 solver.cpp:357] Iteration 57200 (2.79653 iter/s, 35.7586s/100 iters), loss = 0.043285
I0218 17:12:37.434978 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0111029 (* 1 = 0.0111029 loss)
I0218 17:12:37.434988 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0122072 (* 1 = 0.0122072 loss)
I0218 17:12:37.434996 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109789 (* 1 = 0.0109789 loss)
I0218 17:12:37.435004 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00899599 (* 1 = 0.00899599 loss)
I0218 17:12:37.435011 27028 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0218 17:13:13.210839 27028 solver.cpp:357] Iteration 57300 (2.79516 iter/s, 35.7762s/100 iters), loss = 0.0439411
I0218 17:13:13.210989 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103548 (* 1 = 0.0103548 loss)
I0218 17:13:13.210999 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0109093 (* 1 = 0.0109093 loss)
I0218 17:13:13.211009 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0132963 (* 1 = 0.0132963 loss)
I0218 17:13:13.211016 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00938067 (* 1 = 0.00938067 loss)
I0218 17:13:13.211024 27028 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0218 17:13:49.018826 27028 solver.cpp:357] Iteration 57400 (2.79266 iter/s, 35.8081s/100 iters), loss = 0.0407054
I0218 17:13:49.019002 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00998012 (* 1 = 0.00998012 loss)
I0218 17:13:49.019013 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00980556 (* 1 = 0.00980556 loss)
I0218 17:13:49.019022 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0114366 (* 1 = 0.0114366 loss)
I0218 17:13:49.019031 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00948309 (* 1 = 0.00948309 loss)
I0218 17:13:49.019039 27028 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0218 17:14:19.193572 27028 solver.cpp:357] Iteration 57500 (3.31424 iter/s, 30.1729s/100 iters), loss = 0.0408889
I0218 17:14:19.193722 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00978876 (* 1 = 0.00978876 loss)
I0218 17:14:19.193732 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0117062 (* 1 = 0.0117062 loss)
I0218 17:14:19.193742 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0103524 (* 1 = 0.0103524 loss)
I0218 17:14:19.193749 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0090415 (* 1 = 0.0090415 loss)
I0218 17:14:19.193761 27028 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0218 17:14:54.032681 27028 solver.cpp:357] Iteration 57600 (2.87032 iter/s, 34.8393s/100 iters), loss = 0.043383
I0218 17:14:54.032799 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0113106 (* 1 = 0.0113106 loss)
I0218 17:14:54.032809 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00909759 (* 1 = 0.00909759 loss)
I0218 17:14:54.032817 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0120636 (* 1 = 0.0120636 loss)
I0218 17:14:54.032825 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0109112 (* 1 = 0.0109112 loss)
I0218 17:14:54.032833 27028 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0218 17:15:29.854959 27028 solver.cpp:357] Iteration 57700 (2.79154 iter/s, 35.8225s/100 iters), loss = 0.0381825
I0218 17:15:29.855187 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102513 (* 1 = 0.0102513 loss)
I0218 17:15:29.855199 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00871318 (* 1 = 0.00871318 loss)
I0218 17:15:29.855208 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109383 (* 1 = 0.0109383 loss)
I0218 17:15:29.855216 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00827975 (* 1 = 0.00827975 loss)
I0218 17:15:29.855227 27028 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0218 17:16:05.687542 27028 solver.cpp:357] Iteration 57800 (2.7909 iter/s, 35.8307s/100 iters), loss = 0.0371041
I0218 17:16:05.687664 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00895739 (* 1 = 0.00895739 loss)
I0218 17:16:05.687674 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00897321 (* 1 = 0.00897321 loss)
I0218 17:16:05.687682 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0101295 (* 1 = 0.0101295 loss)
I0218 17:16:05.687690 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00904401 (* 1 = 0.00904401 loss)
I0218 17:16:05.687697 27028 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0218 17:16:41.456259 27028 solver.cpp:357] Iteration 57900 (2.79572 iter/s, 35.7689s/100 iters), loss = 0.0416535
I0218 17:16:41.456413 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0105289 (* 1 = 0.0105289 loss)
I0218 17:16:41.456423 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0100155 (* 1 = 0.0100155 loss)
I0218 17:16:41.456432 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0106437 (* 1 = 0.0106437 loss)
I0218 17:16:41.456439 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0104654 (* 1 = 0.0104654 loss)
I0218 17:16:41.456449 27028 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0218 17:17:12.364109 27028 solver.cpp:514] Iteration 58000, Testing net (#0)
I0218 17:17:13.917112 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0670554 (* 1 = 0.0670554 loss)
I0218 17:17:13.917158 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.126705 (* 1 = 0.126705 loss)
I0218 17:17:13.917166 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.142276 (* 1 = 0.142276 loss)
I0218 17:17:13.917176 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.130906 (* 1 = 0.130906 loss)
I0218 17:17:13.917181 27028 solver.cpp:580]     Test net output #4: prob1 = 0.985
I0218 17:17:13.917187 27028 solver.cpp:580]     Test net output #5: prob2 = 0.973
I0218 17:17:13.917192 27028 solver.cpp:580]     Test net output #6: prob3 = 0.964
I0218 17:17:13.917197 27028 solver.cpp:580]     Test net output #7: prob4 = 0.971
I0218 17:17:14.189018 27028 solver.cpp:357] Iteration 58000 (3.05503 iter/s, 32.7329s/100 iters), loss = 0.0434501
I0218 17:17:14.189072 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0117172 (* 1 = 0.0117172 loss)
I0218 17:17:14.189082 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0115449 (* 1 = 0.0115449 loss)
I0218 17:17:14.189091 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0100183 (* 1 = 0.0100183 loss)
I0218 17:17:14.189098 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101696 (* 1 = 0.0101696 loss)
I0218 17:17:14.189107 27028 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0218 17:17:49.141094 27028 solver.cpp:357] Iteration 58100 (2.86102 iter/s, 34.9526s/100 iters), loss = 0.0424798
I0218 17:17:49.141254 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00975945 (* 1 = 0.00975945 loss)
I0218 17:17:49.141265 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00992182 (* 1 = 0.00992182 loss)
I0218 17:17:49.141274 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.010407 (* 1 = 0.010407 loss)
I0218 17:17:49.141283 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0123916 (* 1 = 0.0123916 loss)
I0218 17:17:49.141289 27028 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0218 17:18:24.914124 27028 solver.cpp:357] Iteration 58200 (2.79517 iter/s, 35.776s/100 iters), loss = 0.0401427
I0218 17:18:24.914340 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00968775 (* 1 = 0.00968775 loss)
I0218 17:18:24.914352 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108327 (* 1 = 0.0108327 loss)
I0218 17:18:24.914361 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00964695 (* 1 = 0.00964695 loss)
I0218 17:18:24.914369 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00997535 (* 1 = 0.00997535 loss)
I0218 17:18:24.914377 27028 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0218 17:19:00.705082 27028 solver.cpp:357] Iteration 58300 (2.79379 iter/s, 35.7937s/100 iters), loss = 0.0414615
I0218 17:19:00.705202 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104529 (* 1 = 0.0104529 loss)
I0218 17:19:00.705212 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00985572 (* 1 = 0.00985572 loss)
I0218 17:19:00.705221 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112978 (* 1 = 0.0112978 loss)
I0218 17:19:00.705229 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00985516 (* 1 = 0.00985516 loss)
I0218 17:19:00.705237 27028 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0218 17:19:36.481554 27028 solver.cpp:357] Iteration 58400 (2.79492 iter/s, 35.7791s/100 iters), loss = 0.0405328
I0218 17:19:36.481667 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0086798 (* 1 = 0.0086798 loss)
I0218 17:19:36.481676 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106376 (* 1 = 0.0106376 loss)
I0218 17:19:36.481685 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0105481 (* 1 = 0.0105481 loss)
I0218 17:19:36.481693 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106673 (* 1 = 0.0106673 loss)
I0218 17:19:36.481701 27028 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0218 17:20:07.649593 27028 solver.cpp:357] Iteration 58500 (3.20841 iter/s, 31.1681s/100 iters), loss = 0.0404537
I0218 17:20:07.649746 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106184 (* 1 = 0.0106184 loss)
I0218 17:20:07.649757 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0103565 (* 1 = 0.0103565 loss)
I0218 17:20:07.649766 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104807 (* 1 = 0.0104807 loss)
I0218 17:20:07.649775 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00899811 (* 1 = 0.00899811 loss)
I0218 17:20:07.649782 27028 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0218 17:20:41.695864 27028 solver.cpp:357] Iteration 58600 (2.93698 iter/s, 34.0485s/100 iters), loss = 0.0425949
I0218 17:20:41.696043 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00995264 (* 1 = 0.00995264 loss)
I0218 17:20:41.696053 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0100807 (* 1 = 0.0100807 loss)
I0218 17:20:41.696063 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0113905 (* 1 = 0.0113905 loss)
I0218 17:20:41.696070 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.011171 (* 1 = 0.011171 loss)
I0218 17:20:41.696079 27028 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0218 17:21:17.503293 27028 solver.cpp:357] Iteration 58700 (2.79269 iter/s, 35.8077s/100 iters), loss = 0.0398438
I0218 17:21:17.503404 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00962896 (* 1 = 0.00962896 loss)
I0218 17:21:17.503414 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106222 (* 1 = 0.0106222 loss)
I0218 17:21:17.503423 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00919539 (* 1 = 0.00919539 loss)
I0218 17:21:17.503432 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103972 (* 1 = 0.0103972 loss)
I0218 17:21:17.503439 27028 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0218 17:21:53.272826 27028 solver.cpp:357] Iteration 58800 (2.7955 iter/s, 35.7718s/100 iters), loss = 0.042236
I0218 17:21:53.273041 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0102162 (* 1 = 0.0102162 loss)
I0218 17:21:53.273051 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0112116 (* 1 = 0.0112116 loss)
I0218 17:21:53.273061 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102292 (* 1 = 0.0102292 loss)
I0218 17:21:53.273068 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.010579 (* 1 = 0.010579 loss)
I0218 17:21:53.273077 27028 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0218 17:22:29.052826 27028 solver.cpp:357] Iteration 58900 (2.7947 iter/s, 35.782s/100 iters), loss = 0.0408362
I0218 17:22:29.052944 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00882516 (* 1 = 0.00882516 loss)
I0218 17:22:29.052955 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0111448 (* 1 = 0.0111448 loss)
I0218 17:22:29.052964 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104012 (* 1 = 0.0104012 loss)
I0218 17:22:29.052973 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.010465 (* 1 = 0.010465 loss)
I0218 17:22:29.052979 27028 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0218 17:23:00.728691 27028 solver.cpp:514] Iteration 59000, Testing net (#0)
I0218 17:23:02.299224 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.118209 (* 1 = 0.118209 loss)
I0218 17:23:02.299281 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.142069 (* 1 = 0.142069 loss)
I0218 17:23:02.299290 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.111816 (* 1 = 0.111816 loss)
I0218 17:23:02.299299 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.110648 (* 1 = 0.110648 loss)
I0218 17:23:02.299305 27028 solver.cpp:580]     Test net output #4: prob1 = 0.974
I0218 17:23:02.299311 27028 solver.cpp:580]     Test net output #5: prob2 = 0.964
I0218 17:23:02.299316 27028 solver.cpp:580]     Test net output #6: prob3 = 0.969
I0218 17:23:02.299322 27028 solver.cpp:580]     Test net output #7: prob4 = 0.974
I0218 17:23:02.576115 27028 solver.cpp:357] Iteration 59000 (2.98283 iter/s, 33.5252s/100 iters), loss = 0.0456701
I0218 17:23:02.576170 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00967926 (* 1 = 0.00967926 loss)
I0218 17:23:02.576180 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0116769 (* 1 = 0.0116769 loss)
I0218 17:23:02.576189 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115955 (* 1 = 0.0115955 loss)
I0218 17:23:02.576197 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0127184 (* 1 = 0.0127184 loss)
I0218 17:23:02.576205 27028 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0218 17:23:36.559703 27028 solver.cpp:357] Iteration 59100 (2.94243 iter/s, 33.9855s/100 iters), loss = 0.0407984
I0218 17:23:36.559866 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00985962 (* 1 = 0.00985962 loss)
I0218 17:23:36.559877 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00940729 (* 1 = 0.00940729 loss)
I0218 17:23:36.559885 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102149 (* 1 = 0.0102149 loss)
I0218 17:23:36.559893 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0113166 (* 1 = 0.0113166 loss)
I0218 17:23:36.559901 27028 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0218 17:24:12.320852 27028 solver.cpp:357] Iteration 59200 (2.79619 iter/s, 35.763s/100 iters), loss = 0.0444889
I0218 17:24:12.321056 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0113289 (* 1 = 0.0113289 loss)
I0218 17:24:12.321082 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0123732 (* 1 = 0.0123732 loss)
I0218 17:24:12.321105 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0105484 (* 1 = 0.0105484 loss)
I0218 17:24:12.321127 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102383 (* 1 = 0.0102383 loss)
I0218 17:24:12.321148 27028 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0218 17:24:48.072281 27028 solver.cpp:357] Iteration 59300 (2.79696 iter/s, 35.7531s/100 iters), loss = 0.0413261
I0218 17:24:48.072446 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0115085 (* 1 = 0.0115085 loss)
I0218 17:24:48.072458 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00931813 (* 1 = 0.00931813 loss)
I0218 17:24:48.072466 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104028 (* 1 = 0.0104028 loss)
I0218 17:24:48.072474 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100966 (* 1 = 0.0100966 loss)
I0218 17:24:48.072481 27028 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0218 17:25:23.856950 27028 solver.cpp:357] Iteration 59400 (2.79436 iter/s, 35.7863s/100 iters), loss = 0.0402807
I0218 17:25:23.857080 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0089199 (* 1 = 0.0089199 loss)
I0218 17:25:23.857090 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0088056 (* 1 = 0.0088056 loss)
I0218 17:25:23.857100 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0135019 (* 1 = 0.0135019 loss)
I0218 17:25:23.857107 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00905329 (* 1 = 0.00905329 loss)
I0218 17:25:23.857115 27028 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0218 17:25:55.921690 27028 solver.cpp:357] Iteration 59500 (3.11855 iter/s, 32.0662s/100 iters), loss = 0.0434461
I0218 17:25:55.921816 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0100108 (* 1 = 0.0100108 loss)
I0218 17:25:55.921828 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0102582 (* 1 = 0.0102582 loss)
I0218 17:25:55.921836 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0134835 (* 1 = 0.0134835 loss)
I0218 17:25:55.921844 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00969356 (* 1 = 0.00969356 loss)
I0218 17:25:55.921854 27028 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0218 17:26:28.980758 27028 solver.cpp:357] Iteration 59600 (3.02494 iter/s, 33.0585s/100 iters), loss = 0.0434348
I0218 17:26:28.980916 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010123 (* 1 = 0.010123 loss)
I0218 17:26:28.980927 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0120617 (* 1 = 0.0120617 loss)
I0218 17:26:28.980937 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109057 (* 1 = 0.0109057 loss)
I0218 17:26:28.980944 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103444 (* 1 = 0.0103444 loss)
I0218 17:26:28.980957 27028 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0218 17:27:04.821696 27028 solver.cpp:357] Iteration 59700 (2.79014 iter/s, 35.8405s/100 iters), loss = 0.0442285
I0218 17:27:04.821801 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00955906 (* 1 = 0.00955906 loss)
I0218 17:27:04.821812 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.01085 (* 1 = 0.01085 loss)
I0218 17:27:04.821821 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.011094 (* 1 = 0.011094 loss)
I0218 17:27:04.821830 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0127254 (* 1 = 0.0127254 loss)
I0218 17:27:04.821836 27028 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0218 17:27:40.583029 27028 solver.cpp:357] Iteration 59800 (2.7962 iter/s, 35.7628s/100 iters), loss = 0.0389746
I0218 17:27:40.583142 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00815816 (* 1 = 0.00815816 loss)
I0218 17:27:40.583153 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00932933 (* 1 = 0.00932933 loss)
I0218 17:27:40.583161 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.01101 (* 1 = 0.01101 loss)
I0218 17:27:40.583169 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0104771 (* 1 = 0.0104771 loss)
I0218 17:27:40.583178 27028 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0218 17:28:16.356739 27028 solver.cpp:357] Iteration 59900 (2.79524 iter/s, 35.7752s/100 iters), loss = 0.0448731
I0218 17:28:16.356910 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0121173 (* 1 = 0.0121173 loss)
I0218 17:28:16.356920 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.010226 (* 1 = 0.010226 loss)
I0218 17:28:16.356930 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.010688 (* 1 = 0.010688 loss)
I0218 17:28:16.356937 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0118418 (* 1 = 0.0118418 loss)
I0218 17:28:16.356945 27028 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0218 17:28:49.069408 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.caffemodel
I0218 17:28:49.078234 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_60000.solverstate
I0218 17:28:49.081312 27028 solver.cpp:514] Iteration 60000, Testing net (#0)
I0218 17:28:50.645054 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0726714 (* 1 = 0.0726714 loss)
I0218 17:28:50.645104 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.109073 (* 1 = 0.109073 loss)
I0218 17:28:50.645112 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0971444 (* 1 = 0.0971444 loss)
I0218 17:28:50.645120 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.115853 (* 1 = 0.115853 loss)
I0218 17:28:50.645126 27028 solver.cpp:580]     Test net output #4: prob1 = 0.981
I0218 17:28:50.645133 27028 solver.cpp:580]     Test net output #5: prob2 = 0.969
I0218 17:28:50.645138 27028 solver.cpp:580]     Test net output #6: prob3 = 0.975
I0218 17:28:50.645143 27028 solver.cpp:580]     Test net output #7: prob4 = 0.967
I0218 17:28:50.910850 27028 solver.cpp:357] Iteration 60000 (2.8939 iter/s, 34.5554s/100 iters), loss = 0.0415355
I0218 17:28:50.910903 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00933211 (* 1 = 0.00933211 loss)
I0218 17:28:50.910912 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108867 (* 1 = 0.0108867 loss)
I0218 17:28:50.910921 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0105109 (* 1 = 0.0105109 loss)
I0218 17:28:50.910929 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0108059 (* 1 = 0.0108059 loss)
I0218 17:28:50.910938 27028 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0218 17:29:24.000834 27028 solver.cpp:357] Iteration 60100 (3.02194 iter/s, 33.0913s/100 iters), loss = 0.0404722
I0218 17:29:24.001004 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00946096 (* 1 = 0.00946096 loss)
I0218 17:29:24.001015 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00966162 (* 1 = 0.00966162 loss)
I0218 17:29:24.001024 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0119811 (* 1 = 0.0119811 loss)
I0218 17:29:24.001032 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00936849 (* 1 = 0.00936849 loss)
I0218 17:29:24.001039 27028 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0218 17:29:59.771046 27028 solver.cpp:357] Iteration 60200 (2.79552 iter/s, 35.7715s/100 iters), loss = 0.0432736
I0218 17:29:59.771167 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00912529 (* 1 = 0.00912529 loss)
I0218 17:29:59.771178 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0125679 (* 1 = 0.0125679 loss)
I0218 17:29:59.771186 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0114964 (* 1 = 0.0114964 loss)
I0218 17:29:59.771194 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100839 (* 1 = 0.0100839 loss)
I0218 17:29:59.771201 27028 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0218 17:30:35.534749 27028 solver.cpp:357] Iteration 60300 (2.79603 iter/s, 35.765s/100 iters), loss = 0.0383906
I0218 17:30:35.534865 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010428 (* 1 = 0.010428 loss)
I0218 17:30:35.534876 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00907621 (* 1 = 0.00907621 loss)
I0218 17:30:35.534885 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00979226 (* 1 = 0.00979226 loss)
I0218 17:30:35.534893 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00909411 (* 1 = 0.00909411 loss)
I0218 17:30:35.534901 27028 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0218 17:31:11.305613 27028 solver.cpp:357] Iteration 60400 (2.79547 iter/s, 35.7721s/100 iters), loss = 0.0436904
I0218 17:31:11.305785 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.010289 (* 1 = 0.010289 loss)
I0218 17:31:11.305796 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0116404 (* 1 = 0.0116404 loss)
I0218 17:31:11.305805 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00943144 (* 1 = 0.00943144 loss)
I0218 17:31:11.305814 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0123295 (* 1 = 0.0123295 loss)
I0218 17:31:11.305863 27028 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0218 17:31:44.251919 27028 solver.cpp:357] Iteration 60500 (3.03515 iter/s, 32.9473s/100 iters), loss = 0.0420875
I0218 17:31:44.252072 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101283 (* 1 = 0.0101283 loss)
I0218 17:31:44.252084 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0113554 (* 1 = 0.0113554 loss)
I0218 17:31:44.252092 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00981718 (* 1 = 0.00981718 loss)
I0218 17:31:44.252100 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0107866 (* 1 = 0.0107866 loss)
I0218 17:31:44.252111 27028 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0218 17:32:16.392179 27028 solver.cpp:357] Iteration 60600 (3.11127 iter/s, 32.1413s/100 iters), loss = 0.0429792
I0218 17:32:16.392354 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0117337 (* 1 = 0.0117337 loss)
I0218 17:32:16.392365 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00905483 (* 1 = 0.00905483 loss)
I0218 17:32:16.392374 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0106682 (* 1 = 0.0106682 loss)
I0218 17:32:16.392382 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0115225 (* 1 = 0.0115225 loss)
I0218 17:32:16.392390 27028 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0218 17:32:52.171613 27028 solver.cpp:357] Iteration 60700 (2.79497 iter/s, 35.7785s/100 iters), loss = 0.0372279
I0218 17:32:52.171730 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00928385 (* 1 = 0.00928385 loss)
I0218 17:32:52.171739 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00870484 (* 1 = 0.00870484 loss)
I0218 17:32:52.171748 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00998458 (* 1 = 0.00998458 loss)
I0218 17:32:52.171756 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00925467 (* 1 = 0.00925467 loss)
I0218 17:32:52.171763 27028 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0218 17:33:27.947860 27028 solver.cpp:357] Iteration 60800 (2.79506 iter/s, 35.7774s/100 iters), loss = 0.0409214
I0218 17:33:27.947978 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0100548 (* 1 = 0.0100548 loss)
I0218 17:33:27.947988 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0103572 (* 1 = 0.0103572 loss)
I0218 17:33:27.947996 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104366 (* 1 = 0.0104366 loss)
I0218 17:33:27.948004 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100727 (* 1 = 0.0100727 loss)
I0218 17:33:27.948012 27028 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0218 17:34:03.703120 27028 solver.cpp:357] Iteration 60900 (2.79671 iter/s, 35.7563s/100 iters), loss = 0.0412023
I0218 17:34:03.703238 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00999164 (* 1 = 0.00999164 loss)
I0218 17:34:03.703248 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0103008 (* 1 = 0.0103008 loss)
I0218 17:34:03.703258 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0107191 (* 1 = 0.0107191 loss)
I0218 17:34:03.703265 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101908 (* 1 = 0.0101908 loss)
I0218 17:34:03.703272 27028 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0218 17:34:37.386528 27028 solver.cpp:514] Iteration 61000, Testing net (#0)
I0218 17:34:38.931913 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0916118 (* 1 = 0.0916118 loss)
I0218 17:34:38.931958 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.0691979 (* 1 = 0.0691979 loss)
I0218 17:34:38.931967 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.114918 (* 1 = 0.114918 loss)
I0218 17:34:38.931975 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0988467 (* 1 = 0.0988467 loss)
I0218 17:34:38.931982 27028 solver.cpp:580]     Test net output #4: prob1 = 0.975
I0218 17:34:38.931988 27028 solver.cpp:580]     Test net output #5: prob2 = 0.981
I0218 17:34:38.931993 27028 solver.cpp:580]     Test net output #6: prob3 = 0.963
I0218 17:34:38.931998 27028 solver.cpp:580]     Test net output #7: prob4 = 0.971
I0218 17:34:39.201778 27028 solver.cpp:357] Iteration 61000 (2.81692 iter/s, 35.4997s/100 iters), loss = 0.0437907
I0218 17:34:39.201835 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0126254 (* 1 = 0.0126254 loss)
I0218 17:34:39.201845 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0113769 (* 1 = 0.0113769 loss)
I0218 17:34:39.201854 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00902763 (* 1 = 0.00902763 loss)
I0218 17:34:39.201862 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0107607 (* 1 = 0.0107607 loss)
I0218 17:34:39.201871 27028 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0218 17:35:11.250998 27028 solver.cpp:357] Iteration 61100 (3.12031 iter/s, 32.0481s/100 iters), loss = 0.0439902
I0218 17:35:11.251176 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0105696 (* 1 = 0.0105696 loss)
I0218 17:35:11.251188 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00955281 (* 1 = 0.00955281 loss)
I0218 17:35:11.251196 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0136543 (* 1 = 0.0136543 loss)
I0218 17:35:11.251204 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102134 (* 1 = 0.0102134 loss)
I0218 17:35:11.251211 27028 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0218 17:35:47.054000 27028 solver.cpp:357] Iteration 61200 (2.79299 iter/s, 35.804s/100 iters), loss = 0.0443048
I0218 17:35:47.054113 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00965145 (* 1 = 0.00965145 loss)
I0218 17:35:47.054124 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106262 (* 1 = 0.0106262 loss)
I0218 17:35:47.054133 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0123162 (* 1 = 0.0123162 loss)
I0218 17:35:47.054141 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0117109 (* 1 = 0.0117109 loss)
I0218 17:35:47.054148 27028 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0218 17:36:22.836998 27028 solver.cpp:357] Iteration 61300 (2.79455 iter/s, 35.784s/100 iters), loss = 0.0400927
I0218 17:36:22.837124 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107246 (* 1 = 0.0107246 loss)
I0218 17:36:22.837136 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00989332 (* 1 = 0.00989332 loss)
I0218 17:36:22.837144 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00944486 (* 1 = 0.00944486 loss)
I0218 17:36:22.837152 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100299 (* 1 = 0.0100299 loss)
I0218 17:36:22.837160 27028 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0218 17:36:58.591543 27028 solver.cpp:357] Iteration 61400 (2.79693 iter/s, 35.7535s/100 iters), loss = 0.0423503
I0218 17:36:58.591667 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107509 (* 1 = 0.0107509 loss)
I0218 17:36:58.591677 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104825 (* 1 = 0.0104825 loss)
I0218 17:36:58.591686 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109992 (* 1 = 0.0109992 loss)
I0218 17:36:58.591694 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101177 (* 1 = 0.0101177 loss)
I0218 17:36:58.591702 27028 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0218 17:37:32.608796 27028 solver.cpp:357] Iteration 61500 (2.93978 iter/s, 34.0161s/100 iters), loss = 0.0408834
I0218 17:37:32.608959 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00869965 (* 1 = 0.00869965 loss)
I0218 17:37:32.608969 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011101 (* 1 = 0.011101 loss)
I0218 17:37:32.608978 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115012 (* 1 = 0.0115012 loss)
I0218 17:37:32.608986 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00958152 (* 1 = 0.00958152 loss)
I0218 17:37:32.608994 27028 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0218 17:38:03.806887 27028 solver.cpp:357] Iteration 61600 (3.20545 iter/s, 31.1969s/100 iters), loss = 0.0411177
I0218 17:38:03.807219 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00944391 (* 1 = 0.00944391 loss)
I0218 17:38:03.807231 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106369 (* 1 = 0.0106369 loss)
I0218 17:38:03.807240 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0113507 (* 1 = 0.0113507 loss)
I0218 17:38:03.807247 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00968613 (* 1 = 0.00968613 loss)
I0218 17:38:03.807255 27028 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0218 17:38:39.586529 27028 solver.cpp:357] Iteration 61700 (2.79483 iter/s, 35.7804s/100 iters), loss = 0.0413782
I0218 17:38:39.586694 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0101694 (* 1 = 0.0101694 loss)
I0218 17:38:39.586704 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.011006 (* 1 = 0.011006 loss)
I0218 17:38:39.586712 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.01106 (* 1 = 0.01106 loss)
I0218 17:38:39.586721 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00914291 (* 1 = 0.00914291 loss)
I0218 17:38:39.586728 27028 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0218 17:39:15.358989 27028 solver.cpp:357] Iteration 61800 (2.79538 iter/s, 35.7733s/100 iters), loss = 0.0404576
I0218 17:39:15.359181 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00976592 (* 1 = 0.00976592 loss)
I0218 17:39:15.359194 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00965857 (* 1 = 0.00965857 loss)
I0218 17:39:15.359208 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0108916 (* 1 = 0.0108916 loss)
I0218 17:39:15.359216 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101416 (* 1 = 0.0101416 loss)
I0218 17:39:15.359225 27028 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0218 17:39:51.216424 27028 solver.cpp:357] Iteration 61900 (2.78876 iter/s, 35.8583s/100 iters), loss = 0.0410737
I0218 17:39:51.216711 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0096318 (* 1 = 0.0096318 loss)
I0218 17:39:51.216722 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108047 (* 1 = 0.0108047 loss)
I0218 17:39:51.216732 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0112261 (* 1 = 0.0112261 loss)
I0218 17:39:51.216739 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0094111 (* 1 = 0.0094111 loss)
I0218 17:39:51.216747 27028 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0218 17:40:25.711323 27028 solver.cpp:514] Iteration 62000, Testing net (#0)
I0218 17:40:27.286187 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.0835697 (* 1 = 0.0835697 loss)
I0218 17:40:27.286237 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.0727298 (* 1 = 0.0727298 loss)
I0218 17:40:27.286247 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0849485 (* 1 = 0.0849485 loss)
I0218 17:40:27.286254 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0686786 (* 1 = 0.0686786 loss)
I0218 17:40:27.286264 27028 solver.cpp:580]     Test net output #4: prob1 = 0.979
I0218 17:40:27.286270 27028 solver.cpp:580]     Test net output #5: prob2 = 0.982
I0218 17:40:27.286275 27028 solver.cpp:580]     Test net output #6: prob3 = 0.982
I0218 17:40:27.286281 27028 solver.cpp:580]     Test net output #7: prob4 = 0.981
I0218 17:40:27.550681 27028 solver.cpp:357] Iteration 62000 (2.75231 iter/s, 36.3331s/100 iters), loss = 0.0410609
I0218 17:40:27.550738 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00986366 (* 1 = 0.00986366 loss)
I0218 17:40:27.550748 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00953632 (* 1 = 0.00953632 loss)
I0218 17:40:27.550757 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0116151 (* 1 = 0.0116151 loss)
I0218 17:40:27.550765 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0100459 (* 1 = 0.0100459 loss)
I0218 17:40:27.550773 27028 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0218 17:40:58.523805 27028 solver.cpp:357] Iteration 62100 (3.22852 iter/s, 30.9739s/100 iters), loss = 0.0439647
I0218 17:40:58.524044 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00912758 (* 1 = 0.00912758 loss)
I0218 17:40:58.524056 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0107897 (* 1 = 0.0107897 loss)
I0218 17:40:58.524065 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115128 (* 1 = 0.0115128 loss)
I0218 17:40:58.524073 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0125346 (* 1 = 0.0125346 loss)
I0218 17:40:58.524081 27028 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0218 17:41:34.261296 27028 solver.cpp:357] Iteration 62200 (2.79827 iter/s, 35.7363s/100 iters), loss = 0.043553
I0218 17:41:34.261430 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0093678 (* 1 = 0.0093678 loss)
I0218 17:41:34.261440 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0116667 (* 1 = 0.0116667 loss)
I0218 17:41:34.261448 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0115724 (* 1 = 0.0115724 loss)
I0218 17:41:34.261456 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0109461 (* 1 = 0.0109461 loss)
I0218 17:41:34.261464 27028 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0218 17:42:10.029475 27028 solver.cpp:357] Iteration 62300 (2.79571 iter/s, 35.769s/100 iters), loss = 0.0407791
I0218 17:42:10.029637 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00929517 (* 1 = 0.00929517 loss)
I0218 17:42:10.029647 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0118074 (* 1 = 0.0118074 loss)
I0218 17:42:10.029656 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0101458 (* 1 = 0.0101458 loss)
I0218 17:42:10.029664 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0095307 (* 1 = 0.0095307 loss)
I0218 17:42:10.029671 27028 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0218 17:42:45.828616 27028 solver.cpp:357] Iteration 62400 (2.7933 iter/s, 35.8s/100 iters), loss = 0.0404988
I0218 17:42:45.828770 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0103263 (* 1 = 0.0103263 loss)
I0218 17:42:45.828781 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0101323 (* 1 = 0.0101323 loss)
I0218 17:42:45.828790 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00883555 (* 1 = 0.00883555 loss)
I0218 17:42:45.828799 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0112046 (* 1 = 0.0112046 loss)
I0218 17:42:45.828805 27028 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0218 17:43:20.883239 27028 solver.cpp:357] Iteration 62500 (2.85265 iter/s, 35.0552s/100 iters), loss = 0.0405321
I0218 17:43:20.883358 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0115498 (* 1 = 0.0115498 loss)
I0218 17:43:20.883369 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00994173 (* 1 = 0.00994173 loss)
I0218 17:43:20.883378 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00910711 (* 1 = 0.00910711 loss)
I0218 17:43:20.883386 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00993341 (* 1 = 0.00993341 loss)
I0218 17:43:20.883394 27028 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0218 17:43:50.786304 27028 solver.cpp:357] Iteration 62600 (3.34406 iter/s, 29.9037s/100 iters), loss = 0.0433688
I0218 17:43:50.786382 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0106725 (* 1 = 0.0106725 loss)
I0218 17:43:50.786392 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0114811 (* 1 = 0.0114811 loss)
I0218 17:43:50.786401 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0114684 (* 1 = 0.0114684 loss)
I0218 17:43:50.786411 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00974685 (* 1 = 0.00974685 loss)
I0218 17:43:50.786417 27028 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0218 17:44:26.614490 27028 solver.cpp:357] Iteration 62700 (2.79119 iter/s, 35.827s/100 iters), loss = 0.0405507
I0218 17:44:26.614730 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0100027 (* 1 = 0.0100027 loss)
I0218 17:44:26.614742 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00994969 (* 1 = 0.00994969 loss)
I0218 17:44:26.614750 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0109411 (* 1 = 0.0109411 loss)
I0218 17:44:26.614758 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00965721 (* 1 = 0.00965721 loss)
I0218 17:44:26.614766 27028 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0218 17:45:02.416335 27028 solver.cpp:357] Iteration 62800 (2.79325 iter/s, 35.8006s/100 iters), loss = 0.04122
I0218 17:45:02.416448 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00933077 (* 1 = 0.00933077 loss)
I0218 17:45:02.416460 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104559 (* 1 = 0.0104559 loss)
I0218 17:45:02.416468 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0107749 (* 1 = 0.0107749 loss)
I0218 17:45:02.416476 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106585 (* 1 = 0.0106585 loss)
I0218 17:45:02.416484 27028 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0218 17:45:38.178444 27028 solver.cpp:357] Iteration 62900 (2.79619 iter/s, 35.7629s/100 iters), loss = 0.0411063
I0218 17:45:38.178562 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0104101 (* 1 = 0.0104101 loss)
I0218 17:45:38.178573 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0101659 (* 1 = 0.0101659 loss)
I0218 17:45:38.178582 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0101824 (* 1 = 0.0101824 loss)
I0218 17:45:38.178589 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0103479 (* 1 = 0.0103479 loss)
I0218 17:45:38.178596 27028 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0218 17:46:13.599856 27028 solver.cpp:514] Iteration 63000, Testing net (#0)
I0218 17:46:15.437016 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.137606 (* 1 = 0.137606 loss)
I0218 17:46:15.437064 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.119865 (* 1 = 0.119865 loss)
I0218 17:46:15.437073 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.146836 (* 1 = 0.146836 loss)
I0218 17:46:15.437083 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0794307 (* 1 = 0.0794307 loss)
I0218 17:46:15.437088 27028 solver.cpp:580]     Test net output #4: prob1 = 0.965
I0218 17:46:15.437094 27028 solver.cpp:580]     Test net output #5: prob2 = 0.974
I0218 17:46:15.437100 27028 solver.cpp:580]     Test net output #6: prob3 = 0.967
I0218 17:46:15.437105 27028 solver.cpp:580]     Test net output #7: prob4 = 0.981
I0218 17:46:15.715240 27028 solver.cpp:357] Iteration 63000 (2.66399 iter/s, 37.5377s/100 iters), loss = 0.0399744
I0218 17:46:15.715296 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00943011 (* 1 = 0.00943011 loss)
I0218 17:46:15.715306 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0108395 (* 1 = 0.0108395 loss)
I0218 17:46:15.715314 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0102073 (* 1 = 0.0102073 loss)
I0218 17:46:15.715322 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0094975 (* 1 = 0.0094975 loss)
I0218 17:46:15.715332 27028 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0218 17:46:45.371906 27028 solver.cpp:357] Iteration 63100 (3.37184 iter/s, 29.6574s/100 iters), loss = 0.0407035
I0218 17:46:45.372059 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.00883264 (* 1 = 0.00883264 loss)
I0218 17:46:45.372071 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00910103 (* 1 = 0.00910103 loss)
I0218 17:46:45.372079 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0113003 (* 1 = 0.0113003 loss)
I0218 17:46:45.372087 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0114695 (* 1 = 0.0114695 loss)
I0218 17:46:45.372094 27028 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0218 17:47:21.157305 27028 solver.cpp:357] Iteration 63200 (2.79438 iter/s, 35.7862s/100 iters), loss = 0.0429858
I0218 17:47:21.157598 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0116646 (* 1 = 0.0116646 loss)
I0218 17:47:21.157609 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0100146 (* 1 = 0.0100146 loss)
I0218 17:47:21.157618 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00994566 (* 1 = 0.00994566 loss)
I0218 17:47:21.157626 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0113609 (* 1 = 0.0113609 loss)
I0218 17:47:21.157634 27028 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0218 17:47:56.979928 27028 solver.cpp:357] Iteration 63300 (2.79148 iter/s, 35.8232s/100 iters), loss = 0.0491119
I0218 17:47:56.980247 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0148317 (* 1 = 0.0148317 loss)
I0218 17:47:56.980258 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0104029 (* 1 = 0.0104029 loss)
I0218 17:47:56.980268 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0132718 (* 1 = 0.0132718 loss)
I0218 17:47:56.980275 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0106055 (* 1 = 0.0106055 loss)
I0218 17:47:56.980283 27028 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0218 17:48:32.808998 27028 solver.cpp:357] Iteration 63400 (2.79112 iter/s, 35.8279s/100 iters), loss = 0.0417436
I0218 17:48:32.809149 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109534 (* 1 = 0.0109534 loss)
I0218 17:48:32.809159 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0102008 (* 1 = 0.0102008 loss)
I0218 17:48:32.809167 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0104697 (* 1 = 0.0104697 loss)
I0218 17:48:32.809175 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0101197 (* 1 = 0.0101197 loss)
I0218 17:48:32.809182 27028 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0218 17:49:08.577831 27028 solver.cpp:357] Iteration 63500 (2.79567 iter/s, 35.7696s/100 iters), loss = 0.0442616
I0218 17:49:08.578013 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0120701 (* 1 = 0.0120701 loss)
I0218 17:49:08.578024 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0123299 (* 1 = 0.0123299 loss)
I0218 17:49:08.578033 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00965567 (* 1 = 0.00965567 loss)
I0218 17:49:08.578042 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0102059 (* 1 = 0.0102059 loss)
I0218 17:49:08.578048 27028 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0218 17:49:37.540556 27028 solver.cpp:357] Iteration 63600 (3.45265 iter/s, 28.9633s/100 iters), loss = 0.0427819
I0218 17:49:37.540629 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0099678 (* 1 = 0.0099678 loss)
I0218 17:49:37.540639 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0106164 (* 1 = 0.0106164 loss)
I0218 17:49:37.540648 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0122184 (* 1 = 0.0122184 loss)
I0218 17:49:37.540657 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00997916 (* 1 = 0.00997916 loss)
I0218 17:49:37.540664 27028 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0218 17:50:13.313503 27028 solver.cpp:357] Iteration 63700 (2.79534 iter/s, 35.7738s/100 iters), loss = 0.0416383
I0218 17:50:13.313668 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0107537 (* 1 = 0.0107537 loss)
I0218 17:50:13.313678 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00997659 (* 1 = 0.00997659 loss)
I0218 17:50:13.313688 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0110862 (* 1 = 0.0110862 loss)
I0218 17:50:13.313695 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00982176 (* 1 = 0.00982176 loss)
I0218 17:50:13.313702 27028 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0218 17:50:49.078040 27028 solver.cpp:357] Iteration 63800 (2.79601 iter/s, 35.7653s/100 iters), loss = 0.039884
I0218 17:50:49.078260 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0109611 (* 1 = 0.0109611 loss)
I0218 17:50:49.078272 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.00998367 (* 1 = 0.00998367 loss)
I0218 17:50:49.078281 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.0100283 (* 1 = 0.0100283 loss)
I0218 17:50:49.078289 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.00891096 (* 1 = 0.00891096 loss)
I0218 17:50:49.078296 27028 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0218 17:51:24.903832 27028 solver.cpp:357] Iteration 63900 (2.79123 iter/s, 35.8265s/100 iters), loss = 0.0458818
I0218 17:51:24.903964 27028 solver.cpp:376]     Train net output #0: Softmax1 = 0.0124443 (* 1 = 0.0124443 loss)
I0218 17:51:24.903975 27028 solver.cpp:376]     Train net output #1: Softmax2 = 0.0113426 (* 1 = 0.0113426 loss)
I0218 17:51:24.903983 27028 solver.cpp:376]     Train net output #2: Softmax3 = 0.00999994 (* 1 = 0.00999994 loss)
I0218 17:51:24.903991 27028 solver.cpp:376]     Train net output #3: Softmax4 = 0.0120949 (* 1 = 0.0120949 loss)
I0218 17:51:24.904000 27028 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0218 17:52:00.399802 27028 solver.cpp:684] Snapshotting to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.caffemodel
I0218 17:52:00.409790 27028 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_20_iter_64000.solverstate
I0218 17:52:00.587167 27028 solver.cpp:472] Iteration 64000, loss = 0.0417757
I0218 17:52:00.587224 27028 solver.cpp:514] Iteration 64000, Testing net (#0)
I0218 17:52:03.063390 27028 solver.cpp:580]     Test net output #0: Softmax1 = 0.114066 (* 1 = 0.114066 loss)
I0218 17:52:03.063436 27028 solver.cpp:580]     Test net output #1: Softmax2 = 0.101126 (* 1 = 0.101126 loss)
I0218 17:52:03.063446 27028 solver.cpp:580]     Test net output #2: Softmax3 = 0.0891142 (* 1 = 0.0891142 loss)
I0218 17:52:03.063457 27028 solver.cpp:580]     Test net output #3: Softmax4 = 0.0849787 (* 1 = 0.0849787 loss)
I0218 17:52:03.063462 27028 solver.cpp:580]     Test net output #4: prob1 = 0.975
I0218 17:52:03.063468 27028 solver.cpp:580]     Test net output #5: prob2 = 0.973
I0218 17:52:03.063473 27028 solver.cpp:580]     Test net output #6: prob3 = 0.982
I0218 17:52:03.063478 27028 solver.cpp:580]     Test net output #7: prob4 = 0.978
I0218 17:52:03.063484 27028 solver.cpp:479] Optimization Done.
I0218 17:52:03.063488 27028 caffe.cpp:326] Optimization Done.
