WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 11:10:28.399971  1691 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0825 11:10:28.400110  1691 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0825 11:10:28.400116  1691 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0825 11:10:28.400120  1691 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0825 11:10:28.400123  1691 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0825 11:10:28.400127  1691 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0825 11:10:28.400187  1691 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0825 11:10:28.400380  1691 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0825 11:10:28.417222  1691 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0825 11:10:28.417345  1691 caffe.cpp:269] Using GPUs 0
I0825 11:10:28.429731  1691 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0825 11:10:29.080397  1691 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0825 11:10:29.080444  1691 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0825 11:10:29.173516  1691 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_32.prototxt"
test_net: "./test_ResNet_32.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_32"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
iter_size: 2
type: "Nesterov"
I0825 11:10:29.173761  1691 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_32.prototxt
I0825 11:10:29.174991  1691 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_32.prototxt
I0825 11:10:29.175014  1691 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:10:29.175282  1691 net.cpp:390] layer_param.include_size():1
I0825 11:10:29.175289  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175297  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175302  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175307  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175310  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175314  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175318  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175323  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175325  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175329  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175333  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175338  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175341  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175346  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175349  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175354  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175357  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175361  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175365  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175369  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175374  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175377  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175381  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175385  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175388  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175392  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175396  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175400  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175405  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175408  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175412  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175416  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175420  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175423  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175427  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175431  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175434  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175439  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175470  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175474  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175478  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175482  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175487  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175490  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175494  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175498  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175501  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175506  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175510  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175513  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175518  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175521  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175525  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175529  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175532  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175537  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175541  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175545  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175549  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175552  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175556  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175560  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175565  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175568  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175571  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175575  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175580  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175583  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175587  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175591  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175595  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175599  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175602  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175607  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175611  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175614  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175618  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175622  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175626  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175631  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175633  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175637  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175642  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175645  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175649  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175653  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175657  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175660  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175664  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175668  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175673  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175676  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175680  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175684  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175688  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175691  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175695  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175706  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175710  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175714  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175717  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175722  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175725  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175729  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175734  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175737  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175741  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175745  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175750  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175753  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175756  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175760  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175765  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175768  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175772  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175776  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175779  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175783  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175787  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175791  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175796  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175799  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175803  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175806  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175810  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175814  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175818  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175822  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175838  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175843  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175846  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175850  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175854  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175858  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175861  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175865  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175869  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175873  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175878  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175881  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175884  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175889  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175892  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175896  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175900  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175904  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175907  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175911  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175915  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175920  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175922  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175926  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175930  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175935  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175938  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175948  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175952  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175956  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175959  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175963  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175967  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175971  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175974  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175978  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175982  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175987  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175990  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.175994  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.175997  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176002  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176005  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176009  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176012  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176017  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176020  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176024  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176028  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176031  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176035  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176039  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176043  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176048  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176050  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176054  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176059  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176062  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176066  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176070  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176074  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176077  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176081  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176085  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176090  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176093  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176096  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176100  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176105  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176108  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176112  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176116  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176120  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176123  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176127  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176131  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176136  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176139  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176142  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176146  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176151  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176154  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176158  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176162  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176167  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176170  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176180  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176185  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176188  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176192  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176196  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176200  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176204  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176208  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176213  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176216  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176219  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176223  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176228  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176231  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176235  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176239  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176244  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176247  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176250  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176254  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176259  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176262  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176266  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176270  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176275  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176278  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176282  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176286  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176290  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176293  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176297  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176301  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176306  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176309  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176312  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176316  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176321  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176324  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176328  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176333  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176337  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176340  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176344  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176349  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176352  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176357  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176360  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176364  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176368  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176373  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176376  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176380  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176384  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176388  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176393  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176396  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176399  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176403  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176414  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176419  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176421  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176426  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176429  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176434  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176437  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176441  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176445  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176448  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176452  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176456  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176460  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176465  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176468  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176472  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176476  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176479  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176483  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176487  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176491  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176494  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176498  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.176502  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:29.176506  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:29.177510  1691 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 64
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution24"
  top: "Convolution24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Convolution24"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution25"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution26"
  top: "Convolution26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Convolution26"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution27"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution28"
  top: "Convolution28"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Convolution28"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution29"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution30"
  top: "Convolution30"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Convolution30"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution31"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_par
I0825 11:10:29.178162  1691 layer_factory.hpp:77] Creating layer Data1
I0825 11:10:29.178361  1691 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0825 11:10:29.178426  1691 net.cpp:128] Creating Layer Data1
I0825 11:10:29.178438  1691 net.cpp:522] Data1 -> Data1
I0825 11:10:29.178472  1691 net.cpp:522] Data1 -> Data2
I0825 11:10:29.180395  1691 data_layer.cpp:45] output data size: 64,3,32,32
I0825 11:10:29.191895  1691 net.cpp:172] Setting up Data1
I0825 11:10:29.191941  1691 net.cpp:186] Top shape: 64 3 32 32 (196608)
I0825 11:10:29.191949  1691 net.cpp:186] Top shape: 64 (64)
I0825 11:10:29.191953  1691 net.cpp:194] Memory required for data: 786688
I0825 11:10:29.191969  1691 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:10:29.192008  1691 net.cpp:128] Creating Layer Convolution1
I0825 11:10:29.192018  1691 net.cpp:558] Convolution1 <- Data1
I0825 11:10:29.192039  1691 net.cpp:522] Convolution1 -> Convolution1
I0825 11:10:30.213219  1691 net.cpp:172] Setting up Convolution1
I0825 11:10:30.213359  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.213367  1691 net.cpp:194] Memory required for data: 4980992
I0825 11:10:30.213418  1691 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:10:30.213443  1691 net.cpp:128] Creating Layer BatchNorm1
I0825 11:10:30.213449  1691 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:10:30.213459  1691 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:10:30.213706  1691 net.cpp:172] Setting up BatchNorm1
I0825 11:10:30.213714  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.213718  1691 net.cpp:194] Memory required for data: 9175296
I0825 11:10:30.213733  1691 layer_factory.hpp:77] Creating layer Scale1
I0825 11:10:30.213745  1691 net.cpp:128] Creating Layer Scale1
I0825 11:10:30.213749  1691 net.cpp:558] Scale1 <- Convolution1
I0825 11:10:30.213755  1691 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:10:30.213807  1691 layer_factory.hpp:77] Creating layer Scale1
I0825 11:10:30.213939  1691 net.cpp:172] Setting up Scale1
I0825 11:10:30.213948  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.213951  1691 net.cpp:194] Memory required for data: 13369600
I0825 11:10:30.213959  1691 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:10:30.213970  1691 net.cpp:128] Creating Layer ReLU1
I0825 11:10:30.213975  1691 net.cpp:558] ReLU1 <- Convolution1
I0825 11:10:30.213981  1691 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:10:30.214946  1691 net.cpp:172] Setting up ReLU1
I0825 11:10:30.214962  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.214967  1691 net.cpp:194] Memory required for data: 17563904
I0825 11:10:30.214972  1691 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:10:30.214982  1691 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:10:30.214987  1691 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:10:30.214993  1691 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:10:30.215003  1691 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:10:30.215046  1691 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:10:30.215054  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.215060  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.215065  1691 net.cpp:194] Memory required for data: 25952512
I0825 11:10:30.215068  1691 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:10:30.215085  1691 net.cpp:128] Creating Layer Convolution2
I0825 11:10:30.215090  1691 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:10:30.215097  1691 net.cpp:522] Convolution2 -> Convolution2
I0825 11:10:30.221707  1691 net.cpp:172] Setting up Convolution2
I0825 11:10:30.221736  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.221741  1691 net.cpp:194] Memory required for data: 30146816
I0825 11:10:30.221757  1691 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:10:30.221768  1691 net.cpp:128] Creating Layer BatchNorm2
I0825 11:10:30.221773  1691 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:10:30.221786  1691 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:10:30.222002  1691 net.cpp:172] Setting up BatchNorm2
I0825 11:10:30.222014  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.222019  1691 net.cpp:194] Memory required for data: 34341120
I0825 11:10:30.222029  1691 layer_factory.hpp:77] Creating layer Scale2
I0825 11:10:30.222039  1691 net.cpp:128] Creating Layer Scale2
I0825 11:10:30.222043  1691 net.cpp:558] Scale2 <- Convolution2
I0825 11:10:30.222050  1691 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:10:30.222091  1691 layer_factory.hpp:77] Creating layer Scale2
I0825 11:10:30.222213  1691 net.cpp:172] Setting up Scale2
I0825 11:10:30.222224  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.222231  1691 net.cpp:194] Memory required for data: 38535424
I0825 11:10:30.222239  1691 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:10:30.222270  1691 net.cpp:128] Creating Layer ReLU2
I0825 11:10:30.222278  1691 net.cpp:558] ReLU2 <- Convolution2
I0825 11:10:30.222285  1691 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:10:30.223803  1691 net.cpp:172] Setting up ReLU2
I0825 11:10:30.223819  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.223824  1691 net.cpp:194] Memory required for data: 42729728
I0825 11:10:30.223829  1691 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:10:30.223845  1691 net.cpp:128] Creating Layer Convolution3
I0825 11:10:30.223853  1691 net.cpp:558] Convolution3 <- Convolution2
I0825 11:10:30.223862  1691 net.cpp:522] Convolution3 -> Convolution3
I0825 11:10:30.230547  1691 net.cpp:172] Setting up Convolution3
I0825 11:10:30.230574  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.230578  1691 net.cpp:194] Memory required for data: 46924032
I0825 11:10:30.230589  1691 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:10:30.230599  1691 net.cpp:128] Creating Layer BatchNorm3
I0825 11:10:30.230603  1691 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:10:30.230612  1691 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:10:30.230830  1691 net.cpp:172] Setting up BatchNorm3
I0825 11:10:30.230842  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.230847  1691 net.cpp:194] Memory required for data: 51118336
I0825 11:10:30.230859  1691 layer_factory.hpp:77] Creating layer Scale3
I0825 11:10:30.230868  1691 net.cpp:128] Creating Layer Scale3
I0825 11:10:30.230872  1691 net.cpp:558] Scale3 <- Convolution3
I0825 11:10:30.230880  1691 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:10:30.230921  1691 layer_factory.hpp:77] Creating layer Scale3
I0825 11:10:30.231046  1691 net.cpp:172] Setting up Scale3
I0825 11:10:30.231057  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.231061  1691 net.cpp:194] Memory required for data: 55312640
I0825 11:10:30.231070  1691 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:10:30.231078  1691 net.cpp:128] Creating Layer Eltwise1
I0825 11:10:30.231083  1691 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:10:30.231088  1691 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:10:30.231096  1691 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:10:30.231125  1691 net.cpp:172] Setting up Eltwise1
I0825 11:10:30.231137  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.231140  1691 net.cpp:194] Memory required for data: 59506944
I0825 11:10:30.231145  1691 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:10:30.231151  1691 net.cpp:128] Creating Layer ReLU3
I0825 11:10:30.231156  1691 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:10:30.231163  1691 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:10:30.232653  1691 net.cpp:172] Setting up ReLU3
I0825 11:10:30.232673  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.232678  1691 net.cpp:194] Memory required for data: 63701248
I0825 11:10:30.232684  1691 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:10:30.232693  1691 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:10:30.232697  1691 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:10:30.232709  1691 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:10:30.232719  1691 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:10:30.232762  1691 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:10:30.232770  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.232776  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.232781  1691 net.cpp:194] Memory required for data: 72089856
I0825 11:10:30.232785  1691 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:10:30.232797  1691 net.cpp:128] Creating Layer Convolution4
I0825 11:10:30.232802  1691 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:10:30.232810  1691 net.cpp:522] Convolution4 -> Convolution4
I0825 11:10:30.234036  1691 net.cpp:172] Setting up Convolution4
I0825 11:10:30.234076  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.234081  1691 net.cpp:194] Memory required for data: 76284160
I0825 11:10:30.234091  1691 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:10:30.234099  1691 net.cpp:128] Creating Layer BatchNorm4
I0825 11:10:30.234107  1691 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:10:30.234113  1691 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:10:30.234328  1691 net.cpp:172] Setting up BatchNorm4
I0825 11:10:30.234349  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.234352  1691 net.cpp:194] Memory required for data: 80478464
I0825 11:10:30.234362  1691 layer_factory.hpp:77] Creating layer Scale4
I0825 11:10:30.234370  1691 net.cpp:128] Creating Layer Scale4
I0825 11:10:30.234375  1691 net.cpp:558] Scale4 <- Convolution4
I0825 11:10:30.234380  1691 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:10:30.234421  1691 layer_factory.hpp:77] Creating layer Scale4
I0825 11:10:30.234550  1691 net.cpp:172] Setting up Scale4
I0825 11:10:30.234562  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.234566  1691 net.cpp:194] Memory required for data: 84672768
I0825 11:10:30.234575  1691 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:10:30.234581  1691 net.cpp:128] Creating Layer ReLU4
I0825 11:10:30.234586  1691 net.cpp:558] ReLU4 <- Convolution4
I0825 11:10:30.234591  1691 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:10:30.234808  1691 net.cpp:172] Setting up ReLU4
I0825 11:10:30.234817  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.234822  1691 net.cpp:194] Memory required for data: 88867072
I0825 11:10:30.234827  1691 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:10:30.234836  1691 net.cpp:128] Creating Layer Convolution5
I0825 11:10:30.234841  1691 net.cpp:558] Convolution5 <- Convolution4
I0825 11:10:30.234848  1691 net.cpp:522] Convolution5 -> Convolution5
I0825 11:10:30.240998  1691 net.cpp:172] Setting up Convolution5
I0825 11:10:30.241024  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.241029  1691 net.cpp:194] Memory required for data: 93061376
I0825 11:10:30.241039  1691 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:10:30.241048  1691 net.cpp:128] Creating Layer BatchNorm5
I0825 11:10:30.241053  1691 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:10:30.241060  1691 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:10:30.241294  1691 net.cpp:172] Setting up BatchNorm5
I0825 11:10:30.241307  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.241312  1691 net.cpp:194] Memory required for data: 97255680
I0825 11:10:30.241325  1691 layer_factory.hpp:77] Creating layer Scale5
I0825 11:10:30.241333  1691 net.cpp:128] Creating Layer Scale5
I0825 11:10:30.241338  1691 net.cpp:558] Scale5 <- Convolution5
I0825 11:10:30.241343  1691 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:10:30.241386  1691 layer_factory.hpp:77] Creating layer Scale5
I0825 11:10:30.241518  1691 net.cpp:172] Setting up Scale5
I0825 11:10:30.241529  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.241533  1691 net.cpp:194] Memory required for data: 101449984
I0825 11:10:30.241541  1691 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:10:30.241549  1691 net.cpp:128] Creating Layer Eltwise2
I0825 11:10:30.241554  1691 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:10:30.241559  1691 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:10:30.241565  1691 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:10:30.241590  1691 net.cpp:172] Setting up Eltwise2
I0825 11:10:30.241597  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.241601  1691 net.cpp:194] Memory required for data: 105644288
I0825 11:10:30.241606  1691 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:10:30.241613  1691 net.cpp:128] Creating Layer ReLU5
I0825 11:10:30.241617  1691 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:10:30.241623  1691 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:10:30.243101  1691 net.cpp:172] Setting up ReLU5
I0825 11:10:30.243132  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.243137  1691 net.cpp:194] Memory required for data: 109838592
I0825 11:10:30.243142  1691 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:10:30.243156  1691 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:10:30.243161  1691 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:10:30.243168  1691 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:10:30.243177  1691 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:10:30.243223  1691 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:10:30.243234  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.243240  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.243245  1691 net.cpp:194] Memory required for data: 118227200
I0825 11:10:30.243249  1691 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:10:30.243260  1691 net.cpp:128] Creating Layer Convolution6
I0825 11:10:30.243265  1691 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:10:30.243273  1691 net.cpp:522] Convolution6 -> Convolution6
I0825 11:10:30.249792  1691 net.cpp:172] Setting up Convolution6
I0825 11:10:30.249819  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.249824  1691 net.cpp:194] Memory required for data: 122421504
I0825 11:10:30.249833  1691 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:10:30.249842  1691 net.cpp:128] Creating Layer BatchNorm6
I0825 11:10:30.249847  1691 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:10:30.249855  1691 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:10:30.250089  1691 net.cpp:172] Setting up BatchNorm6
I0825 11:10:30.250100  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.250106  1691 net.cpp:194] Memory required for data: 126615808
I0825 11:10:30.250116  1691 layer_factory.hpp:77] Creating layer Scale6
I0825 11:10:30.250124  1691 net.cpp:128] Creating Layer Scale6
I0825 11:10:30.250129  1691 net.cpp:558] Scale6 <- Convolution6
I0825 11:10:30.250135  1691 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:10:30.250176  1691 layer_factory.hpp:77] Creating layer Scale6
I0825 11:10:30.250308  1691 net.cpp:172] Setting up Scale6
I0825 11:10:30.250320  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.250324  1691 net.cpp:194] Memory required for data: 130810112
I0825 11:10:30.250332  1691 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:10:30.250346  1691 net.cpp:128] Creating Layer ReLU6
I0825 11:10:30.250352  1691 net.cpp:558] ReLU6 <- Convolution6
I0825 11:10:30.250358  1691 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:10:30.251858  1691 net.cpp:172] Setting up ReLU6
I0825 11:10:30.251884  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.251889  1691 net.cpp:194] Memory required for data: 135004416
I0825 11:10:30.251894  1691 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:10:30.251907  1691 net.cpp:128] Creating Layer Convolution7
I0825 11:10:30.251914  1691 net.cpp:558] Convolution7 <- Convolution6
I0825 11:10:30.251921  1691 net.cpp:522] Convolution7 -> Convolution7
I0825 11:10:30.258574  1691 net.cpp:172] Setting up Convolution7
I0825 11:10:30.258602  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.258607  1691 net.cpp:194] Memory required for data: 139198720
I0825 11:10:30.258617  1691 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:10:30.258626  1691 net.cpp:128] Creating Layer BatchNorm7
I0825 11:10:30.258631  1691 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:10:30.258639  1691 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:10:30.258893  1691 net.cpp:172] Setting up BatchNorm7
I0825 11:10:30.258905  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.258909  1691 net.cpp:194] Memory required for data: 143393024
I0825 11:10:30.258920  1691 layer_factory.hpp:77] Creating layer Scale7
I0825 11:10:30.258931  1691 net.cpp:128] Creating Layer Scale7
I0825 11:10:30.258951  1691 net.cpp:558] Scale7 <- Convolution7
I0825 11:10:30.258958  1691 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:10:30.259004  1691 layer_factory.hpp:77] Creating layer Scale7
I0825 11:10:30.259145  1691 net.cpp:172] Setting up Scale7
I0825 11:10:30.259157  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.259161  1691 net.cpp:194] Memory required for data: 147587328
I0825 11:10:30.259169  1691 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:10:30.259176  1691 net.cpp:128] Creating Layer Eltwise3
I0825 11:10:30.259181  1691 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:10:30.259186  1691 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:10:30.259194  1691 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:10:30.259223  1691 net.cpp:172] Setting up Eltwise3
I0825 11:10:30.259234  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.259238  1691 net.cpp:194] Memory required for data: 151781632
I0825 11:10:30.259244  1691 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:10:30.259250  1691 net.cpp:128] Creating Layer ReLU7
I0825 11:10:30.259254  1691 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:10:30.259263  1691 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:10:30.260622  1691 net.cpp:172] Setting up ReLU7
I0825 11:10:30.260637  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.260643  1691 net.cpp:194] Memory required for data: 155975936
I0825 11:10:30.260648  1691 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:10:30.260654  1691 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:10:30.260659  1691 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:10:30.260668  1691 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:10:30.260677  1691 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:10:30.260723  1691 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:10:30.260735  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.260742  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.260746  1691 net.cpp:194] Memory required for data: 164364544
I0825 11:10:30.260751  1691 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:10:30.260766  1691 net.cpp:128] Creating Layer Convolution8
I0825 11:10:30.260771  1691 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:10:30.260779  1691 net.cpp:522] Convolution8 -> Convolution8
I0825 11:10:30.267316  1691 net.cpp:172] Setting up Convolution8
I0825 11:10:30.267341  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.267346  1691 net.cpp:194] Memory required for data: 168558848
I0825 11:10:30.267356  1691 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:10:30.267367  1691 net.cpp:128] Creating Layer BatchNorm8
I0825 11:10:30.267374  1691 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:10:30.267382  1691 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:10:30.267627  1691 net.cpp:172] Setting up BatchNorm8
I0825 11:10:30.267638  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.267643  1691 net.cpp:194] Memory required for data: 172753152
I0825 11:10:30.267653  1691 layer_factory.hpp:77] Creating layer Scale8
I0825 11:10:30.267666  1691 net.cpp:128] Creating Layer Scale8
I0825 11:10:30.267671  1691 net.cpp:558] Scale8 <- Convolution8
I0825 11:10:30.267678  1691 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:10:30.267724  1691 layer_factory.hpp:77] Creating layer Scale8
I0825 11:10:30.267863  1691 net.cpp:172] Setting up Scale8
I0825 11:10:30.267876  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.267880  1691 net.cpp:194] Memory required for data: 176947456
I0825 11:10:30.267889  1691 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:10:30.267897  1691 net.cpp:128] Creating Layer ReLU8
I0825 11:10:30.267907  1691 net.cpp:558] ReLU8 <- Convolution8
I0825 11:10:30.267913  1691 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:10:30.269361  1691 net.cpp:172] Setting up ReLU8
I0825 11:10:30.269392  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.269397  1691 net.cpp:194] Memory required for data: 181141760
I0825 11:10:30.269402  1691 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:10:30.269421  1691 net.cpp:128] Creating Layer Convolution9
I0825 11:10:30.269430  1691 net.cpp:558] Convolution9 <- Convolution8
I0825 11:10:30.269438  1691 net.cpp:522] Convolution9 -> Convolution9
I0825 11:10:30.276091  1691 net.cpp:172] Setting up Convolution9
I0825 11:10:30.276118  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.276123  1691 net.cpp:194] Memory required for data: 185336064
I0825 11:10:30.276132  1691 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:10:30.276144  1691 net.cpp:128] Creating Layer BatchNorm9
I0825 11:10:30.276149  1691 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:10:30.276156  1691 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:10:30.276404  1691 net.cpp:172] Setting up BatchNorm9
I0825 11:10:30.276417  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.276422  1691 net.cpp:194] Memory required for data: 189530368
I0825 11:10:30.276432  1691 layer_factory.hpp:77] Creating layer Scale9
I0825 11:10:30.276438  1691 net.cpp:128] Creating Layer Scale9
I0825 11:10:30.276443  1691 net.cpp:558] Scale9 <- Convolution9
I0825 11:10:30.276450  1691 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:10:30.276495  1691 layer_factory.hpp:77] Creating layer Scale9
I0825 11:10:30.276638  1691 net.cpp:172] Setting up Scale9
I0825 11:10:30.276649  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.276655  1691 net.cpp:194] Memory required for data: 193724672
I0825 11:10:30.276662  1691 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:10:30.276670  1691 net.cpp:128] Creating Layer Eltwise4
I0825 11:10:30.276675  1691 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:10:30.276679  1691 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:10:30.276688  1691 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:10:30.276717  1691 net.cpp:172] Setting up Eltwise4
I0825 11:10:30.276728  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.276733  1691 net.cpp:194] Memory required for data: 197918976
I0825 11:10:30.276737  1691 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:10:30.276744  1691 net.cpp:128] Creating Layer ReLU9
I0825 11:10:30.276748  1691 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:10:30.276754  1691 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:10:30.278141  1691 net.cpp:172] Setting up ReLU9
I0825 11:10:30.278158  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.278162  1691 net.cpp:194] Memory required for data: 202113280
I0825 11:10:30.278167  1691 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:10:30.278174  1691 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:10:30.278178  1691 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:10:30.278187  1691 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:10:30.278196  1691 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:10:30.278240  1691 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:10:30.278247  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.278254  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.278257  1691 net.cpp:194] Memory required for data: 210501888
I0825 11:10:30.278261  1691 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:10:30.278275  1691 net.cpp:128] Creating Layer Convolution10
I0825 11:10:30.278280  1691 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:10:30.278287  1691 net.cpp:522] Convolution10 -> Convolution10
I0825 11:10:30.284884  1691 net.cpp:172] Setting up Convolution10
I0825 11:10:30.284910  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.284915  1691 net.cpp:194] Memory required for data: 214696192
I0825 11:10:30.284934  1691 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:10:30.284946  1691 net.cpp:128] Creating Layer BatchNorm10
I0825 11:10:30.284966  1691 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:10:30.284974  1691 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:10:30.285218  1691 net.cpp:172] Setting up BatchNorm10
I0825 11:10:30.285231  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.285236  1691 net.cpp:194] Memory required for data: 218890496
I0825 11:10:30.285246  1691 layer_factory.hpp:77] Creating layer Scale10
I0825 11:10:30.285254  1691 net.cpp:128] Creating Layer Scale10
I0825 11:10:30.285257  1691 net.cpp:558] Scale10 <- Convolution10
I0825 11:10:30.285264  1691 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:10:30.285308  1691 layer_factory.hpp:77] Creating layer Scale10
I0825 11:10:30.285452  1691 net.cpp:172] Setting up Scale10
I0825 11:10:30.285465  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.285470  1691 net.cpp:194] Memory required for data: 223084800
I0825 11:10:30.285477  1691 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:10:30.285486  1691 net.cpp:128] Creating Layer ReLU10
I0825 11:10:30.285490  1691 net.cpp:558] ReLU10 <- Convolution10
I0825 11:10:30.285496  1691 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:10:30.286978  1691 net.cpp:172] Setting up ReLU10
I0825 11:10:30.287004  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.287009  1691 net.cpp:194] Memory required for data: 227279104
I0825 11:10:30.287014  1691 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:10:30.287029  1691 net.cpp:128] Creating Layer Convolution11
I0825 11:10:30.287035  1691 net.cpp:558] Convolution11 <- Convolution10
I0825 11:10:30.287050  1691 net.cpp:522] Convolution11 -> Convolution11
I0825 11:10:30.293737  1691 net.cpp:172] Setting up Convolution11
I0825 11:10:30.293763  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.293768  1691 net.cpp:194] Memory required for data: 231473408
I0825 11:10:30.293778  1691 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:10:30.293790  1691 net.cpp:128] Creating Layer BatchNorm11
I0825 11:10:30.293797  1691 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:10:30.293810  1691 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:10:30.294055  1691 net.cpp:172] Setting up BatchNorm11
I0825 11:10:30.294068  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.294072  1691 net.cpp:194] Memory required for data: 235667712
I0825 11:10:30.294082  1691 layer_factory.hpp:77] Creating layer Scale11
I0825 11:10:30.294090  1691 net.cpp:128] Creating Layer Scale11
I0825 11:10:30.294095  1691 net.cpp:558] Scale11 <- Convolution11
I0825 11:10:30.294100  1691 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:10:30.294147  1691 layer_factory.hpp:77] Creating layer Scale11
I0825 11:10:30.294278  1691 net.cpp:172] Setting up Scale11
I0825 11:10:30.294291  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.294294  1691 net.cpp:194] Memory required for data: 239862016
I0825 11:10:30.294302  1691 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:10:30.294313  1691 net.cpp:128] Creating Layer Eltwise5
I0825 11:10:30.294318  1691 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:10:30.294323  1691 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:10:30.294329  1691 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:10:30.294365  1691 net.cpp:172] Setting up Eltwise5
I0825 11:10:30.294373  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.294378  1691 net.cpp:194] Memory required for data: 244056320
I0825 11:10:30.294381  1691 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:10:30.294389  1691 net.cpp:128] Creating Layer ReLU11
I0825 11:10:30.294392  1691 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:10:30.294400  1691 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:10:30.295804  1691 net.cpp:172] Setting up ReLU11
I0825 11:10:30.295817  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.295821  1691 net.cpp:194] Memory required for data: 248250624
I0825 11:10:30.295826  1691 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:10:30.295850  1691 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:10:30.295855  1691 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:10:30.295863  1691 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:10:30.295871  1691 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:10:30.295922  1691 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:10:30.295929  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.295935  1691 net.cpp:186] Top shape: 64 16 32 32 (1048576)
I0825 11:10:30.295939  1691 net.cpp:194] Memory required for data: 256639232
I0825 11:10:30.295944  1691 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:10:30.295958  1691 net.cpp:128] Creating Layer Convolution12
I0825 11:10:30.295964  1691 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:10:30.295974  1691 net.cpp:522] Convolution12 -> Convolution12
I0825 11:10:30.302767  1691 net.cpp:172] Setting up Convolution12
I0825 11:10:30.302793  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.302798  1691 net.cpp:194] Memory required for data: 258736384
I0825 11:10:30.302808  1691 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:10:30.302819  1691 net.cpp:128] Creating Layer BatchNorm12
I0825 11:10:30.302824  1691 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:10:30.302830  1691 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:10:30.303076  1691 net.cpp:172] Setting up BatchNorm12
I0825 11:10:30.303103  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.303107  1691 net.cpp:194] Memory required for data: 260833536
I0825 11:10:30.303117  1691 layer_factory.hpp:77] Creating layer Scale12
I0825 11:10:30.303126  1691 net.cpp:128] Creating Layer Scale12
I0825 11:10:30.303131  1691 net.cpp:558] Scale12 <- Convolution12
I0825 11:10:30.303138  1691 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:10:30.303181  1691 layer_factory.hpp:77] Creating layer Scale12
I0825 11:10:30.303319  1691 net.cpp:172] Setting up Scale12
I0825 11:10:30.303326  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.303330  1691 net.cpp:194] Memory required for data: 262930688
I0825 11:10:30.303339  1691 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:10:30.303350  1691 net.cpp:128] Creating Layer Convolution13
I0825 11:10:30.303355  1691 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_1
I0825 11:10:30.303365  1691 net.cpp:522] Convolution13 -> Convolution13
I0825 11:10:30.309175  1691 net.cpp:172] Setting up Convolution13
I0825 11:10:30.309201  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.309206  1691 net.cpp:194] Memory required for data: 265027840
I0825 11:10:30.309216  1691 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:10:30.309227  1691 net.cpp:128] Creating Layer BatchNorm13
I0825 11:10:30.309232  1691 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:10:30.309239  1691 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:10:30.309485  1691 net.cpp:172] Setting up BatchNorm13
I0825 11:10:30.309497  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.309501  1691 net.cpp:194] Memory required for data: 267124992
I0825 11:10:30.309511  1691 layer_factory.hpp:77] Creating layer Scale13
I0825 11:10:30.309518  1691 net.cpp:128] Creating Layer Scale13
I0825 11:10:30.309523  1691 net.cpp:558] Scale13 <- Convolution13
I0825 11:10:30.309531  1691 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:10:30.309573  1691 layer_factory.hpp:77] Creating layer Scale13
I0825 11:10:30.309720  1691 net.cpp:172] Setting up Scale13
I0825 11:10:30.309732  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.309736  1691 net.cpp:194] Memory required for data: 269222144
I0825 11:10:30.309746  1691 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:10:30.309752  1691 net.cpp:128] Creating Layer ReLU12
I0825 11:10:30.309757  1691 net.cpp:558] ReLU12 <- Convolution13
I0825 11:10:30.309780  1691 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0825 11:10:30.311241  1691 net.cpp:172] Setting up ReLU12
I0825 11:10:30.311259  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.311264  1691 net.cpp:194] Memory required for data: 271319296
I0825 11:10:30.311269  1691 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:10:30.311282  1691 net.cpp:128] Creating Layer Convolution14
I0825 11:10:30.311288  1691 net.cpp:558] Convolution14 <- Convolution13
I0825 11:10:30.311297  1691 net.cpp:522] Convolution14 -> Convolution14
I0825 11:10:30.316246  1691 net.cpp:172] Setting up Convolution14
I0825 11:10:30.316272  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.316277  1691 net.cpp:194] Memory required for data: 273416448
I0825 11:10:30.316287  1691 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:10:30.316306  1691 net.cpp:128] Creating Layer BatchNorm14
I0825 11:10:30.316313  1691 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:10:30.316321  1691 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:10:30.316560  1691 net.cpp:172] Setting up BatchNorm14
I0825 11:10:30.316572  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.316576  1691 net.cpp:194] Memory required for data: 275513600
I0825 11:10:30.316586  1691 layer_factory.hpp:77] Creating layer Scale14
I0825 11:10:30.316593  1691 net.cpp:128] Creating Layer Scale14
I0825 11:10:30.316597  1691 net.cpp:558] Scale14 <- Convolution14
I0825 11:10:30.316606  1691 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:10:30.316648  1691 layer_factory.hpp:77] Creating layer Scale14
I0825 11:10:30.316783  1691 net.cpp:172] Setting up Scale14
I0825 11:10:30.316790  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.316794  1691 net.cpp:194] Memory required for data: 277610752
I0825 11:10:30.316802  1691 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:10:30.316809  1691 net.cpp:128] Creating Layer Eltwise6
I0825 11:10:30.316813  1691 net.cpp:558] Eltwise6 <- Convolution12
I0825 11:10:30.316818  1691 net.cpp:558] Eltwise6 <- Convolution14
I0825 11:10:30.316826  1691 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:10:30.316848  1691 net.cpp:172] Setting up Eltwise6
I0825 11:10:30.316854  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.316859  1691 net.cpp:194] Memory required for data: 279707904
I0825 11:10:30.316864  1691 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:10:30.316871  1691 net.cpp:128] Creating Layer ReLU13
I0825 11:10:30.316875  1691 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:10:30.316881  1691 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:10:30.317343  1691 net.cpp:172] Setting up ReLU13
I0825 11:10:30.317364  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.317369  1691 net.cpp:194] Memory required for data: 281805056
I0825 11:10:30.317374  1691 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:10:30.317385  1691 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:10:30.317390  1691 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:10:30.317397  1691 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:10:30.317409  1691 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:10:30.317459  1691 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:10:30.317467  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.317473  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.317477  1691 net.cpp:194] Memory required for data: 285999360
I0825 11:10:30.317481  1691 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:10:30.317493  1691 net.cpp:128] Creating Layer Convolution15
I0825 11:10:30.317498  1691 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0825 11:10:30.317507  1691 net.cpp:522] Convolution15 -> Convolution15
I0825 11:10:30.321949  1691 net.cpp:172] Setting up Convolution15
I0825 11:10:30.321979  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.321983  1691 net.cpp:194] Memory required for data: 288096512
I0825 11:10:30.322016  1691 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:10:30.322028  1691 net.cpp:128] Creating Layer BatchNorm15
I0825 11:10:30.322033  1691 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:10:30.322046  1691 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:10:30.322293  1691 net.cpp:172] Setting up BatchNorm15
I0825 11:10:30.322304  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.322307  1691 net.cpp:194] Memory required for data: 290193664
I0825 11:10:30.322317  1691 layer_factory.hpp:77] Creating layer Scale15
I0825 11:10:30.322325  1691 net.cpp:128] Creating Layer Scale15
I0825 11:10:30.322329  1691 net.cpp:558] Scale15 <- Convolution15
I0825 11:10:30.322335  1691 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:10:30.322389  1691 layer_factory.hpp:77] Creating layer Scale15
I0825 11:10:30.322530  1691 net.cpp:172] Setting up Scale15
I0825 11:10:30.322540  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.322544  1691 net.cpp:194] Memory required for data: 292290816
I0825 11:10:30.322556  1691 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:10:30.322566  1691 net.cpp:128] Creating Layer ReLU14
I0825 11:10:30.322576  1691 net.cpp:558] ReLU14 <- Convolution15
I0825 11:10:30.322582  1691 net.cpp:509] ReLU14 -> Convolution15 (in-place)
I0825 11:10:30.324019  1691 net.cpp:172] Setting up ReLU14
I0825 11:10:30.324036  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.324041  1691 net.cpp:194] Memory required for data: 294387968
I0825 11:10:30.324045  1691 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:10:30.324059  1691 net.cpp:128] Creating Layer Convolution16
I0825 11:10:30.324072  1691 net.cpp:558] Convolution16 <- Convolution15
I0825 11:10:30.324082  1691 net.cpp:522] Convolution16 -> Convolution16
I0825 11:10:30.328773  1691 net.cpp:172] Setting up Convolution16
I0825 11:10:30.328796  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.328801  1691 net.cpp:194] Memory required for data: 296485120
I0825 11:10:30.328814  1691 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:10:30.328828  1691 net.cpp:128] Creating Layer BatchNorm16
I0825 11:10:30.328841  1691 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:10:30.328850  1691 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:10:30.329094  1691 net.cpp:172] Setting up BatchNorm16
I0825 11:10:30.329105  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.329110  1691 net.cpp:194] Memory required for data: 298582272
I0825 11:10:30.329120  1691 layer_factory.hpp:77] Creating layer Scale16
I0825 11:10:30.329130  1691 net.cpp:128] Creating Layer Scale16
I0825 11:10:30.329135  1691 net.cpp:558] Scale16 <- Convolution16
I0825 11:10:30.329141  1691 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:10:30.329186  1691 layer_factory.hpp:77] Creating layer Scale16
I0825 11:10:30.329331  1691 net.cpp:172] Setting up Scale16
I0825 11:10:30.329341  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.329345  1691 net.cpp:194] Memory required for data: 300679424
I0825 11:10:30.329354  1691 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:10:30.329363  1691 net.cpp:128] Creating Layer Eltwise7
I0825 11:10:30.329372  1691 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:10:30.329378  1691 net.cpp:558] Eltwise7 <- Convolution16
I0825 11:10:30.329386  1691 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:10:30.329408  1691 net.cpp:172] Setting up Eltwise7
I0825 11:10:30.329418  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.329422  1691 net.cpp:194] Memory required for data: 302776576
I0825 11:10:30.329427  1691 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:10:30.329433  1691 net.cpp:128] Creating Layer ReLU15
I0825 11:10:30.329440  1691 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:10:30.329449  1691 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:10:30.329905  1691 net.cpp:172] Setting up ReLU15
I0825 11:10:30.329926  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.329951  1691 net.cpp:194] Memory required for data: 304873728
I0825 11:10:30.329957  1691 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:10:30.329967  1691 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:10:30.329973  1691 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:10:30.329980  1691 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:10:30.329989  1691 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:10:30.330049  1691 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:10:30.330058  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.330065  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.330070  1691 net.cpp:194] Memory required for data: 309068032
I0825 11:10:30.330073  1691 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:10:30.330085  1691 net.cpp:128] Creating Layer Convolution17
I0825 11:10:30.330092  1691 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_0
I0825 11:10:30.330101  1691 net.cpp:522] Convolution17 -> Convolution17
I0825 11:10:30.335402  1691 net.cpp:172] Setting up Convolution17
I0825 11:10:30.335429  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.335433  1691 net.cpp:194] Memory required for data: 311165184
I0825 11:10:30.335448  1691 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:10:30.335460  1691 net.cpp:128] Creating Layer BatchNorm17
I0825 11:10:30.335466  1691 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:10:30.335475  1691 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:10:30.335726  1691 net.cpp:172] Setting up BatchNorm17
I0825 11:10:30.335736  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.335741  1691 net.cpp:194] Memory required for data: 313262336
I0825 11:10:30.335750  1691 layer_factory.hpp:77] Creating layer Scale17
I0825 11:10:30.335758  1691 net.cpp:128] Creating Layer Scale17
I0825 11:10:30.335768  1691 net.cpp:558] Scale17 <- Convolution17
I0825 11:10:30.335777  1691 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:10:30.335822  1691 layer_factory.hpp:77] Creating layer Scale17
I0825 11:10:30.335963  1691 net.cpp:172] Setting up Scale17
I0825 11:10:30.335973  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.335978  1691 net.cpp:194] Memory required for data: 315359488
I0825 11:10:30.335986  1691 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:10:30.335994  1691 net.cpp:128] Creating Layer ReLU16
I0825 11:10:30.336000  1691 net.cpp:558] ReLU16 <- Convolution17
I0825 11:10:30.336009  1691 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0825 11:10:30.337502  1691 net.cpp:172] Setting up ReLU16
I0825 11:10:30.337525  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.337530  1691 net.cpp:194] Memory required for data: 317456640
I0825 11:10:30.337535  1691 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:10:30.337554  1691 net.cpp:128] Creating Layer Convolution18
I0825 11:10:30.337561  1691 net.cpp:558] Convolution18 <- Convolution17
I0825 11:10:30.337574  1691 net.cpp:522] Convolution18 -> Convolution18
I0825 11:10:30.344014  1691 net.cpp:172] Setting up Convolution18
I0825 11:10:30.344039  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.344044  1691 net.cpp:194] Memory required for data: 319553792
I0825 11:10:30.344058  1691 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:10:30.344069  1691 net.cpp:128] Creating Layer BatchNorm18
I0825 11:10:30.344074  1691 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:10:30.344084  1691 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:10:30.344331  1691 net.cpp:172] Setting up BatchNorm18
I0825 11:10:30.344342  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.344348  1691 net.cpp:194] Memory required for data: 321650944
I0825 11:10:30.344357  1691 layer_factory.hpp:77] Creating layer Scale18
I0825 11:10:30.344367  1691 net.cpp:128] Creating Layer Scale18
I0825 11:10:30.344390  1691 net.cpp:558] Scale18 <- Convolution18
I0825 11:10:30.344398  1691 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:10:30.344444  1691 layer_factory.hpp:77] Creating layer Scale18
I0825 11:10:30.344589  1691 net.cpp:172] Setting up Scale18
I0825 11:10:30.344600  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.344605  1691 net.cpp:194] Memory required for data: 323748096
I0825 11:10:30.344612  1691 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:10:30.344622  1691 net.cpp:128] Creating Layer Eltwise8
I0825 11:10:30.344627  1691 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:10:30.344632  1691 net.cpp:558] Eltwise8 <- Convolution18
I0825 11:10:30.344640  1691 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:10:30.344662  1691 net.cpp:172] Setting up Eltwise8
I0825 11:10:30.344668  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.344672  1691 net.cpp:194] Memory required for data: 325845248
I0825 11:10:30.344676  1691 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:10:30.344683  1691 net.cpp:128] Creating Layer ReLU17
I0825 11:10:30.344687  1691 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:10:30.344696  1691 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:10:30.344928  1691 net.cpp:172] Setting up ReLU17
I0825 11:10:30.344941  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.344947  1691 net.cpp:194] Memory required for data: 327942400
I0825 11:10:30.344952  1691 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:10:30.344959  1691 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:10:30.344964  1691 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:10:30.344972  1691 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:10:30.344981  1691 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:10:30.345031  1691 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:10:30.345041  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.345047  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.345052  1691 net.cpp:194] Memory required for data: 332136704
I0825 11:10:30.345055  1691 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:10:30.345067  1691 net.cpp:128] Creating Layer Convolution19
I0825 11:10:30.345072  1691 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0825 11:10:30.345082  1691 net.cpp:522] Convolution19 -> Convolution19
I0825 11:10:30.351769  1691 net.cpp:172] Setting up Convolution19
I0825 11:10:30.351796  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.351801  1691 net.cpp:194] Memory required for data: 334233856
I0825 11:10:30.351811  1691 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:10:30.351822  1691 net.cpp:128] Creating Layer BatchNorm19
I0825 11:10:30.351828  1691 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:10:30.351837  1691 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:10:30.352092  1691 net.cpp:172] Setting up BatchNorm19
I0825 11:10:30.352104  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.352108  1691 net.cpp:194] Memory required for data: 336331008
I0825 11:10:30.352133  1691 layer_factory.hpp:77] Creating layer Scale19
I0825 11:10:30.352140  1691 net.cpp:128] Creating Layer Scale19
I0825 11:10:30.352144  1691 net.cpp:558] Scale19 <- Convolution19
I0825 11:10:30.352154  1691 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:10:30.352200  1691 layer_factory.hpp:77] Creating layer Scale19
I0825 11:10:30.352340  1691 net.cpp:172] Setting up Scale19
I0825 11:10:30.352351  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.352355  1691 net.cpp:194] Memory required for data: 338428160
I0825 11:10:30.352363  1691 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:10:30.352370  1691 net.cpp:128] Creating Layer ReLU18
I0825 11:10:30.352375  1691 net.cpp:558] ReLU18 <- Convolution19
I0825 11:10:30.352382  1691 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0825 11:10:30.353617  1691 net.cpp:172] Setting up ReLU18
I0825 11:10:30.353647  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.353653  1691 net.cpp:194] Memory required for data: 340525312
I0825 11:10:30.353657  1691 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:10:30.353677  1691 net.cpp:128] Creating Layer Convolution20
I0825 11:10:30.353683  1691 net.cpp:558] Convolution20 <- Convolution19
I0825 11:10:30.353693  1691 net.cpp:522] Convolution20 -> Convolution20
I0825 11:10:30.360329  1691 net.cpp:172] Setting up Convolution20
I0825 11:10:30.360355  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.360359  1691 net.cpp:194] Memory required for data: 342622464
I0825 11:10:30.360369  1691 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:10:30.360381  1691 net.cpp:128] Creating Layer BatchNorm20
I0825 11:10:30.360386  1691 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:10:30.360396  1691 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:10:30.360642  1691 net.cpp:172] Setting up BatchNorm20
I0825 11:10:30.360654  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.360661  1691 net.cpp:194] Memory required for data: 344719616
I0825 11:10:30.360671  1691 layer_factory.hpp:77] Creating layer Scale20
I0825 11:10:30.360678  1691 net.cpp:128] Creating Layer Scale20
I0825 11:10:30.360683  1691 net.cpp:558] Scale20 <- Convolution20
I0825 11:10:30.360689  1691 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:10:30.360733  1691 layer_factory.hpp:77] Creating layer Scale20
I0825 11:10:30.360877  1691 net.cpp:172] Setting up Scale20
I0825 11:10:30.360889  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.360893  1691 net.cpp:194] Memory required for data: 346816768
I0825 11:10:30.360901  1691 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:10:30.360924  1691 net.cpp:128] Creating Layer Eltwise9
I0825 11:10:30.360929  1691 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:10:30.360934  1691 net.cpp:558] Eltwise9 <- Convolution20
I0825 11:10:30.360942  1691 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:10:30.360968  1691 net.cpp:172] Setting up Eltwise9
I0825 11:10:30.360982  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.360986  1691 net.cpp:194] Memory required for data: 348913920
I0825 11:10:30.360991  1691 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:10:30.360997  1691 net.cpp:128] Creating Layer ReLU19
I0825 11:10:30.361001  1691 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:10:30.361009  1691 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:10:30.362396  1691 net.cpp:172] Setting up ReLU19
I0825 11:10:30.362416  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.362421  1691 net.cpp:194] Memory required for data: 351011072
I0825 11:10:30.362426  1691 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:10:30.362432  1691 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:10:30.362437  1691 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:10:30.362447  1691 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:10:30.362457  1691 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:10:30.362507  1691 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:10:30.362515  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.362521  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.362525  1691 net.cpp:194] Memory required for data: 355205376
I0825 11:10:30.362529  1691 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:10:30.362542  1691 net.cpp:128] Creating Layer Convolution21
I0825 11:10:30.362546  1691 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0825 11:10:30.362557  1691 net.cpp:522] Convolution21 -> Convolution21
I0825 11:10:30.369113  1691 net.cpp:172] Setting up Convolution21
I0825 11:10:30.369139  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.369144  1691 net.cpp:194] Memory required for data: 357302528
I0825 11:10:30.369154  1691 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:10:30.369179  1691 net.cpp:128] Creating Layer BatchNorm21
I0825 11:10:30.369187  1691 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:10:30.369194  1691 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:10:30.369441  1691 net.cpp:172] Setting up BatchNorm21
I0825 11:10:30.369454  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.369458  1691 net.cpp:194] Memory required for data: 359399680
I0825 11:10:30.369469  1691 layer_factory.hpp:77] Creating layer Scale21
I0825 11:10:30.369478  1691 net.cpp:128] Creating Layer Scale21
I0825 11:10:30.369483  1691 net.cpp:558] Scale21 <- Convolution21
I0825 11:10:30.369488  1691 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:10:30.369534  1691 layer_factory.hpp:77] Creating layer Scale21
I0825 11:10:30.369685  1691 net.cpp:172] Setting up Scale21
I0825 11:10:30.369698  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.369701  1691 net.cpp:194] Memory required for data: 361496832
I0825 11:10:30.369709  1691 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:10:30.369716  1691 net.cpp:128] Creating Layer ReLU20
I0825 11:10:30.369720  1691 net.cpp:558] ReLU20 <- Convolution21
I0825 11:10:30.369729  1691 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:10:30.371160  1691 net.cpp:172] Setting up ReLU20
I0825 11:10:30.371182  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.371186  1691 net.cpp:194] Memory required for data: 363593984
I0825 11:10:30.371192  1691 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:10:30.371207  1691 net.cpp:128] Creating Layer Convolution22
I0825 11:10:30.371212  1691 net.cpp:558] Convolution22 <- Convolution21
I0825 11:10:30.371223  1691 net.cpp:522] Convolution22 -> Convolution22
I0825 11:10:30.377853  1691 net.cpp:172] Setting up Convolution22
I0825 11:10:30.377882  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.377887  1691 net.cpp:194] Memory required for data: 365691136
I0825 11:10:30.377897  1691 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:10:30.377907  1691 net.cpp:128] Creating Layer BatchNorm22
I0825 11:10:30.377912  1691 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:10:30.377921  1691 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:10:30.378168  1691 net.cpp:172] Setting up BatchNorm22
I0825 11:10:30.378180  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.378185  1691 net.cpp:194] Memory required for data: 367788288
I0825 11:10:30.378195  1691 layer_factory.hpp:77] Creating layer Scale22
I0825 11:10:30.378201  1691 net.cpp:128] Creating Layer Scale22
I0825 11:10:30.378206  1691 net.cpp:558] Scale22 <- Convolution22
I0825 11:10:30.378211  1691 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:10:30.378257  1691 layer_factory.hpp:77] Creating layer Scale22
I0825 11:10:30.378414  1691 net.cpp:172] Setting up Scale22
I0825 11:10:30.378429  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.378433  1691 net.cpp:194] Memory required for data: 369885440
I0825 11:10:30.378442  1691 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:10:30.378448  1691 net.cpp:128] Creating Layer Eltwise10
I0825 11:10:30.378453  1691 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0825 11:10:30.378458  1691 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:10:30.378465  1691 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:10:30.378490  1691 net.cpp:172] Setting up Eltwise10
I0825 11:10:30.378497  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.378501  1691 net.cpp:194] Memory required for data: 371982592
I0825 11:10:30.378505  1691 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:10:30.378512  1691 net.cpp:128] Creating Layer ReLU21
I0825 11:10:30.378516  1691 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:10:30.378525  1691 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:10:30.379897  1691 net.cpp:172] Setting up ReLU21
I0825 11:10:30.379912  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.379916  1691 net.cpp:194] Memory required for data: 374079744
I0825 11:10:30.379942  1691 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:10:30.379951  1691 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:10:30.379956  1691 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:10:30.379964  1691 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:10:30.379973  1691 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:10:30.380026  1691 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:10:30.380040  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.380046  1691 net.cpp:186] Top shape: 64 32 16 16 (524288)
I0825 11:10:30.380050  1691 net.cpp:194] Memory required for data: 378274048
I0825 11:10:30.380054  1691 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:10:30.380069  1691 net.cpp:128] Creating Layer Convolution23
I0825 11:10:30.380074  1691 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:10:30.380082  1691 net.cpp:522] Convolution23 -> Convolution23
I0825 11:10:30.386602  1691 net.cpp:172] Setting up Convolution23
I0825 11:10:30.386628  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.386632  1691 net.cpp:194] Memory required for data: 379322624
I0825 11:10:30.386642  1691 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:10:30.386656  1691 net.cpp:128] Creating Layer BatchNorm23
I0825 11:10:30.386662  1691 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:10:30.386669  1691 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:10:30.386925  1691 net.cpp:172] Setting up BatchNorm23
I0825 11:10:30.386937  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.386941  1691 net.cpp:194] Memory required for data: 380371200
I0825 11:10:30.386952  1691 layer_factory.hpp:77] Creating layer Scale23
I0825 11:10:30.386962  1691 net.cpp:128] Creating Layer Scale23
I0825 11:10:30.386967  1691 net.cpp:558] Scale23 <- Convolution23
I0825 11:10:30.386973  1691 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:10:30.387018  1691 layer_factory.hpp:77] Creating layer Scale23
I0825 11:10:30.387168  1691 net.cpp:172] Setting up Scale23
I0825 11:10:30.387179  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.387183  1691 net.cpp:194] Memory required for data: 381419776
I0825 11:10:30.387192  1691 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:10:30.387207  1691 net.cpp:128] Creating Layer Convolution24
I0825 11:10:30.387212  1691 net.cpp:558] Convolution24 <- Eltwise10_ReLU21_0_split_1
I0825 11:10:30.387220  1691 net.cpp:522] Convolution24 -> Convolution24
I0825 11:10:30.393419  1691 net.cpp:172] Setting up Convolution24
I0825 11:10:30.393447  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.393451  1691 net.cpp:194] Memory required for data: 382468352
I0825 11:10:30.393463  1691 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:10:30.393476  1691 net.cpp:128] Creating Layer BatchNorm24
I0825 11:10:30.393481  1691 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:10:30.393487  1691 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:10:30.393752  1691 net.cpp:172] Setting up BatchNorm24
I0825 11:10:30.393765  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.393769  1691 net.cpp:194] Memory required for data: 383516928
I0825 11:10:30.393779  1691 layer_factory.hpp:77] Creating layer Scale24
I0825 11:10:30.393787  1691 net.cpp:128] Creating Layer Scale24
I0825 11:10:30.393791  1691 net.cpp:558] Scale24 <- Convolution24
I0825 11:10:30.393797  1691 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:10:30.393844  1691 layer_factory.hpp:77] Creating layer Scale24
I0825 11:10:30.393996  1691 net.cpp:172] Setting up Scale24
I0825 11:10:30.394006  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.394011  1691 net.cpp:194] Memory required for data: 384565504
I0825 11:10:30.394019  1691 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:10:30.394026  1691 net.cpp:128] Creating Layer ReLU22
I0825 11:10:30.394052  1691 net.cpp:558] ReLU22 <- Convolution24
I0825 11:10:30.394062  1691 net.cpp:509] ReLU22 -> Convolution24 (in-place)
I0825 11:10:30.395246  1691 net.cpp:172] Setting up ReLU22
I0825 11:10:30.395262  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.395265  1691 net.cpp:194] Memory required for data: 385614080
I0825 11:10:30.395270  1691 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:10:30.395288  1691 net.cpp:128] Creating Layer Convolution25
I0825 11:10:30.395293  1691 net.cpp:558] Convolution25 <- Convolution24
I0825 11:10:30.395304  1691 net.cpp:522] Convolution25 -> Convolution25
I0825 11:10:30.402230  1691 net.cpp:172] Setting up Convolution25
I0825 11:10:30.402256  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.402261  1691 net.cpp:194] Memory required for data: 386662656
I0825 11:10:30.402271  1691 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:10:30.402282  1691 net.cpp:128] Creating Layer BatchNorm25
I0825 11:10:30.402288  1691 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:10:30.402295  1691 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:10:30.402572  1691 net.cpp:172] Setting up BatchNorm25
I0825 11:10:30.402585  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.402590  1691 net.cpp:194] Memory required for data: 387711232
I0825 11:10:30.402599  1691 layer_factory.hpp:77] Creating layer Scale25
I0825 11:10:30.402606  1691 net.cpp:128] Creating Layer Scale25
I0825 11:10:30.402611  1691 net.cpp:558] Scale25 <- Convolution25
I0825 11:10:30.402617  1691 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:10:30.402665  1691 layer_factory.hpp:77] Creating layer Scale25
I0825 11:10:30.402813  1691 net.cpp:172] Setting up Scale25
I0825 11:10:30.402820  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.402824  1691 net.cpp:194] Memory required for data: 388759808
I0825 11:10:30.402832  1691 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:10:30.402840  1691 net.cpp:128] Creating Layer Eltwise11
I0825 11:10:30.402845  1691 net.cpp:558] Eltwise11 <- Convolution23
I0825 11:10:30.402850  1691 net.cpp:558] Eltwise11 <- Convolution25
I0825 11:10:30.402858  1691 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:10:30.402885  1691 net.cpp:172] Setting up Eltwise11
I0825 11:10:30.402894  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.402899  1691 net.cpp:194] Memory required for data: 389808384
I0825 11:10:30.402902  1691 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:10:30.402909  1691 net.cpp:128] Creating Layer ReLU23
I0825 11:10:30.402914  1691 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:10:30.402920  1691 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:10:30.404078  1691 net.cpp:172] Setting up ReLU23
I0825 11:10:30.404100  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.404105  1691 net.cpp:194] Memory required for data: 390856960
I0825 11:10:30.404110  1691 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:10:30.404121  1691 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:10:30.404162  1691 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:10:30.404173  1691 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:10:30.404182  1691 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:10:30.404235  1691 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:10:30.404244  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.404251  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.404254  1691 net.cpp:194] Memory required for data: 392954112
I0825 11:10:30.404259  1691 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:10:30.404271  1691 net.cpp:128] Creating Layer Convolution26
I0825 11:10:30.404276  1691 net.cpp:558] Convolution26 <- Eltwise11_ReLU23_0_split_0
I0825 11:10:30.404284  1691 net.cpp:522] Convolution26 -> Convolution26
I0825 11:10:30.410812  1691 net.cpp:172] Setting up Convolution26
I0825 11:10:30.410835  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.410858  1691 net.cpp:194] Memory required for data: 394002688
I0825 11:10:30.410869  1691 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:10:30.410881  1691 net.cpp:128] Creating Layer BatchNorm26
I0825 11:10:30.410887  1691 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:10:30.410897  1691 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:10:30.411159  1691 net.cpp:172] Setting up BatchNorm26
I0825 11:10:30.411170  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.411175  1691 net.cpp:194] Memory required for data: 395051264
I0825 11:10:30.411185  1691 layer_factory.hpp:77] Creating layer Scale26
I0825 11:10:30.411191  1691 net.cpp:128] Creating Layer Scale26
I0825 11:10:30.411195  1691 net.cpp:558] Scale26 <- Convolution26
I0825 11:10:30.411203  1691 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:10:30.411247  1691 layer_factory.hpp:77] Creating layer Scale26
I0825 11:10:30.411393  1691 net.cpp:172] Setting up Scale26
I0825 11:10:30.411404  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.411408  1691 net.cpp:194] Memory required for data: 396099840
I0825 11:10:30.411417  1691 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:10:30.411422  1691 net.cpp:128] Creating Layer ReLU24
I0825 11:10:30.411427  1691 net.cpp:558] ReLU24 <- Convolution26
I0825 11:10:30.411435  1691 net.cpp:509] ReLU24 -> Convolution26 (in-place)
I0825 11:10:30.412850  1691 net.cpp:172] Setting up ReLU24
I0825 11:10:30.412869  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.412873  1691 net.cpp:194] Memory required for data: 397148416
I0825 11:10:30.412878  1691 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:10:30.412895  1691 net.cpp:128] Creating Layer Convolution27
I0825 11:10:30.412904  1691 net.cpp:558] Convolution27 <- Convolution26
I0825 11:10:30.412914  1691 net.cpp:522] Convolution27 -> Convolution27
I0825 11:10:30.419809  1691 net.cpp:172] Setting up Convolution27
I0825 11:10:30.419836  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.419840  1691 net.cpp:194] Memory required for data: 398196992
I0825 11:10:30.419855  1691 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:10:30.419867  1691 net.cpp:128] Creating Layer BatchNorm27
I0825 11:10:30.419875  1691 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:10:30.419888  1691 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:10:30.420159  1691 net.cpp:172] Setting up BatchNorm27
I0825 11:10:30.420171  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.420176  1691 net.cpp:194] Memory required for data: 399245568
I0825 11:10:30.420186  1691 layer_factory.hpp:77] Creating layer Scale27
I0825 11:10:30.420204  1691 net.cpp:128] Creating Layer Scale27
I0825 11:10:30.420209  1691 net.cpp:558] Scale27 <- Convolution27
I0825 11:10:30.420215  1691 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:10:30.420260  1691 layer_factory.hpp:77] Creating layer Scale27
I0825 11:10:30.420409  1691 net.cpp:172] Setting up Scale27
I0825 11:10:30.420420  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.420424  1691 net.cpp:194] Memory required for data: 400294144
I0825 11:10:30.420433  1691 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:10:30.420440  1691 net.cpp:128] Creating Layer Eltwise12
I0825 11:10:30.420444  1691 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:10:30.420450  1691 net.cpp:558] Eltwise12 <- Convolution27
I0825 11:10:30.420456  1691 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:10:30.420490  1691 net.cpp:172] Setting up Eltwise12
I0825 11:10:30.420497  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.420501  1691 net.cpp:194] Memory required for data: 401342720
I0825 11:10:30.420506  1691 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:10:30.420512  1691 net.cpp:128] Creating Layer ReLU25
I0825 11:10:30.420516  1691 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:10:30.420522  1691 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:10:30.421663  1691 net.cpp:172] Setting up ReLU25
I0825 11:10:30.421696  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.421702  1691 net.cpp:194] Memory required for data: 402391296
I0825 11:10:30.421706  1691 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:10:30.421717  1691 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:10:30.421722  1691 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:10:30.421730  1691 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:10:30.421741  1691 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:10:30.421794  1691 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:10:30.421802  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.421808  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.421811  1691 net.cpp:194] Memory required for data: 404488448
I0825 11:10:30.421815  1691 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:10:30.421828  1691 net.cpp:128] Creating Layer Convolution28
I0825 11:10:30.421833  1691 net.cpp:558] Convolution28 <- Eltwise12_ReLU25_0_split_0
I0825 11:10:30.421842  1691 net.cpp:522] Convolution28 -> Convolution28
I0825 11:10:30.426964  1691 net.cpp:172] Setting up Convolution28
I0825 11:10:30.426990  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.426993  1691 net.cpp:194] Memory required for data: 405537024
I0825 11:10:30.427004  1691 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:10:30.427016  1691 net.cpp:128] Creating Layer BatchNorm28
I0825 11:10:30.427021  1691 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:10:30.427031  1691 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:10:30.427291  1691 net.cpp:172] Setting up BatchNorm28
I0825 11:10:30.427304  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.427307  1691 net.cpp:194] Memory required for data: 406585600
I0825 11:10:30.427317  1691 layer_factory.hpp:77] Creating layer Scale28
I0825 11:10:30.427325  1691 net.cpp:128] Creating Layer Scale28
I0825 11:10:30.427328  1691 net.cpp:558] Scale28 <- Convolution28
I0825 11:10:30.427336  1691 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:10:30.427381  1691 layer_factory.hpp:77] Creating layer Scale28
I0825 11:10:30.427531  1691 net.cpp:172] Setting up Scale28
I0825 11:10:30.427538  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.427542  1691 net.cpp:194] Memory required for data: 407634176
I0825 11:10:30.427551  1691 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:10:30.427556  1691 net.cpp:128] Creating Layer ReLU26
I0825 11:10:30.427561  1691 net.cpp:558] ReLU26 <- Convolution28
I0825 11:10:30.427568  1691 net.cpp:509] ReLU26 -> Convolution28 (in-place)
I0825 11:10:30.428036  1691 net.cpp:172] Setting up ReLU26
I0825 11:10:30.428053  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.428061  1691 net.cpp:194] Memory required for data: 408682752
I0825 11:10:30.428066  1691 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:10:30.428079  1691 net.cpp:128] Creating Layer Convolution29
I0825 11:10:30.428086  1691 net.cpp:558] Convolution29 <- Convolution28
I0825 11:10:30.428095  1691 net.cpp:522] Convolution29 -> Convolution29
I0825 11:10:30.430455  1691 net.cpp:172] Setting up Convolution29
I0825 11:10:30.430477  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.430482  1691 net.cpp:194] Memory required for data: 409731328
I0825 11:10:30.430492  1691 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:10:30.430502  1691 net.cpp:128] Creating Layer BatchNorm29
I0825 11:10:30.430508  1691 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:10:30.430517  1691 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:10:30.430793  1691 net.cpp:172] Setting up BatchNorm29
I0825 11:10:30.430802  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.430806  1691 net.cpp:194] Memory required for data: 410779904
I0825 11:10:30.430816  1691 layer_factory.hpp:77] Creating layer Scale29
I0825 11:10:30.430826  1691 net.cpp:128] Creating Layer Scale29
I0825 11:10:30.430850  1691 net.cpp:558] Scale29 <- Convolution29
I0825 11:10:30.430856  1691 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:10:30.430905  1691 layer_factory.hpp:77] Creating layer Scale29
I0825 11:10:30.431063  1691 net.cpp:172] Setting up Scale29
I0825 11:10:30.431073  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.431077  1691 net.cpp:194] Memory required for data: 411828480
I0825 11:10:30.431085  1691 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:10:30.431093  1691 net.cpp:128] Creating Layer Eltwise13
I0825 11:10:30.431098  1691 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:10:30.431103  1691 net.cpp:558] Eltwise13 <- Convolution29
I0825 11:10:30.431113  1691 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:10:30.431139  1691 net.cpp:172] Setting up Eltwise13
I0825 11:10:30.431149  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.431152  1691 net.cpp:194] Memory required for data: 412877056
I0825 11:10:30.431156  1691 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:10:30.431165  1691 net.cpp:128] Creating Layer ReLU27
I0825 11:10:30.431170  1691 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:10:30.431175  1691 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:10:30.431417  1691 net.cpp:172] Setting up ReLU27
I0825 11:10:30.431432  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.431435  1691 net.cpp:194] Memory required for data: 413925632
I0825 11:10:30.431440  1691 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:10:30.431452  1691 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:10:30.431457  1691 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:10:30.431465  1691 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:10:30.431474  1691 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:10:30.431529  1691 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:10:30.431541  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.431547  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.431551  1691 net.cpp:194] Memory required for data: 416022784
I0825 11:10:30.431555  1691 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:10:30.431567  1691 net.cpp:128] Creating Layer Convolution30
I0825 11:10:30.431572  1691 net.cpp:558] Convolution30 <- Eltwise13_ReLU27_0_split_0
I0825 11:10:30.431581  1691 net.cpp:522] Convolution30 -> Convolution30
I0825 11:10:30.437121  1691 net.cpp:172] Setting up Convolution30
I0825 11:10:30.437152  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.437157  1691 net.cpp:194] Memory required for data: 417071360
I0825 11:10:30.437170  1691 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:10:30.437178  1691 net.cpp:128] Creating Layer BatchNorm30
I0825 11:10:30.437183  1691 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:10:30.437193  1691 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:10:30.437463  1691 net.cpp:172] Setting up BatchNorm30
I0825 11:10:30.437475  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.437479  1691 net.cpp:194] Memory required for data: 418119936
I0825 11:10:30.437489  1691 layer_factory.hpp:77] Creating layer Scale30
I0825 11:10:30.437497  1691 net.cpp:128] Creating Layer Scale30
I0825 11:10:30.437501  1691 net.cpp:558] Scale30 <- Convolution30
I0825 11:10:30.437510  1691 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:10:30.437556  1691 layer_factory.hpp:77] Creating layer Scale30
I0825 11:10:30.437711  1691 net.cpp:172] Setting up Scale30
I0825 11:10:30.437721  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.437726  1691 net.cpp:194] Memory required for data: 419168512
I0825 11:10:30.437733  1691 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:10:30.437739  1691 net.cpp:128] Creating Layer ReLU28
I0825 11:10:30.437744  1691 net.cpp:558] ReLU28 <- Convolution30
I0825 11:10:30.437752  1691 net.cpp:509] ReLU28 -> Convolution30 (in-place)
I0825 11:10:30.439146  1691 net.cpp:172] Setting up ReLU28
I0825 11:10:30.439162  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.439167  1691 net.cpp:194] Memory required for data: 420217088
I0825 11:10:30.439172  1691 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:10:30.439190  1691 net.cpp:128] Creating Layer Convolution31
I0825 11:10:30.439198  1691 net.cpp:558] Convolution31 <- Convolution30
I0825 11:10:30.439206  1691 net.cpp:522] Convolution31 -> Convolution31
I0825 11:10:30.446106  1691 net.cpp:172] Setting up Convolution31
I0825 11:10:30.446132  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.446137  1691 net.cpp:194] Memory required for data: 421265664
I0825 11:10:30.446152  1691 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:10:30.446163  1691 net.cpp:128] Creating Layer BatchNorm31
I0825 11:10:30.446169  1691 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:10:30.446177  1691 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:10:30.446483  1691 net.cpp:172] Setting up BatchNorm31
I0825 11:10:30.446492  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.446497  1691 net.cpp:194] Memory required for data: 422314240
I0825 11:10:30.446507  1691 layer_factory.hpp:77] Creating layer Scale31
I0825 11:10:30.446516  1691 net.cpp:128] Creating Layer Scale31
I0825 11:10:30.446519  1691 net.cpp:558] Scale31 <- Convolution31
I0825 11:10:30.446528  1691 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:10:30.446574  1691 layer_factory.hpp:77] Creating layer Scale31
I0825 11:10:30.446730  1691 net.cpp:172] Setting up Scale31
I0825 11:10:30.446738  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.446743  1691 net.cpp:194] Memory required for data: 423362816
I0825 11:10:30.446750  1691 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:10:30.446759  1691 net.cpp:128] Creating Layer Eltwise14
I0825 11:10:30.446764  1691 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:10:30.446769  1691 net.cpp:558] Eltwise14 <- Convolution31
I0825 11:10:30.446777  1691 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:10:30.446807  1691 net.cpp:172] Setting up Eltwise14
I0825 11:10:30.446815  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.446818  1691 net.cpp:194] Memory required for data: 424411392
I0825 11:10:30.446822  1691 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:10:30.446828  1691 net.cpp:128] Creating Layer ReLU29
I0825 11:10:30.446833  1691 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:10:30.446841  1691 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:10:30.447963  1691 net.cpp:172] Setting up ReLU29
I0825 11:10:30.447979  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.447984  1691 net.cpp:194] Memory required for data: 425459968
I0825 11:10:30.447989  1691 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:10:30.448001  1691 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:10:30.448006  1691 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:10:30.448016  1691 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:10:30.448024  1691 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:10:30.448081  1691 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:10:30.448088  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.448094  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.448098  1691 net.cpp:194] Memory required for data: 427557120
I0825 11:10:30.448102  1691 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:10:30.448117  1691 net.cpp:128] Creating Layer Convolution32
I0825 11:10:30.448120  1691 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_0
I0825 11:10:30.448130  1691 net.cpp:522] Convolution32 -> Convolution32
I0825 11:10:30.454710  1691 net.cpp:172] Setting up Convolution32
I0825 11:10:30.454735  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.454741  1691 net.cpp:194] Memory required for data: 428605696
I0825 11:10:30.454766  1691 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:10:30.454777  1691 net.cpp:128] Creating Layer BatchNorm32
I0825 11:10:30.454785  1691 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:10:30.454792  1691 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:10:30.455068  1691 net.cpp:172] Setting up BatchNorm32
I0825 11:10:30.455078  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.455083  1691 net.cpp:194] Memory required for data: 429654272
I0825 11:10:30.455093  1691 layer_factory.hpp:77] Creating layer Scale32
I0825 11:10:30.455099  1691 net.cpp:128] Creating Layer Scale32
I0825 11:10:30.455104  1691 net.cpp:558] Scale32 <- Convolution32
I0825 11:10:30.455111  1691 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:10:30.455159  1691 layer_factory.hpp:77] Creating layer Scale32
I0825 11:10:30.455312  1691 net.cpp:172] Setting up Scale32
I0825 11:10:30.455323  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.455328  1691 net.cpp:194] Memory required for data: 430702848
I0825 11:10:30.455335  1691 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:10:30.455343  1691 net.cpp:128] Creating Layer ReLU30
I0825 11:10:30.455346  1691 net.cpp:558] ReLU30 <- Convolution32
I0825 11:10:30.455351  1691 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0825 11:10:30.455817  1691 net.cpp:172] Setting up ReLU30
I0825 11:10:30.455839  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.455843  1691 net.cpp:194] Memory required for data: 431751424
I0825 11:10:30.455848  1691 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:10:30.455862  1691 net.cpp:128] Creating Layer Convolution33
I0825 11:10:30.455868  1691 net.cpp:558] Convolution33 <- Convolution32
I0825 11:10:30.455878  1691 net.cpp:522] Convolution33 -> Convolution33
I0825 11:10:30.461261  1691 net.cpp:172] Setting up Convolution33
I0825 11:10:30.461287  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.461292  1691 net.cpp:194] Memory required for data: 432800000
I0825 11:10:30.461305  1691 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:10:30.461316  1691 net.cpp:128] Creating Layer BatchNorm33
I0825 11:10:30.461324  1691 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:10:30.461334  1691 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:10:30.461616  1691 net.cpp:172] Setting up BatchNorm33
I0825 11:10:30.461627  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.461632  1691 net.cpp:194] Memory required for data: 433848576
I0825 11:10:30.461642  1691 layer_factory.hpp:77] Creating layer Scale33
I0825 11:10:30.461648  1691 net.cpp:128] Creating Layer Scale33
I0825 11:10:30.461652  1691 net.cpp:558] Scale33 <- Convolution33
I0825 11:10:30.461658  1691 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:10:30.461711  1691 layer_factory.hpp:77] Creating layer Scale33
I0825 11:10:30.461866  1691 net.cpp:172] Setting up Scale33
I0825 11:10:30.461876  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.461881  1691 net.cpp:194] Memory required for data: 434897152
I0825 11:10:30.461889  1691 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:10:30.461899  1691 net.cpp:128] Creating Layer Eltwise15
I0825 11:10:30.461905  1691 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0825 11:10:30.461910  1691 net.cpp:558] Eltwise15 <- Convolution33
I0825 11:10:30.461915  1691 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:10:30.461944  1691 net.cpp:172] Setting up Eltwise15
I0825 11:10:30.461952  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.461956  1691 net.cpp:194] Memory required for data: 435945728
I0825 11:10:30.461961  1691 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:10:30.461966  1691 net.cpp:128] Creating Layer ReLU31
I0825 11:10:30.461971  1691 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:10:30.461977  1691 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:10:30.463074  1691 net.cpp:172] Setting up ReLU31
I0825 11:10:30.463090  1691 net.cpp:186] Top shape: 64 64 8 8 (262144)
I0825 11:10:30.463095  1691 net.cpp:194] Memory required for data: 436994304
I0825 11:10:30.463120  1691 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:10:30.463135  1691 net.cpp:128] Creating Layer Pooling1
I0825 11:10:30.463140  1691 net.cpp:558] Pooling1 <- Eltwise15
I0825 11:10:30.463150  1691 net.cpp:522] Pooling1 -> Pooling1
I0825 11:10:30.465342  1691 net.cpp:172] Setting up Pooling1
I0825 11:10:30.465361  1691 net.cpp:186] Top shape: 64 64 1 1 (4096)
I0825 11:10:30.465366  1691 net.cpp:194] Memory required for data: 437010688
I0825 11:10:30.465370  1691 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:10:30.465382  1691 net.cpp:128] Creating Layer InnerProduct1
I0825 11:10:30.465389  1691 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:10:30.465397  1691 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:10:30.465584  1691 net.cpp:172] Setting up InnerProduct1
I0825 11:10:30.465591  1691 net.cpp:186] Top shape: 64 10 (640)
I0825 11:10:30.465595  1691 net.cpp:194] Memory required for data: 437013248
I0825 11:10:30.465605  1691 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:10:30.465615  1691 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:10:30.465620  1691 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0825 11:10:30.465625  1691 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0825 11:10:30.465632  1691 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:10:30.465643  1691 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:10:30.467625  1691 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:10:30.467651  1691 net.cpp:186] Top shape: (1)
I0825 11:10:30.467656  1691 net.cpp:189]     with loss weight 1
I0825 11:10:30.467687  1691 net.cpp:194] Memory required for data: 437013252
I0825 11:10:30.467697  1691 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:10:30.467706  1691 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:10:30.467710  1691 net.cpp:301] Pooling1 needs backward computation.
I0825 11:10:30.467715  1691 net.cpp:301] ReLU31 needs backward computation.
I0825 11:10:30.467720  1691 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:10:30.467725  1691 net.cpp:301] Scale33 needs backward computation.
I0825 11:10:30.467728  1691 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:10:30.467732  1691 net.cpp:301] Convolution33 needs backward computation.
I0825 11:10:30.467736  1691 net.cpp:301] ReLU30 needs backward computation.
I0825 11:10:30.467741  1691 net.cpp:301] Scale32 needs backward computation.
I0825 11:10:30.467746  1691 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:10:30.467749  1691 net.cpp:301] Convolution32 needs backward computation.
I0825 11:10:30.467753  1691 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:10:30.467758  1691 net.cpp:301] ReLU29 needs backward computation.
I0825 11:10:30.467762  1691 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:10:30.467767  1691 net.cpp:301] Scale31 needs backward computation.
I0825 11:10:30.467772  1691 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:10:30.467777  1691 net.cpp:301] Convolution31 needs backward computation.
I0825 11:10:30.467780  1691 net.cpp:301] ReLU28 needs backward computation.
I0825 11:10:30.467784  1691 net.cpp:301] Scale30 needs backward computation.
I0825 11:10:30.467788  1691 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:10:30.467792  1691 net.cpp:301] Convolution30 needs backward computation.
I0825 11:10:30.467797  1691 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:10:30.467803  1691 net.cpp:301] ReLU27 needs backward computation.
I0825 11:10:30.467808  1691 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:10:30.467811  1691 net.cpp:301] Scale29 needs backward computation.
I0825 11:10:30.467816  1691 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:10:30.467820  1691 net.cpp:301] Convolution29 needs backward computation.
I0825 11:10:30.467828  1691 net.cpp:301] ReLU26 needs backward computation.
I0825 11:10:30.467846  1691 net.cpp:301] Scale28 needs backward computation.
I0825 11:10:30.467851  1691 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:10:30.467855  1691 net.cpp:301] Convolution28 needs backward computation.
I0825 11:10:30.467861  1691 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:10:30.467865  1691 net.cpp:301] ReLU25 needs backward computation.
I0825 11:10:30.467870  1691 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:10:30.467876  1691 net.cpp:301] Scale27 needs backward computation.
I0825 11:10:30.467880  1691 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:10:30.467885  1691 net.cpp:301] Convolution27 needs backward computation.
I0825 11:10:30.467890  1691 net.cpp:301] ReLU24 needs backward computation.
I0825 11:10:30.467893  1691 net.cpp:301] Scale26 needs backward computation.
I0825 11:10:30.467898  1691 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:10:30.467902  1691 net.cpp:301] Convolution26 needs backward computation.
I0825 11:10:30.467907  1691 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:10:30.467912  1691 net.cpp:301] ReLU23 needs backward computation.
I0825 11:10:30.467917  1691 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:10:30.467922  1691 net.cpp:301] Scale25 needs backward computation.
I0825 11:10:30.467926  1691 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:10:30.467931  1691 net.cpp:301] Convolution25 needs backward computation.
I0825 11:10:30.467936  1691 net.cpp:301] ReLU22 needs backward computation.
I0825 11:10:30.467941  1691 net.cpp:301] Scale24 needs backward computation.
I0825 11:10:30.467944  1691 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:10:30.467949  1691 net.cpp:301] Convolution24 needs backward computation.
I0825 11:10:30.467954  1691 net.cpp:301] Scale23 needs backward computation.
I0825 11:10:30.467958  1691 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:10:30.467962  1691 net.cpp:301] Convolution23 needs backward computation.
I0825 11:10:30.467967  1691 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:10:30.467972  1691 net.cpp:301] ReLU21 needs backward computation.
I0825 11:10:30.467978  1691 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:10:30.467983  1691 net.cpp:301] Scale22 needs backward computation.
I0825 11:10:30.467986  1691 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:10:30.467991  1691 net.cpp:301] Convolution22 needs backward computation.
I0825 11:10:30.467995  1691 net.cpp:301] ReLU20 needs backward computation.
I0825 11:10:30.468000  1691 net.cpp:301] Scale21 needs backward computation.
I0825 11:10:30.468004  1691 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:10:30.468009  1691 net.cpp:301] Convolution21 needs backward computation.
I0825 11:10:30.468014  1691 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:10:30.468019  1691 net.cpp:301] ReLU19 needs backward computation.
I0825 11:10:30.468026  1691 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:10:30.468031  1691 net.cpp:301] Scale20 needs backward computation.
I0825 11:10:30.468036  1691 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:10:30.468040  1691 net.cpp:301] Convolution20 needs backward computation.
I0825 11:10:30.468045  1691 net.cpp:301] ReLU18 needs backward computation.
I0825 11:10:30.468050  1691 net.cpp:301] Scale19 needs backward computation.
I0825 11:10:30.468055  1691 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:10:30.468058  1691 net.cpp:301] Convolution19 needs backward computation.
I0825 11:10:30.468063  1691 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:10:30.468068  1691 net.cpp:301] ReLU17 needs backward computation.
I0825 11:10:30.468073  1691 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:10:30.468078  1691 net.cpp:301] Scale18 needs backward computation.
I0825 11:10:30.468083  1691 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:10:30.468093  1691 net.cpp:301] Convolution18 needs backward computation.
I0825 11:10:30.468098  1691 net.cpp:301] ReLU16 needs backward computation.
I0825 11:10:30.468102  1691 net.cpp:301] Scale17 needs backward computation.
I0825 11:10:30.468107  1691 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:10:30.468111  1691 net.cpp:301] Convolution17 needs backward computation.
I0825 11:10:30.468116  1691 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:10:30.468120  1691 net.cpp:301] ReLU15 needs backward computation.
I0825 11:10:30.468125  1691 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:10:30.468130  1691 net.cpp:301] Scale16 needs backward computation.
I0825 11:10:30.468135  1691 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:10:30.468148  1691 net.cpp:301] Convolution16 needs backward computation.
I0825 11:10:30.468153  1691 net.cpp:301] ReLU14 needs backward computation.
I0825 11:10:30.468158  1691 net.cpp:301] Scale15 needs backward computation.
I0825 11:10:30.468163  1691 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:10:30.468166  1691 net.cpp:301] Convolution15 needs backward computation.
I0825 11:10:30.468173  1691 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:10:30.468176  1691 net.cpp:301] ReLU13 needs backward computation.
I0825 11:10:30.468181  1691 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:10:30.468186  1691 net.cpp:301] Scale14 needs backward computation.
I0825 11:10:30.468190  1691 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:10:30.468194  1691 net.cpp:301] Convolution14 needs backward computation.
I0825 11:10:30.468199  1691 net.cpp:301] ReLU12 needs backward computation.
I0825 11:10:30.468204  1691 net.cpp:301] Scale13 needs backward computation.
I0825 11:10:30.468209  1691 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:10:30.468214  1691 net.cpp:301] Convolution13 needs backward computation.
I0825 11:10:30.468219  1691 net.cpp:301] Scale12 needs backward computation.
I0825 11:10:30.468222  1691 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:10:30.468226  1691 net.cpp:301] Convolution12 needs backward computation.
I0825 11:10:30.468231  1691 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:10:30.468236  1691 net.cpp:301] ReLU11 needs backward computation.
I0825 11:10:30.468241  1691 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:10:30.468246  1691 net.cpp:301] Scale11 needs backward computation.
I0825 11:10:30.468251  1691 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:10:30.468255  1691 net.cpp:301] Convolution11 needs backward computation.
I0825 11:10:30.468264  1691 net.cpp:301] ReLU10 needs backward computation.
I0825 11:10:30.468269  1691 net.cpp:301] Scale10 needs backward computation.
I0825 11:10:30.468273  1691 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:10:30.468278  1691 net.cpp:301] Convolution10 needs backward computation.
I0825 11:10:30.468283  1691 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:10:30.468288  1691 net.cpp:301] ReLU9 needs backward computation.
I0825 11:10:30.468293  1691 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:10:30.468298  1691 net.cpp:301] Scale9 needs backward computation.
I0825 11:10:30.468303  1691 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:10:30.468308  1691 net.cpp:301] Convolution9 needs backward computation.
I0825 11:10:30.468312  1691 net.cpp:301] ReLU8 needs backward computation.
I0825 11:10:30.468317  1691 net.cpp:301] Scale8 needs backward computation.
I0825 11:10:30.468322  1691 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:10:30.468327  1691 net.cpp:301] Convolution8 needs backward computation.
I0825 11:10:30.468331  1691 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:10:30.468336  1691 net.cpp:301] ReLU7 needs backward computation.
I0825 11:10:30.468340  1691 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:10:30.468353  1691 net.cpp:301] Scale7 needs backward computation.
I0825 11:10:30.468358  1691 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:10:30.468364  1691 net.cpp:301] Convolution7 needs backward computation.
I0825 11:10:30.468367  1691 net.cpp:301] ReLU6 needs backward computation.
I0825 11:10:30.468371  1691 net.cpp:301] Scale6 needs backward computation.
I0825 11:10:30.468376  1691 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:10:30.468380  1691 net.cpp:301] Convolution6 needs backward computation.
I0825 11:10:30.468385  1691 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:10:30.468390  1691 net.cpp:301] ReLU5 needs backward computation.
I0825 11:10:30.468394  1691 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:10:30.468400  1691 net.cpp:301] Scale5 needs backward computation.
I0825 11:10:30.468405  1691 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:10:30.468410  1691 net.cpp:301] Convolution5 needs backward computation.
I0825 11:10:30.468415  1691 net.cpp:301] ReLU4 needs backward computation.
I0825 11:10:30.468418  1691 net.cpp:301] Scale4 needs backward computation.
I0825 11:10:30.468423  1691 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:10:30.468427  1691 net.cpp:301] Convolution4 needs backward computation.
I0825 11:10:30.468432  1691 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:10:30.468437  1691 net.cpp:301] ReLU3 needs backward computation.
I0825 11:10:30.468441  1691 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:10:30.468448  1691 net.cpp:301] Scale3 needs backward computation.
I0825 11:10:30.468453  1691 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:10:30.468458  1691 net.cpp:301] Convolution3 needs backward computation.
I0825 11:10:30.468463  1691 net.cpp:301] ReLU2 needs backward computation.
I0825 11:10:30.468467  1691 net.cpp:301] Scale2 needs backward computation.
I0825 11:10:30.468472  1691 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:10:30.468477  1691 net.cpp:301] Convolution2 needs backward computation.
I0825 11:10:30.468482  1691 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:10:30.468487  1691 net.cpp:301] ReLU1 needs backward computation.
I0825 11:10:30.468492  1691 net.cpp:301] Scale1 needs backward computation.
I0825 11:10:30.468495  1691 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:10:30.468499  1691 net.cpp:301] Convolution1 needs backward computation.
I0825 11:10:30.468505  1691 net.cpp:303] Data1 does not need backward computation.
I0825 11:10:30.468509  1691 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:10:30.468596  1691 net.cpp:363] Network initialization done.
I0825 11:10:30.470264  1691 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_32.prototxt
I0825 11:10:30.470283  1691 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0825 11:10:30.470291  1691 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_32.prototxt
I0825 11:10:30.470449  1691 net.cpp:390] layer_param.include_size():1
I0825 11:10:30.470458  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470463  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470468  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470471  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470475  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470480  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470484  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470489  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470491  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470495  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470499  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470504  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470522  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470527  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470531  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470535  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470538  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470543  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470546  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470551  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470554  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470558  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470562  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470567  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470571  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470576  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470578  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470582  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470587  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470592  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470594  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470598  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470602  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470607  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470610  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470614  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470618  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470623  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470626  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470630  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470634  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470638  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470643  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470646  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470650  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470654  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470657  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470661  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470665  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470669  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470674  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470677  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470681  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470685  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470690  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470693  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470697  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470701  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470705  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470710  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470713  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470717  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470721  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470724  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470728  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470733  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470736  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470741  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470744  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470748  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470752  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470763  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470767  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470772  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470775  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470779  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470783  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470788  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470791  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470795  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470798  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470803  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470806  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470811  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470814  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470818  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470823  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470826  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470830  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470834  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470837  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470842  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470846  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470850  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470854  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470857  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470861  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470865  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470870  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470873  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470876  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470880  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470885  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470888  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470892  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470896  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470901  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470904  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470908  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470912  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470916  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470921  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470923  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470928  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470932  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470935  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470939  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470943  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470947  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470952  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470955  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470959  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470963  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470968  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470970  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470974  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470978  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470983  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.470986  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.470996  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471000  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471004  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471009  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471012  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471016  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471020  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471024  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471029  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471032  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471036  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471040  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471045  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471048  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471052  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471056  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471060  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471063  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471067  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471071  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471076  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471079  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471083  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471087  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471091  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471096  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471099  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471102  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471107  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471110  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471114  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471118  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471122  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471127  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471130  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471134  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471138  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471141  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471145  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471149  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471153  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471158  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471161  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471165  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471169  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471174  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471177  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471180  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471184  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471189  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471194  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471197  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471202  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471205  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471210  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471213  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471217  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471221  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471225  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471235  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471240  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471243  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471247  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471251  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471256  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471258  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471262  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471266  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471271  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471274  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471278  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471282  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471287  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471290  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471294  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471298  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471302  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471307  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471310  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471313  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471318  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471321  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471325  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471329  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471333  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471338  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471341  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471345  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471349  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471354  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471356  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471360  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471364  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471369  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471372  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471376  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471380  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471385  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471388  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471392  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471396  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471400  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471403  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471407  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471411  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471415  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471421  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471423  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471427  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471431  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471436  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471439  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471443  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471447  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471451  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471455  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471459  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471462  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471474  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471478  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471482  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471485  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471489  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471493  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471498  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471501  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471505  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471508  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471513  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471516  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471520  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471524  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471529  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471532  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471536  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471540  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471544  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471549  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471552  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471555  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471560  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471563  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471567  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471571  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471575  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471580  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471583  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471587  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471591  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471595  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471598  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471602  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471606  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471611  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471614  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471618  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471622  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471626  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471629  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471633  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471637  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471642  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471645  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471649  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471653  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471657  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471662  1691 net.cpp:390] layer_param.include_size():0
I0825 11:10:30.471664  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.471669  1691 net.cpp:390] layer_param.include_size():1
I0825 11:10:30.471673  1691 net.cpp:391] layer_param.exclude_size():0
I0825 11:10:30.472602  1691 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution24"
  top: "Convolution24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Convolution24"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution25"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution26"
  top: "Convolution26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Convolution26"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution27"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution28"
  top: "Convolution28"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Convolution28"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution29"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution30"
  top: "Convolution30"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Convolution30"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution31"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msr
I0825 11:10:30.473148  1691 layer_factory.hpp:77] Creating layer Data1
I0825 11:10:30.473228  1691 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0825 11:10:30.473258  1691 net.cpp:128] Creating Layer Data1
I0825 11:10:30.473264  1691 net.cpp:522] Data1 -> Data1
I0825 11:10:30.473275  1691 net.cpp:522] Data1 -> Data2
I0825 11:10:30.473431  1691 data_layer.cpp:45] output data size: 10,3,32,32
I0825 11:10:30.482631  1691 net.cpp:172] Setting up Data1
I0825 11:10:30.482651  1691 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0825 11:10:30.482657  1691 net.cpp:186] Top shape: 10 (10)
I0825 11:10:30.482661  1691 net.cpp:194] Memory required for data: 122920
I0825 11:10:30.482666  1691 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0825 11:10:30.482676  1691 net.cpp:128] Creating Layer Data2_Data1_1_split
I0825 11:10:30.482681  1691 net.cpp:558] Data2_Data1_1_split <- Data2
I0825 11:10:30.482692  1691 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0825 11:10:30.482702  1691 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0825 11:10:30.482758  1691 net.cpp:172] Setting up Data2_Data1_1_split
I0825 11:10:30.482764  1691 net.cpp:186] Top shape: 10 (10)
I0825 11:10:30.482769  1691 net.cpp:186] Top shape: 10 (10)
I0825 11:10:30.482774  1691 net.cpp:194] Memory required for data: 123000
I0825 11:10:30.482777  1691 layer_factory.hpp:77] Creating layer Convolution1
I0825 11:10:30.482792  1691 net.cpp:128] Creating Layer Convolution1
I0825 11:10:30.482797  1691 net.cpp:558] Convolution1 <- Data1
I0825 11:10:30.482807  1691 net.cpp:522] Convolution1 -> Convolution1
I0825 11:10:30.489440  1691 net.cpp:172] Setting up Convolution1
I0825 11:10:30.489468  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.489473  1691 net.cpp:194] Memory required for data: 778360
I0825 11:10:30.489490  1691 layer_factory.hpp:77] Creating layer BatchNorm1
I0825 11:10:30.489503  1691 net.cpp:128] Creating Layer BatchNorm1
I0825 11:10:30.489511  1691 net.cpp:558] BatchNorm1 <- Convolution1
I0825 11:10:30.489521  1691 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0825 11:10:30.491498  1691 net.cpp:172] Setting up BatchNorm1
I0825 11:10:30.491528  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.491531  1691 net.cpp:194] Memory required for data: 1433720
I0825 11:10:30.491547  1691 layer_factory.hpp:77] Creating layer Scale1
I0825 11:10:30.491559  1691 net.cpp:128] Creating Layer Scale1
I0825 11:10:30.491567  1691 net.cpp:558] Scale1 <- Convolution1
I0825 11:10:30.491577  1691 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0825 11:10:30.491632  1691 layer_factory.hpp:77] Creating layer Scale1
I0825 11:10:30.491809  1691 net.cpp:172] Setting up Scale1
I0825 11:10:30.491822  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.491825  1691 net.cpp:194] Memory required for data: 2089080
I0825 11:10:30.491834  1691 layer_factory.hpp:77] Creating layer ReLU1
I0825 11:10:30.491850  1691 net.cpp:128] Creating Layer ReLU1
I0825 11:10:30.491855  1691 net.cpp:558] ReLU1 <- Convolution1
I0825 11:10:30.491861  1691 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0825 11:10:30.492272  1691 net.cpp:172] Setting up ReLU1
I0825 11:10:30.492291  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.492296  1691 net.cpp:194] Memory required for data: 2744440
I0825 11:10:30.492301  1691 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0825 11:10:30.492310  1691 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0825 11:10:30.492314  1691 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0825 11:10:30.492321  1691 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0825 11:10:30.492332  1691 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0825 11:10:30.492389  1691 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0825 11:10:30.492399  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.492405  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.492411  1691 net.cpp:194] Memory required for data: 4055160
I0825 11:10:30.492415  1691 layer_factory.hpp:77] Creating layer Convolution2
I0825 11:10:30.492434  1691 net.cpp:128] Creating Layer Convolution2
I0825 11:10:30.492450  1691 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0825 11:10:30.492486  1691 net.cpp:522] Convolution2 -> Convolution2
I0825 11:10:30.498602  1691 net.cpp:172] Setting up Convolution2
I0825 11:10:30.498627  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.498633  1691 net.cpp:194] Memory required for data: 4710520
I0825 11:10:30.498648  1691 layer_factory.hpp:77] Creating layer BatchNorm2
I0825 11:10:30.498666  1691 net.cpp:128] Creating Layer BatchNorm2
I0825 11:10:30.498677  1691 net.cpp:558] BatchNorm2 <- Convolution2
I0825 11:10:30.498687  1691 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0825 11:10:30.499033  1691 net.cpp:172] Setting up BatchNorm2
I0825 11:10:30.499045  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.499052  1691 net.cpp:194] Memory required for data: 5365880
I0825 11:10:30.499063  1691 layer_factory.hpp:77] Creating layer Scale2
I0825 11:10:30.499074  1691 net.cpp:128] Creating Layer Scale2
I0825 11:10:30.499080  1691 net.cpp:558] Scale2 <- Convolution2
I0825 11:10:30.499088  1691 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0825 11:10:30.499150  1691 layer_factory.hpp:77] Creating layer Scale2
I0825 11:10:30.499342  1691 net.cpp:172] Setting up Scale2
I0825 11:10:30.499356  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.499361  1691 net.cpp:194] Memory required for data: 6021240
I0825 11:10:30.499372  1691 layer_factory.hpp:77] Creating layer ReLU2
I0825 11:10:30.499382  1691 net.cpp:128] Creating Layer ReLU2
I0825 11:10:30.499387  1691 net.cpp:558] ReLU2 <- Convolution2
I0825 11:10:30.499394  1691 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0825 11:10:30.500201  1691 net.cpp:172] Setting up ReLU2
I0825 11:10:30.500221  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.500226  1691 net.cpp:194] Memory required for data: 6676600
I0825 11:10:30.500236  1691 layer_factory.hpp:77] Creating layer Convolution3
I0825 11:10:30.500257  1691 net.cpp:128] Creating Layer Convolution3
I0825 11:10:30.500265  1691 net.cpp:558] Convolution3 <- Convolution2
I0825 11:10:30.500275  1691 net.cpp:522] Convolution3 -> Convolution3
I0825 11:10:30.507020  1691 net.cpp:172] Setting up Convolution3
I0825 11:10:30.507055  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.507066  1691 net.cpp:194] Memory required for data: 7331960
I0825 11:10:30.507081  1691 layer_factory.hpp:77] Creating layer BatchNorm3
I0825 11:10:30.507097  1691 net.cpp:128] Creating Layer BatchNorm3
I0825 11:10:30.507113  1691 net.cpp:558] BatchNorm3 <- Convolution3
I0825 11:10:30.507128  1691 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0825 11:10:30.507546  1691 net.cpp:172] Setting up BatchNorm3
I0825 11:10:30.507563  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.507570  1691 net.cpp:194] Memory required for data: 7987320
I0825 11:10:30.507591  1691 layer_factory.hpp:77] Creating layer Scale3
I0825 11:10:30.507622  1691 net.cpp:128] Creating Layer Scale3
I0825 11:10:30.507630  1691 net.cpp:558] Scale3 <- Convolution3
I0825 11:10:30.507639  1691 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0825 11:10:30.507720  1691 layer_factory.hpp:77] Creating layer Scale3
I0825 11:10:30.507958  1691 net.cpp:172] Setting up Scale3
I0825 11:10:30.507977  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.507983  1691 net.cpp:194] Memory required for data: 8642680
I0825 11:10:30.507997  1691 layer_factory.hpp:77] Creating layer Eltwise1
I0825 11:10:30.508008  1691 net.cpp:128] Creating Layer Eltwise1
I0825 11:10:30.508014  1691 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0825 11:10:30.508021  1691 net.cpp:558] Eltwise1 <- Convolution3
I0825 11:10:30.508034  1691 net.cpp:522] Eltwise1 -> Eltwise1
I0825 11:10:30.508077  1691 net.cpp:172] Setting up Eltwise1
I0825 11:10:30.508088  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.508095  1691 net.cpp:194] Memory required for data: 9298040
I0825 11:10:30.508101  1691 layer_factory.hpp:77] Creating layer ReLU3
I0825 11:10:30.508113  1691 net.cpp:128] Creating Layer ReLU3
I0825 11:10:30.508121  1691 net.cpp:558] ReLU3 <- Eltwise1
I0825 11:10:30.508131  1691 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0825 11:10:30.509008  1691 net.cpp:172] Setting up ReLU3
I0825 11:10:30.509040  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.509048  1691 net.cpp:194] Memory required for data: 9953400
I0825 11:10:30.509055  1691 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0825 11:10:30.509073  1691 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0825 11:10:30.509081  1691 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0825 11:10:30.509093  1691 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0825 11:10:30.509105  1691 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0825 11:10:30.509191  1691 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0825 11:10:30.509205  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.509214  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.509220  1691 net.cpp:194] Memory required for data: 11264120
I0825 11:10:30.509227  1691 layer_factory.hpp:77] Creating layer Convolution4
I0825 11:10:30.509245  1691 net.cpp:128] Creating Layer Convolution4
I0825 11:10:30.509253  1691 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0825 11:10:30.509266  1691 net.cpp:522] Convolution4 -> Convolution4
I0825 11:10:30.515735  1691 net.cpp:172] Setting up Convolution4
I0825 11:10:30.515761  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.515765  1691 net.cpp:194] Memory required for data: 11919480
I0825 11:10:30.515777  1691 layer_factory.hpp:77] Creating layer BatchNorm4
I0825 11:10:30.515789  1691 net.cpp:128] Creating Layer BatchNorm4
I0825 11:10:30.515794  1691 net.cpp:558] BatchNorm4 <- Convolution4
I0825 11:10:30.515808  1691 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0825 11:10:30.516088  1691 net.cpp:172] Setting up BatchNorm4
I0825 11:10:30.516099  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.516104  1691 net.cpp:194] Memory required for data: 12574840
I0825 11:10:30.516114  1691 layer_factory.hpp:77] Creating layer Scale4
I0825 11:10:30.516124  1691 net.cpp:128] Creating Layer Scale4
I0825 11:10:30.516127  1691 net.cpp:558] Scale4 <- Convolution4
I0825 11:10:30.516134  1691 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0825 11:10:30.516188  1691 layer_factory.hpp:77] Creating layer Scale4
I0825 11:10:30.516342  1691 net.cpp:172] Setting up Scale4
I0825 11:10:30.516350  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.516353  1691 net.cpp:194] Memory required for data: 13230200
I0825 11:10:30.516361  1691 layer_factory.hpp:77] Creating layer ReLU4
I0825 11:10:30.516369  1691 net.cpp:128] Creating Layer ReLU4
I0825 11:10:30.516374  1691 net.cpp:558] ReLU4 <- Convolution4
I0825 11:10:30.516381  1691 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0825 11:10:30.517765  1691 net.cpp:172] Setting up ReLU4
I0825 11:10:30.517796  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.517801  1691 net.cpp:194] Memory required for data: 13885560
I0825 11:10:30.517805  1691 layer_factory.hpp:77] Creating layer Convolution5
I0825 11:10:30.517827  1691 net.cpp:128] Creating Layer Convolution5
I0825 11:10:30.517832  1691 net.cpp:558] Convolution5 <- Convolution4
I0825 11:10:30.517839  1691 net.cpp:522] Convolution5 -> Convolution5
I0825 11:10:30.524507  1691 net.cpp:172] Setting up Convolution5
I0825 11:10:30.524533  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.524538  1691 net.cpp:194] Memory required for data: 14540920
I0825 11:10:30.524549  1691 layer_factory.hpp:77] Creating layer BatchNorm5
I0825 11:10:30.524559  1691 net.cpp:128] Creating Layer BatchNorm5
I0825 11:10:30.524564  1691 net.cpp:558] BatchNorm5 <- Convolution5
I0825 11:10:30.524572  1691 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0825 11:10:30.524864  1691 net.cpp:172] Setting up BatchNorm5
I0825 11:10:30.524876  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.524883  1691 net.cpp:194] Memory required for data: 15196280
I0825 11:10:30.524897  1691 layer_factory.hpp:77] Creating layer Scale5
I0825 11:10:30.524904  1691 net.cpp:128] Creating Layer Scale5
I0825 11:10:30.524909  1691 net.cpp:558] Scale5 <- Convolution5
I0825 11:10:30.524914  1691 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0825 11:10:30.524968  1691 layer_factory.hpp:77] Creating layer Scale5
I0825 11:10:30.525123  1691 net.cpp:172] Setting up Scale5
I0825 11:10:30.525131  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.525135  1691 net.cpp:194] Memory required for data: 15851640
I0825 11:10:30.525143  1691 layer_factory.hpp:77] Creating layer Eltwise2
I0825 11:10:30.525152  1691 net.cpp:128] Creating Layer Eltwise2
I0825 11:10:30.525157  1691 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0825 11:10:30.525162  1691 net.cpp:558] Eltwise2 <- Convolution5
I0825 11:10:30.525168  1691 net.cpp:522] Eltwise2 -> Eltwise2
I0825 11:10:30.525198  1691 net.cpp:172] Setting up Eltwise2
I0825 11:10:30.525205  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.525209  1691 net.cpp:194] Memory required for data: 16507000
I0825 11:10:30.525213  1691 layer_factory.hpp:77] Creating layer ReLU5
I0825 11:10:30.525219  1691 net.cpp:128] Creating Layer ReLU5
I0825 11:10:30.525224  1691 net.cpp:558] ReLU5 <- Eltwise2
I0825 11:10:30.525229  1691 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0825 11:10:30.526584  1691 net.cpp:172] Setting up ReLU5
I0825 11:10:30.526602  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.526605  1691 net.cpp:194] Memory required for data: 17162360
I0825 11:10:30.526610  1691 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0825 11:10:30.526618  1691 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0825 11:10:30.526623  1691 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0825 11:10:30.526633  1691 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0825 11:10:30.526640  1691 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0825 11:10:30.526701  1691 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0825 11:10:30.526708  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.526715  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.526720  1691 net.cpp:194] Memory required for data: 18473080
I0825 11:10:30.526724  1691 layer_factory.hpp:77] Creating layer Convolution6
I0825 11:10:30.526738  1691 net.cpp:128] Creating Layer Convolution6
I0825 11:10:30.526743  1691 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0825 11:10:30.526751  1691 net.cpp:522] Convolution6 -> Convolution6
I0825 11:10:30.533303  1691 net.cpp:172] Setting up Convolution6
I0825 11:10:30.533329  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.533334  1691 net.cpp:194] Memory required for data: 19128440
I0825 11:10:30.533344  1691 layer_factory.hpp:77] Creating layer BatchNorm6
I0825 11:10:30.533354  1691 net.cpp:128] Creating Layer BatchNorm6
I0825 11:10:30.533377  1691 net.cpp:558] BatchNorm6 <- Convolution6
I0825 11:10:30.533385  1691 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0825 11:10:30.533676  1691 net.cpp:172] Setting up BatchNorm6
I0825 11:10:30.533689  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.533694  1691 net.cpp:194] Memory required for data: 19783800
I0825 11:10:30.533704  1691 layer_factory.hpp:77] Creating layer Scale6
I0825 11:10:30.533713  1691 net.cpp:128] Creating Layer Scale6
I0825 11:10:30.533717  1691 net.cpp:558] Scale6 <- Convolution6
I0825 11:10:30.533723  1691 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0825 11:10:30.533774  1691 layer_factory.hpp:77] Creating layer Scale6
I0825 11:10:30.533932  1691 net.cpp:172] Setting up Scale6
I0825 11:10:30.533939  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.533943  1691 net.cpp:194] Memory required for data: 20439160
I0825 11:10:30.533951  1691 layer_factory.hpp:77] Creating layer ReLU6
I0825 11:10:30.533958  1691 net.cpp:128] Creating Layer ReLU6
I0825 11:10:30.533962  1691 net.cpp:558] ReLU6 <- Convolution6
I0825 11:10:30.533969  1691 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0825 11:10:30.535373  1691 net.cpp:172] Setting up ReLU6
I0825 11:10:30.535389  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.535393  1691 net.cpp:194] Memory required for data: 21094520
I0825 11:10:30.535398  1691 layer_factory.hpp:77] Creating layer Convolution7
I0825 11:10:30.535410  1691 net.cpp:128] Creating Layer Convolution7
I0825 11:10:30.535416  1691 net.cpp:558] Convolution7 <- Convolution6
I0825 11:10:30.535426  1691 net.cpp:522] Convolution7 -> Convolution7
I0825 11:10:30.538182  1691 net.cpp:172] Setting up Convolution7
I0825 11:10:30.538208  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.538213  1691 net.cpp:194] Memory required for data: 21749880
I0825 11:10:30.538223  1691 layer_factory.hpp:77] Creating layer BatchNorm7
I0825 11:10:30.538235  1691 net.cpp:128] Creating Layer BatchNorm7
I0825 11:10:30.538240  1691 net.cpp:558] BatchNorm7 <- Convolution7
I0825 11:10:30.538247  1691 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0825 11:10:30.538545  1691 net.cpp:172] Setting up BatchNorm7
I0825 11:10:30.538558  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.538563  1691 net.cpp:194] Memory required for data: 22405240
I0825 11:10:30.538573  1691 layer_factory.hpp:77] Creating layer Scale7
I0825 11:10:30.538583  1691 net.cpp:128] Creating Layer Scale7
I0825 11:10:30.538588  1691 net.cpp:558] Scale7 <- Convolution7
I0825 11:10:30.538594  1691 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0825 11:10:30.538645  1691 layer_factory.hpp:77] Creating layer Scale7
I0825 11:10:30.538805  1691 net.cpp:172] Setting up Scale7
I0825 11:10:30.538817  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.538821  1691 net.cpp:194] Memory required for data: 23060600
I0825 11:10:30.538830  1691 layer_factory.hpp:77] Creating layer Eltwise3
I0825 11:10:30.538838  1691 net.cpp:128] Creating Layer Eltwise3
I0825 11:10:30.538844  1691 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0825 11:10:30.538849  1691 net.cpp:558] Eltwise3 <- Convolution7
I0825 11:10:30.538856  1691 net.cpp:522] Eltwise3 -> Eltwise3
I0825 11:10:30.538884  1691 net.cpp:172] Setting up Eltwise3
I0825 11:10:30.538892  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.538895  1691 net.cpp:194] Memory required for data: 23715960
I0825 11:10:30.538899  1691 layer_factory.hpp:77] Creating layer ReLU7
I0825 11:10:30.538906  1691 net.cpp:128] Creating Layer ReLU7
I0825 11:10:30.538910  1691 net.cpp:558] ReLU7 <- Eltwise3
I0825 11:10:30.538918  1691 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0825 11:10:30.539397  1691 net.cpp:172] Setting up ReLU7
I0825 11:10:30.539422  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.539427  1691 net.cpp:194] Memory required for data: 24371320
I0825 11:10:30.539433  1691 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0825 11:10:30.539460  1691 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0825 11:10:30.539469  1691 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0825 11:10:30.539476  1691 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0825 11:10:30.539486  1691 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0825 11:10:30.539548  1691 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0825 11:10:30.539559  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.539566  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.539569  1691 net.cpp:194] Memory required for data: 25682040
I0825 11:10:30.539573  1691 layer_factory.hpp:77] Creating layer Convolution8
I0825 11:10:30.539585  1691 net.cpp:128] Creating Layer Convolution8
I0825 11:10:30.539590  1691 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0825 11:10:30.539599  1691 net.cpp:522] Convolution8 -> Convolution8
I0825 11:10:30.544337  1691 net.cpp:172] Setting up Convolution8
I0825 11:10:30.544363  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.544368  1691 net.cpp:194] Memory required for data: 26337400
I0825 11:10:30.544380  1691 layer_factory.hpp:77] Creating layer BatchNorm8
I0825 11:10:30.544389  1691 net.cpp:128] Creating Layer BatchNorm8
I0825 11:10:30.544395  1691 net.cpp:558] BatchNorm8 <- Convolution8
I0825 11:10:30.544404  1691 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0825 11:10:30.544697  1691 net.cpp:172] Setting up BatchNorm8
I0825 11:10:30.544708  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.544713  1691 net.cpp:194] Memory required for data: 26992760
I0825 11:10:30.544723  1691 layer_factory.hpp:77] Creating layer Scale8
I0825 11:10:30.544730  1691 net.cpp:128] Creating Layer Scale8
I0825 11:10:30.544741  1691 net.cpp:558] Scale8 <- Convolution8
I0825 11:10:30.544749  1691 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0825 11:10:30.544801  1691 layer_factory.hpp:77] Creating layer Scale8
I0825 11:10:30.544970  1691 net.cpp:172] Setting up Scale8
I0825 11:10:30.544984  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.544988  1691 net.cpp:194] Memory required for data: 27648120
I0825 11:10:30.544997  1691 layer_factory.hpp:77] Creating layer ReLU8
I0825 11:10:30.545004  1691 net.cpp:128] Creating Layer ReLU8
I0825 11:10:30.545009  1691 net.cpp:558] ReLU8 <- Convolution8
I0825 11:10:30.545014  1691 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0825 11:10:30.546388  1691 net.cpp:172] Setting up ReLU8
I0825 11:10:30.546409  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.546413  1691 net.cpp:194] Memory required for data: 28303480
I0825 11:10:30.546418  1691 layer_factory.hpp:77] Creating layer Convolution9
I0825 11:10:30.546432  1691 net.cpp:128] Creating Layer Convolution9
I0825 11:10:30.546437  1691 net.cpp:558] Convolution9 <- Convolution8
I0825 11:10:30.546448  1691 net.cpp:522] Convolution9 -> Convolution9
I0825 11:10:30.553375  1691 net.cpp:172] Setting up Convolution9
I0825 11:10:30.553400  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.553406  1691 net.cpp:194] Memory required for data: 28958840
I0825 11:10:30.553416  1691 layer_factory.hpp:77] Creating layer BatchNorm9
I0825 11:10:30.553426  1691 net.cpp:128] Creating Layer BatchNorm9
I0825 11:10:30.553431  1691 net.cpp:558] BatchNorm9 <- Convolution9
I0825 11:10:30.553441  1691 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0825 11:10:30.553741  1691 net.cpp:172] Setting up BatchNorm9
I0825 11:10:30.553753  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.553757  1691 net.cpp:194] Memory required for data: 29614200
I0825 11:10:30.553768  1691 layer_factory.hpp:77] Creating layer Scale9
I0825 11:10:30.553778  1691 net.cpp:128] Creating Layer Scale9
I0825 11:10:30.553782  1691 net.cpp:558] Scale9 <- Convolution9
I0825 11:10:30.553788  1691 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0825 11:10:30.553841  1691 layer_factory.hpp:77] Creating layer Scale9
I0825 11:10:30.554008  1691 net.cpp:172] Setting up Scale9
I0825 11:10:30.554038  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.554042  1691 net.cpp:194] Memory required for data: 30269560
I0825 11:10:30.554051  1691 layer_factory.hpp:77] Creating layer Eltwise4
I0825 11:10:30.554061  1691 net.cpp:128] Creating Layer Eltwise4
I0825 11:10:30.554067  1691 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0825 11:10:30.554072  1691 net.cpp:558] Eltwise4 <- Convolution9
I0825 11:10:30.554080  1691 net.cpp:522] Eltwise4 -> Eltwise4
I0825 11:10:30.554111  1691 net.cpp:172] Setting up Eltwise4
I0825 11:10:30.554119  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.554123  1691 net.cpp:194] Memory required for data: 30924920
I0825 11:10:30.554128  1691 layer_factory.hpp:77] Creating layer ReLU9
I0825 11:10:30.554136  1691 net.cpp:128] Creating Layer ReLU9
I0825 11:10:30.554139  1691 net.cpp:558] ReLU9 <- Eltwise4
I0825 11:10:30.554147  1691 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0825 11:10:30.555177  1691 net.cpp:172] Setting up ReLU9
I0825 11:10:30.555193  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.555199  1691 net.cpp:194] Memory required for data: 31580280
I0825 11:10:30.555204  1691 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0825 11:10:30.555217  1691 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0825 11:10:30.555220  1691 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0825 11:10:30.555230  1691 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0825 11:10:30.555239  1691 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0825 11:10:30.555299  1691 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0825 11:10:30.555306  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.555312  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.555316  1691 net.cpp:194] Memory required for data: 32891000
I0825 11:10:30.555320  1691 layer_factory.hpp:77] Creating layer Convolution10
I0825 11:10:30.555335  1691 net.cpp:128] Creating Layer Convolution10
I0825 11:10:30.555338  1691 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0825 11:10:30.555348  1691 net.cpp:522] Convolution10 -> Convolution10
I0825 11:10:30.561895  1691 net.cpp:172] Setting up Convolution10
I0825 11:10:30.561920  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.561925  1691 net.cpp:194] Memory required for data: 33546360
I0825 11:10:30.561947  1691 layer_factory.hpp:77] Creating layer BatchNorm10
I0825 11:10:30.561957  1691 net.cpp:128] Creating Layer BatchNorm10
I0825 11:10:30.561962  1691 net.cpp:558] BatchNorm10 <- Convolution10
I0825 11:10:30.561975  1691 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0825 11:10:30.562263  1691 net.cpp:172] Setting up BatchNorm10
I0825 11:10:30.562275  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.562279  1691 net.cpp:194] Memory required for data: 34201720
I0825 11:10:30.562289  1691 layer_factory.hpp:77] Creating layer Scale10
I0825 11:10:30.562296  1691 net.cpp:128] Creating Layer Scale10
I0825 11:10:30.562301  1691 net.cpp:558] Scale10 <- Convolution10
I0825 11:10:30.562306  1691 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0825 11:10:30.562376  1691 layer_factory.hpp:77] Creating layer Scale10
I0825 11:10:30.562535  1691 net.cpp:172] Setting up Scale10
I0825 11:10:30.562542  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.562546  1691 net.cpp:194] Memory required for data: 34857080
I0825 11:10:30.562554  1691 layer_factory.hpp:77] Creating layer ReLU10
I0825 11:10:30.562561  1691 net.cpp:128] Creating Layer ReLU10
I0825 11:10:30.562568  1691 net.cpp:558] ReLU10 <- Convolution10
I0825 11:10:30.562574  1691 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0825 11:10:30.563957  1691 net.cpp:172] Setting up ReLU10
I0825 11:10:30.563982  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.563985  1691 net.cpp:194] Memory required for data: 35512440
I0825 11:10:30.563992  1691 layer_factory.hpp:77] Creating layer Convolution11
I0825 11:10:30.564005  1691 net.cpp:128] Creating Layer Convolution11
I0825 11:10:30.564026  1691 net.cpp:558] Convolution11 <- Convolution10
I0825 11:10:30.564040  1691 net.cpp:522] Convolution11 -> Convolution11
I0825 11:10:30.567396  1691 net.cpp:172] Setting up Convolution11
I0825 11:10:30.567423  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.567427  1691 net.cpp:194] Memory required for data: 36167800
I0825 11:10:30.567437  1691 layer_factory.hpp:77] Creating layer BatchNorm11
I0825 11:10:30.567445  1691 net.cpp:128] Creating Layer BatchNorm11
I0825 11:10:30.567451  1691 net.cpp:558] BatchNorm11 <- Convolution11
I0825 11:10:30.567459  1691 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0825 11:10:30.567749  1691 net.cpp:172] Setting up BatchNorm11
I0825 11:10:30.567762  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.567767  1691 net.cpp:194] Memory required for data: 36823160
I0825 11:10:30.567777  1691 layer_factory.hpp:77] Creating layer Scale11
I0825 11:10:30.567785  1691 net.cpp:128] Creating Layer Scale11
I0825 11:10:30.567790  1691 net.cpp:558] Scale11 <- Convolution11
I0825 11:10:30.567796  1691 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0825 11:10:30.567847  1691 layer_factory.hpp:77] Creating layer Scale11
I0825 11:10:30.568006  1691 net.cpp:172] Setting up Scale11
I0825 11:10:30.568013  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.568017  1691 net.cpp:194] Memory required for data: 37478520
I0825 11:10:30.568025  1691 layer_factory.hpp:77] Creating layer Eltwise5
I0825 11:10:30.568032  1691 net.cpp:128] Creating Layer Eltwise5
I0825 11:10:30.568037  1691 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0825 11:10:30.568042  1691 net.cpp:558] Eltwise5 <- Convolution11
I0825 11:10:30.568050  1691 net.cpp:522] Eltwise5 -> Eltwise5
I0825 11:10:30.568079  1691 net.cpp:172] Setting up Eltwise5
I0825 11:10:30.568090  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.568094  1691 net.cpp:194] Memory required for data: 38133880
I0825 11:10:30.568099  1691 layer_factory.hpp:77] Creating layer ReLU11
I0825 11:10:30.568105  1691 net.cpp:128] Creating Layer ReLU11
I0825 11:10:30.568109  1691 net.cpp:558] ReLU11 <- Eltwise5
I0825 11:10:30.568115  1691 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0825 11:10:30.568363  1691 net.cpp:172] Setting up ReLU11
I0825 11:10:30.568374  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.568378  1691 net.cpp:194] Memory required for data: 38789240
I0825 11:10:30.568383  1691 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0825 11:10:30.568392  1691 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0825 11:10:30.568397  1691 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0825 11:10:30.568405  1691 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0825 11:10:30.568415  1691 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0825 11:10:30.568470  1691 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0825 11:10:30.568477  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.568483  1691 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0825 11:10:30.568487  1691 net.cpp:194] Memory required for data: 40099960
I0825 11:10:30.568491  1691 layer_factory.hpp:77] Creating layer Convolution12
I0825 11:10:30.568506  1691 net.cpp:128] Creating Layer Convolution12
I0825 11:10:30.568511  1691 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0825 11:10:30.568517  1691 net.cpp:522] Convolution12 -> Convolution12
I0825 11:10:30.574651  1691 net.cpp:172] Setting up Convolution12
I0825 11:10:30.574677  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.574682  1691 net.cpp:194] Memory required for data: 40427640
I0825 11:10:30.574692  1691 layer_factory.hpp:77] Creating layer BatchNorm12
I0825 11:10:30.574703  1691 net.cpp:128] Creating Layer BatchNorm12
I0825 11:10:30.574709  1691 net.cpp:558] BatchNorm12 <- Convolution12
I0825 11:10:30.574718  1691 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0825 11:10:30.575002  1691 net.cpp:172] Setting up BatchNorm12
I0825 11:10:30.575028  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.575033  1691 net.cpp:194] Memory required for data: 40755320
I0825 11:10:30.575044  1691 layer_factory.hpp:77] Creating layer Scale12
I0825 11:10:30.575052  1691 net.cpp:128] Creating Layer Scale12
I0825 11:10:30.575055  1691 net.cpp:558] Scale12 <- Convolution12
I0825 11:10:30.575062  1691 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0825 11:10:30.575116  1691 layer_factory.hpp:77] Creating layer Scale12
I0825 11:10:30.575276  1691 net.cpp:172] Setting up Scale12
I0825 11:10:30.575284  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.575289  1691 net.cpp:194] Memory required for data: 41083000
I0825 11:10:30.575296  1691 layer_factory.hpp:77] Creating layer Convolution13
I0825 11:10:30.575309  1691 net.cpp:128] Creating Layer Convolution13
I0825 11:10:30.575314  1691 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_1
I0825 11:10:30.575323  1691 net.cpp:522] Convolution13 -> Convolution13
I0825 11:10:30.581200  1691 net.cpp:172] Setting up Convolution13
I0825 11:10:30.581226  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.581231  1691 net.cpp:194] Memory required for data: 41410680
I0825 11:10:30.581241  1691 layer_factory.hpp:77] Creating layer BatchNorm13
I0825 11:10:30.581254  1691 net.cpp:128] Creating Layer BatchNorm13
I0825 11:10:30.581259  1691 net.cpp:558] BatchNorm13 <- Convolution13
I0825 11:10:30.581265  1691 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0825 11:10:30.581557  1691 net.cpp:172] Setting up BatchNorm13
I0825 11:10:30.581568  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.581571  1691 net.cpp:194] Memory required for data: 41738360
I0825 11:10:30.581581  1691 layer_factory.hpp:77] Creating layer Scale13
I0825 11:10:30.581588  1691 net.cpp:128] Creating Layer Scale13
I0825 11:10:30.581593  1691 net.cpp:558] Scale13 <- Convolution13
I0825 11:10:30.581598  1691 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0825 11:10:30.581653  1691 layer_factory.hpp:77] Creating layer Scale13
I0825 11:10:30.581816  1691 net.cpp:172] Setting up Scale13
I0825 11:10:30.581825  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.581828  1691 net.cpp:194] Memory required for data: 42066040
I0825 11:10:30.581836  1691 layer_factory.hpp:77] Creating layer ReLU12
I0825 11:10:30.581843  1691 net.cpp:128] Creating Layer ReLU12
I0825 11:10:30.581847  1691 net.cpp:558] ReLU12 <- Convolution13
I0825 11:10:30.581856  1691 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0825 11:10:30.583264  1691 net.cpp:172] Setting up ReLU12
I0825 11:10:30.583281  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.583284  1691 net.cpp:194] Memory required for data: 42393720
I0825 11:10:30.583289  1691 layer_factory.hpp:77] Creating layer Convolution14
I0825 11:10:30.583315  1691 net.cpp:128] Creating Layer Convolution14
I0825 11:10:30.583320  1691 net.cpp:558] Convolution14 <- Convolution13
I0825 11:10:30.583328  1691 net.cpp:522] Convolution14 -> Convolution14
I0825 11:10:30.590020  1691 net.cpp:172] Setting up Convolution14
I0825 11:10:30.590046  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.590051  1691 net.cpp:194] Memory required for data: 42721400
I0825 11:10:30.590061  1691 layer_factory.hpp:77] Creating layer BatchNorm14
I0825 11:10:30.590072  1691 net.cpp:128] Creating Layer BatchNorm14
I0825 11:10:30.590078  1691 net.cpp:558] BatchNorm14 <- Convolution14
I0825 11:10:30.590086  1691 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0825 11:10:30.590384  1691 net.cpp:172] Setting up BatchNorm14
I0825 11:10:30.590396  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.590400  1691 net.cpp:194] Memory required for data: 43049080
I0825 11:10:30.590410  1691 layer_factory.hpp:77] Creating layer Scale14
I0825 11:10:30.590417  1691 net.cpp:128] Creating Layer Scale14
I0825 11:10:30.590421  1691 net.cpp:558] Scale14 <- Convolution14
I0825 11:10:30.590430  1691 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0825 11:10:30.590482  1691 layer_factory.hpp:77] Creating layer Scale14
I0825 11:10:30.590669  1691 net.cpp:172] Setting up Scale14
I0825 11:10:30.590687  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.590692  1691 net.cpp:194] Memory required for data: 43376760
I0825 11:10:30.590700  1691 layer_factory.hpp:77] Creating layer Eltwise6
I0825 11:10:30.590708  1691 net.cpp:128] Creating Layer Eltwise6
I0825 11:10:30.590713  1691 net.cpp:558] Eltwise6 <- Convolution12
I0825 11:10:30.590718  1691 net.cpp:558] Eltwise6 <- Convolution14
I0825 11:10:30.590726  1691 net.cpp:522] Eltwise6 -> Eltwise6
I0825 11:10:30.590751  1691 net.cpp:172] Setting up Eltwise6
I0825 11:10:30.590761  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.590765  1691 net.cpp:194] Memory required for data: 43704440
I0825 11:10:30.590770  1691 layer_factory.hpp:77] Creating layer ReLU13
I0825 11:10:30.590776  1691 net.cpp:128] Creating Layer ReLU13
I0825 11:10:30.590780  1691 net.cpp:558] ReLU13 <- Eltwise6
I0825 11:10:30.590786  1691 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0825 11:10:30.592070  1691 net.cpp:172] Setting up ReLU13
I0825 11:10:30.592094  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.592099  1691 net.cpp:194] Memory required for data: 44032120
I0825 11:10:30.592104  1691 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0825 11:10:30.592115  1691 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0825 11:10:30.592120  1691 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0825 11:10:30.592130  1691 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0825 11:10:30.592139  1691 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0825 11:10:30.592198  1691 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0825 11:10:30.592208  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.592214  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.592218  1691 net.cpp:194] Memory required for data: 44687480
I0825 11:10:30.592222  1691 layer_factory.hpp:77] Creating layer Convolution15
I0825 11:10:30.592236  1691 net.cpp:128] Creating Layer Convolution15
I0825 11:10:30.592241  1691 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0825 11:10:30.592247  1691 net.cpp:522] Convolution15 -> Convolution15
I0825 11:10:30.598994  1691 net.cpp:172] Setting up Convolution15
I0825 11:10:30.599020  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.599025  1691 net.cpp:194] Memory required for data: 45015160
I0825 11:10:30.599035  1691 layer_factory.hpp:77] Creating layer BatchNorm15
I0825 11:10:30.599045  1691 net.cpp:128] Creating Layer BatchNorm15
I0825 11:10:30.599051  1691 net.cpp:558] BatchNorm15 <- Convolution15
I0825 11:10:30.599061  1691 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0825 11:10:30.599349  1691 net.cpp:172] Setting up BatchNorm15
I0825 11:10:30.599362  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.599366  1691 net.cpp:194] Memory required for data: 45342840
I0825 11:10:30.599376  1691 layer_factory.hpp:77] Creating layer Scale15
I0825 11:10:30.599385  1691 net.cpp:128] Creating Layer Scale15
I0825 11:10:30.599390  1691 net.cpp:558] Scale15 <- Convolution15
I0825 11:10:30.599396  1691 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0825 11:10:30.599450  1691 layer_factory.hpp:77] Creating layer Scale15
I0825 11:10:30.599617  1691 net.cpp:172] Setting up Scale15
I0825 11:10:30.599632  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.599637  1691 net.cpp:194] Memory required for data: 45670520
I0825 11:10:30.599645  1691 layer_factory.hpp:77] Creating layer ReLU14
I0825 11:10:30.599651  1691 net.cpp:128] Creating Layer ReLU14
I0825 11:10:30.599655  1691 net.cpp:558] ReLU14 <- Convolution15
I0825 11:10:30.599664  1691 net.cpp:509] ReLU14 -> Convolution15 (in-place)
I0825 11:10:30.600834  1691 net.cpp:172] Setting up ReLU14
I0825 11:10:30.600852  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.600857  1691 net.cpp:194] Memory required for data: 45998200
I0825 11:10:30.600862  1691 layer_factory.hpp:77] Creating layer Convolution16
I0825 11:10:30.600895  1691 net.cpp:128] Creating Layer Convolution16
I0825 11:10:30.600903  1691 net.cpp:558] Convolution16 <- Convolution15
I0825 11:10:30.600910  1691 net.cpp:522] Convolution16 -> Convolution16
I0825 11:10:30.607581  1691 net.cpp:172] Setting up Convolution16
I0825 11:10:30.607607  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.607612  1691 net.cpp:194] Memory required for data: 46325880
I0825 11:10:30.607622  1691 layer_factory.hpp:77] Creating layer BatchNorm16
I0825 11:10:30.607633  1691 net.cpp:128] Creating Layer BatchNorm16
I0825 11:10:30.607640  1691 net.cpp:558] BatchNorm16 <- Convolution16
I0825 11:10:30.607645  1691 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0825 11:10:30.607942  1691 net.cpp:172] Setting up BatchNorm16
I0825 11:10:30.607954  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.607959  1691 net.cpp:194] Memory required for data: 46653560
I0825 11:10:30.607969  1691 layer_factory.hpp:77] Creating layer Scale16
I0825 11:10:30.607975  1691 net.cpp:128] Creating Layer Scale16
I0825 11:10:30.607980  1691 net.cpp:558] Scale16 <- Convolution16
I0825 11:10:30.607990  1691 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0825 11:10:30.608042  1691 layer_factory.hpp:77] Creating layer Scale16
I0825 11:10:30.608204  1691 net.cpp:172] Setting up Scale16
I0825 11:10:30.608212  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.608217  1691 net.cpp:194] Memory required for data: 46981240
I0825 11:10:30.608224  1691 layer_factory.hpp:77] Creating layer Eltwise7
I0825 11:10:30.608232  1691 net.cpp:128] Creating Layer Eltwise7
I0825 11:10:30.608237  1691 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0825 11:10:30.608242  1691 net.cpp:558] Eltwise7 <- Convolution16
I0825 11:10:30.608249  1691 net.cpp:522] Eltwise7 -> Eltwise7
I0825 11:10:30.608273  1691 net.cpp:172] Setting up Eltwise7
I0825 11:10:30.608280  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.608284  1691 net.cpp:194] Memory required for data: 47308920
I0825 11:10:30.608289  1691 layer_factory.hpp:77] Creating layer ReLU15
I0825 11:10:30.608296  1691 net.cpp:128] Creating Layer ReLU15
I0825 11:10:30.608301  1691 net.cpp:558] ReLU15 <- Eltwise7
I0825 11:10:30.608306  1691 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0825 11:10:30.609609  1691 net.cpp:172] Setting up ReLU15
I0825 11:10:30.609625  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.609629  1691 net.cpp:194] Memory required for data: 47636600
I0825 11:10:30.609634  1691 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0825 11:10:30.609647  1691 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0825 11:10:30.609652  1691 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0825 11:10:30.609658  1691 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0825 11:10:30.609666  1691 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0825 11:10:30.609726  1691 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0825 11:10:30.609733  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.609740  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.609743  1691 net.cpp:194] Memory required for data: 48291960
I0825 11:10:30.609747  1691 layer_factory.hpp:77] Creating layer Convolution17
I0825 11:10:30.609760  1691 net.cpp:128] Creating Layer Convolution17
I0825 11:10:30.609766  1691 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_0
I0825 11:10:30.609774  1691 net.cpp:522] Convolution17 -> Convolution17
I0825 11:10:30.616354  1691 net.cpp:172] Setting up Convolution17
I0825 11:10:30.616380  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.616385  1691 net.cpp:194] Memory required for data: 48619640
I0825 11:10:30.616395  1691 layer_factory.hpp:77] Creating layer BatchNorm17
I0825 11:10:30.616408  1691 net.cpp:128] Creating Layer BatchNorm17
I0825 11:10:30.616413  1691 net.cpp:558] BatchNorm17 <- Convolution17
I0825 11:10:30.616422  1691 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0825 11:10:30.616731  1691 net.cpp:172] Setting up BatchNorm17
I0825 11:10:30.616744  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.616750  1691 net.cpp:194] Memory required for data: 48947320
I0825 11:10:30.616760  1691 layer_factory.hpp:77] Creating layer Scale17
I0825 11:10:30.616766  1691 net.cpp:128] Creating Layer Scale17
I0825 11:10:30.616770  1691 net.cpp:558] Scale17 <- Convolution17
I0825 11:10:30.616780  1691 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0825 11:10:30.616830  1691 layer_factory.hpp:77] Creating layer Scale17
I0825 11:10:30.616997  1691 net.cpp:172] Setting up Scale17
I0825 11:10:30.617008  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.617012  1691 net.cpp:194] Memory required for data: 49275000
I0825 11:10:30.617020  1691 layer_factory.hpp:77] Creating layer ReLU16
I0825 11:10:30.617027  1691 net.cpp:128] Creating Layer ReLU16
I0825 11:10:30.617031  1691 net.cpp:558] ReLU16 <- Convolution17
I0825 11:10:30.617039  1691 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0825 11:10:30.618398  1691 net.cpp:172] Setting up ReLU16
I0825 11:10:30.618415  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.618420  1691 net.cpp:194] Memory required for data: 49602680
I0825 11:10:30.618424  1691 layer_factory.hpp:77] Creating layer Convolution18
I0825 11:10:30.618440  1691 net.cpp:128] Creating Layer Convolution18
I0825 11:10:30.618446  1691 net.cpp:558] Convolution18 <- Convolution17
I0825 11:10:30.618455  1691 net.cpp:522] Convolution18 -> Convolution18
I0825 11:10:30.625149  1691 net.cpp:172] Setting up Convolution18
I0825 11:10:30.625175  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.625180  1691 net.cpp:194] Memory required for data: 49930360
I0825 11:10:30.625190  1691 layer_factory.hpp:77] Creating layer BatchNorm18
I0825 11:10:30.625200  1691 net.cpp:128] Creating Layer BatchNorm18
I0825 11:10:30.625205  1691 net.cpp:558] BatchNorm18 <- Convolution18
I0825 11:10:30.625213  1691 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0825 11:10:30.625510  1691 net.cpp:172] Setting up BatchNorm18
I0825 11:10:30.625524  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.625527  1691 net.cpp:194] Memory required for data: 50258040
I0825 11:10:30.625537  1691 layer_factory.hpp:77] Creating layer Scale18
I0825 11:10:30.625545  1691 net.cpp:128] Creating Layer Scale18
I0825 11:10:30.625550  1691 net.cpp:558] Scale18 <- Convolution18
I0825 11:10:30.625556  1691 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0825 11:10:30.625610  1691 layer_factory.hpp:77] Creating layer Scale18
I0825 11:10:30.625772  1691 net.cpp:172] Setting up Scale18
I0825 11:10:30.625785  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.625790  1691 net.cpp:194] Memory required for data: 50585720
I0825 11:10:30.625798  1691 layer_factory.hpp:77] Creating layer Eltwise8
I0825 11:10:30.625805  1691 net.cpp:128] Creating Layer Eltwise8
I0825 11:10:30.625810  1691 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0825 11:10:30.625815  1691 net.cpp:558] Eltwise8 <- Convolution18
I0825 11:10:30.625823  1691 net.cpp:522] Eltwise8 -> Eltwise8
I0825 11:10:30.625849  1691 net.cpp:172] Setting up Eltwise8
I0825 11:10:30.625856  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.625860  1691 net.cpp:194] Memory required for data: 50913400
I0825 11:10:30.625864  1691 layer_factory.hpp:77] Creating layer ReLU17
I0825 11:10:30.625870  1691 net.cpp:128] Creating Layer ReLU17
I0825 11:10:30.625875  1691 net.cpp:558] ReLU17 <- Eltwise8
I0825 11:10:30.625883  1691 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0825 11:10:30.627197  1691 net.cpp:172] Setting up ReLU17
I0825 11:10:30.627220  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.627225  1691 net.cpp:194] Memory required for data: 51241080
I0825 11:10:30.627229  1691 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0825 11:10:30.627238  1691 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0825 11:10:30.627243  1691 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0825 11:10:30.627267  1691 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0825 11:10:30.627276  1691 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0825 11:10:30.627343  1691 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0825 11:10:30.627351  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.627357  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.627360  1691 net.cpp:194] Memory required for data: 51896440
I0825 11:10:30.627365  1691 layer_factory.hpp:77] Creating layer Convolution19
I0825 11:10:30.627377  1691 net.cpp:128] Creating Layer Convolution19
I0825 11:10:30.627382  1691 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0825 11:10:30.627393  1691 net.cpp:522] Convolution19 -> Convolution19
I0825 11:10:30.633947  1691 net.cpp:172] Setting up Convolution19
I0825 11:10:30.633975  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.633978  1691 net.cpp:194] Memory required for data: 52224120
I0825 11:10:30.633988  1691 layer_factory.hpp:77] Creating layer BatchNorm19
I0825 11:10:30.633999  1691 net.cpp:128] Creating Layer BatchNorm19
I0825 11:10:30.634004  1691 net.cpp:558] BatchNorm19 <- Convolution19
I0825 11:10:30.634011  1691 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0825 11:10:30.634307  1691 net.cpp:172] Setting up BatchNorm19
I0825 11:10:30.634320  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.634323  1691 net.cpp:194] Memory required for data: 52551800
I0825 11:10:30.634363  1691 layer_factory.hpp:77] Creating layer Scale19
I0825 11:10:30.634371  1691 net.cpp:128] Creating Layer Scale19
I0825 11:10:30.634377  1691 net.cpp:558] Scale19 <- Convolution19
I0825 11:10:30.634384  1691 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0825 11:10:30.634441  1691 layer_factory.hpp:77] Creating layer Scale19
I0825 11:10:30.634608  1691 net.cpp:172] Setting up Scale19
I0825 11:10:30.634619  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.634624  1691 net.cpp:194] Memory required for data: 52879480
I0825 11:10:30.634632  1691 layer_factory.hpp:77] Creating layer ReLU18
I0825 11:10:30.634639  1691 net.cpp:128] Creating Layer ReLU18
I0825 11:10:30.634644  1691 net.cpp:558] ReLU18 <- Convolution19
I0825 11:10:30.634652  1691 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0825 11:10:30.635957  1691 net.cpp:172] Setting up ReLU18
I0825 11:10:30.635973  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.635979  1691 net.cpp:194] Memory required for data: 53207160
I0825 11:10:30.635984  1691 layer_factory.hpp:77] Creating layer Convolution20
I0825 11:10:30.636008  1691 net.cpp:128] Creating Layer Convolution20
I0825 11:10:30.636013  1691 net.cpp:558] Convolution20 <- Convolution19
I0825 11:10:30.636023  1691 net.cpp:522] Convolution20 -> Convolution20
I0825 11:10:30.642709  1691 net.cpp:172] Setting up Convolution20
I0825 11:10:30.642735  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.642740  1691 net.cpp:194] Memory required for data: 53534840
I0825 11:10:30.642750  1691 layer_factory.hpp:77] Creating layer BatchNorm20
I0825 11:10:30.642758  1691 net.cpp:128] Creating Layer BatchNorm20
I0825 11:10:30.642763  1691 net.cpp:558] BatchNorm20 <- Convolution20
I0825 11:10:30.642771  1691 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0825 11:10:30.643060  1691 net.cpp:172] Setting up BatchNorm20
I0825 11:10:30.643072  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.643077  1691 net.cpp:194] Memory required for data: 53862520
I0825 11:10:30.643087  1691 layer_factory.hpp:77] Creating layer Scale20
I0825 11:10:30.643095  1691 net.cpp:128] Creating Layer Scale20
I0825 11:10:30.643100  1691 net.cpp:558] Scale20 <- Convolution20
I0825 11:10:30.643106  1691 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0825 11:10:30.643163  1691 layer_factory.hpp:77] Creating layer Scale20
I0825 11:10:30.643328  1691 net.cpp:172] Setting up Scale20
I0825 11:10:30.643338  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.643357  1691 net.cpp:194] Memory required for data: 54190200
I0825 11:10:30.643366  1691 layer_factory.hpp:77] Creating layer Eltwise9
I0825 11:10:30.643375  1691 net.cpp:128] Creating Layer Eltwise9
I0825 11:10:30.643381  1691 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0825 11:10:30.643386  1691 net.cpp:558] Eltwise9 <- Convolution20
I0825 11:10:30.643393  1691 net.cpp:522] Eltwise9 -> Eltwise9
I0825 11:10:30.643421  1691 net.cpp:172] Setting up Eltwise9
I0825 11:10:30.643429  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.643434  1691 net.cpp:194] Memory required for data: 54517880
I0825 11:10:30.643437  1691 layer_factory.hpp:77] Creating layer ReLU19
I0825 11:10:30.643445  1691 net.cpp:128] Creating Layer ReLU19
I0825 11:10:30.643448  1691 net.cpp:558] ReLU19 <- Eltwise9
I0825 11:10:30.643456  1691 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0825 11:10:30.644770  1691 net.cpp:172] Setting up ReLU19
I0825 11:10:30.644785  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.644790  1691 net.cpp:194] Memory required for data: 54845560
I0825 11:10:30.644795  1691 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0825 11:10:30.644803  1691 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0825 11:10:30.644807  1691 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0825 11:10:30.644819  1691 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0825 11:10:30.644826  1691 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0825 11:10:30.644886  1691 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0825 11:10:30.644893  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.644898  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.644903  1691 net.cpp:194] Memory required for data: 55500920
I0825 11:10:30.644907  1691 layer_factory.hpp:77] Creating layer Convolution21
I0825 11:10:30.644920  1691 net.cpp:128] Creating Layer Convolution21
I0825 11:10:30.644925  1691 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0825 11:10:30.644934  1691 net.cpp:522] Convolution21 -> Convolution21
I0825 11:10:30.650153  1691 net.cpp:172] Setting up Convolution21
I0825 11:10:30.650179  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.650184  1691 net.cpp:194] Memory required for data: 55828600
I0825 11:10:30.650194  1691 layer_factory.hpp:77] Creating layer BatchNorm21
I0825 11:10:30.650203  1691 net.cpp:128] Creating Layer BatchNorm21
I0825 11:10:30.650208  1691 net.cpp:558] BatchNorm21 <- Convolution21
I0825 11:10:30.650218  1691 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0825 11:10:30.650524  1691 net.cpp:172] Setting up BatchNorm21
I0825 11:10:30.650537  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.650540  1691 net.cpp:194] Memory required for data: 56156280
I0825 11:10:30.650550  1691 layer_factory.hpp:77] Creating layer Scale21
I0825 11:10:30.650557  1691 net.cpp:128] Creating Layer Scale21
I0825 11:10:30.650562  1691 net.cpp:558] Scale21 <- Convolution21
I0825 11:10:30.650569  1691 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0825 11:10:30.650622  1691 layer_factory.hpp:77] Creating layer Scale21
I0825 11:10:30.650786  1691 net.cpp:172] Setting up Scale21
I0825 11:10:30.650797  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.650801  1691 net.cpp:194] Memory required for data: 56483960
I0825 11:10:30.650810  1691 layer_factory.hpp:77] Creating layer ReLU20
I0825 11:10:30.650815  1691 net.cpp:128] Creating Layer ReLU20
I0825 11:10:30.650820  1691 net.cpp:558] ReLU20 <- Convolution21
I0825 11:10:30.650825  1691 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0825 11:10:30.651321  1691 net.cpp:172] Setting up ReLU20
I0825 11:10:30.651345  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.651350  1691 net.cpp:194] Memory required for data: 56811640
I0825 11:10:30.651355  1691 layer_factory.hpp:77] Creating layer Convolution22
I0825 11:10:30.651371  1691 net.cpp:128] Creating Layer Convolution22
I0825 11:10:30.651376  1691 net.cpp:558] Convolution22 <- Convolution21
I0825 11:10:30.651401  1691 net.cpp:522] Convolution22 -> Convolution22
I0825 11:10:30.653015  1691 net.cpp:172] Setting up Convolution22
I0825 11:10:30.653038  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.653046  1691 net.cpp:194] Memory required for data: 57139320
I0825 11:10:30.653056  1691 layer_factory.hpp:77] Creating layer BatchNorm22
I0825 11:10:30.653069  1691 net.cpp:128] Creating Layer BatchNorm22
I0825 11:10:30.653074  1691 net.cpp:558] BatchNorm22 <- Convolution22
I0825 11:10:30.653082  1691 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0825 11:10:30.653384  1691 net.cpp:172] Setting up BatchNorm22
I0825 11:10:30.653394  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.653398  1691 net.cpp:194] Memory required for data: 57467000
I0825 11:10:30.653409  1691 layer_factory.hpp:77] Creating layer Scale22
I0825 11:10:30.653419  1691 net.cpp:128] Creating Layer Scale22
I0825 11:10:30.653424  1691 net.cpp:558] Scale22 <- Convolution22
I0825 11:10:30.653429  1691 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0825 11:10:30.653486  1691 layer_factory.hpp:77] Creating layer Scale22
I0825 11:10:30.653656  1691 net.cpp:172] Setting up Scale22
I0825 11:10:30.653666  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.653671  1691 net.cpp:194] Memory required for data: 57794680
I0825 11:10:30.653678  1691 layer_factory.hpp:77] Creating layer Eltwise10
I0825 11:10:30.653688  1691 net.cpp:128] Creating Layer Eltwise10
I0825 11:10:30.653693  1691 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0825 11:10:30.653699  1691 net.cpp:558] Eltwise10 <- Convolution22
I0825 11:10:30.653705  1691 net.cpp:522] Eltwise10 -> Eltwise10
I0825 11:10:30.653734  1691 net.cpp:172] Setting up Eltwise10
I0825 11:10:30.653744  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.653748  1691 net.cpp:194] Memory required for data: 58122360
I0825 11:10:30.653753  1691 layer_factory.hpp:77] Creating layer ReLU21
I0825 11:10:30.653759  1691 net.cpp:128] Creating Layer ReLU21
I0825 11:10:30.653764  1691 net.cpp:558] ReLU21 <- Eltwise10
I0825 11:10:30.653769  1691 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0825 11:10:30.654017  1691 net.cpp:172] Setting up ReLU21
I0825 11:10:30.654032  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.654037  1691 net.cpp:194] Memory required for data: 58450040
I0825 11:10:30.654042  1691 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0825 11:10:30.654049  1691 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0825 11:10:30.654054  1691 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0825 11:10:30.654063  1691 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0825 11:10:30.654072  1691 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0825 11:10:30.654130  1691 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0825 11:10:30.654141  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.654147  1691 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0825 11:10:30.654151  1691 net.cpp:194] Memory required for data: 59105400
I0825 11:10:30.654155  1691 layer_factory.hpp:77] Creating layer Convolution23
I0825 11:10:30.654170  1691 net.cpp:128] Creating Layer Convolution23
I0825 11:10:30.654175  1691 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0825 11:10:30.654182  1691 net.cpp:522] Convolution23 -> Convolution23
I0825 11:10:30.659914  1691 net.cpp:172] Setting up Convolution23
I0825 11:10:30.659940  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.659945  1691 net.cpp:194] Memory required for data: 59269240
I0825 11:10:30.659960  1691 layer_factory.hpp:77] Creating layer BatchNorm23
I0825 11:10:30.659971  1691 net.cpp:128] Creating Layer BatchNorm23
I0825 11:10:30.659976  1691 net.cpp:558] BatchNorm23 <- Convolution23
I0825 11:10:30.659983  1691 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0825 11:10:30.660295  1691 net.cpp:172] Setting up BatchNorm23
I0825 11:10:30.660305  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.660332  1691 net.cpp:194] Memory required for data: 59433080
I0825 11:10:30.660346  1691 layer_factory.hpp:77] Creating layer Scale23
I0825 11:10:30.660353  1691 net.cpp:128] Creating Layer Scale23
I0825 11:10:30.660358  1691 net.cpp:558] Scale23 <- Convolution23
I0825 11:10:30.660364  1691 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0825 11:10:30.660426  1691 layer_factory.hpp:77] Creating layer Scale23
I0825 11:10:30.660601  1691 net.cpp:172] Setting up Scale23
I0825 11:10:30.660614  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.660619  1691 net.cpp:194] Memory required for data: 59596920
I0825 11:10:30.660627  1691 layer_factory.hpp:77] Creating layer Convolution24
I0825 11:10:30.660643  1691 net.cpp:128] Creating Layer Convolution24
I0825 11:10:30.660651  1691 net.cpp:558] Convolution24 <- Eltwise10_ReLU21_0_split_1
I0825 11:10:30.660661  1691 net.cpp:522] Convolution24 -> Convolution24
I0825 11:10:30.666509  1691 net.cpp:172] Setting up Convolution24
I0825 11:10:30.666535  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.666540  1691 net.cpp:194] Memory required for data: 59760760
I0825 11:10:30.666554  1691 layer_factory.hpp:77] Creating layer BatchNorm24
I0825 11:10:30.666563  1691 net.cpp:128] Creating Layer BatchNorm24
I0825 11:10:30.666569  1691 net.cpp:558] BatchNorm24 <- Convolution24
I0825 11:10:30.666579  1691 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0825 11:10:30.666888  1691 net.cpp:172] Setting up BatchNorm24
I0825 11:10:30.666899  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.666903  1691 net.cpp:194] Memory required for data: 59924600
I0825 11:10:30.666914  1691 layer_factory.hpp:77] Creating layer Scale24
I0825 11:10:30.666921  1691 net.cpp:128] Creating Layer Scale24
I0825 11:10:30.666925  1691 net.cpp:558] Scale24 <- Convolution24
I0825 11:10:30.666934  1691 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0825 11:10:30.666996  1691 layer_factory.hpp:77] Creating layer Scale24
I0825 11:10:30.667171  1691 net.cpp:172] Setting up Scale24
I0825 11:10:30.667181  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.667184  1691 net.cpp:194] Memory required for data: 60088440
I0825 11:10:30.667192  1691 layer_factory.hpp:77] Creating layer ReLU22
I0825 11:10:30.667207  1691 net.cpp:128] Creating Layer ReLU22
I0825 11:10:30.667212  1691 net.cpp:558] ReLU22 <- Convolution24
I0825 11:10:30.667217  1691 net.cpp:509] ReLU22 -> Convolution24 (in-place)
I0825 11:10:30.668521  1691 net.cpp:172] Setting up ReLU22
I0825 11:10:30.668540  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.668545  1691 net.cpp:194] Memory required for data: 60252280
I0825 11:10:30.668548  1691 layer_factory.hpp:77] Creating layer Convolution25
I0825 11:10:30.668566  1691 net.cpp:128] Creating Layer Convolution25
I0825 11:10:30.668571  1691 net.cpp:558] Convolution25 <- Convolution24
I0825 11:10:30.668579  1691 net.cpp:522] Convolution25 -> Convolution25
I0825 11:10:30.675330  1691 net.cpp:172] Setting up Convolution25
I0825 11:10:30.675361  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.675366  1691 net.cpp:194] Memory required for data: 60416120
I0825 11:10:30.675377  1691 layer_factory.hpp:77] Creating layer BatchNorm25
I0825 11:10:30.675386  1691 net.cpp:128] Creating Layer BatchNorm25
I0825 11:10:30.675392  1691 net.cpp:558] BatchNorm25 <- Convolution25
I0825 11:10:30.675406  1691 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0825 11:10:30.675717  1691 net.cpp:172] Setting up BatchNorm25
I0825 11:10:30.675729  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.675734  1691 net.cpp:194] Memory required for data: 60579960
I0825 11:10:30.675743  1691 layer_factory.hpp:77] Creating layer Scale25
I0825 11:10:30.675750  1691 net.cpp:128] Creating Layer Scale25
I0825 11:10:30.675755  1691 net.cpp:558] Scale25 <- Convolution25
I0825 11:10:30.675762  1691 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0825 11:10:30.675822  1691 layer_factory.hpp:77] Creating layer Scale25
I0825 11:10:30.675994  1691 net.cpp:172] Setting up Scale25
I0825 11:10:30.676023  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.676028  1691 net.cpp:194] Memory required for data: 60743800
I0825 11:10:30.676038  1691 layer_factory.hpp:77] Creating layer Eltwise11
I0825 11:10:30.676048  1691 net.cpp:128] Creating Layer Eltwise11
I0825 11:10:30.676054  1691 net.cpp:558] Eltwise11 <- Convolution23
I0825 11:10:30.676059  1691 net.cpp:558] Eltwise11 <- Convolution25
I0825 11:10:30.676064  1691 net.cpp:522] Eltwise11 -> Eltwise11
I0825 11:10:30.676100  1691 net.cpp:172] Setting up Eltwise11
I0825 11:10:30.676107  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.676111  1691 net.cpp:194] Memory required for data: 60907640
I0825 11:10:30.676116  1691 layer_factory.hpp:77] Creating layer ReLU23
I0825 11:10:30.676122  1691 net.cpp:128] Creating Layer ReLU23
I0825 11:10:30.676126  1691 net.cpp:558] ReLU23 <- Eltwise11
I0825 11:10:30.676134  1691 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0825 11:10:30.677388  1691 net.cpp:172] Setting up ReLU23
I0825 11:10:30.677412  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.677415  1691 net.cpp:194] Memory required for data: 61071480
I0825 11:10:30.677420  1691 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0825 11:10:30.677433  1691 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0825 11:10:30.677438  1691 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0825 11:10:30.677446  1691 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0825 11:10:30.677455  1691 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0825 11:10:30.677520  1691 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0825 11:10:30.677527  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.677533  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.677537  1691 net.cpp:194] Memory required for data: 61399160
I0825 11:10:30.677541  1691 layer_factory.hpp:77] Creating layer Convolution26
I0825 11:10:30.677554  1691 net.cpp:128] Creating Layer Convolution26
I0825 11:10:30.677559  1691 net.cpp:558] Convolution26 <- Eltwise11_ReLU23_0_split_0
I0825 11:10:30.677569  1691 net.cpp:522] Convolution26 -> Convolution26
I0825 11:10:30.680320  1691 net.cpp:172] Setting up Convolution26
I0825 11:10:30.680349  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.680354  1691 net.cpp:194] Memory required for data: 61563000
I0825 11:10:30.680366  1691 layer_factory.hpp:77] Creating layer BatchNorm26
I0825 11:10:30.680377  1691 net.cpp:128] Creating Layer BatchNorm26
I0825 11:10:30.680382  1691 net.cpp:558] BatchNorm26 <- Convolution26
I0825 11:10:30.680393  1691 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0825 11:10:30.680709  1691 net.cpp:172] Setting up BatchNorm26
I0825 11:10:30.680721  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.680725  1691 net.cpp:194] Memory required for data: 61726840
I0825 11:10:30.680735  1691 layer_factory.hpp:77] Creating layer Scale26
I0825 11:10:30.680742  1691 net.cpp:128] Creating Layer Scale26
I0825 11:10:30.680747  1691 net.cpp:558] Scale26 <- Convolution26
I0825 11:10:30.680752  1691 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0825 11:10:30.680810  1691 layer_factory.hpp:77] Creating layer Scale26
I0825 11:10:30.680984  1691 net.cpp:172] Setting up Scale26
I0825 11:10:30.680997  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.681002  1691 net.cpp:194] Memory required for data: 61890680
I0825 11:10:30.681010  1691 layer_factory.hpp:77] Creating layer ReLU24
I0825 11:10:30.681016  1691 net.cpp:128] Creating Layer ReLU24
I0825 11:10:30.681021  1691 net.cpp:558] ReLU24 <- Convolution26
I0825 11:10:30.681027  1691 net.cpp:509] ReLU24 -> Convolution26 (in-place)
I0825 11:10:30.681289  1691 net.cpp:172] Setting up ReLU24
I0825 11:10:30.681305  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.681310  1691 net.cpp:194] Memory required for data: 62054520
I0825 11:10:30.681315  1691 layer_factory.hpp:77] Creating layer Convolution27
I0825 11:10:30.681342  1691 net.cpp:128] Creating Layer Convolution27
I0825 11:10:30.681349  1691 net.cpp:558] Convolution27 <- Convolution26
I0825 11:10:30.681356  1691 net.cpp:522] Convolution27 -> Convolution27
I0825 11:10:30.687285  1691 net.cpp:172] Setting up Convolution27
I0825 11:10:30.687314  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.687319  1691 net.cpp:194] Memory required for data: 62218360
I0825 11:10:30.687328  1691 layer_factory.hpp:77] Creating layer BatchNorm27
I0825 11:10:30.687348  1691 net.cpp:128] Creating Layer BatchNorm27
I0825 11:10:30.687358  1691 net.cpp:558] BatchNorm27 <- Convolution27
I0825 11:10:30.687366  1691 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0825 11:10:30.687680  1691 net.cpp:172] Setting up BatchNorm27
I0825 11:10:30.687691  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.687695  1691 net.cpp:194] Memory required for data: 62382200
I0825 11:10:30.687706  1691 layer_factory.hpp:77] Creating layer Scale27
I0825 11:10:30.687713  1691 net.cpp:128] Creating Layer Scale27
I0825 11:10:30.687718  1691 net.cpp:558] Scale27 <- Convolution27
I0825 11:10:30.687726  1691 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0825 11:10:30.687783  1691 layer_factory.hpp:77] Creating layer Scale27
I0825 11:10:30.687954  1691 net.cpp:172] Setting up Scale27
I0825 11:10:30.687963  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.687966  1691 net.cpp:194] Memory required for data: 62546040
I0825 11:10:30.687975  1691 layer_factory.hpp:77] Creating layer Eltwise12
I0825 11:10:30.687985  1691 net.cpp:128] Creating Layer Eltwise12
I0825 11:10:30.687990  1691 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0825 11:10:30.687995  1691 net.cpp:558] Eltwise12 <- Convolution27
I0825 11:10:30.688001  1691 net.cpp:522] Eltwise12 -> Eltwise12
I0825 11:10:30.688035  1691 net.cpp:172] Setting up Eltwise12
I0825 11:10:30.688041  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.688045  1691 net.cpp:194] Memory required for data: 62709880
I0825 11:10:30.688050  1691 layer_factory.hpp:77] Creating layer ReLU25
I0825 11:10:30.688055  1691 net.cpp:128] Creating Layer ReLU25
I0825 11:10:30.688060  1691 net.cpp:558] ReLU25 <- Eltwise12
I0825 11:10:30.688066  1691 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0825 11:10:30.689306  1691 net.cpp:172] Setting up ReLU25
I0825 11:10:30.689321  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.689327  1691 net.cpp:194] Memory required for data: 62873720
I0825 11:10:30.689332  1691 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0825 11:10:30.689343  1691 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0825 11:10:30.689349  1691 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0825 11:10:30.689359  1691 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0825 11:10:30.689368  1691 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0825 11:10:30.689430  1691 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0825 11:10:30.689437  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.689443  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.689447  1691 net.cpp:194] Memory required for data: 63201400
I0825 11:10:30.689451  1691 layer_factory.hpp:77] Creating layer Convolution28
I0825 11:10:30.689465  1691 net.cpp:128] Creating Layer Convolution28
I0825 11:10:30.689468  1691 net.cpp:558] Convolution28 <- Eltwise12_ReLU25_0_split_0
I0825 11:10:30.689478  1691 net.cpp:522] Convolution28 -> Convolution28
I0825 11:10:30.696250  1691 net.cpp:172] Setting up Convolution28
I0825 11:10:30.696279  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.696282  1691 net.cpp:194] Memory required for data: 63365240
I0825 11:10:30.696293  1691 layer_factory.hpp:77] Creating layer BatchNorm28
I0825 11:10:30.696306  1691 net.cpp:128] Creating Layer BatchNorm28
I0825 11:10:30.696312  1691 net.cpp:558] BatchNorm28 <- Convolution28
I0825 11:10:30.696319  1691 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0825 11:10:30.696642  1691 net.cpp:172] Setting up BatchNorm28
I0825 11:10:30.696672  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.696676  1691 net.cpp:194] Memory required for data: 63529080
I0825 11:10:30.696687  1691 layer_factory.hpp:77] Creating layer Scale28
I0825 11:10:30.696694  1691 net.cpp:128] Creating Layer Scale28
I0825 11:10:30.696699  1691 net.cpp:558] Scale28 <- Convolution28
I0825 11:10:30.696704  1691 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0825 11:10:30.696766  1691 layer_factory.hpp:77] Creating layer Scale28
I0825 11:10:30.696944  1691 net.cpp:172] Setting up Scale28
I0825 11:10:30.696955  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.696959  1691 net.cpp:194] Memory required for data: 63692920
I0825 11:10:30.696967  1691 layer_factory.hpp:77] Creating layer ReLU26
I0825 11:10:30.696974  1691 net.cpp:128] Creating Layer ReLU26
I0825 11:10:30.696979  1691 net.cpp:558] ReLU26 <- Convolution28
I0825 11:10:30.696986  1691 net.cpp:509] ReLU26 -> Convolution28 (in-place)
I0825 11:10:30.698073  1691 net.cpp:172] Setting up ReLU26
I0825 11:10:30.698091  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.698096  1691 net.cpp:194] Memory required for data: 63856760
I0825 11:10:30.698099  1691 layer_factory.hpp:77] Creating layer Convolution29
I0825 11:10:30.698117  1691 net.cpp:128] Creating Layer Convolution29
I0825 11:10:30.698122  1691 net.cpp:558] Convolution29 <- Convolution28
I0825 11:10:30.698132  1691 net.cpp:522] Convolution29 -> Convolution29
I0825 11:10:30.704841  1691 net.cpp:172] Setting up Convolution29
I0825 11:10:30.704869  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.704874  1691 net.cpp:194] Memory required for data: 64020600
I0825 11:10:30.704883  1691 layer_factory.hpp:77] Creating layer BatchNorm29
I0825 11:10:30.704891  1691 net.cpp:128] Creating Layer BatchNorm29
I0825 11:10:30.704896  1691 net.cpp:558] BatchNorm29 <- Convolution29
I0825 11:10:30.704906  1691 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0825 11:10:30.705224  1691 net.cpp:172] Setting up BatchNorm29
I0825 11:10:30.705236  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.705240  1691 net.cpp:194] Memory required for data: 64184440
I0825 11:10:30.705250  1691 layer_factory.hpp:77] Creating layer Scale29
I0825 11:10:30.705261  1691 net.cpp:128] Creating Layer Scale29
I0825 11:10:30.705266  1691 net.cpp:558] Scale29 <- Convolution29
I0825 11:10:30.705271  1691 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0825 11:10:30.705327  1691 layer_factory.hpp:77] Creating layer Scale29
I0825 11:10:30.705503  1691 net.cpp:172] Setting up Scale29
I0825 11:10:30.705514  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.705518  1691 net.cpp:194] Memory required for data: 64348280
I0825 11:10:30.705526  1691 layer_factory.hpp:77] Creating layer Eltwise13
I0825 11:10:30.705535  1691 net.cpp:128] Creating Layer Eltwise13
I0825 11:10:30.705541  1691 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0825 11:10:30.705546  1691 net.cpp:558] Eltwise13 <- Convolution29
I0825 11:10:30.705554  1691 net.cpp:522] Eltwise13 -> Eltwise13
I0825 11:10:30.705585  1691 net.cpp:172] Setting up Eltwise13
I0825 11:10:30.705591  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.705595  1691 net.cpp:194] Memory required for data: 64512120
I0825 11:10:30.705600  1691 layer_factory.hpp:77] Creating layer ReLU27
I0825 11:10:30.705608  1691 net.cpp:128] Creating Layer ReLU27
I0825 11:10:30.705613  1691 net.cpp:558] ReLU27 <- Eltwise13
I0825 11:10:30.705619  1691 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0825 11:10:30.706859  1691 net.cpp:172] Setting up ReLU27
I0825 11:10:30.706881  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.706887  1691 net.cpp:194] Memory required for data: 64675960
I0825 11:10:30.706892  1691 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0825 11:10:30.706902  1691 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0825 11:10:30.706907  1691 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0825 11:10:30.706914  1691 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0825 11:10:30.706939  1691 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0825 11:10:30.707006  1691 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0825 11:10:30.707015  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.707020  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.707023  1691 net.cpp:194] Memory required for data: 65003640
I0825 11:10:30.707028  1691 layer_factory.hpp:77] Creating layer Convolution30
I0825 11:10:30.707041  1691 net.cpp:128] Creating Layer Convolution30
I0825 11:10:30.707046  1691 net.cpp:558] Convolution30 <- Eltwise13_ReLU27_0_split_0
I0825 11:10:30.707054  1691 net.cpp:522] Convolution30 -> Convolution30
I0825 11:10:30.713809  1691 net.cpp:172] Setting up Convolution30
I0825 11:10:30.713836  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.713840  1691 net.cpp:194] Memory required for data: 65167480
I0825 11:10:30.713851  1691 layer_factory.hpp:77] Creating layer BatchNorm30
I0825 11:10:30.713862  1691 net.cpp:128] Creating Layer BatchNorm30
I0825 11:10:30.713868  1691 net.cpp:558] BatchNorm30 <- Convolution30
I0825 11:10:30.713881  1691 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0825 11:10:30.714207  1691 net.cpp:172] Setting up BatchNorm30
I0825 11:10:30.714220  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.714223  1691 net.cpp:194] Memory required for data: 65331320
I0825 11:10:30.714233  1691 layer_factory.hpp:77] Creating layer Scale30
I0825 11:10:30.714241  1691 net.cpp:128] Creating Layer Scale30
I0825 11:10:30.714246  1691 net.cpp:558] Scale30 <- Convolution30
I0825 11:10:30.714253  1691 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0825 11:10:30.714308  1691 layer_factory.hpp:77] Creating layer Scale30
I0825 11:10:30.714495  1691 net.cpp:172] Setting up Scale30
I0825 11:10:30.714509  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.714514  1691 net.cpp:194] Memory required for data: 65495160
I0825 11:10:30.714521  1691 layer_factory.hpp:77] Creating layer ReLU28
I0825 11:10:30.714527  1691 net.cpp:128] Creating Layer ReLU28
I0825 11:10:30.714532  1691 net.cpp:558] ReLU28 <- Convolution30
I0825 11:10:30.714540  1691 net.cpp:509] ReLU28 -> Convolution30 (in-place)
I0825 11:10:30.715634  1691 net.cpp:172] Setting up ReLU28
I0825 11:10:30.715651  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.715656  1691 net.cpp:194] Memory required for data: 65659000
I0825 11:10:30.715659  1691 layer_factory.hpp:77] Creating layer Convolution31
I0825 11:10:30.715673  1691 net.cpp:128] Creating Layer Convolution31
I0825 11:10:30.715678  1691 net.cpp:558] Convolution31 <- Convolution30
I0825 11:10:30.715688  1691 net.cpp:522] Convolution31 -> Convolution31
I0825 11:10:30.722355  1691 net.cpp:172] Setting up Convolution31
I0825 11:10:30.722381  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.722386  1691 net.cpp:194] Memory required for data: 65822840
I0825 11:10:30.722396  1691 layer_factory.hpp:77] Creating layer BatchNorm31
I0825 11:10:30.722405  1691 net.cpp:128] Creating Layer BatchNorm31
I0825 11:10:30.722410  1691 net.cpp:558] BatchNorm31 <- Convolution31
I0825 11:10:30.722420  1691 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0825 11:10:30.722744  1691 net.cpp:172] Setting up BatchNorm31
I0825 11:10:30.722756  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.722761  1691 net.cpp:194] Memory required for data: 65986680
I0825 11:10:30.722771  1691 layer_factory.hpp:77] Creating layer Scale31
I0825 11:10:30.722780  1691 net.cpp:128] Creating Layer Scale31
I0825 11:10:30.722785  1691 net.cpp:558] Scale31 <- Convolution31
I0825 11:10:30.722791  1691 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0825 11:10:30.722847  1691 layer_factory.hpp:77] Creating layer Scale31
I0825 11:10:30.723024  1691 net.cpp:172] Setting up Scale31
I0825 11:10:30.723031  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.723037  1691 net.cpp:194] Memory required for data: 66150520
I0825 11:10:30.723060  1691 layer_factory.hpp:77] Creating layer Eltwise14
I0825 11:10:30.723071  1691 net.cpp:128] Creating Layer Eltwise14
I0825 11:10:30.723078  1691 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0825 11:10:30.723083  1691 net.cpp:558] Eltwise14 <- Convolution31
I0825 11:10:30.723091  1691 net.cpp:522] Eltwise14 -> Eltwise14
I0825 11:10:30.723124  1691 net.cpp:172] Setting up Eltwise14
I0825 11:10:30.723131  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.723135  1691 net.cpp:194] Memory required for data: 66314360
I0825 11:10:30.723140  1691 layer_factory.hpp:77] Creating layer ReLU29
I0825 11:10:30.723146  1691 net.cpp:128] Creating Layer ReLU29
I0825 11:10:30.723151  1691 net.cpp:558] ReLU29 <- Eltwise14
I0825 11:10:30.723160  1691 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0825 11:10:30.724642  1691 net.cpp:172] Setting up ReLU29
I0825 11:10:30.724668  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.724673  1691 net.cpp:194] Memory required for data: 66478200
I0825 11:10:30.724678  1691 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0825 11:10:30.724685  1691 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0825 11:10:30.724690  1691 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0825 11:10:30.724697  1691 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0825 11:10:30.724710  1691 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0825 11:10:30.724778  1691 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0825 11:10:30.724786  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.724792  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.724795  1691 net.cpp:194] Memory required for data: 66805880
I0825 11:10:30.724799  1691 layer_factory.hpp:77] Creating layer Convolution32
I0825 11:10:30.724813  1691 net.cpp:128] Creating Layer Convolution32
I0825 11:10:30.724818  1691 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_0
I0825 11:10:30.724826  1691 net.cpp:522] Convolution32 -> Convolution32
I0825 11:10:30.731369  1691 net.cpp:172] Setting up Convolution32
I0825 11:10:30.731398  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.731403  1691 net.cpp:194] Memory required for data: 66969720
I0825 11:10:30.731415  1691 layer_factory.hpp:77] Creating layer BatchNorm32
I0825 11:10:30.731422  1691 net.cpp:128] Creating Layer BatchNorm32
I0825 11:10:30.731428  1691 net.cpp:558] BatchNorm32 <- Convolution32
I0825 11:10:30.731441  1691 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0825 11:10:30.731776  1691 net.cpp:172] Setting up BatchNorm32
I0825 11:10:30.731788  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.731792  1691 net.cpp:194] Memory required for data: 67133560
I0825 11:10:30.731802  1691 layer_factory.hpp:77] Creating layer Scale32
I0825 11:10:30.731809  1691 net.cpp:128] Creating Layer Scale32
I0825 11:10:30.731814  1691 net.cpp:558] Scale32 <- Convolution32
I0825 11:10:30.731822  1691 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0825 11:10:30.731878  1691 layer_factory.hpp:77] Creating layer Scale32
I0825 11:10:30.732056  1691 net.cpp:172] Setting up Scale32
I0825 11:10:30.732067  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.732071  1691 net.cpp:194] Memory required for data: 67297400
I0825 11:10:30.732079  1691 layer_factory.hpp:77] Creating layer ReLU30
I0825 11:10:30.732087  1691 net.cpp:128] Creating Layer ReLU30
I0825 11:10:30.732092  1691 net.cpp:558] ReLU30 <- Convolution32
I0825 11:10:30.732098  1691 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0825 11:10:30.733177  1691 net.cpp:172] Setting up ReLU30
I0825 11:10:30.733199  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.733204  1691 net.cpp:194] Memory required for data: 67461240
I0825 11:10:30.733209  1691 layer_factory.hpp:77] Creating layer Convolution33
I0825 11:10:30.733224  1691 net.cpp:128] Creating Layer Convolution33
I0825 11:10:30.733230  1691 net.cpp:558] Convolution33 <- Convolution32
I0825 11:10:30.733242  1691 net.cpp:522] Convolution33 -> Convolution33
I0825 11:10:30.739961  1691 net.cpp:172] Setting up Convolution33
I0825 11:10:30.739987  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.739991  1691 net.cpp:194] Memory required for data: 67625080
I0825 11:10:30.740002  1691 layer_factory.hpp:77] Creating layer BatchNorm33
I0825 11:10:30.740013  1691 net.cpp:128] Creating Layer BatchNorm33
I0825 11:10:30.740020  1691 net.cpp:558] BatchNorm33 <- Convolution33
I0825 11:10:30.740026  1691 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0825 11:10:30.740345  1691 net.cpp:172] Setting up BatchNorm33
I0825 11:10:30.740356  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.740361  1691 net.cpp:194] Memory required for data: 67788920
I0825 11:10:30.740371  1691 layer_factory.hpp:77] Creating layer Scale33
I0825 11:10:30.740378  1691 net.cpp:128] Creating Layer Scale33
I0825 11:10:30.740382  1691 net.cpp:558] Scale33 <- Convolution33
I0825 11:10:30.740391  1691 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0825 11:10:30.740448  1691 layer_factory.hpp:77] Creating layer Scale33
I0825 11:10:30.740630  1691 net.cpp:172] Setting up Scale33
I0825 11:10:30.740643  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.740646  1691 net.cpp:194] Memory required for data: 67952760
I0825 11:10:30.740654  1691 layer_factory.hpp:77] Creating layer Eltwise15
I0825 11:10:30.740661  1691 net.cpp:128] Creating Layer Eltwise15
I0825 11:10:30.740666  1691 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0825 11:10:30.740671  1691 net.cpp:558] Eltwise15 <- Convolution33
I0825 11:10:30.740680  1691 net.cpp:522] Eltwise15 -> Eltwise15
I0825 11:10:30.740713  1691 net.cpp:172] Setting up Eltwise15
I0825 11:10:30.740720  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.740725  1691 net.cpp:194] Memory required for data: 68116600
I0825 11:10:30.740728  1691 layer_factory.hpp:77] Creating layer ReLU31
I0825 11:10:30.740736  1691 net.cpp:128] Creating Layer ReLU31
I0825 11:10:30.740739  1691 net.cpp:558] ReLU31 <- Eltwise15
I0825 11:10:30.740746  1691 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0825 11:10:30.741974  1691 net.cpp:172] Setting up ReLU31
I0825 11:10:30.741989  1691 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0825 11:10:30.741994  1691 net.cpp:194] Memory required for data: 68280440
I0825 11:10:30.741999  1691 layer_factory.hpp:77] Creating layer Pooling1
I0825 11:10:30.742008  1691 net.cpp:128] Creating Layer Pooling1
I0825 11:10:30.742012  1691 net.cpp:558] Pooling1 <- Eltwise15
I0825 11:10:30.742023  1691 net.cpp:522] Pooling1 -> Pooling1
I0825 11:10:30.744220  1691 net.cpp:172] Setting up Pooling1
I0825 11:10:30.744238  1691 net.cpp:186] Top shape: 10 64 1 1 (640)
I0825 11:10:30.744242  1691 net.cpp:194] Memory required for data: 68283000
I0825 11:10:30.744247  1691 layer_factory.hpp:77] Creating layer InnerProduct1
I0825 11:10:30.744259  1691 net.cpp:128] Creating Layer InnerProduct1
I0825 11:10:30.744267  1691 net.cpp:558] InnerProduct1 <- Pooling1
I0825 11:10:30.744274  1691 net.cpp:522] InnerProduct1 -> InnerProduct1
I0825 11:10:30.744468  1691 net.cpp:172] Setting up InnerProduct1
I0825 11:10:30.744480  1691 net.cpp:186] Top shape: 10 10 (100)
I0825 11:10:30.744484  1691 net.cpp:194] Memory required for data: 68283400
I0825 11:10:30.744493  1691 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0825 11:10:30.744500  1691 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0825 11:10:30.744505  1691 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0825 11:10:30.744514  1691 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0825 11:10:30.744524  1691 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0825 11:10:30.744578  1691 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0825 11:10:30.744585  1691 net.cpp:186] Top shape: 10 10 (100)
I0825 11:10:30.744590  1691 net.cpp:186] Top shape: 10 10 (100)
I0825 11:10:30.744594  1691 net.cpp:194] Memory required for data: 68284200
I0825 11:10:30.744616  1691 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:10:30.744622  1691 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0825 11:10:30.744627  1691 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0825 11:10:30.744633  1691 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0825 11:10:30.744642  1691 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0825 11:10:30.744652  1691 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0825 11:10:30.746567  1691 net.cpp:172] Setting up SoftmaxWithLoss1
I0825 11:10:30.746588  1691 net.cpp:186] Top shape: (1)
I0825 11:10:30.746593  1691 net.cpp:189]     with loss weight 1
I0825 11:10:30.746613  1691 net.cpp:194] Memory required for data: 68284204
I0825 11:10:30.746618  1691 layer_factory.hpp:77] Creating layer Accuracy1
I0825 11:10:30.746626  1691 net.cpp:128] Creating Layer Accuracy1
I0825 11:10:30.746632  1691 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0825 11:10:30.746639  1691 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0825 11:10:30.746649  1691 net.cpp:522] Accuracy1 -> Accuracy1
I0825 11:10:30.746659  1691 net.cpp:172] Setting up Accuracy1
I0825 11:10:30.746665  1691 net.cpp:186] Top shape: (1)
I0825 11:10:30.746668  1691 net.cpp:194] Memory required for data: 68284208
I0825 11:10:30.746672  1691 net.cpp:303] Accuracy1 does not need backward computation.
I0825 11:10:30.746678  1691 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0825 11:10:30.746683  1691 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0825 11:10:30.746687  1691 net.cpp:301] InnerProduct1 needs backward computation.
I0825 11:10:30.746691  1691 net.cpp:301] Pooling1 needs backward computation.
I0825 11:10:30.746696  1691 net.cpp:301] ReLU31 needs backward computation.
I0825 11:10:30.746701  1691 net.cpp:301] Eltwise15 needs backward computation.
I0825 11:10:30.746706  1691 net.cpp:301] Scale33 needs backward computation.
I0825 11:10:30.746709  1691 net.cpp:301] BatchNorm33 needs backward computation.
I0825 11:10:30.746713  1691 net.cpp:301] Convolution33 needs backward computation.
I0825 11:10:30.746717  1691 net.cpp:301] ReLU30 needs backward computation.
I0825 11:10:30.746721  1691 net.cpp:301] Scale32 needs backward computation.
I0825 11:10:30.746726  1691 net.cpp:301] BatchNorm32 needs backward computation.
I0825 11:10:30.746729  1691 net.cpp:301] Convolution32 needs backward computation.
I0825 11:10:30.746734  1691 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0825 11:10:30.746738  1691 net.cpp:301] ReLU29 needs backward computation.
I0825 11:10:30.746742  1691 net.cpp:301] Eltwise14 needs backward computation.
I0825 11:10:30.746749  1691 net.cpp:301] Scale31 needs backward computation.
I0825 11:10:30.746753  1691 net.cpp:301] BatchNorm31 needs backward computation.
I0825 11:10:30.746757  1691 net.cpp:301] Convolution31 needs backward computation.
I0825 11:10:30.746762  1691 net.cpp:301] ReLU28 needs backward computation.
I0825 11:10:30.746767  1691 net.cpp:301] Scale30 needs backward computation.
I0825 11:10:30.746770  1691 net.cpp:301] BatchNorm30 needs backward computation.
I0825 11:10:30.746774  1691 net.cpp:301] Convolution30 needs backward computation.
I0825 11:10:30.746779  1691 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0825 11:10:30.746783  1691 net.cpp:301] ReLU27 needs backward computation.
I0825 11:10:30.746788  1691 net.cpp:301] Eltwise13 needs backward computation.
I0825 11:10:30.746793  1691 net.cpp:301] Scale29 needs backward computation.
I0825 11:10:30.746796  1691 net.cpp:301] BatchNorm29 needs backward computation.
I0825 11:10:30.746801  1691 net.cpp:301] Convolution29 needs backward computation.
I0825 11:10:30.746805  1691 net.cpp:301] ReLU26 needs backward computation.
I0825 11:10:30.746809  1691 net.cpp:301] Scale28 needs backward computation.
I0825 11:10:30.746814  1691 net.cpp:301] BatchNorm28 needs backward computation.
I0825 11:10:30.746819  1691 net.cpp:301] Convolution28 needs backward computation.
I0825 11:10:30.746837  1691 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0825 11:10:30.746842  1691 net.cpp:301] ReLU25 needs backward computation.
I0825 11:10:30.746846  1691 net.cpp:301] Eltwise12 needs backward computation.
I0825 11:10:30.746851  1691 net.cpp:301] Scale27 needs backward computation.
I0825 11:10:30.746855  1691 net.cpp:301] BatchNorm27 needs backward computation.
I0825 11:10:30.746860  1691 net.cpp:301] Convolution27 needs backward computation.
I0825 11:10:30.746865  1691 net.cpp:301] ReLU24 needs backward computation.
I0825 11:10:30.746868  1691 net.cpp:301] Scale26 needs backward computation.
I0825 11:10:30.746872  1691 net.cpp:301] BatchNorm26 needs backward computation.
I0825 11:10:30.746876  1691 net.cpp:301] Convolution26 needs backward computation.
I0825 11:10:30.746881  1691 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0825 11:10:30.746886  1691 net.cpp:301] ReLU23 needs backward computation.
I0825 11:10:30.746891  1691 net.cpp:301] Eltwise11 needs backward computation.
I0825 11:10:30.746896  1691 net.cpp:301] Scale25 needs backward computation.
I0825 11:10:30.746901  1691 net.cpp:301] BatchNorm25 needs backward computation.
I0825 11:10:30.746904  1691 net.cpp:301] Convolution25 needs backward computation.
I0825 11:10:30.746908  1691 net.cpp:301] ReLU22 needs backward computation.
I0825 11:10:30.746913  1691 net.cpp:301] Scale24 needs backward computation.
I0825 11:10:30.746917  1691 net.cpp:301] BatchNorm24 needs backward computation.
I0825 11:10:30.746922  1691 net.cpp:301] Convolution24 needs backward computation.
I0825 11:10:30.746927  1691 net.cpp:301] Scale23 needs backward computation.
I0825 11:10:30.746930  1691 net.cpp:301] BatchNorm23 needs backward computation.
I0825 11:10:30.746935  1691 net.cpp:301] Convolution23 needs backward computation.
I0825 11:10:30.746939  1691 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0825 11:10:30.746944  1691 net.cpp:301] ReLU21 needs backward computation.
I0825 11:10:30.746949  1691 net.cpp:301] Eltwise10 needs backward computation.
I0825 11:10:30.746954  1691 net.cpp:301] Scale22 needs backward computation.
I0825 11:10:30.746958  1691 net.cpp:301] BatchNorm22 needs backward computation.
I0825 11:10:30.746963  1691 net.cpp:301] Convolution22 needs backward computation.
I0825 11:10:30.746968  1691 net.cpp:301] ReLU20 needs backward computation.
I0825 11:10:30.746971  1691 net.cpp:301] Scale21 needs backward computation.
I0825 11:10:30.746975  1691 net.cpp:301] BatchNorm21 needs backward computation.
I0825 11:10:30.746979  1691 net.cpp:301] Convolution21 needs backward computation.
I0825 11:10:30.746987  1691 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0825 11:10:30.746992  1691 net.cpp:301] ReLU19 needs backward computation.
I0825 11:10:30.746997  1691 net.cpp:301] Eltwise9 needs backward computation.
I0825 11:10:30.747002  1691 net.cpp:301] Scale20 needs backward computation.
I0825 11:10:30.747006  1691 net.cpp:301] BatchNorm20 needs backward computation.
I0825 11:10:30.747010  1691 net.cpp:301] Convolution20 needs backward computation.
I0825 11:10:30.747015  1691 net.cpp:301] ReLU18 needs backward computation.
I0825 11:10:30.747020  1691 net.cpp:301] Scale19 needs backward computation.
I0825 11:10:30.747025  1691 net.cpp:301] BatchNorm19 needs backward computation.
I0825 11:10:30.747028  1691 net.cpp:301] Convolution19 needs backward computation.
I0825 11:10:30.747033  1691 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0825 11:10:30.747038  1691 net.cpp:301] ReLU17 needs backward computation.
I0825 11:10:30.747042  1691 net.cpp:301] Eltwise8 needs backward computation.
I0825 11:10:30.747047  1691 net.cpp:301] Scale18 needs backward computation.
I0825 11:10:30.747051  1691 net.cpp:301] BatchNorm18 needs backward computation.
I0825 11:10:30.747056  1691 net.cpp:301] Convolution18 needs backward computation.
I0825 11:10:30.747061  1691 net.cpp:301] ReLU16 needs backward computation.
I0825 11:10:30.747064  1691 net.cpp:301] Scale17 needs backward computation.
I0825 11:10:30.747076  1691 net.cpp:301] BatchNorm17 needs backward computation.
I0825 11:10:30.747079  1691 net.cpp:301] Convolution17 needs backward computation.
I0825 11:10:30.747084  1691 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0825 11:10:30.747089  1691 net.cpp:301] ReLU15 needs backward computation.
I0825 11:10:30.747094  1691 net.cpp:301] Eltwise7 needs backward computation.
I0825 11:10:30.747099  1691 net.cpp:301] Scale16 needs backward computation.
I0825 11:10:30.747103  1691 net.cpp:301] BatchNorm16 needs backward computation.
I0825 11:10:30.747107  1691 net.cpp:301] Convolution16 needs backward computation.
I0825 11:10:30.747112  1691 net.cpp:301] ReLU14 needs backward computation.
I0825 11:10:30.747117  1691 net.cpp:301] Scale15 needs backward computation.
I0825 11:10:30.747120  1691 net.cpp:301] BatchNorm15 needs backward computation.
I0825 11:10:30.747125  1691 net.cpp:301] Convolution15 needs backward computation.
I0825 11:10:30.747130  1691 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0825 11:10:30.747134  1691 net.cpp:301] ReLU13 needs backward computation.
I0825 11:10:30.747139  1691 net.cpp:301] Eltwise6 needs backward computation.
I0825 11:10:30.747144  1691 net.cpp:301] Scale14 needs backward computation.
I0825 11:10:30.747148  1691 net.cpp:301] BatchNorm14 needs backward computation.
I0825 11:10:30.747153  1691 net.cpp:301] Convolution14 needs backward computation.
I0825 11:10:30.747157  1691 net.cpp:301] ReLU12 needs backward computation.
I0825 11:10:30.747164  1691 net.cpp:301] Scale13 needs backward computation.
I0825 11:10:30.747169  1691 net.cpp:301] BatchNorm13 needs backward computation.
I0825 11:10:30.747174  1691 net.cpp:301] Convolution13 needs backward computation.
I0825 11:10:30.747179  1691 net.cpp:301] Scale12 needs backward computation.
I0825 11:10:30.747182  1691 net.cpp:301] BatchNorm12 needs backward computation.
I0825 11:10:30.747187  1691 net.cpp:301] Convolution12 needs backward computation.
I0825 11:10:30.747192  1691 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0825 11:10:30.747196  1691 net.cpp:301] ReLU11 needs backward computation.
I0825 11:10:30.747201  1691 net.cpp:301] Eltwise5 needs backward computation.
I0825 11:10:30.747206  1691 net.cpp:301] Scale11 needs backward computation.
I0825 11:10:30.747211  1691 net.cpp:301] BatchNorm11 needs backward computation.
I0825 11:10:30.747215  1691 net.cpp:301] Convolution11 needs backward computation.
I0825 11:10:30.747220  1691 net.cpp:301] ReLU10 needs backward computation.
I0825 11:10:30.747225  1691 net.cpp:301] Scale10 needs backward computation.
I0825 11:10:30.747228  1691 net.cpp:301] BatchNorm10 needs backward computation.
I0825 11:10:30.747232  1691 net.cpp:301] Convolution10 needs backward computation.
I0825 11:10:30.747238  1691 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0825 11:10:30.747243  1691 net.cpp:301] ReLU9 needs backward computation.
I0825 11:10:30.747247  1691 net.cpp:301] Eltwise4 needs backward computation.
I0825 11:10:30.747253  1691 net.cpp:301] Scale9 needs backward computation.
I0825 11:10:30.747258  1691 net.cpp:301] BatchNorm9 needs backward computation.
I0825 11:10:30.747262  1691 net.cpp:301] Convolution9 needs backward computation.
I0825 11:10:30.747267  1691 net.cpp:301] ReLU8 needs backward computation.
I0825 11:10:30.747272  1691 net.cpp:301] Scale8 needs backward computation.
I0825 11:10:30.747277  1691 net.cpp:301] BatchNorm8 needs backward computation.
I0825 11:10:30.747280  1691 net.cpp:301] Convolution8 needs backward computation.
I0825 11:10:30.747285  1691 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0825 11:10:30.747290  1691 net.cpp:301] ReLU7 needs backward computation.
I0825 11:10:30.747294  1691 net.cpp:301] Eltwise3 needs backward computation.
I0825 11:10:30.747300  1691 net.cpp:301] Scale7 needs backward computation.
I0825 11:10:30.747304  1691 net.cpp:301] BatchNorm7 needs backward computation.
I0825 11:10:30.747309  1691 net.cpp:301] Convolution7 needs backward computation.
I0825 11:10:30.747321  1691 net.cpp:301] ReLU6 needs backward computation.
I0825 11:10:30.747326  1691 net.cpp:301] Scale6 needs backward computation.
I0825 11:10:30.747330  1691 net.cpp:301] BatchNorm6 needs backward computation.
I0825 11:10:30.747335  1691 net.cpp:301] Convolution6 needs backward computation.
I0825 11:10:30.747340  1691 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0825 11:10:30.747345  1691 net.cpp:301] ReLU5 needs backward computation.
I0825 11:10:30.747349  1691 net.cpp:301] Eltwise2 needs backward computation.
I0825 11:10:30.747354  1691 net.cpp:301] Scale5 needs backward computation.
I0825 11:10:30.747359  1691 net.cpp:301] BatchNorm5 needs backward computation.
I0825 11:10:30.747364  1691 net.cpp:301] Convolution5 needs backward computation.
I0825 11:10:30.747377  1691 net.cpp:301] ReLU4 needs backward computation.
I0825 11:10:30.747381  1691 net.cpp:301] Scale4 needs backward computation.
I0825 11:10:30.747385  1691 net.cpp:301] BatchNorm4 needs backward computation.
I0825 11:10:30.747390  1691 net.cpp:301] Convolution4 needs backward computation.
I0825 11:10:30.747395  1691 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0825 11:10:30.747402  1691 net.cpp:301] ReLU3 needs backward computation.
I0825 11:10:30.747407  1691 net.cpp:301] Eltwise1 needs backward computation.
I0825 11:10:30.747413  1691 net.cpp:301] Scale3 needs backward computation.
I0825 11:10:30.747417  1691 net.cpp:301] BatchNorm3 needs backward computation.
I0825 11:10:30.747422  1691 net.cpp:301] Convolution3 needs backward computation.
I0825 11:10:30.747427  1691 net.cpp:301] ReLU2 needs backward computation.
I0825 11:10:30.747431  1691 net.cpp:301] Scale2 needs backward computation.
I0825 11:10:30.747437  1691 net.cpp:301] BatchNorm2 needs backward computation.
I0825 11:10:30.747440  1691 net.cpp:301] Convolution2 needs backward computation.
I0825 11:10:30.747445  1691 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0825 11:10:30.747450  1691 net.cpp:301] ReLU1 needs backward computation.
I0825 11:10:30.747454  1691 net.cpp:301] Scale1 needs backward computation.
I0825 11:10:30.747459  1691 net.cpp:301] BatchNorm1 needs backward computation.
I0825 11:10:30.747463  1691 net.cpp:301] Convolution1 needs backward computation.
I0825 11:10:30.747468  1691 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0825 11:10:30.747474  1691 net.cpp:303] Data1 does not need backward computation.
I0825 11:10:30.747478  1691 net.cpp:348] This network produces output Accuracy1
I0825 11:10:30.747483  1691 net.cpp:348] This network produces output SoftmaxWithLoss1
I0825 11:10:30.747568  1691 net.cpp:363] Network initialization done.
I0825 11:10:30.748176  1691 solver.cpp:110] Solver scaffolding done.
I0825 11:10:30.761883  1691 caffe.cpp:313] Starting Optimization
I0825 11:10:30.761904  1691 solver.cpp:425] Solving resnet_cifar10
I0825 11:10:30.761909  1691 solver.cpp:427] Learning Rate Policy: multistep
I0825 11:10:30.767158  1691 solver.cpp:514] Iteration 0, Testing net (#0)
I0825 11:10:58.313755  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:10:58.406100  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0
I0825 11:10:58.406289  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.30259 (* 1 = 2.30259 loss)
I0825 11:10:58.911281  1691 solver.cpp:357] Iteration 0 (0 iter/s, 28.1447s/100 iters), loss = 3.32535
I0825 11:10:58.911355  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 3.4712 (* 1 = 3.4712 loss)
I0825 11:10:58.911402  1691 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0825 11:11:31.298027  1691 solver.cpp:357] Iteration 100 (3.08876 iter/s, 32.3755s/100 iters), loss = 1.87777
I0825 11:11:31.298143  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.91453 (* 1 = 1.91453 loss)
I0825 11:11:31.298156  1691 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0825 11:12:19.831881  1691 solver.cpp:357] Iteration 200 (2.0539 iter/s, 48.6879s/100 iters), loss = 1.62886
I0825 11:12:19.832113  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.82495 (* 1 = 1.82495 loss)
I0825 11:12:19.832129  1691 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0825 11:13:37.504101  1691 solver.cpp:357] Iteration 300 (1.28824 iter/s, 77.6255s/100 iters), loss = 1.5571
I0825 11:13:37.504339  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.60383 (* 1 = 1.60383 loss)
I0825 11:13:37.504386  1691 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0825 11:14:49.114159  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:14:58.794754  1691 solver.cpp:357] Iteration 400 (1.23023 iter/s, 81.2857s/100 iters), loss = 1.37333
I0825 11:14:58.794937  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.19636 (* 1 = 1.19636 loss)
I0825 11:14:58.794983  1691 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0825 11:16:15.365865  1691 solver.cpp:514] Iteration 500, Testing net (#0)
I0825 11:17:08.842331  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:17:09.117255  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.130299
I0825 11:17:09.117414  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.26175 (* 1 = 3.26175 loss)
I0825 11:17:09.640811  1691 solver.cpp:357] Iteration 500 (0.764291 iter/s, 130.84s/100 iters), loss = 1.20196
I0825 11:17:09.640985  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.25804 (* 1 = 1.25804 loss)
I0825 11:17:09.641031  1691 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0825 11:18:32.311980  1691 solver.cpp:357] Iteration 600 (1.20975 iter/s, 82.6619s/100 iters), loss = 1.19539
I0825 11:18:32.313583  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.36433 (* 1 = 1.36433 loss)
I0825 11:18:32.313632  1691 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0825 11:19:54.018633  1691 solver.cpp:357] Iteration 700 (1.22401 iter/s, 81.6987s/100 iters), loss = 1.07728
I0825 11:19:54.018883  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.14819 (* 1 = 1.14819 loss)
I0825 11:19:54.018929  1691 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0825 11:20:54.778906  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:21:12.242249  1691 solver.cpp:357] Iteration 800 (1.27846 iter/s, 78.2188s/100 iters), loss = 0.953155
I0825 11:21:12.242398  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.859212 (* 1 = 0.859212 loss)
I0825 11:21:12.242424  1691 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0825 11:22:31.916491  1691 solver.cpp:357] Iteration 900 (1.25517 iter/s, 79.6707s/100 iters), loss = 1.00297
I0825 11:22:31.916733  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.16184 (* 1 = 1.16184 loss)
I0825 11:22:31.916779  1691 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0825 11:23:52.979375  1691 solver.cpp:514] Iteration 1000, Testing net (#0)
I0825 11:24:43.848126  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:24:44.083209  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1839
I0825 11:24:44.083350  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.66766 (* 1 = 2.66766 loss)
I0825 11:24:44.530046  1691 solver.cpp:357] Iteration 1000 (0.75407 iter/s, 132.614s/100 iters), loss = 0.91031
I0825 11:24:44.530225  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.643179 (* 1 = 0.643179 loss)
I0825 11:24:44.530289  1691 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0825 11:26:04.761867  1691 solver.cpp:357] Iteration 1100 (1.24641 iter/s, 80.2304s/100 iters), loss = 0.709559
I0825 11:26:04.762073  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.565086 (* 1 = 0.565086 loss)
I0825 11:26:04.762118  1691 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0825 11:27:01.370465  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:27:25.695367  1691 solver.cpp:357] Iteration 1200 (1.23562 iter/s, 80.9313s/100 iters), loss = 0.795646
I0825 11:27:25.695533  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.765433 (* 1 = 0.765433 loss)
I0825 11:27:25.695565  1691 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0825 11:28:47.791735  1691 solver.cpp:357] Iteration 1300 (1.21815 iter/s, 82.0918s/100 iters), loss = 0.767794
I0825 11:28:47.791983  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.82697 (* 1 = 0.82697 loss)
I0825 11:28:47.792029  1691 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0825 11:30:04.728847  1691 solver.cpp:357] Iteration 1400 (1.29981 iter/s, 76.9345s/100 iters), loss = 0.701759
I0825 11:30:04.729094  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.65988 (* 1 = 0.65988 loss)
I0825 11:30:04.729141  1691 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0825 11:31:26.224582  1691 solver.cpp:514] Iteration 1500, Testing net (#0)
I0825 11:32:18.960670  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:32:19.130677  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.127799
I0825 11:32:19.130803  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.59967 (* 1 = 2.59967 loss)
I0825 11:32:19.782464  1691 solver.cpp:357] Iteration 1500 (0.740551 iter/s, 135.035s/100 iters), loss = 0.652844
I0825 11:32:19.782636  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.695475 (* 1 = 0.695475 loss)
I0825 11:32:19.782680  1691 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0825 11:33:07.328390  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:33:38.332685  1691 solver.cpp:357] Iteration 1600 (1.27323 iter/s, 78.5403s/100 iters), loss = 0.590057
I0825 11:33:38.333214  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.607258 (* 1 = 0.607258 loss)
I0825 11:33:38.333393  1691 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0825 11:34:56.707305  1691 solver.cpp:357] Iteration 1700 (1.27608 iter/s, 78.3647s/100 iters), loss = 0.844886
I0825 11:34:56.707514  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.73533 (* 1 = 0.73533 loss)
I0825 11:34:56.707558  1691 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0825 11:36:18.112435  1691 solver.cpp:357] Iteration 1800 (1.22859 iter/s, 81.3939s/100 iters), loss = 0.87724
I0825 11:36:18.112695  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.823056 (* 1 = 0.823056 loss)
I0825 11:36:18.112741  1691 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0825 11:37:39.726184  1691 solver.cpp:357] Iteration 1900 (1.22535 iter/s, 81.6094s/100 iters), loss = 0.839505
I0825 11:37:39.726415  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.806329 (* 1 = 0.806329 loss)
I0825 11:37:39.726462  1691 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0825 11:38:19.192441  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:38:55.787593  1691 solver.cpp:514] Iteration 2000, Testing net (#0)
I0825 11:39:49.882407  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:39:50.074434  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.100099
I0825 11:39:50.074549  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.85495 (* 1 = 2.85495 loss)
I0825 11:39:50.753710  1691 solver.cpp:357] Iteration 2000 (0.763242 iter/s, 131.02s/100 iters), loss = 0.633667
I0825 11:39:50.753890  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.493649 (* 1 = 0.493649 loss)
I0825 11:39:50.753935  1691 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0825 11:41:12.415792  1691 solver.cpp:357] Iteration 2100 (1.22467 iter/s, 81.6549s/100 iters), loss = 0.838398
I0825 11:41:12.416153  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.8216 (* 1 = 0.8216 loss)
I0825 11:41:12.416242  1691 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0825 11:42:32.288456  1691 solver.cpp:357] Iteration 2200 (1.25204 iter/s, 79.8698s/100 iters), loss = 0.763304
I0825 11:42:32.288763  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.667375 (* 1 = 0.667375 loss)
I0825 11:42:32.288813  1691 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0825 11:43:45.711987  1691 solver.cpp:357] Iteration 2300 (1.36204 iter/s, 73.4192s/100 iters), loss = 0.589213
I0825 11:43:45.712215  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.491027 (* 1 = 0.491027 loss)
I0825 11:43:45.712260  1691 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0825 11:44:19.622251  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:45:07.435223  1691 solver.cpp:357] Iteration 2400 (1.22371 iter/s, 81.7188s/100 iters), loss = 0.603597
I0825 11:45:07.435632  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.6284 (* 1 = 0.6284 loss)
I0825 11:45:07.435729  1691 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0825 11:46:28.885118  1691 solver.cpp:514] Iteration 2500, Testing net (#0)
I0825 11:47:21.405316  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:47:21.486585  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2309
I0825 11:47:21.486723  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.10075 (* 1 = 2.10075 loss)
I0825 11:47:22.177390  1691 solver.cpp:357] Iteration 2500 (0.742212 iter/s, 134.732s/100 iters), loss = 0.708098
I0825 11:47:22.177516  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.701469 (* 1 = 0.701469 loss)
I0825 11:47:22.177598  1691 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0825 11:48:41.049746  1691 solver.cpp:357] Iteration 2600 (1.26765 iter/s, 78.886s/100 iters), loss = 0.784112
I0825 11:48:41.051837  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.792649 (* 1 = 0.792649 loss)
I0825 11:48:41.052022  1691 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0825 11:50:03.027474  1691 solver.cpp:357] Iteration 2700 (1.21963 iter/s, 81.9921s/100 iters), loss = 0.733764
I0825 11:50:03.027706  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.694775 (* 1 = 0.694775 loss)
I0825 11:50:03.027752  1691 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0825 11:50:27.974016  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:51:23.155686  1691 solver.cpp:357] Iteration 2800 (1.24788 iter/s, 80.1357s/100 iters), loss = 0.446261
I0825 11:51:23.155922  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.536876 (* 1 = 0.536876 loss)
I0825 11:51:23.155972  1691 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0825 11:52:35.701390  1691 solver.cpp:357] Iteration 2900 (1.3783 iter/s, 72.5529s/100 iters), loss = 0.629928
I0825 11:52:35.701606  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.537416 (* 1 = 0.537416 loss)
I0825 11:52:35.701651  1691 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0825 11:53:55.608723  1691 solver.cpp:514] Iteration 3000, Testing net (#0)
I0825 11:54:49.035378  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:54:49.264885  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.421699
I0825 11:54:49.265046  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.58844 (* 1 = 1.58844 loss)
I0825 11:54:49.899519  1691 solver.cpp:357] Iteration 3000 (0.745121 iter/s, 134.206s/100 iters), loss = 0.592214
I0825 11:54:49.899646  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.658409 (* 1 = 0.658409 loss)
I0825 11:54:49.899688  1691 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0825 11:56:11.860698  1691 solver.cpp:357] Iteration 3100 (1.22004 iter/s, 81.9644s/100 iters), loss = 0.638994
I0825 11:56:11.860962  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.816167 (* 1 = 0.816167 loss)
I0825 11:56:11.861007  1691 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0825 11:56:28.659981  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 11:57:28.479099  1691 solver.cpp:357] Iteration 3200 (1.30514 iter/s, 76.6202s/100 iters), loss = 0.747182
I0825 11:57:28.481016  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.651432 (* 1 = 0.651432 loss)
I0825 11:57:28.481045  1691 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0825 11:58:50.522686  1691 solver.cpp:357] Iteration 3300 (1.21884 iter/s, 82.0449s/100 iters), loss = 0.538222
I0825 11:58:50.522996  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.594349 (* 1 = 0.594349 loss)
I0825 11:58:50.523043  1691 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0825 12:00:12.401319  1691 solver.cpp:357] Iteration 3400 (1.22134 iter/s, 81.8773s/100 iters), loss = 0.493553
I0825 12:00:12.401547  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499059 (* 1 = 0.499059 loss)
I0825 12:00:12.401594  1691 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0825 12:01:28.733192  1691 solver.cpp:514] Iteration 3500, Testing net (#0)
I0825 12:02:20.186980  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:02:20.391563  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4875
I0825 12:02:20.391680  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.51924 (* 1 = 1.51924 loss)
I0825 12:02:20.993908  1691 solver.cpp:357] Iteration 3500 (0.777665 iter/s, 128.59s/100 iters), loss = 0.686267
I0825 12:02:20.994052  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.8106 (* 1 = 0.8106 loss)
I0825 12:02:20.994084  1691 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0825 12:02:31.486618  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:03:38.734712  1691 solver.cpp:357] Iteration 3600 (1.28637 iter/s, 77.7384s/100 iters), loss = 0.659026
I0825 12:03:38.734884  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.64719 (* 1 = 0.64719 loss)
I0825 12:03:38.734915  1691 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0825 12:05:00.663811  1691 solver.cpp:357] Iteration 3700 (1.22055 iter/s, 81.9303s/100 iters), loss = 0.458513
I0825 12:05:00.665876  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426483 (* 1 = 0.426483 loss)
I0825 12:05:00.666066  1691 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0825 12:06:17.834370  1691 solver.cpp:357] Iteration 3800 (1.29594 iter/s, 77.1642s/100 iters), loss = 0.431061
I0825 12:06:17.834612  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.554991 (* 1 = 0.554991 loss)
I0825 12:06:17.834659  1691 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0825 12:07:40.113227  1691 solver.cpp:357] Iteration 3900 (1.21544 iter/s, 82.2747s/100 iters), loss = 0.684651
I0825 12:07:40.113446  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.821244 (* 1 = 0.821244 loss)
I0825 12:07:40.113492  1691 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0825 12:07:43.154347  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:09:01.306720  1691 solver.cpp:514] Iteration 4000, Testing net (#0)
I0825 12:09:54.956436  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:09:55.082559  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6841
I0825 12:09:55.082670  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.874782 (* 1 = 0.874782 loss)
I0825 12:09:55.800027  1691 solver.cpp:357] Iteration 4000 (0.737011 iter/s, 135.683s/100 iters), loss = 0.616311
I0825 12:09:55.800436  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.514887 (* 1 = 0.514887 loss)
I0825 12:09:55.800612  1691 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0825 12:11:12.191083  1691 solver.cpp:357] Iteration 4100 (1.30916 iter/s, 76.3851s/100 iters), loss = 0.614849
I0825 12:11:12.191336  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.670407 (* 1 = 0.670407 loss)
I0825 12:11:12.191401  1691 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0825 12:12:32.718185  1691 solver.cpp:357] Iteration 4200 (1.24187 iter/s, 80.5235s/100 iters), loss = 0.475841
I0825 12:12:32.718421  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.332017 (* 1 = 0.332017 loss)
I0825 12:12:32.718469  1691 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0825 12:13:47.199203  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:13:51.878832  1691 solver.cpp:357] Iteration 4300 (1.26334 iter/s, 79.155s/100 iters), loss = 0.50438
I0825 12:13:51.879009  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.524897 (* 1 = 0.524897 loss)
I0825 12:13:51.879053  1691 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0825 12:15:09.260558  1691 solver.cpp:357] Iteration 4400 (1.29235 iter/s, 77.3782s/100 iters), loss = 0.471234
I0825 12:15:09.260725  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468685 (* 1 = 0.468685 loss)
I0825 12:15:09.260753  1691 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0825 12:16:28.491381  1691 solver.cpp:514] Iteration 4500, Testing net (#0)
I0825 12:17:22.040483  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:17:22.283771  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.561301
I0825 12:17:22.283934  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.45232 (* 1 = 1.45232 loss)
I0825 12:17:22.793407  1691 solver.cpp:357] Iteration 4500 (0.748884 iter/s, 133.532s/100 iters), loss = 0.581409
I0825 12:17:22.793583  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387549 (* 1 = 0.387549 loss)
I0825 12:17:22.793630  1691 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0825 12:18:45.198925  1691 solver.cpp:357] Iteration 4600 (1.21356 iter/s, 82.4021s/100 iters), loss = 0.488422
I0825 12:18:45.199131  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457758 (* 1 = 0.457758 loss)
I0825 12:18:45.199177  1691 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0825 12:19:50.357270  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:20:02.525837  1691 solver.cpp:357] Iteration 4700 (1.29327 iter/s, 77.3235s/100 iters), loss = 0.485701
I0825 12:20:02.526247  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467408 (* 1 = 0.467408 loss)
I0825 12:20:02.526440  1691 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0825 12:21:24.834008  1691 solver.cpp:357] Iteration 4800 (1.21503 iter/s, 82.3028s/100 iters), loss = 0.584164
I0825 12:21:24.834250  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.778877 (* 1 = 0.778877 loss)
I0825 12:21:24.834298  1691 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0825 12:22:45.050144  1691 solver.cpp:357] Iteration 4900 (1.24672 iter/s, 80.2106s/100 iters), loss = 0.577042
I0825 12:22:45.050386  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.511812 (* 1 = 0.511812 loss)
I0825 12:22:45.050433  1691 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0825 12:24:02.437372  1691 solver.cpp:514] Iteration 5000, Testing net (#0)
I0825 12:24:52.800361  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:24:53.028616  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.631101
I0825 12:24:53.028766  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.17444 (* 1 = 1.17444 loss)
I0825 12:24:53.694103  1691 solver.cpp:357] Iteration 5000 (0.777355 iter/s, 128.641s/100 iters), loss = 0.416232
I0825 12:24:53.694275  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.51694 (* 1 = 0.51694 loss)
I0825 12:24:53.694322  1691 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0825 12:25:55.582304  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:26:15.690660  1691 solver.cpp:357] Iteration 5100 (1.21964 iter/s, 81.9912s/100 iters), loss = 0.459987
I0825 12:26:15.690841  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390623 (* 1 = 0.390623 loss)
I0825 12:26:15.690907  1691 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0825 12:27:37.635823  1691 solver.cpp:357] Iteration 5200 (1.22038 iter/s, 81.9419s/100 iters), loss = 0.611751
I0825 12:27:37.636055  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.633486 (* 1 = 0.633486 loss)
I0825 12:27:37.636101  1691 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0825 12:28:53.283551  1691 solver.cpp:357] Iteration 5300 (1.32194 iter/s, 75.6466s/100 iters), loss = 0.582986
I0825 12:28:53.283866  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.664577 (* 1 = 0.664577 loss)
I0825 12:28:53.283918  1691 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0825 12:30:13.562199  1691 solver.cpp:357] Iteration 5400 (1.24567 iter/s, 80.2778s/100 iters), loss = 0.400171
I0825 12:30:13.562495  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.443225 (* 1 = 0.443225 loss)
I0825 12:30:13.562544  1691 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0825 12:31:08.220968  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:31:34.644788  1691 solver.cpp:514] Iteration 5500, Testing net (#0)
I0825 12:32:27.715494  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:32:27.928910  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5631
I0825 12:32:27.929060  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.43367 (* 1 = 1.43367 loss)
I0825 12:32:28.479657  1691 solver.cpp:357] Iteration 5500 (0.741219 iter/s, 134.913s/100 iters), loss = 0.39724
I0825 12:32:28.479825  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379169 (* 1 = 0.379169 loss)
I0825 12:32:28.479871  1691 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0825 12:33:40.601178  1691 solver.cpp:357] Iteration 5600 (1.38661 iter/s, 72.1182s/100 iters), loss = 0.559953
I0825 12:33:40.601399  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368506 (* 1 = 0.368506 loss)
I0825 12:33:40.601446  1691 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0825 12:35:02.618294  1691 solver.cpp:357] Iteration 5700 (1.2193 iter/s, 82.014s/100 iters), loss = 0.571282
I0825 12:35:02.618454  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462272 (* 1 = 0.462272 loss)
I0825 12:35:02.618479  1691 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0825 12:36:24.834461  1691 solver.cpp:357] Iteration 5800 (1.21629 iter/s, 82.2171s/100 iters), loss = 0.354803
I0825 12:36:24.834676  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367203 (* 1 = 0.367203 loss)
I0825 12:36:24.834722  1691 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0825 12:37:11.570158  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:37:45.483346  1691 solver.cpp:357] Iteration 5900 (1.23999 iter/s, 80.6458s/100 iters), loss = 0.715817
I0825 12:37:45.483589  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.570532 (* 1 = 0.570532 loss)
I0825 12:37:45.483636  1691 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0825 12:39:02.637182  1691 solver.cpp:514] Iteration 6000, Testing net (#0)
I0825 12:39:56.787039  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:39:57.068267  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7538
I0825 12:39:57.068619  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.74477 (* 1 = 0.74477 loss)
I0825 12:39:57.568491  1691 solver.cpp:357] Iteration 6000 (0.757082 iter/s, 132.086s/100 iters), loss = 0.534109
I0825 12:39:57.568892  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387075 (* 1 = 0.387075 loss)
I0825 12:39:57.569069  1691 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0825 12:41:19.540127  1691 solver.cpp:357] Iteration 6100 (1.21983 iter/s, 81.9784s/100 iters), loss = 0.513504
I0825 12:41:19.540345  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.554816 (* 1 = 0.554816 loss)
I0825 12:41:19.540379  1691 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0825 12:42:35.829152  1691 solver.cpp:357] Iteration 6200 (1.31075 iter/s, 76.2922s/100 iters), loss = 0.452093
I0825 12:42:35.829404  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462839 (* 1 = 0.462839 loss)
I0825 12:42:35.829452  1691 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0825 12:43:11.807489  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:43:53.837855  1691 solver.cpp:357] Iteration 6300 (1.28187 iter/s, 78.0113s/100 iters), loss = 0.447864
I0825 12:43:53.838105  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410291 (* 1 = 0.410291 loss)
I0825 12:43:53.838133  1691 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0825 12:45:15.621991  1691 solver.cpp:357] Iteration 6400 (1.2227 iter/s, 81.7864s/100 iters), loss = 0.579952
I0825 12:45:15.622527  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.615191 (* 1 = 0.615191 loss)
I0825 12:45:15.622709  1691 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0825 12:46:37.139217  1691 solver.cpp:514] Iteration 6500, Testing net (#0)
I0825 12:47:25.396394  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:47:25.669523  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.447399
I0825 12:47:25.669780  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.7059 (* 1 = 2.7059 loss)
I0825 12:47:26.441766  1691 solver.cpp:357] Iteration 6500 (0.764381 iter/s, 130.825s/100 iters), loss = 0.465519
I0825 12:47:26.441931  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.519287 (* 1 = 0.519287 loss)
I0825 12:47:26.441977  1691 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0825 12:48:48.448112  1691 solver.cpp:357] Iteration 6600 (1.21939 iter/s, 82.0082s/100 iters), loss = 0.456651
I0825 12:48:48.448348  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413776 (* 1 = 0.413776 loss)
I0825 12:48:48.448393  1691 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0825 12:49:19.941082  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:50:10.252189  1691 solver.cpp:357] Iteration 6700 (1.22242 iter/s, 81.8046s/100 iters), loss = 0.408755
I0825 12:50:10.252445  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44123 (* 1 = 0.44123 loss)
I0825 12:50:10.252475  1691 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0825 12:51:30.486328  1691 solver.cpp:357] Iteration 6800 (1.24632 iter/s, 80.2363s/100 iters), loss = 0.440898
I0825 12:51:30.486879  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396004 (* 1 = 0.396004 loss)
I0825 12:51:30.487061  1691 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0825 12:52:48.085618  1691 solver.cpp:357] Iteration 6900 (1.28868 iter/s, 77.5991s/100 iters), loss = 0.511296
I0825 12:52:48.085794  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392804 (* 1 = 0.392804 loss)
I0825 12:52:48.085839  1691 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0825 12:54:05.648458  1691 solver.cpp:514] Iteration 7000, Testing net (#0)
I0825 12:55:00.408228  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:55:00.651996  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5373
I0825 12:55:00.652158  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.84279 (* 1 = 1.84279 loss)
I0825 12:55:01.269513  1691 solver.cpp:357] Iteration 7000 (0.750806 iter/s, 133.19s/100 iters), loss = 0.490791
I0825 12:55:01.269778  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.648117 (* 1 = 0.648117 loss)
I0825 12:55:01.269791  1691 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0825 12:55:24.876783  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 12:56:19.062435  1691 solver.cpp:357] Iteration 7100 (1.28548 iter/s, 77.7921s/100 iters), loss = 0.440182
I0825 12:56:19.062611  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.460258 (* 1 = 0.460258 loss)
I0825 12:56:19.062638  1691 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0825 12:57:39.647454  1691 solver.cpp:357] Iteration 7200 (1.24091 iter/s, 80.5863s/100 iters), loss = 0.605661
I0825 12:57:39.647972  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.484042 (* 1 = 0.484042 loss)
I0825 12:57:39.648151  1691 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0825 12:59:01.579241  1691 solver.cpp:357] Iteration 7300 (1.22054 iter/s, 81.9309s/100 iters), loss = 0.498238
I0825 12:59:01.579424  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468709 (* 1 = 0.468709 loss)
I0825 12:59:01.579469  1691 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0825 13:00:23.642832  1691 solver.cpp:357] Iteration 7400 (1.21861 iter/s, 82.0605s/100 iters), loss = 0.466235
I0825 13:00:23.643092  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437372 (* 1 = 0.437372 loss)
I0825 13:00:23.643141  1691 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0825 13:00:38.277509  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:01:39.049260  1691 solver.cpp:514] Iteration 7500, Testing net (#0)
I0825 13:02:32.375573  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:02:32.635965  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6738
I0825 13:02:32.636121  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.02032 (* 1 = 1.02032 loss)
I0825 13:02:33.243010  1691 solver.cpp:357] Iteration 7500 (0.771575 iter/s, 129.605s/100 iters), loss = 0.393143
I0825 13:02:33.243165  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425749 (* 1 = 0.425749 loss)
I0825 13:02:33.243208  1691 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0825 13:03:50.814951  1691 solver.cpp:357] Iteration 7600 (1.28908 iter/s, 77.5746s/100 iters), loss = 0.510971
I0825 13:03:50.815186  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450206 (* 1 = 0.450206 loss)
I0825 13:03:50.815232  1691 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0825 13:05:11.358572  1691 solver.cpp:357] Iteration 7700 (1.24155 iter/s, 80.5448s/100 iters), loss = 0.434186
I0825 13:05:11.358800  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.536037 (* 1 = 0.536037 loss)
I0825 13:05:11.358847  1691 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0825 13:06:28.281546  1691 solver.cpp:357] Iteration 7800 (1.29999 iter/s, 76.9235s/100 iters), loss = 0.400136
I0825 13:06:28.281998  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401914 (* 1 = 0.401914 loss)
I0825 13:06:28.282174  1691 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0825 13:06:36.146098  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:07:50.077611  1691 solver.cpp:357] Iteration 7900 (1.22255 iter/s, 81.796s/100 iters), loss = 0.417651
I0825 13:07:50.077852  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371074 (* 1 = 0.371074 loss)
I0825 13:07:50.077898  1691 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0825 13:09:11.315682  1691 solver.cpp:514] Iteration 8000, Testing net (#0)
I0825 13:10:01.171522  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:10:01.391923  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.654201
I0825 13:10:01.392060  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05419 (* 1 = 1.05419 loss)
I0825 13:10:01.926244  1691 solver.cpp:357] Iteration 8000 (0.758456 iter/s, 131.847s/100 iters), loss = 0.433086
I0825 13:10:01.926407  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.444203 (* 1 = 0.444203 loss)
I0825 13:10:01.926453  1691 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0825 13:11:22.397742  1691 solver.cpp:357] Iteration 8100 (1.24267 iter/s, 80.472s/100 iters), loss = 0.463859
I0825 13:11:22.398273  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502094 (* 1 = 0.502094 loss)
I0825 13:11:22.398483  1691 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0825 13:12:44.159240  1691 solver.cpp:357] Iteration 8200 (1.22309 iter/s, 81.76s/100 iters), loss = 0.501765
I0825 13:12:44.159458  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.555563 (* 1 = 0.555563 loss)
I0825 13:12:44.159521  1691 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0825 13:12:44.808308  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:14:02.263118  1691 solver.cpp:357] Iteration 8300 (1.28048 iter/s, 78.0954s/100 iters), loss = 0.462279
I0825 13:14:02.263321  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.462241 (* 1 = 0.462241 loss)
I0825 13:14:02.263366  1691 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0825 13:15:18.647467  1691 solver.cpp:357] Iteration 8400 (1.30932 iter/s, 76.3757s/100 iters), loss = 0.399953
I0825 13:15:18.647737  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347939 (* 1 = 0.347939 loss)
I0825 13:15:18.647765  1691 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0825 13:16:40.129691  1691 solver.cpp:514] Iteration 8500, Testing net (#0)
I0825 13:17:34.114862  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:17:34.336370  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6666
I0825 13:17:34.336697  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.12003 (* 1 = 1.12003 loss)
I0825 13:17:34.903614  1691 solver.cpp:357] Iteration 8500 (0.734011 iter/s, 136.238s/100 iters), loss = 0.512673
I0825 13:17:34.904004  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.59376 (* 1 = 0.59376 loss)
I0825 13:17:34.904181  1691 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0825 13:18:48.918756  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:18:54.303812  1691 solver.cpp:357] Iteration 8600 (1.25963 iter/s, 79.3883s/100 iters), loss = 0.417071
I0825 13:18:54.304214  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377705 (* 1 = 0.377705 loss)
I0825 13:18:54.304389  1691 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0825 13:20:10.383069  1691 solver.cpp:357] Iteration 8700 (1.31452 iter/s, 76.0733s/100 iters), loss = 0.42325
I0825 13:20:10.383313  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475598 (* 1 = 0.475598 loss)
I0825 13:20:10.383359  1691 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0825 13:21:32.766396  1691 solver.cpp:357] Iteration 8800 (1.21393 iter/s, 82.3772s/100 iters), loss = 0.520587
I0825 13:21:32.766610  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416052 (* 1 = 0.416052 loss)
I0825 13:21:32.766656  1691 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0825 13:22:53.925410  1691 solver.cpp:357] Iteration 8900 (1.23226 iter/s, 81.1517s/100 iters), loss = 0.489368
I0825 13:22:53.925639  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.466377 (* 1 = 0.466377 loss)
I0825 13:22:53.925688  1691 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0825 13:23:53.050966  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:24:06.601953  1691 solver.cpp:514] Iteration 9000, Testing net (#0)
I0825 13:25:01.161824  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:25:01.338701  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5085
I0825 13:25:01.338856  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.84057 (* 1 = 1.84057 loss)
I0825 13:25:01.983350  1691 solver.cpp:357] Iteration 9000 (0.780948 iter/s, 128.049s/100 iters), loss = 0.364838
I0825 13:25:01.983465  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416717 (* 1 = 0.416717 loss)
I0825 13:25:01.983494  1691 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0825 13:26:23.717969  1691 solver.cpp:357] Iteration 9100 (1.22359 iter/s, 81.7265s/100 iters), loss = 0.491931
I0825 13:26:23.718188  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.557292 (* 1 = 0.557292 loss)
I0825 13:26:23.718235  1691 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0825 13:27:45.558851  1691 solver.cpp:357] Iteration 9200 (1.222 iter/s, 81.8332s/100 iters), loss = 0.465423
I0825 13:27:45.559079  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.515063 (* 1 = 0.515063 loss)
I0825 13:27:45.559123  1691 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0825 13:29:02.799397  1691 solver.cpp:357] Iteration 9300 (1.29474 iter/s, 77.2353s/100 iters), loss = 0.397059
I0825 13:29:02.799566  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414275 (* 1 = 0.414275 loss)
I0825 13:29:02.799593  1691 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0825 13:30:02.079279  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:30:24.590857  1691 solver.cpp:357] Iteration 9400 (1.22266 iter/s, 81.7887s/100 iters), loss = 0.534958
I0825 13:30:24.590977  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338179 (* 1 = 0.338179 loss)
I0825 13:30:24.591002  1691 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0825 13:31:44.834642  1691 solver.cpp:514] Iteration 9500, Testing net (#0)
I0825 13:32:36.829773  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:32:37.066551  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6579
I0825 13:32:37.066709  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.06209 (* 1 = 1.06209 loss)
I0825 13:32:37.587260  1691 solver.cpp:357] Iteration 9500 (0.751904 iter/s, 132.996s/100 iters), loss = 0.511103
I0825 13:32:37.587437  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.585888 (* 1 = 0.585888 loss)
I0825 13:32:37.587482  1691 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0825 13:33:50.191076  1691 solver.cpp:357] Iteration 9600 (1.37738 iter/s, 72.6016s/100 iters), loss = 0.501483
I0825 13:33:50.191316  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.631207 (* 1 = 0.631207 loss)
I0825 13:33:50.191361  1691 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0825 13:35:12.513402  1691 solver.cpp:357] Iteration 9700 (1.21477 iter/s, 82.3203s/100 iters), loss = 0.428954
I0825 13:35:12.513651  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395759 (* 1 = 0.395759 loss)
I0825 13:35:12.513700  1691 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0825 13:36:04.253795  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:36:34.502537  1691 solver.cpp:357] Iteration 9800 (1.21977 iter/s, 81.983s/100 iters), loss = 0.492243
I0825 13:36:34.502776  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53193 (* 1 = 0.53193 loss)
I0825 13:36:34.502822  1691 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0825 13:37:51.452649  1691 solver.cpp:357] Iteration 9900 (1.29964 iter/s, 76.9446s/100 iters), loss = 0.375458
I0825 13:37:51.452873  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389042 (* 1 = 0.389042 loss)
I0825 13:37:51.452919  1691 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0825 13:39:12.395165  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_10000.caffemodel
I0825 13:39:12.442807  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_10000.solverstate
I0825 13:39:12.448050  1691 solver.cpp:514] Iteration 10000, Testing net (#0)
I0825 13:40:07.371706  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:40:07.650758  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7605
I0825 13:40:07.650920  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.722551 (* 1 = 0.722551 loss)
I0825 13:40:08.219023  1691 solver.cpp:357] Iteration 10000 (0.731192 iter/s, 136.763s/100 iters), loss = 0.399834
I0825 13:40:08.219203  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.293645 (* 1 = 0.293645 loss)
I0825 13:40:08.219249  1691 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0825 13:41:30.151404  1691 solver.cpp:357] Iteration 10100 (1.22054 iter/s, 81.9308s/100 iters), loss = 0.453185
I0825 13:41:30.151638  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374258 (* 1 = 0.374258 loss)
I0825 13:41:30.151684  1691 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0825 13:42:09.952827  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:42:46.698283  1691 solver.cpp:357] Iteration 10200 (1.30645 iter/s, 76.5433s/100 iters), loss = 0.365624
I0825 13:42:46.698559  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344804 (* 1 = 0.344804 loss)
I0825 13:42:46.698604  1691 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0825 13:44:02.934188  1691 solver.cpp:357] Iteration 10300 (1.31171 iter/s, 76.2363s/100 iters), loss = 0.330197
I0825 13:44:02.934383  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356105 (* 1 = 0.356105 loss)
I0825 13:44:02.934412  1691 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0825 13:45:24.143959  1691 solver.cpp:357] Iteration 10400 (1.23146 iter/s, 81.2042s/100 iters), loss = 0.480207
I0825 13:45:24.144225  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.55155 (* 1 = 0.55155 loss)
I0825 13:45:24.144271  1691 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0825 13:46:41.438941  1691 solver.cpp:514] Iteration 10500, Testing net (#0)
I0825 13:47:34.596668  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:47:34.818680  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.396999
I0825 13:47:34.819042  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.3283 (* 1 = 2.3283 loss)
I0825 13:47:35.443020  1691 solver.cpp:357] Iteration 10500 (0.761625 iter/s, 131.298s/100 iters), loss = 0.525628
I0825 13:47:35.443441  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442003 (* 1 = 0.442003 loss)
I0825 13:47:35.443619  1691 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0825 13:48:11.929498  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:48:57.323523  1691 solver.cpp:357] Iteration 10600 (1.22131 iter/s, 81.8794s/100 iters), loss = 0.483845
I0825 13:48:57.323750  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.493744 (* 1 = 0.493744 loss)
I0825 13:48:57.323797  1691 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0825 13:50:19.246909  1691 solver.cpp:357] Iteration 10700 (1.22064 iter/s, 81.9241s/100 iters), loss = 0.328058
I0825 13:50:19.247122  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366558 (* 1 = 0.366558 loss)
I0825 13:50:19.247169  1691 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0825 13:51:36.071815  1691 solver.cpp:357] Iteration 10800 (1.3017 iter/s, 76.8228s/100 iters), loss = 0.39808
I0825 13:51:36.072340  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426233 (* 1 = 0.426233 loss)
I0825 13:51:36.072517  1691 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0825 13:52:57.501611  1691 solver.cpp:357] Iteration 10900 (1.22805 iter/s, 81.4297s/100 iters), loss = 0.43924
I0825 13:52:57.502043  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.524236 (* 1 = 0.524236 loss)
I0825 13:52:57.502215  1691 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0825 13:53:24.661551  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:54:15.410609  1691 solver.cpp:514] Iteration 11000, Testing net (#0)
I0825 13:55:09.136651  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 13:55:09.307117  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.704
I0825 13:55:09.307276  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.886795 (* 1 = 0.886795 loss)
I0825 13:55:10.025135  1691 solver.cpp:357] Iteration 11000 (0.75455 iter/s, 132.529s/100 iters), loss = 0.345117
I0825 13:55:10.025311  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.32411 (* 1 = 0.32411 loss)
I0825 13:55:10.025355  1691 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0825 13:56:26.126633  1691 solver.cpp:357] Iteration 11100 (1.31405 iter/s, 76.1004s/100 iters), loss = 0.442665
I0825 13:56:26.126804  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.476517 (* 1 = 0.476517 loss)
I0825 13:56:26.126830  1691 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0825 13:57:46.483970  1691 solver.cpp:357] Iteration 11200 (1.24442 iter/s, 80.359s/100 iters), loss = 0.429687
I0825 13:57:46.484262  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.417791 (* 1 = 0.417791 loss)
I0825 13:57:46.484323  1691 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0825 13:59:08.881937  1691 solver.cpp:357] Iteration 11300 (1.21361 iter/s, 82.3988s/100 iters), loss = 0.452872
I0825 13:59:08.882141  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.506098 (* 1 = 0.506098 loss)
I0825 13:59:08.882189  1691 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0825 13:59:29.903561  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:00:26.766113  1691 solver.cpp:357] Iteration 11400 (1.28398 iter/s, 77.8826s/100 iters), loss = 0.408891
I0825 14:00:26.766402  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399038 (* 1 = 0.399038 loss)
I0825 14:00:26.766450  1691 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0825 14:01:46.906005  1691 solver.cpp:514] Iteration 11500, Testing net (#0)
I0825 14:02:41.786803  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:02:42.040688  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6641
I0825 14:02:42.040798  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.09045 (* 1 = 1.09045 loss)
I0825 14:02:42.612304  1691 solver.cpp:357] Iteration 11500 (0.736114 iter/s, 135.848s/100 iters), loss = 0.413225
I0825 14:02:42.612480  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377624 (* 1 = 0.377624 loss)
I0825 14:02:42.612526  1691 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0825 14:04:00.716701  1691 solver.cpp:357] Iteration 11600 (1.28041 iter/s, 78.1003s/100 iters), loss = 0.403025
I0825 14:04:00.716904  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403255 (* 1 = 0.403255 loss)
I0825 14:04:00.716950  1691 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0825 14:05:16.583755  1691 solver.cpp:357] Iteration 11700 (1.31813 iter/s, 75.865s/100 iters), loss = 0.426354
I0825 14:05:16.584002  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.314501 (* 1 = 0.314501 loss)
I0825 14:05:16.584050  1691 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0825 14:05:30.083415  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:06:38.833660  1691 solver.cpp:357] Iteration 11800 (1.21587 iter/s, 82.2459s/100 iters), loss = 0.568615
I0825 14:06:38.833896  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497962 (* 1 = 0.497962 loss)
I0825 14:06:38.833942  1691 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0825 14:08:00.773900  1691 solver.cpp:357] Iteration 11900 (1.2204 iter/s, 81.9403s/100 iters), loss = 0.400268
I0825 14:08:00.774137  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503947 (* 1 = 0.503947 loss)
I0825 14:08:00.774185  1691 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0825 14:09:18.803719  1691 solver.cpp:514] Iteration 12000, Testing net (#0)
I0825 14:10:09.465327  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:10:09.672405  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6502
I0825 14:10:09.672762  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07585 (* 1 = 1.07585 loss)
I0825 14:10:10.383340  1691 solver.cpp:357] Iteration 12000 (0.771554 iter/s, 129.609s/100 iters), loss = 0.414248
I0825 14:10:10.383723  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.354231 (* 1 = 0.354231 loss)
I0825 14:10:10.383906  1691 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0825 14:11:32.030369  1691 solver.cpp:357] Iteration 12100 (1.22476 iter/s, 81.6486s/100 iters), loss = 0.434745
I0825 14:11:32.030737  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.469416 (* 1 = 0.469416 loss)
I0825 14:11:32.030830  1691 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0825 14:11:37.462013  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:12:54.133998  1691 solver.cpp:357] Iteration 12200 (1.21801 iter/s, 82.1014s/100 iters), loss = 0.388248
I0825 14:12:54.134536  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430101 (* 1 = 0.430101 loss)
I0825 14:12:54.134747  1691 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0825 14:14:08.000497  1691 solver.cpp:357] Iteration 12300 (1.3538 iter/s, 73.8662s/100 iters), loss = 0.303131
I0825 14:14:08.000852  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.238021 (* 1 = 0.238021 loss)
I0825 14:14:08.001027  1691 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0825 14:15:28.898833  1691 solver.cpp:357] Iteration 12400 (1.23612 iter/s, 80.8982s/100 iters), loss = 0.589454
I0825 14:15:28.899484  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.571491 (* 1 = 0.571491 loss)
I0825 14:15:28.899663  1691 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0825 14:16:48.383244  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:16:49.837625  1691 solver.cpp:514] Iteration 12500, Testing net (#0)
I0825 14:17:43.692895  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:17:43.854887  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6245
I0825 14:17:43.855044  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.32053 (* 1 = 1.32053 loss)
I0825 14:17:44.497831  1691 solver.cpp:357] Iteration 12500 (0.737484 iter/s, 135.596s/100 iters), loss = 0.408457
I0825 14:17:44.498010  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327298 (* 1 = 0.327298 loss)
I0825 14:17:44.498054  1691 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0825 14:19:00.628209  1691 solver.cpp:357] Iteration 12600 (1.31354 iter/s, 76.13s/100 iters), loss = 0.446534
I0825 14:19:00.628393  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373548 (* 1 = 0.373548 loss)
I0825 14:19:00.628422  1691 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0825 14:20:22.531103  1691 solver.cpp:357] Iteration 12700 (1.22098 iter/s, 81.9017s/100 iters), loss = 0.508932
I0825 14:20:22.531338  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.521926 (* 1 = 0.521926 loss)
I0825 14:20:22.531383  1691 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0825 14:21:43.571488  1691 solver.cpp:357] Iteration 12800 (1.23399 iter/s, 81.038s/100 iters), loss = 0.414322
I0825 14:21:43.571698  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.57319 (* 1 = 0.57319 loss)
I0825 14:21:43.571748  1691 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0825 14:22:53.902839  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:23:02.655534  1691 solver.cpp:357] Iteration 12900 (1.26446 iter/s, 79.0853s/100 iters), loss = 0.408797
I0825 14:23:02.655714  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.204543 (* 1 = 0.204543 loss)
I0825 14:23:02.655761  1691 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0825 14:24:15.738108  1691 solver.cpp:514] Iteration 13000, Testing net (#0)
I0825 14:25:10.152065  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:25:10.352041  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7472
I0825 14:25:10.352170  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.761693 (* 1 = 0.761693 loss)
I0825 14:25:11.029214  1691 solver.cpp:357] Iteration 13000 (0.778973 iter/s, 128.374s/100 iters), loss = 0.318581
I0825 14:25:11.029379  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352844 (* 1 = 0.352844 loss)
I0825 14:25:11.029424  1691 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0825 14:26:32.531857  1691 solver.cpp:357] Iteration 13100 (1.22696 iter/s, 81.5021s/100 iters), loss = 0.395637
I0825 14:26:32.532042  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410852 (* 1 = 0.410852 loss)
I0825 14:26:32.532086  1691 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0825 14:27:50.337496  1691 solver.cpp:357] Iteration 13200 (1.28523 iter/s, 77.807s/100 iters), loss = 0.374794
I0825 14:27:50.337679  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36588 (* 1 = 0.36588 loss)
I0825 14:27:50.337723  1691 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0825 14:28:53.271201  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:29:10.808535  1691 solver.cpp:357] Iteration 13300 (1.24266 iter/s, 80.4725s/100 iters), loss = 0.468734
I0825 14:29:10.808719  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373078 (* 1 = 0.373078 loss)
I0825 14:29:10.808765  1691 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0825 14:30:33.016368  1691 solver.cpp:357] Iteration 13400 (1.2165 iter/s, 82.2031s/100 iters), loss = 0.617431
I0825 14:30:33.016664  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.632774 (* 1 = 0.632774 loss)
I0825 14:30:33.016711  1691 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0825 14:31:54.446537  1691 solver.cpp:514] Iteration 13500, Testing net (#0)
I0825 14:32:43.400444  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:32:43.694824  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7339
I0825 14:32:43.695184  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.83837 (* 1 = 0.83837 loss)
I0825 14:32:44.250099  1691 solver.cpp:357] Iteration 13500 (0.762008 iter/s, 131.232s/100 iters), loss = 0.500811
I0825 14:32:44.250537  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.423318 (* 1 = 0.423318 loss)
I0825 14:32:44.250717  1691 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0825 14:34:02.166494  1691 solver.cpp:357] Iteration 13600 (1.28344 iter/s, 77.9159s/100 iters), loss = 0.353408
I0825 14:34:02.167016  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.298138 (* 1 = 0.298138 loss)
I0825 14:34:02.167196  1691 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0825 14:34:57.283124  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:35:22.116431  1691 solver.cpp:357] Iteration 13700 (1.25082 iter/s, 79.9474s/100 iters), loss = 0.458432
I0825 14:35:22.116848  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48219 (* 1 = 0.48219 loss)
I0825 14:35:22.117022  1691 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0825 14:36:41.211107  1691 solver.cpp:357] Iteration 13800 (1.26438 iter/s, 79.09s/100 iters), loss = 0.337374
I0825 14:36:41.211342  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356996 (* 1 = 0.356996 loss)
I0825 14:36:41.211391  1691 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0825 14:38:00.217193  1691 solver.cpp:357] Iteration 13900 (1.26577 iter/s, 79.0035s/100 iters), loss = 0.274223
I0825 14:38:00.217427  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.217226 (* 1 = 0.217226 loss)
I0825 14:38:00.217473  1691 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0825 14:39:21.376502  1691 solver.cpp:514] Iteration 14000, Testing net (#0)
I0825 14:40:16.139344  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:40:16.408524  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6651
I0825 14:40:16.408659  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07744 (* 1 = 1.07744 loss)
I0825 14:40:17.140821  1691 solver.cpp:357] Iteration 14000 (0.730341 iter/s, 136.922s/100 iters), loss = 0.360668
I0825 14:40:17.141079  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351965 (* 1 = 0.351965 loss)
I0825 14:40:17.141093  1691 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0825 14:41:05.295842  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:41:34.092850  1691 solver.cpp:357] Iteration 14100 (1.29952 iter/s, 76.9515s/100 iters), loss = 0.377084
I0825 14:41:34.093118  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413844 (* 1 = 0.413844 loss)
I0825 14:41:34.093132  1691 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0825 14:42:55.461022  1691 solver.cpp:357] Iteration 14200 (1.22902 iter/s, 81.3656s/100 iters), loss = 0.401965
I0825 14:42:55.461256  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368741 (* 1 = 0.368741 loss)
I0825 14:42:55.461302  1691 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0825 14:44:13.603570  1691 solver.cpp:357] Iteration 14300 (1.27976 iter/s, 78.1399s/100 iters), loss = 0.48852
I0825 14:44:13.603821  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513051 (* 1 = 0.513051 loss)
I0825 14:44:13.603868  1691 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0825 14:45:35.052143  1691 solver.cpp:357] Iteration 14400 (1.22781 iter/s, 81.4461s/100 iters), loss = 0.345353
I0825 14:45:35.052294  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281107 (* 1 = 0.281107 loss)
I0825 14:45:35.052338  1691 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0825 14:46:10.615559  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:46:48.229357  1691 solver.cpp:514] Iteration 14500, Testing net (#0)
I0825 14:47:42.491060  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:47:42.697557  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5375
I0825 14:47:42.697723  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.03925 (* 1 = 2.03925 loss)
I0825 14:47:43.375546  1691 solver.cpp:357] Iteration 14500 (0.779301 iter/s, 128.32s/100 iters), loss = 0.396015
I0825 14:47:43.375730  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343644 (* 1 = 0.343644 loss)
I0825 14:47:43.375775  1691 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0825 14:49:05.250025  1691 solver.cpp:357] Iteration 14600 (1.22145 iter/s, 81.8698s/100 iters), loss = 0.529278
I0825 14:49:05.250221  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.598567 (* 1 = 0.598567 loss)
I0825 14:49:05.250264  1691 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0825 14:50:25.064219  1691 solver.cpp:357] Iteration 14700 (1.25295 iter/s, 79.8116s/100 iters), loss = 0.435678
I0825 14:50:25.064452  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.433725 (* 1 = 0.433725 loss)
I0825 14:50:25.064499  1691 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0825 14:51:43.592867  1691 solver.cpp:357] Iteration 14800 (1.27349 iter/s, 78.5241s/100 iters), loss = 0.342409
I0825 14:51:43.593189  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.253204 (* 1 = 0.253204 loss)
I0825 14:51:43.593236  1691 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0825 14:52:17.632777  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:53:05.708705  1691 solver.cpp:357] Iteration 14900 (1.21783 iter/s, 82.1134s/100 iters), loss = 0.310191
I0825 14:53:05.708938  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.362995 (* 1 = 0.362995 loss)
I0825 14:53:05.708982  1691 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0825 14:54:23.184784  1691 solver.cpp:514] Iteration 15000, Testing net (#0)
I0825 14:55:12.884507  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:55:13.123371  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7474
I0825 14:55:13.123512  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.784329 (* 1 = 0.784329 loss)
I0825 14:55:13.796962  1691 solver.cpp:357] Iteration 15000 (0.78072 iter/s, 128.087s/100 iters), loss = 0.481056
I0825 14:55:13.797116  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.479853 (* 1 = 0.479853 loss)
I0825 14:55:13.797160  1691 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0825 14:56:34.805426  1691 solver.cpp:357] Iteration 15100 (1.23435 iter/s, 81.0142s/100 iters), loss = 0.499967
I0825 14:56:34.805634  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.441367 (* 1 = 0.441367 loss)
I0825 14:56:34.805678  1691 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0825 14:57:57.023830  1691 solver.cpp:357] Iteration 15200 (1.21613 iter/s, 82.228s/100 iters), loss = 0.640865
I0825 14:57:57.024065  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503617 (* 1 = 0.503617 loss)
I0825 14:57:57.024111  1691 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0825 14:58:22.774708  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 14:59:17.630120  1691 solver.cpp:357] Iteration 15300 (1.24051 iter/s, 80.6121s/100 iters), loss = 0.388216
I0825 14:59:17.630342  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432713 (* 1 = 0.432713 loss)
I0825 14:59:17.630388  1691 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0825 15:00:32.565569  1691 solver.cpp:357] Iteration 15400 (1.33434 iter/s, 74.9435s/100 iters), loss = 0.54838
I0825 15:00:32.565711  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513403 (* 1 = 0.513403 loss)
I0825 15:00:32.565735  1691 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0825 15:01:53.673897  1691 solver.cpp:514] Iteration 15500, Testing net (#0)
I0825 15:02:48.363544  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:02:48.525650  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6401
I0825 15:02:48.525810  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.23402 (* 1 = 1.23402 loss)
I0825 15:02:49.172725  1691 solver.cpp:357] Iteration 15500 (0.731939 iter/s, 136.623s/100 iters), loss = 0.428941
I0825 15:02:49.172904  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397369 (* 1 = 0.397369 loss)
I0825 15:02:49.172950  1691 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0825 15:04:05.598839  1691 solver.cpp:357] Iteration 15600 (1.30842 iter/s, 76.4278s/100 iters), loss = 0.419719
I0825 15:04:05.599068  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.476364 (* 1 = 0.476364 loss)
I0825 15:04:05.599114  1691 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0825 15:04:21.411494  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:05:23.978428  1691 solver.cpp:357] Iteration 15700 (1.27575 iter/s, 78.3851s/100 iters), loss = 0.465039
I0825 15:05:23.978667  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339391 (* 1 = 0.339391 loss)
I0825 15:05:23.978713  1691 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0825 15:06:46.349493  1691 solver.cpp:357] Iteration 15800 (1.21394 iter/s, 82.3764s/100 iters), loss = 0.435493
I0825 15:06:46.349738  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.434115 (* 1 = 0.434115 loss)
I0825 15:06:46.349787  1691 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0825 15:08:08.572736  1691 solver.cpp:357] Iteration 15900 (1.21619 iter/s, 82.2238s/100 iters), loss = 0.409198
I0825 15:08:08.573251  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.315352 (* 1 = 0.315352 loss)
I0825 15:08:08.573431  1691 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0825 15:09:24.698686  1691 solver.cpp:514] Iteration 16000, Testing net (#0)
I0825 15:10:19.046895  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:10:19.251924  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6818
I0825 15:10:19.252013  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08559 (* 1 = 1.08559 loss)
I0825 15:10:19.830314  1691 solver.cpp:357] Iteration 16000 (0.761838 iter/s, 131.262s/100 iters), loss = 0.374348
I0825 15:10:19.830497  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400999 (* 1 = 0.400999 loss)
I0825 15:10:19.830543  1691 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0825 15:10:30.441615  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:11:40.782441  1691 solver.cpp:357] Iteration 16100 (1.23526 iter/s, 80.9548s/100 iters), loss = 0.388038
I0825 15:11:40.782785  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445427 (* 1 = 0.445427 loss)
I0825 15:11:40.782883  1691 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0825 15:13:00.727906  1691 solver.cpp:357] Iteration 16200 (1.25083 iter/s, 79.9467s/100 iters), loss = 0.358724
I0825 15:13:00.728375  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388043 (* 1 = 0.388043 loss)
I0825 15:13:00.728550  1691 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0825 15:14:13.782431  1691 solver.cpp:357] Iteration 16300 (1.36879 iter/s, 73.0572s/100 iters), loss = 0.319975
I0825 15:14:13.782943  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.455945 (* 1 = 0.455945 loss)
I0825 15:14:13.783149  1691 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0825 15:15:34.811800  1691 solver.cpp:357] Iteration 16400 (1.23411 iter/s, 81.0302s/100 iters), loss = 0.567361
I0825 15:15:34.811970  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.647417 (* 1 = 0.647417 loss)
I0825 15:15:34.811998  1691 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0825 15:15:37.916218  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:16:55.666836  1691 solver.cpp:514] Iteration 16500, Testing net (#0)
I0825 15:17:47.348963  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:17:47.499336  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7097
I0825 15:17:47.499652  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.966273 (* 1 = 0.966273 loss)
I0825 15:17:48.121376  1691 solver.cpp:357] Iteration 16500 (0.750098 iter/s, 133.316s/100 iters), loss = 0.459247
I0825 15:17:48.121774  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410893 (* 1 = 0.410893 loss)
I0825 15:17:48.121946  1691 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0825 15:19:05.875742  1691 solver.cpp:357] Iteration 16600 (1.28605 iter/s, 77.7578s/100 iters), loss = 0.477943
I0825 15:19:05.876273  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.523247 (* 1 = 0.523247 loss)
I0825 15:19:05.876457  1691 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0825 15:20:27.621984  1691 solver.cpp:357] Iteration 16700 (1.22329 iter/s, 81.7466s/100 iters), loss = 0.371859
I0825 15:20:27.622179  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.245977 (* 1 = 0.245977 loss)
I0825 15:20:27.622211  1691 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0825 15:21:44.207805  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:21:48.982447  1691 solver.cpp:357] Iteration 16800 (1.22909 iter/s, 81.3607s/100 iters), loss = 0.39113
I0825 15:21:48.982626  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.257176 (* 1 = 0.257176 loss)
I0825 15:21:48.982672  1691 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0825 15:23:05.074573  1691 solver.cpp:357] Iteration 16900 (1.3142 iter/s, 76.092s/100 iters), loss = 0.39651
I0825 15:23:05.074765  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373105 (* 1 = 0.373105 loss)
I0825 15:23:05.074810  1691 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0825 15:24:20.122306  1691 solver.cpp:514] Iteration 17000, Testing net (#0)
I0825 15:25:14.142241  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:25:14.293160  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7126
I0825 15:25:14.293277  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00627 (* 1 = 1.00627 loss)
I0825 15:25:14.960794  1691 solver.cpp:357] Iteration 17000 (0.769876 iter/s, 129.891s/100 iters), loss = 0.339088
I0825 15:25:14.960943  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343737 (* 1 = 0.343737 loss)
I0825 15:25:14.960971  1691 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0825 15:26:36.957872  1691 solver.cpp:357] Iteration 17100 (1.21958 iter/s, 81.9954s/100 iters), loss = 0.297786
I0825 15:26:36.958041  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.238976 (* 1 = 0.238976 loss)
I0825 15:26:36.958070  1691 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0825 15:27:41.518307  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:27:54.088968  1691 solver.cpp:357] Iteration 17200 (1.29653 iter/s, 77.1287s/100 iters), loss = 0.341516
I0825 15:27:54.089161  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397431 (* 1 = 0.397431 loss)
I0825 15:27:54.089207  1691 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0825 15:29:16.401309  1691 solver.cpp:357] Iteration 17300 (1.21492 iter/s, 82.3102s/100 iters), loss = 0.343464
I0825 15:29:16.401545  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410045 (* 1 = 0.410045 loss)
I0825 15:29:16.401608  1691 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0825 15:30:38.495868  1691 solver.cpp:357] Iteration 17400 (1.21814 iter/s, 82.0923s/100 iters), loss = 0.364097
I0825 15:30:38.496090  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300143 (* 1 = 0.300143 loss)
I0825 15:30:38.496135  1691 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0825 15:31:54.724748  1691 solver.cpp:514] Iteration 17500, Testing net (#0)
I0825 15:32:48.944473  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:32:49.137660  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6634
I0825 15:32:49.137782  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.19248 (* 1 = 1.19248 loss)
I0825 15:32:49.889434  1691 solver.cpp:357] Iteration 17500 (0.761046 iter/s, 131.398s/100 iters), loss = 0.316985
I0825 15:32:49.889609  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4694 (* 1 = 0.4694 loss)
I0825 15:32:49.889655  1691 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0825 15:33:49.571115  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:34:08.683703  1691 solver.cpp:357] Iteration 17600 (1.26913 iter/s, 78.794s/100 iters), loss = 0.396217
I0825 15:34:08.683872  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.227 (* 1 = 0.227 loss)
I0825 15:34:08.683918  1691 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0825 15:35:29.301156  1691 solver.cpp:357] Iteration 17700 (1.2404 iter/s, 80.6193s/100 iters), loss = 0.491877
I0825 15:35:29.301389  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.566877 (* 1 = 0.566877 loss)
I0825 15:35:29.301436  1691 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0825 15:36:43.968863  1691 solver.cpp:357] Iteration 17800 (1.33928 iter/s, 74.6671s/100 iters), loss = 0.462896
I0825 15:36:43.969112  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543476 (* 1 = 0.543476 loss)
I0825 15:36:43.969161  1691 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0825 15:38:04.944660  1691 solver.cpp:357] Iteration 17900 (1.23494 iter/s, 80.9755s/100 iters), loss = 0.228629
I0825 15:38:04.945087  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.24948 (* 1 = 0.24948 loss)
I0825 15:38:04.945206  1691 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0825 15:38:58.799384  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:39:25.668031  1691 solver.cpp:514] Iteration 18000, Testing net (#0)
I0825 15:40:19.733536  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:40:19.822549  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7448
I0825 15:40:19.822686  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.885984 (* 1 = 0.885984 loss)
I0825 15:40:20.512910  1691 solver.cpp:357] Iteration 18000 (0.737623 iter/s, 135.571s/100 iters), loss = 0.340358
I0825 15:40:20.513099  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371033 (* 1 = 0.371033 loss)
I0825 15:40:20.513149  1691 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0825 15:41:36.649432  1691 solver.cpp:357] Iteration 18100 (1.31348 iter/s, 76.1339s/100 iters), loss = 0.310966
I0825 15:41:36.651093  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.321858 (* 1 = 0.321858 loss)
I0825 15:41:36.651144  1691 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0825 15:42:58.462913  1691 solver.cpp:357] Iteration 18200 (1.22233 iter/s, 81.8111s/100 iters), loss = 0.503256
I0825 15:42:58.463285  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.415522 (* 1 = 0.415522 loss)
I0825 15:42:58.463379  1691 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0825 15:44:16.878602  1691 solver.cpp:357] Iteration 18300 (1.27523 iter/s, 78.417s/100 iters), loss = 0.221172
I0825 15:44:16.879120  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.25709 (* 1 = 0.25709 loss)
I0825 15:44:16.879312  1691 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0825 15:45:01.937096  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:45:32.509553  1691 solver.cpp:357] Iteration 18400 (1.32222 iter/s, 75.6304s/100 iters), loss = 0.546967
I0825 15:45:32.510080  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.51794 (* 1 = 0.51794 loss)
I0825 15:45:32.510264  1691 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0825 15:46:52.429263  1691 solver.cpp:514] Iteration 18500, Testing net (#0)
I0825 15:47:46.751863  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:47:47.019413  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6447
I0825 15:47:47.019767  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05806 (* 1 = 1.05806 loss)
I0825 15:47:47.654705  1691 solver.cpp:357] Iteration 18500 (0.740051 iter/s, 135.126s/100 iters), loss = 0.497262
I0825 15:47:47.655124  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.47825 (* 1 = 0.47825 loss)
I0825 15:47:47.655303  1691 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0825 15:49:08.263648  1691 solver.cpp:357] Iteration 18600 (1.24122 iter/s, 80.566s/100 iters), loss = 0.490363
I0825 15:49:08.263816  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.507035 (* 1 = 0.507035 loss)
I0825 15:49:08.263842  1691 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0825 15:50:24.582474  1691 solver.cpp:357] Iteration 18700 (1.31078 iter/s, 76.2905s/100 iters), loss = 0.279491
I0825 15:50:24.582686  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.239142 (* 1 = 0.239142 loss)
I0825 15:50:24.582732  1691 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0825 15:51:03.671341  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:51:46.382805  1691 solver.cpp:357] Iteration 18800 (1.22284 iter/s, 81.7769s/100 iters), loss = 0.405862
I0825 15:51:46.383030  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339244 (* 1 = 0.339244 loss)
I0825 15:51:46.383077  1691 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0825 15:53:08.054816  1691 solver.cpp:357] Iteration 18900 (1.22468 iter/s, 81.6537s/100 iters), loss = 0.44937
I0825 15:53:08.055058  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.520922 (* 1 = 0.520922 loss)
I0825 15:53:08.055102  1691 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0825 15:54:23.685248  1691 solver.cpp:514] Iteration 19000, Testing net (#0)
I0825 15:55:14.582294  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:55:14.679738  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.777001
I0825 15:55:14.679883  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.697655 (* 1 = 0.697655 loss)
I0825 15:55:15.410590  1691 solver.cpp:357] Iteration 19000 (0.785341 iter/s, 127.333s/100 iters), loss = 0.384109
I0825 15:55:15.410780  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439627 (* 1 = 0.439627 loss)
I0825 15:55:15.410828  1691 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0825 15:56:37.258716  1691 solver.cpp:357] Iteration 19100 (1.222 iter/s, 81.833s/100 iters), loss = 0.300377
I0825 15:56:37.258942  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.269082 (* 1 = 0.269082 loss)
I0825 15:56:37.258986  1691 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0825 15:57:08.531553  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 15:57:59.244983  1691 solver.cpp:357] Iteration 19200 (1.21989 iter/s, 81.9749s/100 iters), loss = 0.447929
I0825 15:57:59.245196  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483121 (* 1 = 0.483121 loss)
I0825 15:57:59.245241  1691 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0825 15:59:17.248320  1691 solver.cpp:357] Iteration 19300 (1.28219 iter/s, 77.9913s/100 iters), loss = 0.33996
I0825 15:59:17.248554  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328592 (* 1 = 0.328592 loss)
I0825 15:59:17.248602  1691 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0825 16:00:38.274277  1691 solver.cpp:357] Iteration 19400 (1.23434 iter/s, 81.0149s/100 iters), loss = 0.455439
I0825 16:00:38.274865  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.369311 (* 1 = 0.369311 loss)
I0825 16:00:38.275050  1691 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0825 16:01:57.646581  1691 solver.cpp:514] Iteration 19500, Testing net (#0)
I0825 16:02:51.497891  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:02:51.714934  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7751
I0825 16:02:51.715092  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.660658 (* 1 = 0.660658 loss)
I0825 16:02:52.255193  1691 solver.cpp:357] Iteration 19500 (0.746425 iter/s, 133.972s/100 iters), loss = 0.416922
I0825 16:02:52.255365  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.511147 (* 1 = 0.511147 loss)
I0825 16:02:52.255411  1691 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0825 16:03:15.344792  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:04:06.811347  1691 solver.cpp:357] Iteration 19600 (1.34145 iter/s, 74.5463s/100 iters), loss = 0.403709
I0825 16:04:06.811666  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461031 (* 1 = 0.461031 loss)
I0825 16:04:06.811717  1691 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0825 16:05:27.209195  1691 solver.cpp:357] Iteration 19700 (1.24365 iter/s, 80.4087s/100 iters), loss = 0.610553
I0825 16:05:27.209507  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.623658 (* 1 = 0.623658 loss)
I0825 16:05:27.209597  1691 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0825 16:06:48.967598  1691 solver.cpp:357] Iteration 19800 (1.22301 iter/s, 81.7652s/100 iters), loss = 0.311884
I0825 16:06:48.968027  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.266906 (* 1 = 0.266906 loss)
I0825 16:06:48.968142  1691 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0825 16:08:07.731837  1691 solver.cpp:357] Iteration 19900 (1.26953 iter/s, 78.7691s/100 iters), loss = 0.370254
I0825 16:08:07.732364  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338079 (* 1 = 0.338079 loss)
I0825 16:08:07.732547  1691 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0825 16:08:21.890959  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:09:25.260411  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_20000.caffemodel
I0825 16:09:25.281031  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_20000.solverstate
I0825 16:09:25.286087  1691 solver.cpp:514] Iteration 20000, Testing net (#0)
I0825 16:10:18.816129  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:10:18.991719  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5983
I0825 16:10:18.991807  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.81594 (* 1 = 1.81594 loss)
I0825 16:10:19.744530  1691 solver.cpp:357] Iteration 20000 (0.757456 iter/s, 132.021s/100 iters), loss = 0.412035
I0825 16:10:19.744710  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497586 (* 1 = 0.497586 loss)
I0825 16:10:19.744755  1691 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0825 16:11:41.258723  1691 solver.cpp:357] Iteration 20100 (1.22682 iter/s, 81.5118s/100 iters), loss = 0.432079
I0825 16:11:41.259155  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340862 (* 1 = 0.340862 loss)
I0825 16:11:41.259327  1691 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0825 16:12:59.417976  1691 solver.cpp:357] Iteration 20200 (1.2794 iter/s, 78.1617s/100 iters), loss = 0.417733
I0825 16:12:59.418208  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.53806 (* 1 = 0.53806 loss)
I0825 16:12:59.418256  1691 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0825 16:14:16.820725  1691 solver.cpp:357] Iteration 20300 (1.29195 iter/s, 77.4026s/100 iters), loss = 0.378527
I0825 16:14:16.820986  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.451058 (* 1 = 0.451058 loss)
I0825 16:14:16.821049  1691 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0825 16:14:24.370705  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:15:36.373538  1691 solver.cpp:357] Iteration 20400 (1.25707 iter/s, 79.55s/100 iters), loss = 0.389761
I0825 16:15:36.373720  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365903 (* 1 = 0.365903 loss)
I0825 16:15:36.373749  1691 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0825 16:16:57.726557  1691 solver.cpp:514] Iteration 20500, Testing net (#0)
I0825 16:17:45.697966  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:17:45.865741  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7209
I0825 16:17:45.865867  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.93124 (* 1 = 0.93124 loss)
I0825 16:17:46.525796  1691 solver.cpp:357] Iteration 20500 (0.768347 iter/s, 130.15s/100 iters), loss = 0.389038
I0825 16:17:46.525929  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397181 (* 1 = 0.397181 loss)
I0825 16:17:46.525959  1691 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0825 16:19:08.274096  1691 solver.cpp:357] Iteration 20600 (1.22335 iter/s, 81.7425s/100 iters), loss = 0.36428
I0825 16:19:08.274327  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.372143 (* 1 = 0.372143 loss)
I0825 16:19:08.274379  1691 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0825 16:20:30.143991  1691 solver.cpp:357] Iteration 20700 (1.22154 iter/s, 81.8638s/100 iters), loss = 0.426956
I0825 16:20:30.144217  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.451767 (* 1 = 0.451767 loss)
I0825 16:20:30.144263  1691 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0825 16:20:30.677393  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:21:49.768257  1691 solver.cpp:357] Iteration 20800 (1.25596 iter/s, 79.6203s/100 iters), loss = 0.492801
I0825 16:21:49.768474  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.555827 (* 1 = 0.555827 loss)
I0825 16:21:49.768522  1691 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0825 16:23:08.825407  1691 solver.cpp:357] Iteration 20900 (1.26497 iter/s, 79.0534s/100 iters), loss = 0.399365
I0825 16:23:08.825824  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.461602 (* 1 = 0.461602 loss)
I0825 16:23:08.825939  1691 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0825 16:24:27.411018  1691 solver.cpp:514] Iteration 21000, Testing net (#0)
I0825 16:25:20.734156  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:25:21.015095  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6116
I0825 16:25:21.015210  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.31648 (* 1 = 1.31648 loss)
I0825 16:25:21.686048  1691 solver.cpp:357] Iteration 21000 (0.752688 iter/s, 132.857s/100 iters), loss = 0.407937
I0825 16:25:21.686203  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.508814 (* 1 = 0.508814 loss)
I0825 16:25:21.686247  1691 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0825 16:26:30.829344  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:26:37.119804  1691 solver.cpp:357] Iteration 21100 (1.32566 iter/s, 75.4339s/100 iters), loss = 0.41535
I0825 16:26:37.119992  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326087 (* 1 = 0.326087 loss)
I0825 16:26:37.120039  1691 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0825 16:27:57.668125  1691 solver.cpp:357] Iteration 21200 (1.24152 iter/s, 80.5464s/100 iters), loss = 0.365805
I0825 16:27:57.668323  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.383954 (* 1 = 0.383954 loss)
I0825 16:27:57.668360  1691 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0825 16:29:19.574051  1691 solver.cpp:357] Iteration 21300 (1.22097 iter/s, 81.9019s/100 iters), loss = 0.39448
I0825 16:29:19.574278  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.266753 (* 1 = 0.266753 loss)
I0825 16:29:19.574343  1691 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0825 16:30:41.474787  1691 solver.cpp:357] Iteration 21400 (1.22102 iter/s, 81.8987s/100 iters), loss = 0.477007
I0825 16:30:41.474957  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.547986 (* 1 = 0.547986 loss)
I0825 16:30:41.474988  1691 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0825 16:31:43.811399  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:31:57.750200  1691 solver.cpp:514] Iteration 21500, Testing net (#0)
I0825 16:32:51.858557  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:32:52.085249  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6443
I0825 16:32:52.085366  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07541 (* 1 = 1.07541 loss)
I0825 16:32:52.772719  1691 solver.cpp:357] Iteration 21500 (0.76165 iter/s, 131.294s/100 iters), loss = 0.317161
I0825 16:32:52.772910  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.289754 (* 1 = 0.289754 loss)
I0825 16:32:52.772955  1691 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0825 16:34:11.997519  1691 solver.cpp:357] Iteration 21600 (1.2623 iter/s, 79.2206s/100 iters), loss = 0.431051
I0825 16:34:11.997704  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.482998 (* 1 = 0.482998 loss)
I0825 16:34:11.997735  1691 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0825 16:35:29.796638  1691 solver.cpp:357] Iteration 21700 (1.28547 iter/s, 77.7928s/100 iters), loss = 0.386322
I0825 16:35:29.796861  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325612 (* 1 = 0.325612 loss)
I0825 16:35:29.796910  1691 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0825 16:36:48.514114  1691 solver.cpp:357] Iteration 21800 (1.27043 iter/s, 78.7132s/100 iters), loss = 0.337787
I0825 16:36:48.514353  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375522 (* 1 = 0.375522 loss)
I0825 16:36:48.514401  1691 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0825 16:37:47.755986  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:38:10.028779  1691 solver.cpp:357] Iteration 21900 (1.22687 iter/s, 81.5083s/100 iters), loss = 0.604523
I0825 16:38:10.028918  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408062 (* 1 = 0.408062 loss)
I0825 16:38:10.028944  1691 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0825 16:39:29.693081  1691 solver.cpp:514] Iteration 22000, Testing net (#0)
I0825 16:40:18.858460  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:40:19.070919  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6623
I0825 16:40:19.071069  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.08595 (* 1 = 1.08595 loss)
I0825 16:40:19.531021  1691 solver.cpp:357] Iteration 22000 (0.772213 iter/s, 129.498s/100 iters), loss = 0.417887
I0825 16:40:19.531196  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475126 (* 1 = 0.475126 loss)
I0825 16:40:19.531242  1691 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0825 16:41:40.131438  1691 solver.cpp:357] Iteration 22100 (1.24072 iter/s, 80.5982s/100 iters), loss = 0.41215
I0825 16:41:40.131688  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.516919 (* 1 = 0.516919 loss)
I0825 16:41:40.131736  1691 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0825 16:43:01.667876  1691 solver.cpp:357] Iteration 22200 (1.22648 iter/s, 81.5343s/100 iters), loss = 0.383931
I0825 16:43:01.668087  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.371555 (* 1 = 0.371555 loss)
I0825 16:43:01.668133  1691 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0825 16:43:52.209300  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:44:20.819710  1691 solver.cpp:357] Iteration 22300 (1.26343 iter/s, 79.1496s/100 iters), loss = 0.408433
I0825 16:44:20.819892  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.50525 (* 1 = 0.50525 loss)
I0825 16:44:20.819937  1691 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0825 16:45:36.539438  1691 solver.cpp:357] Iteration 22400 (1.32074 iter/s, 75.7154s/100 iters), loss = 0.314924
I0825 16:45:36.539634  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.28978 (* 1 = 0.28978 loss)
I0825 16:45:36.539678  1691 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0825 16:46:57.248232  1691 solver.cpp:514] Iteration 22500, Testing net (#0)
I0825 16:47:50.556102  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:47:50.731148  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7487
I0825 16:47:50.731256  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.811481 (* 1 = 0.811481 loss)
I0825 16:47:51.442895  1691 solver.cpp:357] Iteration 22500 (0.741272 iter/s, 134.903s/100 iters), loss = 0.281389
I0825 16:47:51.443078  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.215058 (* 1 = 0.215058 loss)
I0825 16:47:51.443122  1691 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0825 16:49:10.979677  1691 solver.cpp:357] Iteration 22600 (1.25731 iter/s, 79.5346s/100 iters), loss = 0.471706
I0825 16:49:10.979859  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.464097 (* 1 = 0.464097 loss)
I0825 16:49:10.979887  1691 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0825 16:49:52.636760  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:50:29.871631  1691 solver.cpp:357] Iteration 22700 (1.26759 iter/s, 78.8897s/100 iters), loss = 0.403125
I0825 16:50:29.871837  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422821 (* 1 = 0.422821 loss)
I0825 16:50:29.871881  1691 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0825 16:51:50.273449  1691 solver.cpp:357] Iteration 22800 (1.24376 iter/s, 80.4016s/100 iters), loss = 0.364896
I0825 16:51:50.273635  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421157 (* 1 = 0.421157 loss)
I0825 16:51:50.273663  1691 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0825 16:53:11.875473  1691 solver.cpp:357] Iteration 22900 (1.22556 iter/s, 81.5956s/100 iters), loss = 0.392814
I0825 16:53:11.875911  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424654 (* 1 = 0.424654 loss)
I0825 16:53:11.876030  1691 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0825 16:54:26.142926  1691 solver.cpp:514] Iteration 23000, Testing net (#0)
I0825 16:55:19.144850  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:55:19.387688  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6286
I0825 16:55:19.387861  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.54907 (* 1 = 1.54907 loss)
I0825 16:55:20.070191  1691 solver.cpp:357] Iteration 23000 (0.78009 iter/s, 128.19s/100 iters), loss = 0.456681
I0825 16:55:20.070361  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.477953 (* 1 = 0.477953 loss)
I0825 16:55:20.070407  1691 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0825 16:55:56.677994  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 16:56:41.713518  1691 solver.cpp:357] Iteration 23100 (1.22469 iter/s, 81.6535s/100 iters), loss = 0.448686
I0825 16:56:41.713634  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.502353 (* 1 = 0.502353 loss)
I0825 16:56:41.713662  1691 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0825 16:58:02.205554  1691 solver.cpp:357] Iteration 23200 (1.24214 iter/s, 80.5061s/100 iters), loss = 0.319913
I0825 16:58:02.205754  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.274225 (* 1 = 0.274225 loss)
I0825 16:58:02.205799  1691 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0825 16:59:19.512688  1691 solver.cpp:357] Iteration 23300 (1.29338 iter/s, 77.3168s/100 iters), loss = 0.38068
I0825 16:59:19.512895  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458659 (* 1 = 0.458659 loss)
I0825 16:59:19.512928  1691 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0825 17:00:41.482110  1691 solver.cpp:357] Iteration 23400 (1.21987 iter/s, 81.9761s/100 iters), loss = 0.369398
I0825 17:00:41.482551  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.321138 (* 1 = 0.321138 loss)
I0825 17:00:41.482661  1691 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0825 17:01:09.536763  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:02:02.879243  1691 solver.cpp:514] Iteration 23500, Testing net (#0)
I0825 17:02:54.531080  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:02:54.674878  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4505
I0825 17:02:54.674965  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.55015 (* 1 = 2.55015 loss)
I0825 17:02:55.261099  1691 solver.cpp:357] Iteration 23500 (0.747452 iter/s, 133.788s/100 iters), loss = 0.40316
I0825 17:02:55.261232  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468657 (* 1 = 0.468657 loss)
I0825 17:02:55.261260  1691 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0825 17:04:11.605012  1691 solver.cpp:357] Iteration 23600 (1.30981 iter/s, 76.347s/100 iters), loss = 0.35355
I0825 17:04:11.605260  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497565 (* 1 = 0.497565 loss)
I0825 17:04:11.605309  1691 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0825 17:05:31.566942  1691 solver.cpp:357] Iteration 23700 (1.25056 iter/s, 79.9645s/100 iters), loss = 0.405091
I0825 17:05:31.567456  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.468173 (* 1 = 0.468173 loss)
I0825 17:05:31.567633  1691 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0825 17:06:53.556882  1691 solver.cpp:357] Iteration 23800 (1.21963 iter/s, 81.992s/100 iters), loss = 0.405931
I0825 17:06:53.557097  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331491 (* 1 = 0.331491 loss)
I0825 17:06:53.557143  1691 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0825 17:07:12.569164  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:08:09.580703  1691 solver.cpp:357] Iteration 23900 (1.31536 iter/s, 76.0248s/100 iters), loss = 0.409462
I0825 17:08:09.580956  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.336946 (* 1 = 0.336946 loss)
I0825 17:08:09.581002  1691 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0825 17:09:30.721612  1691 solver.cpp:514] Iteration 24000, Testing net (#0)
I0825 17:10:23.898478  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:10:24.122093  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5402
I0825 17:10:24.122254  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.84973 (* 1 = 1.84973 loss)
I0825 17:10:24.783444  1691 solver.cpp:357] Iteration 24000 (0.739597 iter/s, 135.209s/100 iters), loss = 0.418454
I0825 17:10:24.783619  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378944 (* 1 = 0.378944 loss)
I0825 17:10:24.783666  1691 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0825 17:11:45.289778  1691 solver.cpp:357] Iteration 24100 (1.24214 iter/s, 80.5063s/100 iters), loss = 0.384235
I0825 17:11:45.289995  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366301 (* 1 = 0.366301 loss)
I0825 17:11:45.290024  1691 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0825 17:13:03.401911  1691 solver.cpp:357] Iteration 24200 (1.28025 iter/s, 78.1096s/100 iters), loss = 0.310431
I0825 17:13:03.402137  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155118 (* 1 = 0.155118 loss)
I0825 17:13:03.402182  1691 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0825 17:13:16.692421  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:14:23.720160  1691 solver.cpp:357] Iteration 24300 (1.24506 iter/s, 80.3176s/100 iters), loss = 0.535169
I0825 17:14:23.720690  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.44757 (* 1 = 0.44757 loss)
I0825 17:14:23.720866  1691 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0825 17:15:43.501011  1691 solver.cpp:357] Iteration 24400 (1.25345 iter/s, 79.78s/100 iters), loss = 0.344932
I0825 17:15:43.501272  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.436501 (* 1 = 0.436501 loss)
I0825 17:15:43.501319  1691 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0825 17:16:56.059851  1691 solver.cpp:514] Iteration 24500, Testing net (#0)
I0825 17:17:49.105417  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:17:49.265055  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7605
I0825 17:17:49.265199  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.693859 (* 1 = 0.693859 loss)
I0825 17:17:49.918740  1691 solver.cpp:357] Iteration 24500 (0.791031 iter/s, 126.417s/100 iters), loss = 0.447527
I0825 17:17:49.918913  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450831 (* 1 = 0.450831 loss)
I0825 17:17:49.918959  1691 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0825 17:19:11.387279  1691 solver.cpp:357] Iteration 24600 (1.22752 iter/s, 81.465s/100 iters), loss = 0.405422
I0825 17:19:11.387542  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.419348 (* 1 = 0.419348 loss)
I0825 17:19:11.387570  1691 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0825 17:19:16.930708  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:20:33.068511  1691 solver.cpp:357] Iteration 24700 (1.22429 iter/s, 81.6797s/100 iters), loss = 0.329115
I0825 17:20:33.068718  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358 (* 1 = 0.358 loss)
I0825 17:20:33.068753  1691 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0825 17:21:48.867979  1691 solver.cpp:357] Iteration 24800 (1.31934 iter/s, 75.7955s/100 iters), loss = 0.364223
I0825 17:21:48.868203  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.251034 (* 1 = 0.251034 loss)
I0825 17:21:48.868249  1691 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0825 17:23:10.371477  1691 solver.cpp:357] Iteration 24900 (1.22697 iter/s, 81.5017s/100 iters), loss = 0.583536
I0825 17:23:10.371696  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.719331 (* 1 = 0.719331 loss)
I0825 17:23:10.371743  1691 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0825 17:24:29.048547  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:24:30.489295  1691 solver.cpp:514] Iteration 25000, Testing net (#0)
I0825 17:25:22.126740  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:25:22.375490  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7099
I0825 17:25:22.375646  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.909179 (* 1 = 0.909179 loss)
I0825 17:25:22.849145  1691 solver.cpp:357] Iteration 25000 (0.754846 iter/s, 132.477s/100 iters), loss = 0.397698
I0825 17:25:22.849273  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.348645 (* 1 = 0.348645 loss)
I0825 17:25:22.849300  1691 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0825 17:26:40.766229  1691 solver.cpp:357] Iteration 25100 (1.28342 iter/s, 77.9171s/100 iters), loss = 0.442458
I0825 17:26:40.766783  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.408368 (* 1 = 0.408368 loss)
I0825 17:26:40.766963  1691 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0825 17:28:02.537824  1691 solver.cpp:357] Iteration 25200 (1.22295 iter/s, 81.7696s/100 iters), loss = 0.546423
I0825 17:28:02.538051  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487684 (* 1 = 0.487684 loss)
I0825 17:28:02.538098  1691 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0825 17:29:23.605588  1691 solver.cpp:357] Iteration 25300 (1.2336 iter/s, 81.0635s/100 iters), loss = 0.443426
I0825 17:29:23.605821  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.536426 (* 1 = 0.536426 loss)
I0825 17:29:23.605868  1691 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0825 17:30:30.143970  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:30:39.293535  1691 solver.cpp:357] Iteration 25400 (1.32122 iter/s, 75.6878s/100 iters), loss = 0.47837
I0825 17:30:39.293725  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.282174 (* 1 = 0.282174 loss)
I0825 17:30:39.293792  1691 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0825 17:32:00.396891  1691 solver.cpp:514] Iteration 25500, Testing net (#0)
I0825 17:32:53.597297  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:32:53.874603  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6193
I0825 17:32:53.874755  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.57021 (* 1 = 1.57021 loss)
I0825 17:32:54.578423  1691 solver.cpp:357] Iteration 25500 (0.739253 iter/s, 135.272s/100 iters), loss = 0.414775
I0825 17:32:54.578591  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.393191 (* 1 = 0.393191 loss)
I0825 17:32:54.578636  1691 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0825 17:34:15.323596  1691 solver.cpp:357] Iteration 25600 (1.23858 iter/s, 80.7379s/100 iters), loss = 0.377385
I0825 17:34:15.323829  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378132 (* 1 = 0.378132 loss)
I0825 17:34:15.323860  1691 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0825 17:35:27.000385  1691 solver.cpp:357] Iteration 25700 (1.39531 iter/s, 71.6687s/100 iters), loss = 0.314754
I0825 17:35:27.000598  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.307061 (* 1 = 0.307061 loss)
I0825 17:35:27.000643  1691 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0825 17:36:31.604753  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:36:48.529494  1691 solver.cpp:357] Iteration 25800 (1.22664 iter/s, 81.5233s/100 iters), loss = 0.39377
I0825 17:36:48.529630  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.256291 (* 1 = 0.256291 loss)
I0825 17:36:48.529657  1691 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0825 17:38:10.934391  1691 solver.cpp:357] Iteration 25900 (1.21363 iter/s, 82.3975s/100 iters), loss = 0.464058
I0825 17:38:10.934603  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.483342 (* 1 = 0.483342 loss)
I0825 17:38:10.934648  1691 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0825 17:39:28.928895  1691 solver.cpp:514] Iteration 26000, Testing net (#0)
I0825 17:40:21.506327  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:40:21.674356  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.683099
I0825 17:40:21.674473  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00013 (* 1 = 1.00013 loss)
I0825 17:40:22.337805  1691 solver.cpp:357] Iteration 26000 (0.761068 iter/s, 131.394s/100 iters), loss = 0.416544
I0825 17:40:22.337992  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.481228 (* 1 = 0.481228 loss)
I0825 17:40:22.338039  1691 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0825 17:41:43.936134  1691 solver.cpp:357] Iteration 26100 (1.22561 iter/s, 81.5922s/100 iters), loss = 0.282725
I0825 17:41:43.936380  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.235241 (* 1 = 0.235241 loss)
I0825 17:41:43.936426  1691 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0825 17:42:39.364297  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:43:04.699753  1691 solver.cpp:357] Iteration 26200 (1.2383 iter/s, 80.7558s/100 iters), loss = 0.463016
I0825 17:43:04.699906  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472519 (* 1 = 0.472519 loss)
I0825 17:43:04.699934  1691 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0825 17:44:19.379528  1691 solver.cpp:357] Iteration 26300 (1.33911 iter/s, 74.6764s/100 iters), loss = 0.308184
I0825 17:44:19.379693  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338507 (* 1 = 0.338507 loss)
I0825 17:44:19.379722  1691 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0825 17:45:39.283283  1691 solver.cpp:357] Iteration 26400 (1.25159 iter/s, 79.8985s/100 iters), loss = 0.364916
I0825 17:45:39.283479  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381612 (* 1 = 0.381612 loss)
I0825 17:45:39.283507  1691 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0825 17:47:00.558439  1691 solver.cpp:514] Iteration 26500, Testing net (#0)
I0825 17:47:55.257901  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:47:55.369653  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5886
I0825 17:47:55.369804  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.44633 (* 1 = 1.44633 loss)
I0825 17:47:56.028192  1691 solver.cpp:357] Iteration 26500 (0.731329 iter/s, 136.737s/100 iters), loss = 0.405925
I0825 17:47:56.028372  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413957 (* 1 = 0.413957 loss)
I0825 17:47:56.028419  1691 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0825 17:48:41.036244  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:49:12.626601  1691 solver.cpp:357] Iteration 26600 (1.30563 iter/s, 76.5915s/100 iters), loss = 0.249091
I0825 17:49:12.627140  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.209634 (* 1 = 0.209634 loss)
I0825 17:49:12.627318  1691 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0825 17:50:34.992359  1691 solver.cpp:357] Iteration 26700 (1.21413 iter/s, 82.3633s/100 iters), loss = 0.466221
I0825 17:50:34.992889  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.471498 (* 1 = 0.471498 loss)
I0825 17:50:34.993068  1691 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0825 17:51:56.656594  1691 solver.cpp:357] Iteration 26800 (1.22463 iter/s, 81.6576s/100 iters), loss = 0.453783
I0825 17:51:56.656747  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428123 (* 1 = 0.428123 loss)
I0825 17:51:56.656775  1691 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0825 17:53:14.620402  1691 solver.cpp:357] Iteration 26900 (1.28268 iter/s, 77.9615s/100 iters), loss = 0.360686
I0825 17:53:14.620633  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.262179 (* 1 = 0.262179 loss)
I0825 17:53:14.620681  1691 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0825 17:53:54.575708  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:54:31.138849  1691 solver.cpp:514] Iteration 27000, Testing net (#0)
I0825 17:55:21.905544  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 17:55:22.187496  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.665
I0825 17:55:22.187665  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.10678 (* 1 = 1.10678 loss)
I0825 17:55:22.685966  1691 solver.cpp:357] Iteration 27000 (0.780875 iter/s, 128.062s/100 iters), loss = 0.413607
I0825 17:55:22.686142  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.364405 (* 1 = 0.364405 loss)
I0825 17:55:22.686187  1691 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0825 17:56:44.371069  1691 solver.cpp:357] Iteration 27100 (1.22428 iter/s, 81.6809s/100 iters), loss = 0.393292
I0825 17:56:44.371328  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.395669 (* 1 = 0.395669 loss)
I0825 17:56:44.371373  1691 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0825 17:58:01.620101  1691 solver.cpp:357] Iteration 27200 (1.29452 iter/s, 77.2488s/100 iters), loss = 0.438681
I0825 17:58:01.620563  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41871 (* 1 = 0.41871 loss)
I0825 17:58:01.620736  1691 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0825 17:59:23.851791  1691 solver.cpp:357] Iteration 27300 (1.21608 iter/s, 82.2313s/100 iters), loss = 0.394659
I0825 17:59:23.851985  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392158 (* 1 = 0.392158 loss)
I0825 17:59:23.852030  1691 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0825 17:59:57.720765  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:00:45.767673  1691 solver.cpp:357] Iteration 27400 (1.22076 iter/s, 81.9159s/100 iters), loss = 0.276527
I0825 18:00:45.767904  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.311667 (* 1 = 0.311667 loss)
I0825 18:00:45.767951  1691 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0825 18:02:05.026229  1691 solver.cpp:514] Iteration 27500, Testing net (#0)
I0825 18:02:55.190985  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:02:55.379124  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5032
I0825 18:02:55.379281  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.28655 (* 1 = 2.28655 loss)
I0825 18:02:56.116225  1691 solver.cpp:357] Iteration 27500 (0.767196 iter/s, 130.345s/100 iters), loss = 0.364061
I0825 18:02:56.116396  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.232445 (* 1 = 0.232445 loss)
I0825 18:02:56.116443  1691 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0825 18:04:17.830406  1691 solver.cpp:357] Iteration 27600 (1.22387 iter/s, 81.708s/100 iters), loss = 0.457532
I0825 18:04:17.830735  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380746 (* 1 = 0.380746 loss)
I0825 18:04:17.830783  1691 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0825 18:05:35.793993  1691 solver.cpp:357] Iteration 27700 (1.28268 iter/s, 77.9619s/100 iters), loss = 0.558301
I0825 18:05:35.794164  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.571941 (* 1 = 0.571941 loss)
I0825 18:05:35.794193  1691 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0825 18:06:01.808190  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:06:51.583431  1691 solver.cpp:357] Iteration 27800 (1.3195 iter/s, 75.7862s/100 iters), loss = 0.311117
I0825 18:06:51.583955  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.356208 (* 1 = 0.356208 loss)
I0825 18:06:51.584134  1691 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0825 18:08:11.317821  1691 solver.cpp:357] Iteration 27900 (1.25421 iter/s, 79.7312s/100 iters), loss = 0.510623
I0825 18:08:11.318015  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.466138 (* 1 = 0.466138 loss)
I0825 18:08:11.318042  1691 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0825 18:09:32.431280  1691 solver.cpp:514] Iteration 28000, Testing net (#0)
I0825 18:10:26.025326  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:10:26.267204  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.699
I0825 18:10:26.267552  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.963956 (* 1 = 0.963956 loss)
I0825 18:10:26.887475  1691 solver.cpp:357] Iteration 28000 (0.737631 iter/s, 135.569s/100 iters), loss = 0.292902
I0825 18:10:26.887881  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.333999 (* 1 = 0.333999 loss)
I0825 18:10:26.888058  1691 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0825 18:11:44.129927  1691 solver.cpp:357] Iteration 28100 (1.29468 iter/s, 77.239s/100 iters), loss = 0.379342
I0825 18:11:44.130107  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.505333 (* 1 = 0.505333 loss)
I0825 18:11:44.130136  1691 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0825 18:12:02.591670  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:13:06.148313  1691 solver.cpp:357] Iteration 28200 (1.21926 iter/s, 82.0171s/100 iters), loss = 0.496742
I0825 18:13:06.148485  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430249 (* 1 = 0.430249 loss)
I0825 18:13:06.148514  1691 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0825 18:14:27.385491  1691 solver.cpp:357] Iteration 28300 (1.23105 iter/s, 81.2316s/100 iters), loss = 0.485325
I0825 18:14:27.385733  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.584111 (* 1 = 0.584111 loss)
I0825 18:14:27.385781  1691 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0825 18:15:44.669837  1691 solver.cpp:357] Iteration 28400 (1.29398 iter/s, 77.2808s/100 iters), loss = 0.31615
I0825 18:15:44.670367  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.325468 (* 1 = 0.325468 loss)
I0825 18:15:44.670549  1691 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0825 18:17:01.854662  1691 solver.cpp:514] Iteration 28500, Testing net (#0)
I0825 18:17:56.157233  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:17:56.430974  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.744599
I0825 18:17:56.431195  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.80335 (* 1 = 0.80335 loss)
I0825 18:17:56.954056  1691 solver.cpp:357] Iteration 28500 (0.755955 iter/s, 132.283s/100 iters), loss = 0.391571
I0825 18:17:56.954214  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.397341 (* 1 = 0.397341 loss)
I0825 18:17:56.954257  1691 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0825 18:18:07.987809  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:19:17.592340  1691 solver.cpp:357] Iteration 28600 (1.2401 iter/s, 80.6388s/100 iters), loss = 0.462364
I0825 18:19:17.592641  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.473057 (* 1 = 0.473057 loss)
I0825 18:19:17.592690  1691 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0825 18:20:35.314291  1691 solver.cpp:357] Iteration 28700 (1.2867 iter/s, 77.7184s/100 iters), loss = 0.32265
I0825 18:20:35.314848  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.278991 (* 1 = 0.278991 loss)
I0825 18:20:35.315026  1691 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0825 18:21:56.172272  1691 solver.cpp:357] Iteration 28800 (1.23676 iter/s, 80.8565s/100 iters), loss = 0.287551
I0825 18:21:56.172513  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374329 (* 1 = 0.374329 loss)
I0825 18:21:56.172559  1691 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0825 18:23:17.815802  1691 solver.cpp:357] Iteration 28900 (1.22492 iter/s, 81.6378s/100 iters), loss = 0.445349
I0825 18:23:17.816037  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409371 (* 1 = 0.409371 loss)
I0825 18:23:17.816085  1691 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0825 18:23:20.896330  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:24:38.133496  1691 solver.cpp:514] Iteration 29000, Testing net (#0)
I0825 18:25:24.119798  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:25:24.329046  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4823
I0825 18:25:24.329149  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.10205 (* 1 = 2.10205 loss)
I0825 18:25:24.993775  1691 solver.cpp:357] Iteration 29000 (0.786332 iter/s, 127.173s/100 iters), loss = 0.431085
I0825 18:25:24.993938  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373641 (* 1 = 0.373641 loss)
I0825 18:25:24.993983  1691 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0825 18:26:46.774974  1691 solver.cpp:357] Iteration 29100 (1.2228 iter/s, 81.7797s/100 iters), loss = 0.36909
I0825 18:26:46.775212  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367975 (* 1 = 0.367975 loss)
I0825 18:26:46.775259  1691 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0825 18:28:08.335422  1691 solver.cpp:357] Iteration 29200 (1.22617 iter/s, 81.5547s/100 iters), loss = 0.343874
I0825 18:28:08.335676  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.295232 (* 1 = 0.295232 loss)
I0825 18:28:08.335722  1691 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0825 18:29:23.872797  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:29:28.093031  1691 solver.cpp:357] Iteration 29300 (1.25386 iter/s, 79.7539s/100 iters), loss = 0.347142
I0825 18:29:28.093204  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296925 (* 1 = 0.296925 loss)
I0825 18:29:28.093250  1691 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0825 18:30:46.555647  1691 solver.cpp:357] Iteration 29400 (1.27459 iter/s, 78.4568s/100 iters), loss = 0.351783
I0825 18:30:46.555860  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.326529 (* 1 = 0.326529 loss)
I0825 18:30:46.555904  1691 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0825 18:32:06.688376  1691 solver.cpp:514] Iteration 29500, Testing net (#0)
I0825 18:32:59.513245  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:32:59.736634  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6923
I0825 18:32:59.736753  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05553 (* 1 = 1.05553 loss)
I0825 18:33:00.394371  1691 solver.cpp:357] Iteration 29500 (0.747174 iter/s, 133.838s/100 iters), loss = 0.493149
I0825 18:33:00.394783  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.367699 (* 1 = 0.367699 loss)
I0825 18:33:00.394961  1691 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0825 18:34:18.091204  1691 solver.cpp:357] Iteration 29600 (1.28711 iter/s, 77.6932s/100 iters), loss = 0.270045
I0825 18:34:18.091511  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.249071 (* 1 = 0.249071 loss)
I0825 18:34:18.091562  1691 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0825 18:35:22.515862  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:35:34.598145  1691 solver.cpp:357] Iteration 29700 (1.3071 iter/s, 76.5053s/100 iters), loss = 0.320678
I0825 18:35:34.598331  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260163 (* 1 = 0.260163 loss)
I0825 18:35:34.598381  1691 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0825 18:36:56.262346  1691 solver.cpp:357] Iteration 29800 (1.22458 iter/s, 81.6606s/100 iters), loss = 0.409324
I0825 18:36:56.262590  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.4607 (* 1 = 0.4607 loss)
I0825 18:36:56.262637  1691 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0825 18:38:17.839264  1691 solver.cpp:357] Iteration 29900 (1.22592 iter/s, 81.5712s/100 iters), loss = 0.369878
I0825 18:38:17.839398  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.365635 (* 1 = 0.365635 loss)
I0825 18:38:17.839429  1691 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0825 18:39:33.397032  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_30000.caffemodel
I0825 18:39:33.415077  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_30000.solverstate
I0825 18:39:33.420459  1691 solver.cpp:514] Iteration 30000, Testing net (#0)
I0825 18:40:27.630540  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:40:27.857054  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.554801
I0825 18:40:27.857302  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.44349 (* 1 = 1.44349 loss)
I0825 18:40:28.474956  1691 solver.cpp:357] Iteration 30000 (0.765456 iter/s, 130.641s/100 iters), loss = 0.261523
I0825 18:40:28.475248  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.307034 (* 1 = 0.307034 loss)
I0825 18:40:28.475340  1691 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0825 18:41:30.578054  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:41:49.942603  1691 solver.cpp:357] Iteration 30100 (1.22743 iter/s, 81.4713s/100 iters), loss = 0.457565
I0825 18:41:49.943028  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.304715 (* 1 = 0.304715 loss)
I0825 18:41:49.943209  1691 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0825 18:43:09.529218  1691 solver.cpp:357] Iteration 30200 (1.25646 iter/s, 79.5889s/100 iters), loss = 0.54336
I0825 18:43:09.529695  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.639028 (* 1 = 0.639028 loss)
I0825 18:43:09.529877  1691 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0825 18:44:27.293047  1691 solver.cpp:357] Iteration 30300 (1.28586 iter/s, 77.7691s/100 iters), loss = 0.446468
I0825 18:44:27.293517  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.499049 (* 1 = 0.499049 loss)
I0825 18:44:27.293697  1691 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0825 18:45:43.836881  1691 solver.cpp:357] Iteration 30400 (1.3064 iter/s, 76.5461s/100 iters), loss = 0.244077
I0825 18:45:43.837085  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.231258 (* 1 = 0.231258 loss)
I0825 18:45:43.837131  1691 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0825 18:46:37.393010  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:47:04.333415  1691 solver.cpp:514] Iteration 30500, Testing net (#0)
I0825 18:47:54.410181  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:47:54.590185  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7005
I0825 18:47:54.590281  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.993211 (* 1 = 0.993211 loss)
I0825 18:47:55.201946  1691 solver.cpp:357] Iteration 30500 (0.761181 iter/s, 131.375s/100 iters), loss = 0.319033
I0825 18:47:55.202121  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.406696 (* 1 = 0.406696 loss)
I0825 18:47:55.202167  1691 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0825 18:49:15.599225  1691 solver.cpp:357] Iteration 30600 (1.24381 iter/s, 80.3981s/100 iters), loss = 0.36011
I0825 18:49:15.600862  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.217544 (* 1 = 0.217544 loss)
I0825 18:49:15.600891  1691 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0825 18:50:37.205446  1691 solver.cpp:357] Iteration 30700 (1.22536 iter/s, 81.6088s/100 iters), loss = 0.430098
I0825 18:50:37.205682  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.287052 (* 1 = 0.287052 loss)
I0825 18:50:37.205729  1691 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0825 18:51:59.024945  1691 solver.cpp:357] Iteration 30800 (1.2222 iter/s, 81.8196s/100 iters), loss = 0.223063
I0825 18:51:59.025116  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.268163 (* 1 = 0.268163 loss)
I0825 18:51:59.025161  1691 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0825 18:52:40.842005  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:53:15.495971  1691 solver.cpp:357] Iteration 30900 (1.30765 iter/s, 76.4733s/100 iters), loss = 0.501639
I0825 18:53:15.496156  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.353001 (* 1 = 0.353001 loss)
I0825 18:53:15.496187  1691 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0825 18:54:36.598207  1691 solver.cpp:514] Iteration 31000, Testing net (#0)
I0825 18:55:27.890671  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:55:28.100592  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6044
I0825 18:55:28.100908  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.36598 (* 1 = 1.36598 loss)
I0825 18:55:28.717068  1691 solver.cpp:357] Iteration 31000 (0.750623 iter/s, 133.223s/100 iters), loss = 0.326603
I0825 18:55:28.717437  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.29183 (* 1 = 0.29183 loss)
I0825 18:55:28.717608  1691 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0825 18:56:46.493158  1691 solver.cpp:357] Iteration 31100 (1.2857 iter/s, 77.7785s/100 iters), loss = 0.302044
I0825 18:56:46.493698  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296785 (* 1 = 0.296785 loss)
I0825 18:56:46.493878  1691 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0825 18:58:02.681324  1691 solver.cpp:357] Iteration 31200 (1.31256 iter/s, 76.1872s/100 iters), loss = 0.268414
I0825 18:58:02.681836  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.221967 (* 1 = 0.221967 loss)
I0825 18:58:02.682018  1691 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0825 18:58:41.283875  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 18:59:24.371887  1691 solver.cpp:357] Iteration 31300 (1.22415 iter/s, 81.6892s/100 iters), loss = 0.455302
I0825 18:59:24.373935  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.429672 (* 1 = 0.429672 loss)
I0825 18:59:24.374125  1691 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0825 19:00:46.127928  1691 solver.cpp:357] Iteration 31400 (1.22321 iter/s, 81.7524s/100 iters), loss = 0.513034
I0825 19:00:46.128464  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.577797 (* 1 = 0.577797 loss)
I0825 19:00:46.128644  1691 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0825 19:02:02.678907  1691 solver.cpp:514] Iteration 31500, Testing net (#0)
I0825 19:02:57.120082  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:02:57.303251  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6441
I0825 19:02:57.303377  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.31126 (* 1 = 1.31126 loss)
I0825 19:02:57.887907  1691 solver.cpp:357] Iteration 31500 (0.758968 iter/s, 131.758s/100 iters), loss = 0.298794
I0825 19:02:57.888084  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.320599 (* 1 = 0.320599 loss)
I0825 19:02:57.888131  1691 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0825 19:04:19.514888  1691 solver.cpp:357] Iteration 31600 (1.22511 iter/s, 81.6251s/100 iters), loss = 0.342999
I0825 19:04:19.515163  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.260553 (* 1 = 0.260553 loss)
I0825 19:04:19.515199  1691 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0825 19:04:49.331280  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:05:37.521633  1691 solver.cpp:357] Iteration 31700 (1.28201 iter/s, 78.0026s/100 iters), loss = 0.406515
I0825 19:05:37.521878  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.472776 (* 1 = 0.472776 loss)
I0825 19:05:37.521924  1691 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0825 19:06:52.731716  1691 solver.cpp:357] Iteration 31800 (1.32965 iter/s, 75.2079s/100 iters), loss = 0.336804
I0825 19:06:52.731958  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347085 (* 1 = 0.347085 loss)
I0825 19:06:52.732003  1691 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0825 19:08:14.463912  1691 solver.cpp:357] Iteration 31900 (1.22354 iter/s, 81.7301s/100 iters), loss = 0.447928
I0825 19:08:14.464110  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403751 (* 1 = 0.403751 loss)
I0825 19:08:14.464143  1691 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0825 19:09:35.082974  1691 solver.cpp:514] Iteration 32000, Testing net (#0)
I0825 19:10:27.132997  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:10:27.288589  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6041
I0825 19:10:27.288668  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.83523 (* 1 = 1.83523 loss)
I0825 19:10:27.959048  1691 solver.cpp:357] Iteration 32000 (0.749083 iter/s, 133.497s/100 iters), loss = 0.415301
I0825 19:10:27.959219  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.603774 (* 1 = 0.603774 loss)
I0825 19:10:27.959264  1691 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0825 19:10:27.959300  1691 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0825 19:10:49.104387  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:11:46.824405  1691 solver.cpp:357] Iteration 32100 (1.26802 iter/s, 78.8633s/100 iters), loss = 0.229652
I0825 19:11:46.824632  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.266099 (* 1 = 0.266099 loss)
I0825 19:11:46.824679  1691 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0825 19:13:08.208947  1691 solver.cpp:357] Iteration 32200 (1.22877 iter/s, 81.3823s/100 iters), loss = 0.320607
I0825 19:13:08.209188  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.263226 (* 1 = 0.263226 loss)
I0825 19:13:08.209236  1691 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0825 19:14:30.118939  1691 solver.cpp:357] Iteration 32300 (1.22099 iter/s, 81.9006s/100 iters), loss = 0.209381
I0825 19:14:30.119171  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.190002 (* 1 = 0.190002 loss)
I0825 19:14:30.119218  1691 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0825 19:15:40.986845  1691 solver.cpp:357] Iteration 32400 (1.41126 iter/s, 70.8589s/100 iters), loss = 0.240413
I0825 19:15:40.987054  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.186108 (* 1 = 0.186108 loss)
I0825 19:15:40.987100  1691 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0825 19:15:57.007164  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:17:02.487103  1691 solver.cpp:514] Iteration 32500, Testing net (#0)
I0825 19:17:56.756785  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:17:57.019093  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888102
I0825 19:17:57.019245  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.321739 (* 1 = 0.321739 loss)
I0825 19:17:57.639739  1691 solver.cpp:357] Iteration 32500 (0.731854 iter/s, 136.639s/100 iters), loss = 0.152443
I0825 19:17:57.639922  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.178578 (* 1 = 0.178578 loss)
I0825 19:17:57.639967  1691 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0825 19:19:19.047811  1691 solver.cpp:357] Iteration 32600 (1.2285 iter/s, 81.4004s/100 iters), loss = 0.183477
I0825 19:19:19.048039  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.133616 (* 1 = 0.133616 loss)
I0825 19:19:19.048069  1691 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0825 19:20:34.969784  1691 solver.cpp:357] Iteration 32700 (1.31722 iter/s, 75.9172s/100 iters), loss = 0.193602
I0825 19:20:34.970022  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.215761 (* 1 = 0.215761 loss)
I0825 19:20:34.970069  1691 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0825 19:21:56.193555  1691 solver.cpp:357] Iteration 32800 (1.23127 iter/s, 81.2172s/100 iters), loss = 0.187099
I0825 19:21:56.193805  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.190501 (* 1 = 0.190501 loss)
I0825 19:21:56.193852  1691 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0825 19:22:04.098009  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:23:16.634385  1691 solver.cpp:357] Iteration 32900 (1.24325 iter/s, 80.4347s/100 iters), loss = 0.125081
I0825 19:23:16.634614  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114356 (* 1 = 0.114356 loss)
I0825 19:23:16.634661  1691 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0825 19:24:33.310751  1691 solver.cpp:514] Iteration 33000, Testing net (#0)
I0825 19:25:24.280386  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:25:24.535185  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.873002
I0825 19:25:24.535326  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.361466 (* 1 = 0.361466 loss)
I0825 19:25:25.113324  1691 solver.cpp:357] Iteration 33000 (0.778377 iter/s, 128.472s/100 iters), loss = 0.154514
I0825 19:25:25.113497  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0948576 (* 1 = 0.0948576 loss)
I0825 19:25:25.113543  1691 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0825 19:26:46.111640  1691 solver.cpp:357] Iteration 33100 (1.23464 iter/s, 80.9951s/100 iters), loss = 0.209596
I0825 19:26:46.111861  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.124376 (* 1 = 0.124376 loss)
I0825 19:26:46.111908  1691 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0825 19:28:08.012765  1691 solver.cpp:357] Iteration 33200 (1.22108 iter/s, 81.8947s/100 iters), loss = 0.175452
I0825 19:28:08.013005  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136561 (* 1 = 0.136561 loss)
I0825 19:28:08.013053  1691 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0825 19:28:08.566028  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:29:24.402000  1691 solver.cpp:357] Iteration 33300 (1.30921 iter/s, 76.3822s/100 iters), loss = 0.225141
I0825 19:29:24.402235  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.158093 (* 1 = 0.158093 loss)
I0825 19:29:24.402281  1691 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0825 19:30:45.984169  1691 solver.cpp:357] Iteration 33400 (1.22586 iter/s, 81.5753s/100 iters), loss = 0.105227
I0825 19:30:45.984387  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142039 (* 1 = 0.142039 loss)
I0825 19:30:45.984433  1691 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0825 19:32:06.827276  1691 solver.cpp:514] Iteration 33500, Testing net (#0)
I0825 19:33:01.362565  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:33:01.597805  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.887402
I0825 19:33:01.597957  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.3342 (* 1 = 0.3342 loss)
I0825 19:33:02.169723  1691 solver.cpp:357] Iteration 33500 (0.734318 iter/s, 136.181s/100 iters), loss = 0.19888
I0825 19:33:02.169903  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.246814 (* 1 = 0.246814 loss)
I0825 19:33:02.169948  1691 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0825 19:34:11.449810  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:34:18.091231  1691 solver.cpp:357] Iteration 33600 (1.31719 iter/s, 75.9192s/100 iters), loss = 0.213323
I0825 19:34:18.091655  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.278141 (* 1 = 0.278141 loss)
I0825 19:34:18.091836  1691 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0825 19:35:35.932646  1691 solver.cpp:357] Iteration 33700 (1.28474 iter/s, 77.837s/100 iters), loss = 0.152182
I0825 19:35:35.933192  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.188174 (* 1 = 0.188174 loss)
I0825 19:35:35.933372  1691 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0825 19:36:57.235546  1691 solver.cpp:357] Iteration 33800 (1.23007 iter/s, 81.2965s/100 iters), loss = 0.169187
I0825 19:36:57.235870  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0995543 (* 1 = 0.0995543 loss)
I0825 19:36:57.235968  1691 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0825 19:38:14.251487  1691 solver.cpp:357] Iteration 33900 (1.29844 iter/s, 77.0156s/100 iters), loss = 0.124705
I0825 19:38:14.251655  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.16241 (* 1 = 0.16241 loss)
I0825 19:38:14.251699  1691 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0825 19:39:21.146667  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:39:35.532018  1691 solver.cpp:514] Iteration 34000, Testing net (#0)
I0825 19:40:29.349335  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:40:29.591483  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.883402
I0825 19:40:29.591603  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.35028 (* 1 = 0.35028 loss)
I0825 19:40:30.244571  1691 solver.cpp:357] Iteration 34000 (0.735341 iter/s, 135.991s/100 iters), loss = 0.108417
I0825 19:40:30.244731  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101554 (* 1 = 0.101554 loss)
I0825 19:40:30.244776  1691 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0825 19:41:52.015609  1691 solver.cpp:357] Iteration 34100 (1.22296 iter/s, 81.7691s/100 iters), loss = 0.122658
I0825 19:41:52.015811  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170753 (* 1 = 0.170753 loss)
I0825 19:41:52.015857  1691 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0825 19:43:09.436594  1691 solver.cpp:357] Iteration 34200 (1.29171 iter/s, 77.417s/100 iters), loss = 0.113191
I0825 19:43:09.437132  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0648081 (* 1 = 0.0648081 loss)
I0825 19:43:09.437314  1691 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0825 19:44:31.579252  1691 solver.cpp:357] Iteration 34300 (1.21748 iter/s, 82.1366s/100 iters), loss = 0.125241
I0825 19:44:31.579488  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169574 (* 1 = 0.169574 loss)
I0825 19:44:31.579533  1691 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0825 19:45:27.825884  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:45:49.543742  1691 solver.cpp:357] Iteration 34400 (1.28267 iter/s, 77.9627s/100 iters), loss = 0.19907
I0825 19:45:49.543925  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.179076 (* 1 = 0.179076 loss)
I0825 19:45:49.543972  1691 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0825 19:47:06.271587  1691 solver.cpp:514] Iteration 34500, Testing net (#0)
I0825 19:47:58.922828  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:47:59.119249  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.885102
I0825 19:47:59.119345  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.345965 (* 1 = 0.345965 loss)
I0825 19:47:59.847283  1691 solver.cpp:357] Iteration 34500 (0.767455 iter/s, 130.301s/100 iters), loss = 0.139688
I0825 19:47:59.847407  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.175976 (* 1 = 0.175976 loss)
I0825 19:47:59.847432  1691 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0825 19:49:21.548575  1691 solver.cpp:357] Iteration 34600 (1.22391 iter/s, 81.7053s/100 iters), loss = 0.134462
I0825 19:49:21.549163  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126112 (* 1 = 0.126112 loss)
I0825 19:49:21.549345  1691 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0825 19:50:43.478090  1691 solver.cpp:357] Iteration 34700 (1.22051 iter/s, 81.9328s/100 iters), loss = 0.127321
I0825 19:50:43.478822  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129147 (* 1 = 0.129147 loss)
I0825 19:50:43.479004  1691 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0825 19:51:32.709973  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:52:00.602807  1691 solver.cpp:357] Iteration 34800 (1.29656 iter/s, 77.1271s/100 iters), loss = 0.125023
I0825 19:52:00.602948  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142784 (* 1 = 0.142784 loss)
I0825 19:52:00.602977  1691 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0825 19:53:22.363577  1691 solver.cpp:357] Iteration 34900 (1.22311 iter/s, 81.7587s/100 iters), loss = 0.165241
I0825 19:53:22.363827  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.116721 (* 1 = 0.116721 loss)
I0825 19:53:22.363874  1691 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0825 19:54:43.311856  1691 solver.cpp:514] Iteration 35000, Testing net (#0)
I0825 19:55:34.397058  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:55:34.641465  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.901802
I0825 19:55:34.641613  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.297143 (* 1 = 0.297143 loss)
I0825 19:55:35.132871  1691 solver.cpp:357] Iteration 35000 (0.753177 iter/s, 132.771s/100 iters), loss = 0.0836482
I0825 19:55:35.133049  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0880175 (* 1 = 0.0880175 loss)
I0825 19:55:35.133093  1691 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0825 19:56:49.290210  1691 solver.cpp:357] Iteration 35100 (1.34851 iter/s, 74.156s/100 iters), loss = 0.148293
I0825 19:56:49.290444  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121011 (* 1 = 0.121011 loss)
I0825 19:56:49.290488  1691 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0825 19:57:33.126670  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 19:58:11.341588  1691 solver.cpp:357] Iteration 35200 (1.2188 iter/s, 82.048s/100 iters), loss = 0.149307
I0825 19:58:11.341799  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102243 (* 1 = 0.102243 loss)
I0825 19:58:11.341842  1691 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0825 19:59:31.863976  1691 solver.cpp:357] Iteration 35300 (1.24188 iter/s, 80.5229s/100 iters), loss = 0.213975
I0825 19:59:31.864171  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.262151 (* 1 = 0.262151 loss)
I0825 19:59:31.864217  1691 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0825 20:00:49.511512  1691 solver.cpp:357] Iteration 35400 (1.28787 iter/s, 77.6477s/100 iters), loss = 0.136871
I0825 20:00:49.511768  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126647 (* 1 = 0.126647 loss)
I0825 20:00:49.511816  1691 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0825 20:02:09.055384  1691 solver.cpp:514] Iteration 35500, Testing net (#0)
I0825 20:03:02.266314  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:03:02.537081  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.893102
I0825 20:03:02.537187  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.337442 (* 1 = 0.337442 loss)
I0825 20:03:03.075608  1691 solver.cpp:357] Iteration 35500 (0.748719 iter/s, 133.562s/100 iters), loss = 0.146206
I0825 20:03:03.075801  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137232 (* 1 = 0.137232 loss)
I0825 20:03:03.075848  1691 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0825 20:03:39.330765  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:04:24.892380  1691 solver.cpp:357] Iteration 35600 (1.22225 iter/s, 81.8167s/100 iters), loss = 0.150824
I0825 20:04:24.892664  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117564 (* 1 = 0.117564 loss)
I0825 20:04:24.892711  1691 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0825 20:05:36.580533  1691 solver.cpp:357] Iteration 35700 (1.39494 iter/s, 71.6877s/100 iters), loss = 0.0906172
I0825 20:05:36.580684  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0743293 (* 1 = 0.0743293 loss)
I0825 20:05:36.580711  1691 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0825 20:06:57.663019  1691 solver.cpp:357] Iteration 35800 (1.23337 iter/s, 81.0785s/100 iters), loss = 0.126977
I0825 20:06:57.663533  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.198092 (* 1 = 0.198092 loss)
I0825 20:06:57.663713  1691 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0825 20:08:19.612542  1691 solver.cpp:357] Iteration 35900 (1.2203 iter/s, 81.947s/100 iters), loss = 0.106992
I0825 20:08:19.612781  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.111521 (* 1 = 0.111521 loss)
I0825 20:08:19.612828  1691 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0825 20:08:47.507228  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:09:39.359035  1691 solver.cpp:514] Iteration 36000, Testing net (#0)
I0825 20:10:29.720721  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:10:29.933491  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894602
I0825 20:10:29.933838  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.315977 (* 1 = 0.315977 loss)
I0825 20:10:30.678181  1691 solver.cpp:357] Iteration 36000 (0.762986 iter/s, 131.064s/100 iters), loss = 0.187868
I0825 20:10:30.678587  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.259298 (* 1 = 0.259298 loss)
I0825 20:10:30.678763  1691 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0825 20:11:52.397853  1691 solver.cpp:357] Iteration 36100 (1.2237 iter/s, 81.7191s/100 iters), loss = 0.0816219
I0825 20:11:52.398363  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101368 (* 1 = 0.101368 loss)
I0825 20:11:52.398558  1691 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0825 20:13:13.077805  1691 solver.cpp:357] Iteration 36200 (1.23954 iter/s, 80.6751s/100 iters), loss = 0.0797353
I0825 20:13:13.078016  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0527599 (* 1 = 0.0527599 loss)
I0825 20:13:13.078047  1691 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0825 20:14:31.495404  1691 solver.cpp:357] Iteration 36300 (1.27527 iter/s, 78.4147s/100 iters), loss = 0.138908
I0825 20:14:31.495640  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.106632 (* 1 = 0.106632 loss)
I0825 20:14:31.495687  1691 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0825 20:14:51.130074  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:15:48.459141  1691 solver.cpp:357] Iteration 36400 (1.29936 iter/s, 76.961s/100 iters), loss = 0.127473
I0825 20:15:48.459378  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0986708 (* 1 = 0.0986708 loss)
I0825 20:15:48.459426  1691 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0825 20:17:09.062073  1691 solver.cpp:514] Iteration 36500, Testing net (#0)
I0825 20:18:02.497781  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:18:02.713748  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.900002
I0825 20:18:02.713852  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.323768 (* 1 = 0.323768 loss)
I0825 20:18:03.338088  1691 solver.cpp:357] Iteration 36500 (0.741428 iter/s, 134.875s/100 iters), loss = 0.0971929
I0825 20:18:03.338260  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0739329 (* 1 = 0.0739329 loss)
I0825 20:18:03.338306  1691 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0825 20:19:20.339715  1691 solver.cpp:357] Iteration 36600 (1.29873 iter/s, 76.9986s/100 iters), loss = 0.13892
I0825 20:19:20.341326  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144764 (* 1 = 0.144764 loss)
I0825 20:19:20.341356  1691 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0825 20:20:41.844838  1691 solver.cpp:357] Iteration 36700 (1.22697 iter/s, 81.5013s/100 iters), loss = 0.071406
I0825 20:20:41.845037  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0796932 (* 1 = 0.0796932 loss)
I0825 20:20:41.845082  1691 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0825 20:20:55.269323  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:22:03.383296  1691 solver.cpp:357] Iteration 36800 (1.22639 iter/s, 81.5399s/100 iters), loss = 0.138711
I0825 20:22:03.383775  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.174347 (* 1 = 0.174347 loss)
I0825 20:22:03.383952  1691 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0825 20:23:24.310479  1691 solver.cpp:357] Iteration 36900 (1.23564 iter/s, 80.9296s/100 iters), loss = 0.112054
I0825 20:23:24.310698  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169243 (* 1 = 0.169243 loss)
I0825 20:23:24.310745  1691 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0825 20:24:40.540833  1691 solver.cpp:514] Iteration 37000, Testing net (#0)
I0825 20:25:31.683948  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:25:31.828183  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.897402
I0825 20:25:31.828303  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.333121 (* 1 = 0.333121 loss)
I0825 20:25:32.483079  1691 solver.cpp:357] Iteration 37000 (0.780172 iter/s, 128.177s/100 iters), loss = 0.116232
I0825 20:25:32.483253  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137595 (* 1 = 0.137595 loss)
I0825 20:25:32.483297  1691 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0825 20:26:52.640930  1691 solver.cpp:357] Iteration 37100 (1.24758 iter/s, 80.155s/100 iters), loss = 0.111485
I0825 20:26:52.641414  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100141 (* 1 = 0.100141 loss)
I0825 20:26:52.641594  1691 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0825 20:26:58.148888  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:28:10.968473  1691 solver.cpp:357] Iteration 37200 (1.27674 iter/s, 78.3244s/100 iters), loss = 0.123486
I0825 20:28:10.968662  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0769174 (* 1 = 0.0769174 loss)
I0825 20:28:10.968691  1691 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0825 20:29:31.686657  1691 solver.cpp:357] Iteration 37300 (1.2389 iter/s, 80.7171s/100 iters), loss = 0.0990151
I0825 20:29:31.686894  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0711591 (* 1 = 0.0711591 loss)
I0825 20:29:31.686941  1691 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0825 20:30:53.637219  1691 solver.cpp:357] Iteration 37400 (1.2203 iter/s, 81.9471s/100 iters), loss = 0.133412
I0825 20:30:53.637634  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.182631 (* 1 = 0.182631 loss)
I0825 20:30:53.637681  1691 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0825 20:32:13.221621  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:32:14.783828  1691 solver.cpp:514] Iteration 37500, Testing net (#0)
I0825 20:33:03.409376  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:33:03.546305  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.890302
I0825 20:33:03.546442  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.365012 (* 1 = 0.365012 loss)
I0825 20:33:04.143370  1691 solver.cpp:357] Iteration 37500 (0.76624 iter/s, 130.507s/100 iters), loss = 0.160566
I0825 20:33:04.143538  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.161982 (* 1 = 0.161982 loss)
I0825 20:33:04.143584  1691 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0825 20:34:25.528584  1691 solver.cpp:357] Iteration 37600 (1.22878 iter/s, 81.3814s/100 iters), loss = 0.0728922
I0825 20:34:25.528923  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.071894 (* 1 = 0.071894 loss)
I0825 20:34:25.528959  1691 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0825 20:35:44.717852  1691 solver.cpp:357] Iteration 37700 (1.26279 iter/s, 79.1895s/100 iters), loss = 0.160659
I0825 20:35:44.718042  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.169 (* 1 = 0.169 loss)
I0825 20:35:44.718070  1691 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0825 20:37:01.295498  1691 solver.cpp:357] Iteration 37800 (1.30586 iter/s, 76.5777s/100 iters), loss = 0.0662282
I0825 20:37:01.296003  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0708681 (* 1 = 0.0708681 loss)
I0825 20:37:01.296180  1691 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0825 20:38:09.388540  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:38:18.913738  1691 solver.cpp:357] Iteration 37900 (1.28836 iter/s, 77.6183s/100 iters), loss = 0.103316
I0825 20:38:18.913908  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0953655 (* 1 = 0.0953655 loss)
I0825 20:38:18.913954  1691 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0825 20:39:40.169886  1691 solver.cpp:514] Iteration 38000, Testing net (#0)
I0825 20:40:34.817317  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:40:35.054651  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.904902
I0825 20:40:35.054807  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.298146 (* 1 = 0.298146 loss)
I0825 20:40:35.614522  1691 solver.cpp:357] Iteration 38000 (0.731523 iter/s, 136.701s/100 iters), loss = 0.118317
I0825 20:40:35.614698  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0856058 (* 1 = 0.0856058 loss)
I0825 20:40:35.614744  1691 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0825 20:41:53.349755  1691 solver.cpp:357] Iteration 38100 (1.28646 iter/s, 77.733s/100 iters), loss = 0.133481
I0825 20:41:53.352449  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.191271 (* 1 = 0.191271 loss)
I0825 20:41:53.352519  1691 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0825 20:43:14.152673  1691 solver.cpp:357] Iteration 38200 (1.23761 iter/s, 80.8006s/100 iters), loss = 0.0747791
I0825 20:43:14.152886  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0862385 (* 1 = 0.0862385 loss)
I0825 20:43:14.152920  1691 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0825 20:44:18.786492  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:44:36.201103  1691 solver.cpp:357] Iteration 38300 (1.21879 iter/s, 82.0483s/100 iters), loss = 0.15919
I0825 20:44:36.201292  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0399941 (* 1 = 0.0399941 loss)
I0825 20:44:36.201339  1691 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0825 20:45:54.493849  1691 solver.cpp:357] Iteration 38400 (1.2773 iter/s, 78.2904s/100 iters), loss = 0.135903
I0825 20:45:54.494091  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.139144 (* 1 = 0.139144 loss)
I0825 20:45:54.494139  1691 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0825 20:47:02.558529  1691 solver.cpp:514] Iteration 38500, Testing net (#0)
I0825 20:47:47.473482  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:47:47.689484  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.900703
I0825 20:47:47.689543  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.320989 (* 1 = 0.320989 loss)
I0825 20:47:48.207898  1691 solver.cpp:357] Iteration 38500 (0.87941 iter/s, 113.713s/100 iters), loss = 0.090482
I0825 20:47:48.207964  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11713 (* 1 = 0.11713 loss)
I0825 20:47:48.207974  1691 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0825 20:48:48.419571  1691 solver.cpp:357] Iteration 38600 (1.66077 iter/s, 60.2131s/100 iters), loss = 0.0730731
I0825 20:48:48.419775  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0417586 (* 1 = 0.0417586 loss)
I0825 20:48:48.419788  1691 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0825 20:49:28.856500  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:49:47.899003  1691 solver.cpp:357] Iteration 38700 (1.68134 iter/s, 59.4765s/100 iters), loss = 0.082445
I0825 20:49:47.899073  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125644 (* 1 = 0.125644 loss)
I0825 20:49:47.899085  1691 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0825 20:50:49.954001  1691 solver.cpp:357] Iteration 38800 (1.61149 iter/s, 62.0543s/100 iters), loss = 0.0756721
I0825 20:50:49.954145  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0610326 (* 1 = 0.0610326 loss)
I0825 20:50:49.954157  1691 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0825 20:51:52.043879  1691 solver.cpp:357] Iteration 38900 (1.61064 iter/s, 62.0871s/100 iters), loss = 0.0576435
I0825 20:51:52.044049  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.046925 (* 1 = 0.046925 loss)
I0825 20:51:52.044061  1691 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0825 20:52:53.405577  1691 solver.cpp:514] Iteration 39000, Testing net (#0)
I0825 20:53:34.543825  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:53:34.735616  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.885702
I0825 20:53:34.735667  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.368719 (* 1 = 0.368719 loss)
I0825 20:53:35.132462  1691 solver.cpp:357] Iteration 39000 (0.970056 iter/s, 103.087s/100 iters), loss = 0.082174
I0825 20:53:35.132539  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114696 (* 1 = 0.114696 loss)
I0825 20:53:35.132550  1691 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0825 20:54:10.007561  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:54:35.180606  1691 solver.cpp:357] Iteration 39100 (1.66541 iter/s, 60.0453s/100 iters), loss = 0.0663705
I0825 20:54:35.180685  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0524266 (* 1 = 0.0524266 loss)
I0825 20:54:35.180698  1691 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0825 20:55:37.262164  1691 solver.cpp:357] Iteration 39200 (1.61086 iter/s, 62.0787s/100 iters), loss = 0.150076
I0825 20:55:37.262500  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142923 (* 1 = 0.142923 loss)
I0825 20:55:37.262567  1691 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0825 20:56:39.208875  1691 solver.cpp:357] Iteration 39300 (1.61429 iter/s, 61.9467s/100 iters), loss = 0.154669
I0825 20:56:39.209082  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170239 (* 1 = 0.170239 loss)
I0825 20:56:39.209115  1691 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0825 20:57:41.205811  1691 solver.cpp:357] Iteration 39400 (1.61293 iter/s, 61.9988s/100 iters), loss = 0.0739292
I0825 20:57:41.205965  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0471796 (* 1 = 0.0471796 loss)
I0825 20:57:41.205977  1691 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0825 20:58:12.400930  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:58:40.754164  1691 solver.cpp:514] Iteration 39500, Testing net (#0)
I0825 20:59:22.744663  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 20:59:22.971132  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.877801
I0825 20:59:22.971184  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.404064 (* 1 = 0.404064 loss)
I0825 20:59:23.463881  1691 solver.cpp:357] Iteration 39500 (0.97787 iter/s, 102.263s/100 iters), loss = 0.0833386
I0825 20:59:23.463956  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0536966 (* 1 = 0.0536966 loss)
I0825 20:59:23.463969  1691 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0825 21:00:25.779002  1691 solver.cpp:357] Iteration 39600 (1.60473 iter/s, 62.3159s/100 iters), loss = 0.110713
I0825 21:00:25.779179  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160393 (* 1 = 0.160393 loss)
I0825 21:00:25.779191  1691 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0825 21:01:24.109403  1691 solver.cpp:357] Iteration 39700 (1.71425 iter/s, 58.3346s/100 iters), loss = 0.115646
I0825 21:01:24.109565  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.110574 (* 1 = 0.110574 loss)
I0825 21:01:24.109575  1691 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0825 21:02:24.208900  1691 solver.cpp:357] Iteration 39800 (1.66379 iter/s, 60.1036s/100 iters), loss = 0.0950226
I0825 21:02:24.209053  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0859203 (* 1 = 0.0859203 loss)
I0825 21:02:24.209064  1691 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0825 21:02:49.856539  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:03:26.348695  1691 solver.cpp:357] Iteration 39900 (1.60917 iter/s, 62.1438s/100 iters), loss = 0.0966826
I0825 21:03:26.348829  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.165353 (* 1 = 0.165353 loss)
I0825 21:03:26.348842  1691 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0825 21:04:27.872946  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_40000.caffemodel
I0825 21:04:27.890911  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_40000.solverstate
I0825 21:04:27.895920  1691 solver.cpp:514] Iteration 40000, Testing net (#0)
I0825 21:05:12.665416  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:05:12.908435  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.874301
I0825 21:05:12.908499  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.424054 (* 1 = 0.424054 loss)
I0825 21:05:13.287397  1691 solver.cpp:357] Iteration 40000 (0.935076 iter/s, 106.943s/100 iters), loss = 0.207977
I0825 21:05:13.287466  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.1442 (* 1 = 0.1442 loss)
I0825 21:05:13.287475  1691 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0825 21:06:15.477761  1691 solver.cpp:357] Iteration 40100 (1.60793 iter/s, 62.1917s/100 iters), loss = 0.1078
I0825 21:06:15.478122  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.117802 (* 1 = 0.117802 loss)
I0825 21:06:15.478309  1691 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0825 21:07:17.618605  1691 solver.cpp:357] Iteration 40200 (1.60922 iter/s, 62.142s/100 iters), loss = 0.110306
I0825 21:07:17.618718  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0861262 (* 1 = 0.0861262 loss)
I0825 21:07:17.618731  1691 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0825 21:07:37.267443  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:08:18.093941  1691 solver.cpp:357] Iteration 40300 (1.65354 iter/s, 60.4763s/100 iters), loss = 0.105795
I0825 21:08:18.094116  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.186638 (* 1 = 0.186638 loss)
I0825 21:08:18.094130  1691 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0825 21:09:13.653895  1691 solver.cpp:357] Iteration 40400 (1.79984 iter/s, 55.5605s/100 iters), loss = 0.110385
I0825 21:09:13.654023  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0881344 (* 1 = 0.0881344 loss)
I0825 21:09:13.654036  1691 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0825 21:10:13.230154  1691 solver.cpp:514] Iteration 40500, Testing net (#0)
I0825 21:10:58.047202  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:10:58.236213  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880301
I0825 21:10:58.236279  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.402399 (* 1 = 0.402399 loss)
I0825 21:10:58.802755  1691 solver.cpp:357] Iteration 40500 (0.951001 iter/s, 105.152s/100 iters), loss = 0.0840706
I0825 21:10:58.802824  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0419727 (* 1 = 0.0419727 loss)
I0825 21:10:58.802835  1691 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0825 21:12:00.421121  1691 solver.cpp:357] Iteration 40600 (1.62288 iter/s, 61.619s/100 iters), loss = 0.0580654
I0825 21:12:00.421366  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0781088 (* 1 = 0.0781088 loss)
I0825 21:12:00.421381  1691 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0825 21:12:14.127724  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:13:02.410055  1691 solver.cpp:357] Iteration 40700 (1.61323 iter/s, 61.9873s/100 iters), loss = 0.132008
I0825 21:13:02.410213  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0829534 (* 1 = 0.0829534 loss)
I0825 21:13:02.410224  1691 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0825 21:14:04.509979  1691 solver.cpp:357] Iteration 40800 (1.6103 iter/s, 62.1003s/100 iters), loss = 0.107822
I0825 21:14:04.510115  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114455 (* 1 = 0.114455 loss)
I0825 21:14:04.510126  1691 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0825 21:15:06.398993  1691 solver.cpp:357] Iteration 40900 (1.61579 iter/s, 61.8894s/100 iters), loss = 0.0814573
I0825 21:15:06.399250  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0798281 (* 1 = 0.0798281 loss)
I0825 21:15:06.399300  1691 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0825 21:16:07.808054  1691 solver.cpp:514] Iteration 41000, Testing net (#0)
I0825 21:16:49.632969  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:16:49.819736  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.884502
I0825 21:16:49.819792  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.39692 (* 1 = 0.39692 loss)
I0825 21:16:50.316182  1691 solver.cpp:357] Iteration 41000 (0.962306 iter/s, 103.917s/100 iters), loss = 0.0944511
I0825 21:16:50.316248  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0546669 (* 1 = 0.0546669 loss)
I0825 21:16:50.316259  1691 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0825 21:16:57.743453  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:17:48.350433  1691 solver.cpp:357] Iteration 41100 (1.72306 iter/s, 58.0364s/100 iters), loss = 0.112369
I0825 21:17:48.350572  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122038 (* 1 = 0.122038 loss)
I0825 21:17:48.350585  1691 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0825 21:18:55.535928  1691 solver.cpp:357] Iteration 41200 (1.48846 iter/s, 67.1837s/100 iters), loss = 0.0943696
I0825 21:18:55.536088  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114862 (* 1 = 0.114862 loss)
I0825 21:18:55.536101  1691 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0825 21:19:57.269361  1691 solver.cpp:357] Iteration 41300 (1.61987 iter/s, 61.7335s/100 iters), loss = 0.0728828
I0825 21:19:57.269538  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120929 (* 1 = 0.120929 loss)
I0825 21:19:57.269551  1691 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0825 21:20:59.487939  1691 solver.cpp:357] Iteration 41400 (1.60729 iter/s, 62.2166s/100 iters), loss = 0.107949
I0825 21:20:59.488180  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156955 (* 1 = 0.156955 loss)
I0825 21:20:59.488225  1691 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0825 21:21:01.785284  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:22:02.426733  1691 solver.cpp:514] Iteration 41500, Testing net (#0)
I0825 21:22:48.921802  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:22:49.161921  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.891202
I0825 21:22:49.162032  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.368491 (* 1 = 0.368491 loss)
I0825 21:22:49.536551  1691 solver.cpp:357] Iteration 41500 (0.908675 iter/s, 110.05s/100 iters), loss = 0.0946733
I0825 21:22:49.536617  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0721603 (* 1 = 0.0721603 loss)
I0825 21:22:49.536628  1691 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0825 21:23:51.587436  1691 solver.cpp:357] Iteration 41600 (1.61153 iter/s, 62.0529s/100 iters), loss = 0.139006
I0825 21:23:51.587694  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.213117 (* 1 = 0.213117 loss)
I0825 21:23:51.587723  1691 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0825 21:24:51.138655  1691 solver.cpp:357] Iteration 41700 (1.67919 iter/s, 59.5526s/100 iters), loss = 0.0681621
I0825 21:24:51.138804  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.023536 (* 1 = 0.023536 loss)
I0825 21:24:51.138816  1691 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0825 21:25:46.475082  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:25:50.158685  1691 solver.cpp:357] Iteration 41800 (1.6944 iter/s, 59.0178s/100 iters), loss = 0.108807
I0825 21:25:50.158763  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0705434 (* 1 = 0.0705434 loss)
I0825 21:25:50.158776  1691 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0825 21:26:51.948421  1691 solver.cpp:357] Iteration 41900 (1.61845 iter/s, 61.7875s/100 iters), loss = 0.078221
I0825 21:26:51.948534  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288426 (* 1 = 0.0288426 loss)
I0825 21:26:51.948545  1691 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0825 21:27:52.481003  1691 solver.cpp:514] Iteration 42000, Testing net (#0)
I0825 21:28:33.650537  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:28:33.777971  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.857502
I0825 21:28:33.778079  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.537141 (* 1 = 0.537141 loss)
I0825 21:28:34.308423  1691 solver.cpp:357] Iteration 42000 (0.976931 iter/s, 102.361s/100 iters), loss = 0.114965
I0825 21:28:34.308547  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127921 (* 1 = 0.127921 loss)
I0825 21:28:34.308576  1691 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0825 21:29:36.100862  1691 solver.cpp:357] Iteration 42100 (1.61835 iter/s, 61.7911s/100 iters), loss = 0.0340408
I0825 21:29:36.101038  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0405111 (* 1 = 0.0405111 loss)
I0825 21:29:36.101052  1691 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0825 21:30:28.581940  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:30:37.936815  1691 solver.cpp:357] Iteration 42200 (1.61737 iter/s, 61.8287s/100 iters), loss = 0.213086
I0825 21:30:37.936897  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.142804 (* 1 = 0.142804 loss)
I0825 21:30:37.936909  1691 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0825 21:31:39.845007  1691 solver.cpp:357] Iteration 42300 (1.61567 iter/s, 61.8939s/100 iters), loss = 0.145478
I0825 21:31:39.845175  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130718 (* 1 = 0.130718 loss)
I0825 21:31:39.845186  1691 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0825 21:32:39.345474  1691 solver.cpp:357] Iteration 42400 (1.68096 iter/s, 59.4897s/100 iters), loss = 0.0987586
I0825 21:32:39.345772  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0569086 (* 1 = 0.0569086 loss)
I0825 21:32:39.345836  1691 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0825 21:33:37.606953  1691 solver.cpp:514] Iteration 42500, Testing net (#0)
I0825 21:34:22.398203  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:34:22.634785  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.888003
I0825 21:34:22.634855  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.3648 (* 1 = 0.3648 loss)
I0825 21:34:23.114012  1691 solver.cpp:357] Iteration 42500 (0.963822 iter/s, 103.754s/100 iters), loss = 0.0560873
I0825 21:34:23.114086  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0711263 (* 1 = 0.0711263 loss)
I0825 21:34:23.114097  1691 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0825 21:35:10.212873  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:35:25.147167  1691 solver.cpp:357] Iteration 42600 (1.61227 iter/s, 62.0245s/100 iters), loss = 0.13228
I0825 21:35:25.147241  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0646656 (* 1 = 0.0646656 loss)
I0825 21:35:25.147253  1691 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0825 21:36:27.404577  1691 solver.cpp:357] Iteration 42700 (1.60649 iter/s, 62.2473s/100 iters), loss = 0.144556
I0825 21:36:27.404783  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104695 (* 1 = 0.104695 loss)
I0825 21:36:27.404814  1691 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0825 21:37:28.706591  1691 solver.cpp:357] Iteration 42800 (1.63146 iter/s, 61.2948s/100 iters), loss = 0.138925
I0825 21:37:28.706702  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125379 (* 1 = 0.125379 loss)
I0825 21:37:28.706714  1691 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0825 21:38:27.245317  1691 solver.cpp:357] Iteration 42900 (1.7084 iter/s, 58.5342s/100 iters), loss = 0.0797819
I0825 21:38:27.245437  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108187 (* 1 = 0.108187 loss)
I0825 21:38:27.245450  1691 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0825 21:39:08.467007  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:39:28.961526  1691 solver.cpp:514] Iteration 43000, Testing net (#0)
I0825 21:40:11.893736  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:40:12.060531  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.886502
I0825 21:40:12.060586  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.405364 (* 1 = 0.405364 loss)
I0825 21:40:12.550484  1691 solver.cpp:357] Iteration 43000 (0.949719 iter/s, 105.294s/100 iters), loss = 0.0471074
I0825 21:40:12.550652  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0745378 (* 1 = 0.0745378 loss)
I0825 21:40:12.550698  1691 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0825 21:41:10.899405  1691 solver.cpp:357] Iteration 43100 (1.71398 iter/s, 58.3436s/100 iters), loss = 0.0768882
I0825 21:41:10.899549  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0571384 (* 1 = 0.0571384 loss)
I0825 21:41:10.899560  1691 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0825 21:42:13.068564  1691 solver.cpp:357] Iteration 43200 (1.6087 iter/s, 62.1619s/100 iters), loss = 0.141759
I0825 21:42:13.068696  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0318287 (* 1 = 0.0318287 loss)
I0825 21:42:13.068707  1691 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0825 21:43:14.901774  1691 solver.cpp:357] Iteration 43300 (1.61738 iter/s, 61.8284s/100 iters), loss = 0.0719076
I0825 21:43:14.901893  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10813 (* 1 = 0.10813 loss)
I0825 21:43:14.901906  1691 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0825 21:43:49.889894  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:44:16.671615  1691 solver.cpp:357] Iteration 43400 (1.61909 iter/s, 61.7632s/100 iters), loss = 0.177459
I0825 21:44:16.671696  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156417 (* 1 = 0.156417 loss)
I0825 21:44:16.671708  1691 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0825 21:45:18.248225  1691 solver.cpp:514] Iteration 43500, Testing net (#0)
I0825 21:46:03.054461  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:46:03.209597  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.891902
I0825 21:46:03.209661  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.351915 (* 1 = 0.351915 loss)
I0825 21:46:03.712723  1691 solver.cpp:357] Iteration 43500 (0.93427 iter/s, 107.035s/100 iters), loss = 0.0853626
I0825 21:46:03.712795  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0736431 (* 1 = 0.0736431 loss)
I0825 21:46:03.712806  1691 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0825 21:47:05.425842  1691 solver.cpp:357] Iteration 43600 (1.62046 iter/s, 61.7109s/100 iters), loss = 0.100067
I0825 21:47:05.426213  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0935401 (* 1 = 0.0935401 loss)
I0825 21:47:05.426275  1691 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0825 21:48:01.774760  1691 solver.cpp:357] Iteration 43700 (1.77482 iter/s, 56.3438s/100 iters), loss = 0.0610333
I0825 21:48:01.774924  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0755226 (* 1 = 0.0755226 loss)
I0825 21:48:01.774937  1691 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0825 21:48:28.004066  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:48:59.544476  1691 solver.cpp:357] Iteration 43800 (1.73118 iter/s, 57.7642s/100 iters), loss = 0.038333
I0825 21:48:59.544613  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0304714 (* 1 = 0.0304714 loss)
I0825 21:48:59.544625  1691 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0825 21:50:01.553979  1691 solver.cpp:357] Iteration 43900 (1.6128 iter/s, 62.0039s/100 iters), loss = 0.122733
I0825 21:50:01.554152  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112414 (* 1 = 0.112414 loss)
I0825 21:50:01.554165  1691 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0825 21:51:02.993654  1691 solver.cpp:514] Iteration 44000, Testing net (#0)
I0825 21:51:47.742513  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:51:47.923612  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.839101
I0825 21:51:47.923676  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.651595 (* 1 = 0.651595 loss)
I0825 21:51:48.419230  1691 solver.cpp:357] Iteration 44000 (0.935811 iter/s, 106.859s/100 iters), loss = 0.105987
I0825 21:51:48.419297  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0644464 (* 1 = 0.0644464 loss)
I0825 21:51:48.419309  1691 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0825 21:52:50.550915  1691 solver.cpp:357] Iteration 44100 (1.60957 iter/s, 62.1286s/100 iters), loss = 0.0804953
I0825 21:52:50.551031  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0875384 (* 1 = 0.0875384 loss)
I0825 21:52:50.551044  1691 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0825 21:53:14.283890  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:53:52.418805  1691 solver.cpp:357] Iteration 44200 (1.61648 iter/s, 61.8627s/100 iters), loss = 0.0472159
I0825 21:53:52.418975  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0690097 (* 1 = 0.0690097 loss)
I0825 21:53:52.418987  1691 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0825 21:54:54.569077  1691 solver.cpp:357] Iteration 44300 (1.60909 iter/s, 62.147s/100 iters), loss = 0.108101
I0825 21:54:54.569222  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.132097 (* 1 = 0.132097 loss)
I0825 21:54:54.569234  1691 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0825 21:55:54.440747  1691 solver.cpp:357] Iteration 44400 (1.67032 iter/s, 59.8688s/100 iters), loss = 0.0682473
I0825 21:55:54.440908  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0667976 (* 1 = 0.0667976 loss)
I0825 21:55:54.440920  1691 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0825 21:56:51.727844  1691 solver.cpp:514] Iteration 44500, Testing net (#0)
I0825 21:57:32.851812  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:57:33.041038  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894402
I0825 21:57:33.041167  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.376174 (* 1 = 0.376174 loss)
I0825 21:57:33.517311  1691 solver.cpp:357] Iteration 44500 (1.00937 iter/s, 99.0714s/100 iters), loss = 0.0714142
I0825 21:57:33.517379  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0559786 (* 1 = 0.0559786 loss)
I0825 21:57:33.517390  1691 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0825 21:57:50.524530  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 21:58:34.982034  1691 solver.cpp:357] Iteration 44600 (1.62702 iter/s, 61.462s/100 iters), loss = 0.0726504
I0825 21:58:34.982194  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0857783 (* 1 = 0.0857783 loss)
I0825 21:58:34.982208  1691 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0825 21:59:36.908955  1691 solver.cpp:357] Iteration 44700 (1.61487 iter/s, 61.9243s/100 iters), loss = 0.154924
I0825 21:59:36.909077  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101243 (* 1 = 0.101243 loss)
I0825 21:59:36.909090  1691 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0825 22:00:38.942814  1691 solver.cpp:357] Iteration 44800 (1.61212 iter/s, 62.0301s/100 iters), loss = 0.0854948
I0825 22:00:38.942970  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.103377 (* 1 = 0.103377 loss)
I0825 22:00:38.942982  1691 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0825 22:01:40.920011  1691 solver.cpp:357] Iteration 44900 (1.61362 iter/s, 61.9725s/100 iters), loss = 0.13927
I0825 22:01:40.920166  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0343346 (* 1 = 0.0343346 loss)
I0825 22:01:40.920176  1691 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0825 22:01:52.797377  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:02:42.284096  1691 solver.cpp:514] Iteration 45000, Testing net (#0)
I0825 22:03:25.882710  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:03:26.055105  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.860701
I0825 22:03:26.055164  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.518326 (* 1 = 0.518326 loss)
I0825 22:03:26.452158  1691 solver.cpp:357] Iteration 45000 (0.947585 iter/s, 105.531s/100 iters), loss = 0.112478
I0825 22:03:26.452242  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122462 (* 1 = 0.122462 loss)
I0825 22:03:26.452256  1691 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0825 22:04:23.813891  1691 solver.cpp:357] Iteration 45100 (1.74338 iter/s, 57.3597s/100 iters), loss = 0.0908207
I0825 22:04:23.814002  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0905227 (* 1 = 0.0905227 loss)
I0825 22:04:23.814013  1691 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0825 22:05:25.861724  1691 solver.cpp:357] Iteration 45200 (1.61139 iter/s, 62.0582s/100 iters), loss = 0.097745
I0825 22:05:25.861851  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10402 (* 1 = 0.10402 loss)
I0825 22:05:25.861862  1691 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0825 22:06:28.015338  1691 solver.cpp:357] Iteration 45300 (1.60863 iter/s, 62.1648s/100 iters), loss = 0.112036
I0825 22:06:28.015507  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.141491 (* 1 = 0.141491 loss)
I0825 22:06:28.015521  1691 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0825 22:06:33.649402  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:07:26.321496  1691 solver.cpp:357] Iteration 45400 (1.71487 iter/s, 58.3135s/100 iters), loss = 0.0905216
I0825 22:07:26.321641  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136076 (* 1 = 0.136076 loss)
I0825 22:07:26.321655  1691 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0825 22:08:26.848624  1691 solver.cpp:514] Iteration 45500, Testing net (#0)
I0825 22:09:11.864186  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:09:12.053175  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.886201
I0825 22:09:12.053222  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.396629 (* 1 = 0.396629 loss)
I0825 22:09:12.446102  1691 solver.cpp:357] Iteration 45500 (0.942172 iter/s, 106.138s/100 iters), loss = 0.106909
I0825 22:09:12.446173  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0811143 (* 1 = 0.0811143 loss)
I0825 22:09:12.446187  1691 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0825 22:10:14.446251  1691 solver.cpp:357] Iteration 45600 (1.61281 iter/s, 62.0036s/100 iters), loss = 0.123273
I0825 22:10:14.446478  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118943 (* 1 = 0.118943 loss)
I0825 22:10:14.446491  1691 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0825 22:11:15.224372  1691 solver.cpp:357] Iteration 45700 (1.6452 iter/s, 60.7828s/100 iters), loss = 0.130583
I0825 22:11:15.224525  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.167447 (* 1 = 0.167447 loss)
I0825 22:11:15.224539  1691 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0825 22:11:15.505321  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:12:12.702509  1691 solver.cpp:357] Iteration 45800 (1.73968 iter/s, 57.4817s/100 iters), loss = 0.136777
I0825 22:12:12.702656  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.170825 (* 1 = 0.170825 loss)
I0825 22:12:12.702668  1691 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0825 22:13:14.954383  1691 solver.cpp:357] Iteration 45900 (1.60634 iter/s, 62.2535s/100 iters), loss = 0.0792535
I0825 22:13:14.954557  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0982492 (* 1 = 0.0982492 loss)
I0825 22:13:14.954569  1691 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0825 22:14:16.176658  1691 solver.cpp:514] Iteration 46000, Testing net (#0)
I0825 22:15:01.070662  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:15:01.304661  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.867001
I0825 22:15:01.304715  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.472568 (* 1 = 0.472568 loss)
I0825 22:15:01.748458  1691 solver.cpp:357] Iteration 46000 (0.93634 iter/s, 106.799s/100 iters), loss = 0.171678
I0825 22:15:01.748525  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.163662 (* 1 = 0.163662 loss)
I0825 22:15:01.748536  1691 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0825 22:15:58.213835  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:16:03.672236  1691 solver.cpp:357] Iteration 46100 (1.61482 iter/s, 61.9262s/100 iters), loss = 0.119088
I0825 22:16:03.672309  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0675694 (* 1 = 0.0675694 loss)
I0825 22:16:03.672320  1691 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0825 22:17:02.314908  1691 solver.cpp:357] Iteration 46200 (1.70519 iter/s, 58.6445s/100 iters), loss = 0.0713919
I0825 22:17:02.315047  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0948003 (* 1 = 0.0948003 loss)
I0825 22:17:02.315059  1691 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0825 22:18:03.224606  1691 solver.cpp:357] Iteration 46300 (1.64179 iter/s, 60.9093s/100 iters), loss = 0.124498
I0825 22:18:03.224779  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107161 (* 1 = 0.107161 loss)
I0825 22:18:03.224793  1691 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0825 22:19:04.181262  1691 solver.cpp:357] Iteration 46400 (1.64053 iter/s, 60.956s/100 iters), loss = 0.104342
I0825 22:19:04.181428  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0864994 (* 1 = 0.0864994 loss)
I0825 22:19:04.181442  1691 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0825 22:19:50.315263  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:20:01.273031  1691 solver.cpp:514] Iteration 46500, Testing net (#0)
I0825 22:20:45.992573  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:20:46.112200  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.848501
I0825 22:20:46.112253  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.554353 (* 1 = 0.554353 loss)
I0825 22:20:46.687109  1691 solver.cpp:357] Iteration 46500 (0.975542 iter/s, 102.507s/100 iters), loss = 0.118394
I0825 22:20:46.687191  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.131114 (* 1 = 0.131114 loss)
I0825 22:20:46.687203  1691 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0825 22:21:48.493700  1691 solver.cpp:357] Iteration 46600 (1.61798 iter/s, 61.8054s/100 iters), loss = 0.0962557
I0825 22:21:48.493927  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0973914 (* 1 = 0.0973914 loss)
I0825 22:21:48.493940  1691 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0825 22:22:50.703236  1691 solver.cpp:357] Iteration 46700 (1.60751 iter/s, 62.2081s/100 iters), loss = 0.100589
I0825 22:22:50.703378  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0418575 (* 1 = 0.0418575 loss)
I0825 22:22:50.703392  1691 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0825 22:23:52.807844  1691 solver.cpp:357] Iteration 46800 (1.61023 iter/s, 62.103s/100 iters), loss = 0.157295
I0825 22:23:52.807993  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.243478 (* 1 = 0.243478 loss)
I0825 22:23:52.808007  1691 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0825 22:24:37.425119  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:24:54.705701  1691 solver.cpp:357] Iteration 46900 (1.61561 iter/s, 61.8961s/100 iters), loss = 0.0916367
I0825 22:24:54.705780  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0813246 (* 1 = 0.0813246 loss)
I0825 22:24:54.705791  1691 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0825 22:25:56.172190  1691 solver.cpp:514] Iteration 47000, Testing net (#0)
I0825 22:26:37.489877  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:26:37.631635  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.852101
I0825 22:26:37.631688  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.590455 (* 1 = 0.590455 loss)
I0825 22:26:38.057060  1691 solver.cpp:357] Iteration 47000 (0.967556 iter/s, 103.353s/100 iters), loss = 0.103623
I0825 22:26:38.057135  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.127977 (* 1 = 0.127977 loss)
I0825 22:26:38.057148  1691 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0825 22:27:32.862314  1691 solver.cpp:357] Iteration 47100 (1.82465 iter/s, 54.8051s/100 iters), loss = 0.0870757
I0825 22:27:32.862431  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11019 (* 1 = 0.11019 loss)
I0825 22:27:32.862442  1691 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0825 22:28:35.089880  1691 solver.cpp:357] Iteration 47200 (1.60701 iter/s, 62.2275s/100 iters), loss = 0.124424
I0825 22:28:35.090064  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0975963 (* 1 = 0.0975963 loss)
I0825 22:28:35.090075  1691 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0825 22:29:14.161955  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:29:37.111495  1691 solver.cpp:357] Iteration 47300 (1.6124 iter/s, 62.0194s/100 iters), loss = 0.0808378
I0825 22:29:37.111562  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0592305 (* 1 = 0.0592305 loss)
I0825 22:29:37.111574  1691 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0825 22:30:38.843888  1691 solver.cpp:357] Iteration 47400 (1.6199 iter/s, 61.7322s/100 iters), loss = 0.0933297
I0825 22:30:38.844040  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0757778 (* 1 = 0.0757778 loss)
I0825 22:30:38.844053  1691 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0825 22:31:40.038985  1691 solver.cpp:514] Iteration 47500, Testing net (#0)
I0825 22:32:25.005914  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:32:25.164489  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.904502
I0825 22:32:25.164533  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.319225 (* 1 = 0.319225 loss)
I0825 22:32:25.655814  1691 solver.cpp:357] Iteration 47500 (0.936234 iter/s, 106.811s/100 iters), loss = 0.0888335
I0825 22:32:25.655889  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0483564 (* 1 = 0.0483564 loss)
I0825 22:32:25.655901  1691 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0825 22:33:27.576735  1691 solver.cpp:357] Iteration 47600 (1.61503 iter/s, 61.9184s/100 iters), loss = 0.104865
I0825 22:33:27.576941  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0805309 (* 1 = 0.0805309 loss)
I0825 22:33:27.576953  1691 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0825 22:34:00.859817  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:34:29.198709  1691 solver.cpp:357] Iteration 47700 (1.62281 iter/s, 61.6216s/100 iters), loss = 0.0549829
I0825 22:34:29.198781  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0508526 (* 1 = 0.0508526 loss)
I0825 22:34:29.198793  1691 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0825 22:35:26.035581  1691 solver.cpp:357] Iteration 47800 (1.75944 iter/s, 56.8363s/100 iters), loss = 0.0901184
I0825 22:35:26.035699  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143964 (* 1 = 0.143964 loss)
I0825 22:35:26.035712  1691 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0825 22:36:25.516842  1691 solver.cpp:357] Iteration 47900 (1.68128 iter/s, 59.4786s/100 iters), loss = 0.130303
I0825 22:36:25.517158  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.146554 (* 1 = 0.146554 loss)
I0825 22:36:25.517220  1691 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0825 22:37:25.459163  1691 solver.cpp:514] Iteration 48000, Testing net (#0)
I0825 22:38:10.246498  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:38:10.322043  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.864301
I0825 22:38:10.322096  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.504716 (* 1 = 0.504716 loss)
I0825 22:38:10.926348  1691 solver.cpp:357] Iteration 48000 (0.948694 iter/s, 105.408s/100 iters), loss = 0.0540092
I0825 22:38:10.926419  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0476521 (* 1 = 0.0476521 loss)
I0825 22:38:10.926430  1691 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0825 22:38:10.926437  1691 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0825 22:38:38.249428  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:39:13.040513  1691 solver.cpp:357] Iteration 48100 (1.61001 iter/s, 62.1114s/100 iters), loss = 0.08601
I0825 22:39:13.040647  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.108019 (* 1 = 0.108019 loss)
I0825 22:39:13.040660  1691 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0825 22:40:15.050843  1691 solver.cpp:357] Iteration 48200 (1.61271 iter/s, 62.0073s/100 iters), loss = 0.0409948
I0825 22:40:15.051017  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0339589 (* 1 = 0.0339589 loss)
I0825 22:40:15.051046  1691 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0825 22:41:16.831753  1691 solver.cpp:357] Iteration 48300 (1.61865 iter/s, 61.7799s/100 iters), loss = 0.0404461
I0825 22:41:16.831928  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0433726 (* 1 = 0.0433726 loss)
I0825 22:41:16.831941  1691 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0825 22:42:18.272341  1691 solver.cpp:357] Iteration 48400 (1.62761 iter/s, 61.4397s/100 iters), loss = 0.0274674
I0825 22:42:18.272449  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0398054 (* 1 = 0.0398054 loss)
I0825 22:42:18.272462  1691 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0825 22:42:37.834798  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:43:14.653568  1691 solver.cpp:514] Iteration 48500, Testing net (#0)
I0825 22:43:59.677975  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:43:59.771064  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922002
I0825 22:43:59.771117  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.26179 (* 1 = 0.26179 loss)
I0825 22:44:00.325199  1691 solver.cpp:357] Iteration 48500 (0.979904 iter/s, 102.051s/100 iters), loss = 0.0830568
I0825 22:44:00.325269  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0581772 (* 1 = 0.0581772 loss)
I0825 22:44:00.325282  1691 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0825 22:45:02.287374  1691 solver.cpp:357] Iteration 48600 (1.61393 iter/s, 61.9606s/100 iters), loss = 0.0695127
I0825 22:45:02.287633  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.105569 (* 1 = 0.105569 loss)
I0825 22:45:02.287647  1691 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0825 22:46:01.993903  1691 solver.cpp:357] Iteration 48700 (1.67494 iter/s, 59.7035s/100 iters), loss = 0.022558
I0825 22:46:01.994094  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0189948 (* 1 = 0.0189948 loss)
I0825 22:46:01.994123  1691 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0825 22:47:01.802633  1691 solver.cpp:357] Iteration 48800 (1.67208 iter/s, 59.8057s/100 iters), loss = 0.0357807
I0825 22:47:01.802762  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.028236 (* 1 = 0.028236 loss)
I0825 22:47:01.802773  1691 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0825 22:47:17.631105  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:48:03.835050  1691 solver.cpp:357] Iteration 48900 (1.61208 iter/s, 62.0315s/100 iters), loss = 0.0239078
I0825 22:48:03.835193  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0178117 (* 1 = 0.0178117 loss)
I0825 22:48:03.835204  1691 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0825 22:49:05.362953  1691 solver.cpp:514] Iteration 49000, Testing net (#0)
I0825 22:49:50.326082  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:49:50.454313  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925902
I0825 22:49:50.454377  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.260207 (* 1 = 0.260207 loss)
I0825 22:49:50.949798  1691 solver.cpp:357] Iteration 49000 (0.933559 iter/s, 107.117s/100 iters), loss = 0.0304708
I0825 22:49:50.949882  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0436478 (* 1 = 0.0436478 loss)
I0825 22:49:50.949894  1691 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0825 22:50:47.877236  1691 solver.cpp:357] Iteration 49100 (1.75672 iter/s, 56.9244s/100 iters), loss = 0.0303882
I0825 22:50:47.877416  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0397745 (* 1 = 0.0397745 loss)
I0825 22:50:47.877446  1691 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0825 22:51:49.468734  1691 solver.cpp:357] Iteration 49200 (1.62362 iter/s, 61.5906s/100 iters), loss = 0.0164739
I0825 22:51:49.468907  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0183433 (* 1 = 0.0183433 loss)
I0825 22:51:49.468935  1691 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0825 22:51:59.480916  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:52:51.604276  1691 solver.cpp:357] Iteration 49300 (1.60936 iter/s, 62.1367s/100 iters), loss = 0.0961331
I0825 22:52:51.604482  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.134511 (* 1 = 0.134511 loss)
I0825 22:52:51.604511  1691 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0825 22:53:53.498489  1691 solver.cpp:357] Iteration 49400 (1.61568 iter/s, 61.8934s/100 iters), loss = 0.0128196
I0825 22:53:53.498617  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0106204 (* 1 = 0.0106204 loss)
I0825 22:53:53.498631  1691 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0825 22:54:54.907999  1691 solver.cpp:514] Iteration 49500, Testing net (#0)
I0825 22:55:37.708593  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:55:37.866456  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925902
I0825 22:55:37.866500  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.260427 (* 1 = 0.260427 loss)
I0825 22:55:38.329644  1691 solver.cpp:357] Iteration 49500 (0.953914 iter/s, 104.831s/100 iters), loss = 0.0361988
I0825 22:55:38.329720  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0150234 (* 1 = 0.0150234 loss)
I0825 22:55:38.329732  1691 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0825 22:56:38.058549  1691 solver.cpp:357] Iteration 49600 (1.67432 iter/s, 59.7259s/100 iters), loss = 0.0273132
I0825 22:56:38.058773  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0434401 (* 1 = 0.0434401 loss)
I0825 22:56:38.058787  1691 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0825 22:56:42.166587  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 22:57:40.200314  1691 solver.cpp:357] Iteration 49700 (1.6093 iter/s, 62.1388s/100 iters), loss = 0.0172271
I0825 22:57:40.200631  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0105963 (* 1 = 0.0105963 loss)
I0825 22:57:40.200698  1691 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0825 22:58:36.570729  1691 solver.cpp:357] Iteration 49800 (1.77408 iter/s, 56.3673s/100 iters), loss = 0.0584541
I0825 22:58:36.570961  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0265427 (* 1 = 0.0265427 loss)
I0825 22:58:36.570993  1691 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0825 22:59:38.703303  1691 solver.cpp:357] Iteration 49900 (1.60943 iter/s, 62.1337s/100 iters), loss = 0.0301569
I0825 22:59:38.703425  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.047574 (* 1 = 0.047574 loss)
I0825 22:59:38.703438  1691 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0825 23:00:38.965466  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:00:40.100836  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_50000.caffemodel
I0825 23:00:40.121270  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_50000.solverstate
I0825 23:00:40.126495  1691 solver.cpp:514] Iteration 50000, Testing net (#0)
I0825 23:01:25.215068  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:01:25.357543  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929203
I0825 23:01:25.357600  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.257904 (* 1 = 0.257904 loss)
I0825 23:01:25.959507  1691 solver.cpp:357] Iteration 50000 (0.932345 iter/s, 107.256s/100 iters), loss = 0.0270315
I0825 23:01:25.959581  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0382052 (* 1 = 0.0382052 loss)
I0825 23:01:25.959592  1691 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0825 23:02:28.093999  1691 solver.cpp:357] Iteration 50100 (1.60943 iter/s, 62.1337s/100 iters), loss = 0.0297597
I0825 23:02:28.094275  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.039333 (* 1 = 0.039333 loss)
I0825 23:02:28.094357  1691 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0825 23:03:30.204280  1691 solver.cpp:357] Iteration 50200 (1.61001 iter/s, 62.1114s/100 iters), loss = 0.022671
I0825 23:03:30.204403  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0317012 (* 1 = 0.0317012 loss)
I0825 23:03:30.204414  1691 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0825 23:04:31.910297  1691 solver.cpp:357] Iteration 50300 (1.62061 iter/s, 61.7052s/100 iters), loss = 0.0220113
I0825 23:04:31.910446  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.036197 (* 1 = 0.036197 loss)
I0825 23:04:31.910459  1691 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0825 23:05:25.404461  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:05:32.176457  1691 solver.cpp:357] Iteration 50400 (1.65933 iter/s, 60.2653s/100 iters), loss = 0.0326153
I0825 23:05:32.176532  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0473463 (* 1 = 0.0473463 loss)
I0825 23:05:32.176544  1691 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0825 23:06:25.546353  1691 solver.cpp:514] Iteration 50500, Testing net (#0)
I0825 23:07:10.292043  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:07:10.495949  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929903
I0825 23:07:10.496016  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.261557 (* 1 = 0.261557 loss)
I0825 23:07:11.051976  1691 solver.cpp:357] Iteration 50500 (1.01137 iter/s, 98.8756s/100 iters), loss = 0.0335711
I0825 23:07:11.052048  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.022668 (* 1 = 0.022668 loss)
I0825 23:07:11.052058  1691 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0825 23:08:12.867409  1691 solver.cpp:357] Iteration 50600 (1.61774 iter/s, 61.8146s/100 iters), loss = 0.0119676
I0825 23:08:12.867619  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00893181 (* 1 = 0.00893181 loss)
I0825 23:08:12.867632  1691 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0825 23:09:14.588078  1691 solver.cpp:357] Iteration 50700 (1.62028 iter/s, 61.7178s/100 iters), loss = 0.0310313
I0825 23:09:14.588230  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122385 (* 1 = 0.0122385 loss)
I0825 23:09:14.588243  1691 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0825 23:10:03.097506  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:10:16.391773  1691 solver.cpp:357] Iteration 50800 (1.6181 iter/s, 61.8008s/100 iters), loss = 0.020166
I0825 23:10:16.391847  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00755166 (* 1 = 0.00755166 loss)
I0825 23:10:16.391860  1691 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0825 23:11:18.359323  1691 solver.cpp:357] Iteration 50900 (1.61382 iter/s, 61.9647s/100 iters), loss = 0.0457585
I0825 23:11:18.359447  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0779629 (* 1 = 0.0779629 loss)
I0825 23:11:18.359459  1691 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0825 23:12:19.561098  1691 solver.cpp:514] Iteration 51000, Testing net (#0)
I0825 23:13:04.490674  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:13:04.590263  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930703
I0825 23:13:04.590324  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265082 (* 1 = 0.265082 loss)
I0825 23:13:05.191658  1691 solver.cpp:357] Iteration 51000 (0.936063 iter/s, 106.83s/100 iters), loss = 0.0187695
I0825 23:13:05.191735  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0120343 (* 1 = 0.0120343 loss)
I0825 23:13:05.191747  1691 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0825 23:14:02.344372  1691 solver.cpp:357] Iteration 51100 (1.74986 iter/s, 57.1473s/100 iters), loss = 0.0179387
I0825 23:14:02.344682  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00805901 (* 1 = 0.00805901 loss)
I0825 23:14:02.344749  1691 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0825 23:14:44.032521  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:15:01.976493  1691 solver.cpp:357] Iteration 51200 (1.6771 iter/s, 59.6266s/100 iters), loss = 0.0475995
I0825 23:15:01.976572  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0855416 (* 1 = 0.0855416 loss)
I0825 23:15:01.976584  1691 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0825 23:16:01.325031  1691 solver.cpp:357] Iteration 51300 (1.68505 iter/s, 59.3455s/100 iters), loss = 0.00759573
I0825 23:16:01.325134  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0057842 (* 1 = 0.0057842 loss)
I0825 23:16:01.325146  1691 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0825 23:17:02.901018  1691 solver.cpp:357] Iteration 51400 (1.62404 iter/s, 61.575s/100 iters), loss = 0.0449824
I0825 23:17:02.901157  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0261251 (* 1 = 0.0261251 loss)
I0825 23:17:02.901170  1691 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0825 23:18:04.371045  1691 solver.cpp:514] Iteration 51500, Testing net (#0)
I0825 23:18:49.089939  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:18:49.197091  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0825 23:18:49.197139  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270219 (* 1 = 0.270219 loss)
I0825 23:18:49.737191  1691 solver.cpp:357] Iteration 51500 (0.936054 iter/s, 106.831s/100 iters), loss = 0.00775702
I0825 23:18:49.737265  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00890976 (* 1 = 0.00890976 loss)
I0825 23:18:49.737278  1691 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0825 23:19:26.819533  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:19:52.035981  1691 solver.cpp:357] Iteration 51600 (1.60529 iter/s, 62.2941s/100 iters), loss = 0.0224328
I0825 23:19:52.036160  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0170444 (* 1 = 0.0170444 loss)
I0825 23:19:52.036206  1691 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0825 23:20:54.100694  1691 solver.cpp:357] Iteration 51700 (1.61128 iter/s, 62.0623s/100 iters), loss = 0.0596307
I0825 23:20:54.100884  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0140879 (* 1 = 0.0140879 loss)
I0825 23:20:54.100898  1691 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0825 23:21:51.031544  1691 solver.cpp:357] Iteration 51800 (1.75659 iter/s, 56.9286s/100 iters), loss = 0.0177785
I0825 23:21:51.031708  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138928 (* 1 = 0.0138928 loss)
I0825 23:21:51.031723  1691 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0825 23:22:51.958003  1691 solver.cpp:357] Iteration 51900 (1.64144 iter/s, 60.9222s/100 iters), loss = 0.0327308
I0825 23:22:51.958117  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0270203 (* 1 = 0.0270203 loss)
I0825 23:22:51.958127  1691 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0825 23:23:23.341434  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:23:53.595213  1691 solver.cpp:514] Iteration 52000, Testing net (#0)
I0825 23:24:37.208441  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:24:37.304375  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930202
I0825 23:24:37.304438  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.269528 (* 1 = 0.269528 loss)
I0825 23:24:37.835739  1691 solver.cpp:357] Iteration 52000 (0.944502 iter/s, 105.876s/100 iters), loss = 0.0342366
I0825 23:24:37.835819  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0346401 (* 1 = 0.0346401 loss)
I0825 23:24:37.835831  1691 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0825 23:25:36.503558  1691 solver.cpp:357] Iteration 52100 (1.70457 iter/s, 58.6659s/100 iters), loss = 0.0128141
I0825 23:25:36.503705  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00794297 (* 1 = 0.00794297 loss)
I0825 23:25:36.503718  1691 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0825 23:26:38.597800  1691 solver.cpp:357] Iteration 52200 (1.61056 iter/s, 62.0903s/100 iters), loss = 0.0159327
I0825 23:26:38.597929  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157839 (* 1 = 0.0157839 loss)
I0825 23:26:38.597941  1691 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0825 23:27:40.986641  1691 solver.cpp:357] Iteration 52300 (1.60295 iter/s, 62.3849s/100 iters), loss = 0.0332941
I0825 23:27:40.986778  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0211976 (* 1 = 0.0211976 loss)
I0825 23:27:40.986790  1691 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0825 23:28:06.556380  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:28:43.022346  1691 solver.cpp:357] Iteration 52400 (1.61207 iter/s, 62.0319s/100 iters), loss = 0.0150344
I0825 23:28:43.022480  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.014863 (* 1 = 0.014863 loss)
I0825 23:28:43.022491  1691 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0825 23:29:39.486467  1691 solver.cpp:514] Iteration 52500, Testing net (#0)
I0825 23:30:23.393565  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:30:23.550341  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931203
I0825 23:30:23.550387  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.268064 (* 1 = 0.268064 loss)
I0825 23:30:23.968224  1691 solver.cpp:357] Iteration 52500 (0.990623 iter/s, 100.947s/100 iters), loss = 0.021055
I0825 23:30:23.968297  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00361965 (* 1 = 0.00361965 loss)
I0825 23:30:23.968309  1691 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0825 23:31:25.847821  1691 solver.cpp:357] Iteration 52600 (1.61614 iter/s, 61.8759s/100 iters), loss = 0.0254159
I0825 23:31:25.848026  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0124128 (* 1 = 0.0124128 loss)
I0825 23:31:25.848037  1691 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0825 23:32:27.698853  1691 solver.cpp:357] Iteration 52700 (1.61678 iter/s, 61.8514s/100 iters), loss = 0.0255235
I0825 23:32:27.698993  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0101653 (* 1 = 0.0101653 loss)
I0825 23:32:27.699005  1691 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0825 23:32:47.399516  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:33:30.001770  1691 solver.cpp:357] Iteration 52800 (1.60516 iter/s, 62.2992s/100 iters), loss = 0.00850802
I0825 23:33:30.001922  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00837342 (* 1 = 0.00837342 loss)
I0825 23:33:30.001933  1691 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0825 23:34:30.769321  1691 solver.cpp:357] Iteration 52900 (1.64562 iter/s, 60.7672s/100 iters), loss = 0.0158897
I0825 23:34:30.769440  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00814465 (* 1 = 0.00814465 loss)
I0825 23:34:30.769451  1691 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0825 23:35:28.627472  1691 solver.cpp:514] Iteration 53000, Testing net (#0)
I0825 23:36:13.470665  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:36:13.703723  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928703
I0825 23:36:13.703786  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.275416 (* 1 = 0.275416 loss)
I0825 23:36:14.152678  1691 solver.cpp:357] Iteration 53000 (0.967302 iter/s, 103.38s/100 iters), loss = 0.0169613
I0825 23:36:14.152750  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187979 (* 1 = 0.0187979 loss)
I0825 23:36:14.152760  1691 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0825 23:37:12.205548  1691 solver.cpp:357] Iteration 53100 (1.72261 iter/s, 58.0514s/100 iters), loss = 0.0134907
I0825 23:37:12.205673  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00973014 (* 1 = 0.00973014 loss)
I0825 23:37:12.205687  1691 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0825 23:37:24.683327  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:38:12.466481  1691 solver.cpp:357] Iteration 53200 (1.65944 iter/s, 60.2612s/100 iters), loss = 0.0221408
I0825 23:38:12.466641  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222878 (* 1 = 0.0222878 loss)
I0825 23:38:12.466655  1691 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0825 23:39:14.287111  1691 solver.cpp:357] Iteration 53300 (1.61768 iter/s, 61.8171s/100 iters), loss = 0.0152499
I0825 23:39:14.287325  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0167172 (* 1 = 0.0167172 loss)
I0825 23:39:14.287355  1691 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0825 23:40:16.045357  1691 solver.cpp:357] Iteration 53400 (1.61925 iter/s, 61.7568s/100 iters), loss = 0.0165075
I0825 23:40:16.045523  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00561005 (* 1 = 0.00561005 loss)
I0825 23:40:16.045536  1691 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0825 23:41:17.612738  1691 solver.cpp:514] Iteration 53500, Testing net (#0)
I0825 23:42:02.221233  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:42:02.451252  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931002
I0825 23:42:02.451313  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.270036 (* 1 = 0.270036 loss)
I0825 23:42:03.008530  1691 solver.cpp:357] Iteration 53500 (0.934908 iter/s, 106.962s/100 iters), loss = 0.0128066
I0825 23:42:03.008597  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00700321 (* 1 = 0.00700321 loss)
I0825 23:42:03.008608  1691 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0825 23:42:11.156038  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:43:05.003701  1691 solver.cpp:357] Iteration 53600 (1.61306 iter/s, 61.9938s/100 iters), loss = 0.018062
I0825 23:43:05.004076  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0219406 (* 1 = 0.0219406 loss)
I0825 23:43:05.004143  1691 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0825 23:44:06.668632  1691 solver.cpp:357] Iteration 53700 (1.6217 iter/s, 61.6636s/100 iters), loss = 0.0165121
I0825 23:44:06.668799  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0202873 (* 1 = 0.0202873 loss)
I0825 23:44:06.668812  1691 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0825 23:45:00.520848  1691 solver.cpp:357] Iteration 53800 (1.85706 iter/s, 53.8486s/100 iters), loss = 0.010119
I0825 23:45:00.520990  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157151 (* 1 = 0.0157151 loss)
I0825 23:45:00.521003  1691 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0825 23:46:00.836514  1691 solver.cpp:357] Iteration 53900 (1.65804 iter/s, 60.3122s/100 iters), loss = 0.0277861
I0825 23:46:00.836637  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0321348 (* 1 = 0.0321348 loss)
I0825 23:46:00.836648  1691 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0825 23:46:03.057101  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:47:02.436944  1691 solver.cpp:514] Iteration 54000, Testing net (#0)
I0825 23:47:47.114910  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:47:47.351474  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930002
I0825 23:47:47.351521  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.274734 (* 1 = 0.274734 loss)
I0825 23:47:47.706637  1691 solver.cpp:357] Iteration 54000 (0.935739 iter/s, 106.867s/100 iters), loss = 0.0102177
I0825 23:47:47.706707  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0081976 (* 1 = 0.0081976 loss)
I0825 23:47:47.706719  1691 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0825 23:48:49.689323  1691 solver.cpp:357] Iteration 54100 (1.61338 iter/s, 61.9816s/100 iters), loss = 0.039098
I0825 23:48:49.689481  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00949524 (* 1 = 0.00949524 loss)
I0825 23:48:49.689494  1691 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0825 23:49:51.517094  1691 solver.cpp:357] Iteration 54200 (1.61748 iter/s, 61.8246s/100 iters), loss = 0.00633596
I0825 23:49:51.517290  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00331651 (* 1 = 0.00331651 loss)
I0825 23:49:51.517318  1691 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0825 23:50:49.918759  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:50:53.375771  1691 solver.cpp:357] Iteration 54300 (1.61662 iter/s, 61.8576s/100 iters), loss = 0.0223096
I0825 23:50:53.375847  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00950954 (* 1 = 0.00950954 loss)
I0825 23:50:53.375860  1691 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0825 23:51:55.285086  1691 solver.cpp:357] Iteration 54400 (1.61535 iter/s, 61.9061s/100 iters), loss = 0.0132542
I0825 23:51:55.285253  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00581749 (* 1 = 0.00581749 loss)
I0825 23:51:55.285266  1691 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0825 23:52:52.555940  1691 solver.cpp:514] Iteration 54500, Testing net (#0)
I0825 23:53:35.846303  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:53:35.949427  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930102
I0825 23:53:35.949491  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.280468 (* 1 = 0.280468 loss)
I0825 23:53:36.526226  1691 solver.cpp:357] Iteration 54500 (0.987765 iter/s, 101.239s/100 iters), loss = 0.0178466
I0825 23:53:36.526306  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00575925 (* 1 = 0.00575925 loss)
I0825 23:53:36.526319  1691 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0825 23:54:34.743793  1691 solver.cpp:357] Iteration 54600 (1.71777 iter/s, 58.215s/100 iters), loss = 0.00462785
I0825 23:54:34.744024  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00487449 (* 1 = 0.00487449 loss)
I0825 23:54:34.744038  1691 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0825 23:55:26.586813  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:55:36.057451  1691 solver.cpp:357] Iteration 54700 (1.63105 iter/s, 61.3104s/100 iters), loss = 0.0274761
I0825 23:55:36.057524  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0109747 (* 1 = 0.0109747 loss)
I0825 23:55:36.057536  1691 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0825 23:56:38.099275  1691 solver.cpp:357] Iteration 54800 (1.6119 iter/s, 62.0386s/100 iters), loss = 0.0134728
I0825 23:56:38.099417  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00981565 (* 1 = 0.00981565 loss)
I0825 23:56:38.099431  1691 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0825 23:57:40.176710  1691 solver.cpp:357] Iteration 54900 (1.61098 iter/s, 62.0742s/100 iters), loss = 0.0290889
I0825 23:57:40.176909  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00847313 (* 1 = 0.00847313 loss)
I0825 23:57:40.176935  1691 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0825 23:58:41.558291  1691 solver.cpp:514] Iteration 55000, Testing net (#0)
I0825 23:59:26.384101  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0825 23:59:26.587496  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928602
I0825 23:59:26.587549  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.283379 (* 1 = 0.283379 loss)
I0825 23:59:27.110963  1691 solver.cpp:357] Iteration 55000 (0.935158 iter/s, 106.934s/100 iters), loss = 0.0250182
I0825 23:59:27.111043  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0206373 (* 1 = 0.0206373 loss)
I0825 23:59:27.111057  1691 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0826 00:00:11.912255  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:00:25.721424  1691 solver.cpp:357] Iteration 55100 (1.70628 iter/s, 58.6071s/100 iters), loss = 0.0328567
I0826 00:00:25.721491  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0127633 (* 1 = 0.0127633 loss)
I0826 00:00:25.721503  1691 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0826 00:01:25.212481  1691 solver.cpp:357] Iteration 55200 (1.68096 iter/s, 59.4898s/100 iters), loss = 0.00589061
I0826 00:01:25.212808  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00548833 (* 1 = 0.00548833 loss)
I0826 00:01:25.212841  1691 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0826 00:02:27.125061  1691 solver.cpp:357] Iteration 55300 (1.61527 iter/s, 61.9092s/100 iters), loss = 0.0325852
I0826 00:02:27.125200  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0397768 (* 1 = 0.0397768 loss)
I0826 00:02:27.125212  1691 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0826 00:03:29.036198  1691 solver.cpp:357] Iteration 55400 (1.61525 iter/s, 61.9099s/100 iters), loss = 0.00858143
I0826 00:03:29.036314  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00480731 (* 1 = 0.00480731 loss)
I0826 00:03:29.036326  1691 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0826 00:04:07.885695  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:04:26.952375  1691 solver.cpp:514] Iteration 55500, Testing net (#0)
I0826 00:05:11.005802  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:05:11.182566  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929502
I0826 00:05:11.182610  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.279248 (* 1 = 0.279248 loss)
I0826 00:05:11.651698  1691 solver.cpp:357] Iteration 55500 (0.974517 iter/s, 102.615s/100 iters), loss = 0.0143888
I0826 00:05:11.651770  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0147014 (* 1 = 0.0147014 loss)
I0826 00:05:11.651782  1691 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0826 00:06:13.629262  1691 solver.cpp:357] Iteration 55600 (1.61357 iter/s, 61.9742s/100 iters), loss = 0.00831842
I0826 00:06:13.629514  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00370408 (* 1 = 0.00370408 loss)
I0826 00:06:13.629528  1691 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0826 00:07:15.577050  1691 solver.cpp:357] Iteration 55700 (1.61435 iter/s, 61.9445s/100 iters), loss = 0.0401736
I0826 00:07:15.577204  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0588927 (* 1 = 0.0588927 loss)
I0826 00:07:15.577216  1691 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0826 00:08:14.144749  1691 solver.cpp:357] Iteration 55800 (1.70753 iter/s, 58.5643s/100 iters), loss = 0.007937
I0826 00:08:14.144856  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00919692 (* 1 = 0.00919692 loss)
I0826 00:08:14.144870  1691 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0826 00:08:47.080667  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:09:14.118824  1691 solver.cpp:357] Iteration 55900 (1.66742 iter/s, 59.9728s/100 iters), loss = 0.0298325
I0826 00:09:14.118903  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0216658 (* 1 = 0.0216658 loss)
I0826 00:09:14.118916  1691 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0826 00:10:15.625568  1691 solver.cpp:514] Iteration 56000, Testing net (#0)
I0826 00:11:00.344986  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:11:00.558522  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930702
I0826 00:11:00.558574  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.280727 (* 1 = 0.280727 loss)
I0826 00:11:01.023892  1691 solver.cpp:357] Iteration 56000 (0.935414 iter/s, 106.905s/100 iters), loss = 0.0709491
I0826 00:11:01.023962  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0157088 (* 1 = 0.0157088 loss)
I0826 00:11:01.023974  1691 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0826 00:12:03.001268  1691 solver.cpp:357] Iteration 56100 (1.61352 iter/s, 61.9761s/100 iters), loss = 0.0313804
I0826 00:12:03.001477  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00970031 (* 1 = 0.00970031 loss)
I0826 00:12:03.001505  1691 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0826 00:13:05.108942  1691 solver.cpp:357] Iteration 56200 (1.61014 iter/s, 62.1064s/100 iters), loss = 0.0159151
I0826 00:13:05.109040  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00593019 (* 1 = 0.00593019 loss)
I0826 00:13:05.109050  1691 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0826 00:13:33.262890  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:14:04.195680  1691 solver.cpp:357] Iteration 56300 (1.69241 iter/s, 59.0875s/100 iters), loss = 0.00583333
I0826 00:14:04.195832  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00441837 (* 1 = 0.00441837 loss)
I0826 00:14:04.195844  1691 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0826 00:15:05.494721  1691 solver.cpp:357] Iteration 56400 (1.63138 iter/s, 61.2978s/100 iters), loss = 0.0326342
I0826 00:15:05.494877  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.015172 (* 1 = 0.015172 loss)
I0826 00:15:05.494891  1691 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0826 00:16:03.587170  1691 solver.cpp:514] Iteration 56500, Testing net (#0)
I0826 00:16:45.897677  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:16:45.966953  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928702
I0826 00:16:45.967022  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.289316 (* 1 = 0.289316 loss)
I0826 00:16:46.588284  1691 solver.cpp:357] Iteration 56500 (0.989189 iter/s, 101.093s/100 iters), loss = 0.0454591
I0826 00:16:46.588366  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.013885 (* 1 = 0.013885 loss)
I0826 00:16:46.588378  1691 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0826 00:17:48.532727  1691 solver.cpp:357] Iteration 56600 (1.61438 iter/s, 61.9432s/100 iters), loss = 0.00753413
I0826 00:17:48.532924  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00444857 (* 1 = 0.00444857 loss)
I0826 00:17:48.532939  1691 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0826 00:18:12.333425  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:18:50.592038  1691 solver.cpp:357] Iteration 56700 (1.61145 iter/s, 62.056s/100 iters), loss = 0.00742446
I0826 00:18:50.592207  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122447 (* 1 = 0.0122447 loss)
I0826 00:18:50.592218  1691 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0826 00:19:52.625465  1691 solver.cpp:357] Iteration 56800 (1.61207 iter/s, 62.0321s/100 iters), loss = 0.0105093
I0826 00:19:52.625605  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0171638 (* 1 = 0.0171638 loss)
I0826 00:19:52.625617  1691 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0826 00:20:54.556175  1691 solver.cpp:357] Iteration 56900 (1.61469 iter/s, 61.9314s/100 iters), loss = 0.0186111
I0826 00:20:54.556293  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00950845 (* 1 = 0.00950845 loss)
I0826 00:20:54.556305  1691 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0826 00:21:55.824471  1691 solver.cpp:514] Iteration 57000, Testing net (#0)
I0826 00:22:40.494110  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:22:40.717056  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931502
I0826 00:22:40.717120  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.28428 (* 1 = 0.28428 loss)
I0826 00:22:41.250262  1691 solver.cpp:357] Iteration 57000 (0.937297 iter/s, 106.69s/100 iters), loss = 0.0296875
I0826 00:22:41.250327  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0414984 (* 1 = 0.0414984 loss)
I0826 00:22:41.250342  1691 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0826 00:22:58.872635  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:23:37.815714  1691 solver.cpp:357] Iteration 57100 (1.76792 iter/s, 56.5636s/100 iters), loss = 0.0109185
I0826 00:23:37.815824  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.012068 (* 1 = 0.012068 loss)
I0826 00:23:37.815836  1691 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0826 00:24:35.235186  1691 solver.cpp:357] Iteration 57200 (1.74162 iter/s, 57.4177s/100 iters), loss = 0.0101006
I0826 00:24:35.235307  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00654761 (* 1 = 0.00654761 loss)
I0826 00:24:35.235319  1691 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0826 00:25:37.438961  1691 solver.cpp:357] Iteration 57300 (1.60766 iter/s, 62.2021s/100 iters), loss = 0.0141051
I0826 00:25:37.439122  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0107369 (* 1 = 0.0107369 loss)
I0826 00:25:37.439133  1691 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0826 00:26:39.416672  1691 solver.cpp:357] Iteration 57400 (1.61357 iter/s, 61.9743s/100 iters), loss = 0.0367834
I0826 00:26:39.416820  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0265456 (* 1 = 0.0265456 loss)
I0826 00:26:39.416831  1691 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0826 00:26:51.273147  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:27:40.807255  1691 solver.cpp:514] Iteration 57500, Testing net (#0)
I0826 00:28:25.539033  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:28:25.711490  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930702
I0826 00:28:25.711550  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.286158 (* 1 = 0.286158 loss)
I0826 00:28:26.211829  1691 solver.cpp:357] Iteration 57500 (0.936425 iter/s, 106.789s/100 iters), loss = 0.0151823
I0826 00:28:26.211901  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0214807 (* 1 = 0.0214807 loss)
I0826 00:28:26.211912  1691 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0826 00:29:27.990361  1691 solver.cpp:357] Iteration 57600 (1.61876 iter/s, 61.7755s/100 iters), loss = 0.0444459
I0826 00:29:27.990586  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0610561 (* 1 = 0.0610561 loss)
I0826 00:29:27.990598  1691 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0826 00:30:29.732480  1691 solver.cpp:357] Iteration 57700 (1.61971 iter/s, 61.7393s/100 iters), loss = 0.0450514
I0826 00:30:29.732657  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.054098 (* 1 = 0.054098 loss)
I0826 00:30:29.732671  1691 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0826 00:31:28.954282  1691 solver.cpp:357] Iteration 57800 (1.68867 iter/s, 59.2182s/100 iters), loss = 0.0126707
I0826 00:31:28.954460  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0208497 (* 1 = 0.0208497 loss)
I0826 00:31:28.954473  1691 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0826 00:31:34.440793  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:32:27.771132  1691 solver.cpp:357] Iteration 57900 (1.70027 iter/s, 58.8143s/100 iters), loss = 0.0101698
I0826 00:32:27.771306  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00854909 (* 1 = 0.00854909 loss)
I0826 00:32:27.771317  1691 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0826 00:33:26.369985  1691 solver.cpp:514] Iteration 58000, Testing net (#0)
I0826 00:34:09.474318  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:34:09.657042  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931402
I0826 00:34:09.657101  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.285998 (* 1 = 0.285998 loss)
I0826 00:34:10.190250  1691 solver.cpp:357] Iteration 58000 (0.976391 iter/s, 102.418s/100 iters), loss = 0.017621
I0826 00:34:10.190323  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0276858 (* 1 = 0.0276858 loss)
I0826 00:34:10.190343  1691 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0826 00:35:12.216138  1691 solver.cpp:357] Iteration 58100 (1.61229 iter/s, 62.0235s/100 iters), loss = 0.0133073
I0826 00:35:12.216253  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00691413 (* 1 = 0.00691413 loss)
I0826 00:35:12.216265  1691 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0826 00:36:14.008169  1691 solver.cpp:357] Iteration 58200 (1.61839 iter/s, 61.7897s/100 iters), loss = 0.0272232
I0826 00:36:14.008328  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0407534 (* 1 = 0.0407534 loss)
I0826 00:36:14.008340  1691 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0826 00:36:14.255971  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:37:15.762272  1691 solver.cpp:357] Iteration 58300 (1.61938 iter/s, 61.7518s/100 iters), loss = 0.0188368
I0826 00:37:15.762420  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.026427 (* 1 = 0.026427 loss)
I0826 00:37:15.762434  1691 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0826 00:38:17.697098  1691 solver.cpp:357] Iteration 58400 (1.61471 iter/s, 61.9305s/100 iters), loss = 0.00875749
I0826 00:38:17.697232  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0142432 (* 1 = 0.0142432 loss)
I0826 00:38:17.697247  1691 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0826 00:39:16.412569  1691 solver.cpp:514] Iteration 58500, Testing net (#0)
I0826 00:39:57.858073  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:39:58.088706  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929102
I0826 00:39:58.088758  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294644 (* 1 = 0.294644 loss)
I0826 00:39:58.518030  1691 solver.cpp:357] Iteration 58500 (0.991899 iter/s, 100.817s/100 iters), loss = 0.0145626
I0826 00:39:58.518097  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0230286 (* 1 = 0.0230286 loss)
I0826 00:39:58.518107  1691 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0826 00:40:55.000192  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:41:00.478282  1691 solver.cpp:357] Iteration 58600 (1.61394 iter/s, 61.9603s/100 iters), loss = 0.0131179
I0826 00:41:00.478359  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0218309 (* 1 = 0.0218309 loss)
I0826 00:41:00.478371  1691 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0826 00:42:02.553589  1691 solver.cpp:357] Iteration 58700 (1.61106 iter/s, 62.0711s/100 iters), loss = 0.020046
I0826 00:42:02.553716  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0316881 (* 1 = 0.0316881 loss)
I0826 00:42:02.553730  1691 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0826 00:43:01.912012  1691 solver.cpp:357] Iteration 58800 (1.6848 iter/s, 59.3543s/100 iters), loss = 0.0144179
I0826 00:43:01.912149  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0151369 (* 1 = 0.0151369 loss)
I0826 00:43:01.912163  1691 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0826 00:44:01.770051  1691 solver.cpp:357] Iteration 58900 (1.67073 iter/s, 59.8539s/100 iters), loss = 0.00820348
I0826 00:44:01.770185  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0108731 (* 1 = 0.0108731 loss)
I0826 00:44:01.770197  1691 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0826 00:44:52.340451  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:45:03.171058  1691 solver.cpp:514] Iteration 59000, Testing net (#0)
I0826 00:45:48.005093  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:45:48.093070  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928902
I0826 00:45:48.093122  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.2923 (* 1 = 0.2923 loss)
I0826 00:45:48.657317  1691 solver.cpp:357] Iteration 59000 (0.935581 iter/s, 106.885s/100 iters), loss = 0.0163235
I0826 00:45:48.657397  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0299585 (* 1 = 0.0299585 loss)
I0826 00:45:48.657409  1691 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0826 00:46:48.989634  1691 solver.cpp:357] Iteration 59100 (1.6576 iter/s, 60.3282s/100 iters), loss = 0.014641
I0826 00:46:48.989745  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.022206 (* 1 = 0.022206 loss)
I0826 00:46:48.989758  1691 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0826 00:47:47.117707  1691 solver.cpp:357] Iteration 59200 (1.7204 iter/s, 58.1261s/100 iters), loss = 0.0154368
I0826 00:47:47.117857  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138971 (* 1 = 0.0138971 loss)
I0826 00:47:47.117871  1691 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0826 00:48:48.926179  1691 solver.cpp:357] Iteration 59300 (1.61801 iter/s, 61.8044s/100 iters), loss = 0.00850405
I0826 00:48:48.926319  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0119234 (* 1 = 0.0119234 loss)
I0826 00:48:48.926332  1691 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0826 00:49:33.805233  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:49:50.957036  1691 solver.cpp:357] Iteration 59400 (1.6122 iter/s, 62.0269s/100 iters), loss = 0.0253169
I0826 00:49:50.957104  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0311633 (* 1 = 0.0311633 loss)
I0826 00:49:50.957118  1691 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0826 00:50:52.416203  1691 solver.cpp:514] Iteration 59500, Testing net (#0)
I0826 00:51:37.355131  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:51:37.485993  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931302
I0826 00:51:37.486038  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.284984 (* 1 = 0.284984 loss)
I0826 00:51:37.940323  1691 solver.cpp:357] Iteration 59500 (0.93474 iter/s, 106.982s/100 iters), loss = 0.0114835
I0826 00:51:37.940399  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0136484 (* 1 = 0.0136484 loss)
I0826 00:51:37.940412  1691 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0826 00:52:38.023725  1691 solver.cpp:357] Iteration 59600 (1.66446 iter/s, 60.0794s/100 iters), loss = 0.0101581
I0826 00:52:38.023952  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.014817 (* 1 = 0.014817 loss)
I0826 00:52:38.023965  1691 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0826 00:53:37.767915  1691 solver.cpp:357] Iteration 59700 (1.6738 iter/s, 59.7442s/100 iters), loss = 0.00870638
I0826 00:53:37.768087  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00899493 (* 1 = 0.00899493 loss)
I0826 00:53:37.768100  1691 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0826 00:54:16.986918  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:54:37.944555  1691 solver.cpp:357] Iteration 59800 (1.66188 iter/s, 60.1726s/100 iters), loss = 0.00653871
I0826 00:54:37.944633  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00838156 (* 1 = 0.00838156 loss)
I0826 00:54:37.944644  1691 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0826 00:55:36.369210  1691 solver.cpp:357] Iteration 59900 (1.71172 iter/s, 58.4207s/100 iters), loss = 0.0141802
I0826 00:55:36.369369  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0227052 (* 1 = 0.0227052 loss)
I0826 00:55:36.369380  1691 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0826 00:56:37.561699  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_60000.caffemodel
I0826 00:56:37.583907  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_60000.solverstate
I0826 00:56:37.589161  1691 solver.cpp:514] Iteration 60000, Testing net (#0)
I0826 00:57:22.374866  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:57:22.605368  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0826 00:57:22.605425  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.289639 (* 1 = 0.289639 loss)
I0826 00:57:23.005909  1691 solver.cpp:357] Iteration 60000 (0.937737 iter/s, 106.64s/100 iters), loss = 0.00449801
I0826 00:57:23.005982  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00393167 (* 1 = 0.00393167 loss)
I0826 00:57:23.005995  1691 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0826 00:58:25.103971  1691 solver.cpp:357] Iteration 60100 (1.6104 iter/s, 62.0964s/100 iters), loss = 0.0247673
I0826 00:58:25.104144  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0159241 (* 1 = 0.0159241 loss)
I0826 00:58:25.104157  1691 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0826 00:58:58.330325  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 00:59:27.077168  1691 solver.cpp:357] Iteration 60200 (1.61365 iter/s, 61.9713s/100 iters), loss = 0.0101457
I0826 00:59:27.077251  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00811335 (* 1 = 0.00811335 loss)
I0826 00:59:27.077265  1691 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0826 01:00:28.578315  1691 solver.cpp:357] Iteration 60300 (1.62604 iter/s, 61.4991s/100 iters), loss = 0.00239719
I0826 01:00:28.578471  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.002597 (* 1 = 0.002597 loss)
I0826 01:00:28.578485  1691 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0826 01:01:30.617592  1691 solver.cpp:357] Iteration 60400 (1.61188 iter/s, 62.0392s/100 iters), loss = 0.0100842
I0826 01:01:30.617736  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122928 (* 1 = 0.0122928 loss)
I0826 01:01:30.617748  1691 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0826 01:02:28.201198  1691 solver.cpp:514] Iteration 60500, Testing net (#0)
I0826 01:03:06.072264  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:03:06.244805  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931102
I0826 01:03:06.244868  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294206 (* 1 = 0.294206 loss)
I0826 01:03:06.642223  1691 solver.cpp:357] Iteration 60500 (1.04137 iter/s, 96.0274s/100 iters), loss = 0.0128918
I0826 01:03:06.642292  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0177431 (* 1 = 0.0177431 loss)
I0826 01:03:06.642302  1691 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0826 01:03:34.108724  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:04:08.539022  1691 solver.cpp:357] Iteration 60600 (1.61555 iter/s, 61.8984s/100 iters), loss = 0.00504946
I0826 01:04:08.539294  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00450616 (* 1 = 0.00450616 loss)
I0826 01:04:08.539309  1691 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0826 01:05:10.397311  1691 solver.cpp:357] Iteration 60700 (1.61661 iter/s, 61.8577s/100 iters), loss = 0.00794759
I0826 01:05:10.397451  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00819959 (* 1 = 0.00819959 loss)
I0826 01:05:10.397464  1691 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0826 01:06:12.315572  1691 solver.cpp:357] Iteration 60800 (1.61505 iter/s, 61.9176s/100 iters), loss = 0.0200063
I0826 01:06:12.315852  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0314388 (* 1 = 0.0314388 loss)
I0826 01:06:12.315912  1691 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0826 01:07:14.199483  1691 solver.cpp:357] Iteration 60900 (1.6159 iter/s, 61.885s/100 iters), loss = 0.0406397
I0826 01:07:14.199596  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0735347 (* 1 = 0.0735347 loss)
I0826 01:07:14.199609  1691 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0826 01:07:35.699026  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:08:15.627251  1691 solver.cpp:514] Iteration 61000, Testing net (#0)
I0826 01:09:00.411936  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:09:00.596771  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932002
I0826 01:09:00.596829  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.292756 (* 1 = 0.292756 loss)
I0826 01:09:01.086625  1691 solver.cpp:357] Iteration 61000 (0.935566 iter/s, 106.887s/100 iters), loss = 0.0151914
I0826 01:09:01.086691  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0222234 (* 1 = 0.0222234 loss)
I0826 01:09:01.086702  1691 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0826 01:10:02.180084  1691 solver.cpp:357] Iteration 61100 (1.63686 iter/s, 61.0924s/100 iters), loss = 0.00720165
I0826 01:10:02.180244  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0102319 (* 1 = 0.0102319 loss)
I0826 01:10:02.180258  1691 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0826 01:10:59.853011  1691 solver.cpp:357] Iteration 61200 (1.73395 iter/s, 57.6718s/100 iters), loss = 0.0168066
I0826 01:10:59.853140  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00537994 (* 1 = 0.00537994 loss)
I0826 01:10:59.853154  1691 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0826 01:12:00.103209  1691 solver.cpp:357] Iteration 61300 (1.65984 iter/s, 60.2469s/100 iters), loss = 0.0139381
I0826 01:12:00.106405  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00486318 (* 1 = 0.00486318 loss)
I0826 01:12:00.106422  1691 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0826 01:12:15.047425  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:12:59.336513  1691 solver.cpp:357] Iteration 61400 (1.6884 iter/s, 59.2276s/100 iters), loss = 0.02862
I0826 01:12:59.336827  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00509981 (* 1 = 0.00509981 loss)
I0826 01:12:59.336891  1691 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0826 01:14:00.793835  1691 solver.cpp:514] Iteration 61500, Testing net (#0)
I0826 01:14:45.726616  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:14:45.927788  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929402
I0826 01:14:45.927836  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.294904 (* 1 = 0.294904 loss)
I0826 01:14:46.454500  1691 solver.cpp:357] Iteration 61500 (0.933572 iter/s, 107.115s/100 iters), loss = 0.0155949
I0826 01:14:46.454577  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0255478 (* 1 = 0.0255478 loss)
I0826 01:14:46.454591  1691 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0826 01:15:48.318719  1691 solver.cpp:357] Iteration 61600 (1.61653 iter/s, 61.8608s/100 iters), loss = 0.00607848
I0826 01:15:48.318951  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00549406 (* 1 = 0.00549406 loss)
I0826 01:15:48.318966  1691 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0826 01:16:50.159229  1691 solver.cpp:357] Iteration 61700 (1.61715 iter/s, 61.8371s/100 iters), loss = 0.0126879
I0826 01:16:50.159411  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0170907 (* 1 = 0.0170907 loss)
I0826 01:16:50.159442  1691 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0826 01:17:00.305502  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:17:50.929229  1691 solver.cpp:357] Iteration 61800 (1.64558 iter/s, 60.7687s/100 iters), loss = 0.0261342
I0826 01:17:50.929363  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0408902 (* 1 = 0.0408902 loss)
I0826 01:17:50.929374  1691 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0826 01:18:48.098218  1691 solver.cpp:357] Iteration 61900 (1.74924 iter/s, 57.1676s/100 iters), loss = 0.00433864
I0826 01:18:48.098353  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00292728 (* 1 = 0.00292728 loss)
I0826 01:18:48.098364  1691 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0826 01:19:49.488942  1691 solver.cpp:514] Iteration 62000, Testing net (#0)
I0826 01:20:34.276448  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:20:34.413285  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929502
I0826 01:20:34.413349  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.297175 (* 1 = 0.297175 loss)
I0826 01:20:34.915705  1691 solver.cpp:357] Iteration 62000 (0.936165 iter/s, 106.819s/100 iters), loss = 0.0105971
I0826 01:20:34.915787  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0154376 (* 1 = 0.0154376 loss)
I0826 01:20:34.915801  1691 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0826 01:21:35.870635  1691 solver.cpp:357] Iteration 62100 (1.64065 iter/s, 60.9514s/100 iters), loss = 0.00502041
I0826 01:21:35.870746  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00705652 (* 1 = 0.00705652 loss)
I0826 01:21:35.870759  1691 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0826 01:21:39.724390  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:22:34.841576  1691 solver.cpp:357] Iteration 62200 (1.69578 iter/s, 58.9698s/100 iters), loss = 0.00564066
I0826 01:22:34.841701  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00503691 (* 1 = 0.00503691 loss)
I0826 01:22:34.841712  1691 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0826 01:23:36.514763  1691 solver.cpp:357] Iteration 62300 (1.62144 iter/s, 61.6735s/100 iters), loss = 0.00633683
I0826 01:23:36.514935  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00343195 (* 1 = 0.00343195 loss)
I0826 01:23:36.514947  1691 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0826 01:24:38.417609  1691 solver.cpp:357] Iteration 62400 (1.61553 iter/s, 61.8993s/100 iters), loss = 0.00916491
I0826 01:24:38.417775  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0126599 (* 1 = 0.0126599 loss)
I0826 01:24:38.417788  1691 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0826 01:25:37.973390  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:25:39.087157  1691 solver.cpp:514] Iteration 62500, Testing net (#0)
I0826 01:26:18.789273  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:26:19.021191  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929202
I0826 01:26:19.021240  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.293852 (* 1 = 0.293852 loss)
I0826 01:26:19.573300  1691 solver.cpp:357] Iteration 62500 (0.988584 iter/s, 101.155s/100 iters), loss = 0.012913
I0826 01:26:19.573370  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0151067 (* 1 = 0.0151067 loss)
I0826 01:26:19.573381  1691 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0826 01:27:21.393416  1691 solver.cpp:357] Iteration 62600 (1.61763 iter/s, 61.8187s/100 iters), loss = 0.0118273
I0826 01:27:21.393599  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0153789 (* 1 = 0.0153789 loss)
I0826 01:27:21.393612  1691 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0826 01:28:23.390362  1691 solver.cpp:357] Iteration 62700 (1.61307 iter/s, 61.9934s/100 iters), loss = 0.0339604
I0826 01:28:23.390524  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.051487 (* 1 = 0.051487 loss)
I0826 01:28:23.390537  1691 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0826 01:29:25.343020  1691 solver.cpp:357] Iteration 62800 (1.61417 iter/s, 61.9512s/100 iters), loss = 0.00573636
I0826 01:29:25.343140  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00863992 (* 1 = 0.00863992 loss)
I0826 01:29:25.343153  1691 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0826 01:30:19.934154  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:30:27.351655  1691 solver.cpp:357] Iteration 62900 (1.61269 iter/s, 62.0082s/100 iters), loss = 0.00730682
I0826 01:30:27.351737  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00497451 (* 1 = 0.00497451 loss)
I0826 01:30:27.351749  1691 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0826 01:31:27.823789  1691 solver.cpp:514] Iteration 63000, Testing net (#0)
I0826 01:32:08.845669  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:32:09.044266  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.930902
I0826 01:32:09.044332  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.2982 (* 1 = 0.2982 loss)
I0826 01:32:09.493276  1691 solver.cpp:357] Iteration 63000 (0.978976 iter/s, 102.148s/100 iters), loss = 0.0073752
I0826 01:32:09.493340  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00463089 (* 1 = 0.00463089 loss)
I0826 01:32:09.493352  1691 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0826 01:33:11.173938  1691 solver.cpp:357] Iteration 63100 (1.62116 iter/s, 61.6841s/100 iters), loss = 0.0147519
I0826 01:33:11.174039  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0259439 (* 1 = 0.0259439 loss)
I0826 01:33:11.174051  1691 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0826 01:34:07.565721  1691 solver.cpp:357] Iteration 63200 (1.7732 iter/s, 56.3953s/100 iters), loss = 0.0110192
I0826 01:34:07.565961  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0174841 (* 1 = 0.0174841 loss)
I0826 01:34:07.566009  1691 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0826 01:34:56.424614  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:35:09.620434  1691 solver.cpp:357] Iteration 63300 (1.61144 iter/s, 62.0563s/100 iters), loss = 0.046405
I0826 01:35:09.620496  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0756858 (* 1 = 0.0756858 loss)
I0826 01:35:09.620506  1691 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0826 01:36:11.607386  1691 solver.cpp:357] Iteration 63400 (1.61316 iter/s, 61.9902s/100 iters), loss = 0.0150617
I0826 01:36:11.607512  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122949 (* 1 = 0.0122949 loss)
I0826 01:36:11.607524  1691 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0826 01:37:13.121073  1691 solver.cpp:514] Iteration 63500, Testing net (#0)
I0826 01:37:57.835080  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:37:57.972812  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.931602
I0826 01:37:57.972867  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.291256 (* 1 = 0.291256 loss)
I0826 01:37:58.565415  1691 solver.cpp:357] Iteration 63500 (0.934899 iter/s, 106.963s/100 iters), loss = 0.0198991
I0826 01:37:58.565486  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0133257 (* 1 = 0.0133257 loss)
I0826 01:37:58.565500  1691 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0826 01:39:00.501780  1691 solver.cpp:357] Iteration 63600 (1.61454 iter/s, 61.9371s/100 iters), loss = 0.0108075
I0826 01:39:00.501969  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00505801 (* 1 = 0.00505801 loss)
I0826 01:39:00.501982  1691 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0826 01:39:43.313140  1696 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:40:02.404628  1691 solver.cpp:357] Iteration 63700 (1.61547 iter/s, 61.9013s/100 iters), loss = 0.0212339
I0826 01:40:02.404706  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0393352 (* 1 = 0.0393352 loss)
I0826 01:40:02.404718  1691 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0826 01:41:03.480600  1691 solver.cpp:357] Iteration 63800 (1.63735 iter/s, 61.0743s/100 iters), loss = 0.0236089
I0826 01:41:03.480710  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00560411 (* 1 = 0.00560411 loss)
I0826 01:41:03.480724  1691 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0826 01:41:55.565675  1691 solver.cpp:357] Iteration 63900 (1.91994 iter/s, 52.085s/100 iters), loss = 0.0137153
I0826 01:41:55.565822  1691 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00917961 (* 1 = 0.00917961 loss)
I0826 01:41:55.565836  1691 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0826 01:42:56.872651  1691 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_64000.caffemodel
I0826 01:42:56.893009  1691 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_64000.solverstate
I0826 01:42:56.996976  1691 solver.cpp:472] Iteration 64000, loss = 0.00502695
I0826 01:42:56.997027  1691 solver.cpp:514] Iteration 64000, Testing net (#0)
I0826 01:43:41.692900  1697 data_layer.cpp:73] Restarting data prefetching from start.
I0826 01:43:41.792107  1691 solver.cpp:580]     Test net output #0: Accuracy1 = 0.932402
I0826 01:43:41.792153  1691 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.289833 (* 1 = 0.289833 loss)
I0826 01:43:41.792160  1691 solver.cpp:479] Optimization Done.
I0826 01:43:41.792165  1691 caffe.cpp:326] Optimization Done.
