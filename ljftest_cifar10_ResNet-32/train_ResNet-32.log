WARNING: Logging before InitGoogleLogging() is written to STDERR
I0818 10:44:54.077419  2034 caffe.cpp:530] argc:5 lijianfei debug!!!!!!!!!!
I0818 10:44:54.077711  2034 caffe.cpp:533] argv[0]:../../build/tools/caffe lijianfei debug!!!!!!!!!!
I0818 10:44:54.077733  2034 caffe.cpp:533] argv[1]:train lijianfei debug!!!!!!!!!!
I0818 10:44:54.077747  2034 caffe.cpp:533] argv[2]:--solver=./solver.prototxt lijianfei debug!!!!!!!!!!
I0818 10:44:54.077765  2034 caffe.cpp:533] argv[3]:--weights= lijianfei debug!!!!!!!!!!
I0818 10:44:54.077780  2034 caffe.cpp:533] argv[4]:--gpu=0 lijianfei debug!!!!!!!!!!
I0818 10:44:54.077929  2034 caffe.cpp:548] use WITH_PYTHON_LAYER lijianfei debug!!!!!!!!!!
I0818 10:44:54.078270  2034 caffe.cpp:553] caffe::string(argv[1]):train lijianfei debug!!!!!!!!!!
I0818 10:44:54.114723  2034 caffe.cpp:238] stages: lijianfei debug!!!!!!!!!!!!
I0818 10:44:54.114764  2034 caffe.cpp:269] Using GPUs 0
I0818 10:44:54.346863  2034 caffe.cpp:274] GPU 0: GeForce GTX 1060 6GB
I0818 10:44:56.538010  2034 solver_factory.hpp:111] function Solver<Dtype>* CreateSolver()  lijianfei debug!!!!!!!!!!
I0818 10:44:56.538089  2034 solver_factory.hpp:113] type:Nesterov lijianfei debug!!!!!!!!!!
I0818 10:44:56.831804  2034 solver.cpp:97] Initializing solver from parameters: 
train_net: "./train_ResNet_32.prototxt"
test_net: "./test_ResNet_32.prototxt"
test_iter: 1000
test_interval: 500
base_lr: 0.1
display: 100
max_iter: 64000
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "./model_save/cifar10_ResNet_32"
solver_mode: GPU
device_id: 0
train_state {
  level: 0
  stage: ""
}
stepvalue: 32000
stepvalue: 48000
type: "Nesterov"
I0818 10:44:56.855501  2034 solver.cpp:167] Creating training net from train_net file: ./train_ResNet_32.prototxt
I0818 10:44:56.884625  2034 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./train_ResNet_32.prototxt
I0818 10:44:56.884712  2034 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0818 10:44:56.885610  2034 net.cpp:390] layer_param.include_size():1
I0818 10:44:56.885651  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885679  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885696  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885715  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885731  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885748  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885764  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885782  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885797  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885814  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885829  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885848  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885862  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885879  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885895  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885912  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885928  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885944  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.885960  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.885978  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886004  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886021  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886036  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886054  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886080  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886097  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886123  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886140  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886155  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886173  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886188  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886205  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886229  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886245  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886260  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886276  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886292  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886309  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886384  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886404  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886418  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886443  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886459  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886476  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886497  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886514  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886534  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886554  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886574  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886591  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886607  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886629  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886687  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886713  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886729  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886747  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886772  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886790  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886806  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886821  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886837  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886853  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886876  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886893  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886909  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886926  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886942  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886960  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.886981  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.886998  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887014  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887030  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887045  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887064  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887084  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887101  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887117  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887133  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887150  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887166  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887189  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887207  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887223  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887239  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887262  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887280  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887300  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887317  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887333  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887349  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887364  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887382  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887403  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887419  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887440  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887457  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887478  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887528  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887545  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887562  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887578  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887595  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887616  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887634  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887650  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887665  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887689  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887707  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887722  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887739  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887755  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887771  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887794  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887811  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887826  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887843  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887859  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887876  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887898  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887917  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887931  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887948  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.887964  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.887980  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888002  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888020  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888036  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888052  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888075  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888092  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888108  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888124  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888139  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888156  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888177  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888195  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888211  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888226  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888243  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888259  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888280  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888298  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888312  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888329  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888352  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888370  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888386  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888401  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888417  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888432  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888448  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888466  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888489  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888505  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888520  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888567  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888586  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888602  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888623  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888640  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888656  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888674  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888694  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888711  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888726  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888742  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888758  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888774  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888797  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888814  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888829  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888846  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888869  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888885  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888908  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888926  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888947  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888965  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.888981  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.888998  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889014  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889030  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889046  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889062  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889077  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889094  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889109  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889127  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889142  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889158  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889173  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889190  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889205  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889221  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889237  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889253  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889268  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889286  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889302  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889318  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889333  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889349  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889365  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889382  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889405  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889423  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889438  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889456  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889470  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889487  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889502  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889519  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889541  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889559  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889605  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889623  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889647  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889663  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889678  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889695  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889717  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889735  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889750  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889767  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889788  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889806  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889820  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889837  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889853  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889870  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889890  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889909  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889924  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889940  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889955  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.889972  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.889993  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890010  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890025  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890043  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890058  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890076  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890097  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890115  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890130  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890146  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890168  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890187  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890202  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890218  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890241  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890259  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890274  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890290  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890306  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890324  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890345  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890363  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890379  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890398  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890417  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890434  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890450  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890465  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890481  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890498  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890514  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890539  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890555  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890573  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890588  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890604  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890647  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890719  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890735  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890753  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890774  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890792  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890807  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890825  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890846  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890863  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890878  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890897  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890919  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890940  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890944  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890947  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890951  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890955  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890959  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890964  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890966  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890970  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890976  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.890980  2034 net.cpp:390] layer_param.include_size():0
I0818 10:44:56.890983  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:44:56.891988  2034 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215684
    mirror: true
    crop_size: 32
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/train_lmdb"
    batch_size: 128
    backend: LMDB
  }
  image_data_param {
    shuffle: true
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution24"
  top: "Convolution24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Convolution24"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution25"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution26"
  top: "Convolution26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Convolution26"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution27"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution28"
  top: "Convolution28"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Convolution28"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution29"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution30"
  top: "Convolution30"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Convolution30"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution31"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_
I0818 10:44:56.892637  2034 layer_factory.hpp:77] Creating layer Data1
I0818 10:44:57.030931  2034 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/train_lmdb
I0818 10:44:57.037544  2034 net.cpp:128] Creating Layer Data1
I0818 10:44:57.037621  2034 net.cpp:522] Data1 -> Data1
I0818 10:44:57.037714  2034 net.cpp:522] Data1 -> Data2
I0818 10:44:57.041829  2034 data_layer.cpp:45] output data size: 128,3,32,32
I0818 10:44:57.052096  2034 net.cpp:172] Setting up Data1
I0818 10:44:57.052155  2034 net.cpp:186] Top shape: 128 3 32 32 (393216)
I0818 10:44:57.052165  2034 net.cpp:186] Top shape: 128 (128)
I0818 10:44:57.052171  2034 net.cpp:194] Memory required for data: 1573376
I0818 10:44:57.052188  2034 layer_factory.hpp:77] Creating layer Convolution1
I0818 10:44:57.052230  2034 net.cpp:128] Creating Layer Convolution1
I0818 10:44:57.052242  2034 net.cpp:558] Convolution1 <- Data1
I0818 10:44:57.052268  2034 net.cpp:522] Convolution1 -> Convolution1
I0818 10:45:00.829525  2034 net.cpp:172] Setting up Convolution1
I0818 10:45:00.829695  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.829717  2034 net.cpp:194] Memory required for data: 9961984
I0818 10:45:00.829821  2034 layer_factory.hpp:77] Creating layer BatchNorm1
I0818 10:45:00.829872  2034 net.cpp:128] Creating Layer BatchNorm1
I0818 10:45:00.829900  2034 net.cpp:558] BatchNorm1 <- Convolution1
I0818 10:45:00.829929  2034 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0818 10:45:00.830826  2034 net.cpp:172] Setting up BatchNorm1
I0818 10:45:00.830935  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.830962  2034 net.cpp:194] Memory required for data: 18350592
I0818 10:45:00.831017  2034 layer_factory.hpp:77] Creating layer Scale1
I0818 10:45:00.831058  2034 net.cpp:128] Creating Layer Scale1
I0818 10:45:00.831084  2034 net.cpp:558] Scale1 <- Convolution1
I0818 10:45:00.831110  2034 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0818 10:45:00.831303  2034 layer_factory.hpp:77] Creating layer Scale1
I0818 10:45:00.831794  2034 net.cpp:172] Setting up Scale1
I0818 10:45:00.831835  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.831853  2034 net.cpp:194] Memory required for data: 26739200
I0818 10:45:00.831887  2034 layer_factory.hpp:77] Creating layer ReLU1
I0818 10:45:00.831928  2034 net.cpp:128] Creating Layer ReLU1
I0818 10:45:00.831946  2034 net.cpp:558] ReLU1 <- Convolution1
I0818 10:45:00.831970  2034 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0818 10:45:00.832808  2034 net.cpp:172] Setting up ReLU1
I0818 10:45:00.832855  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.832875  2034 net.cpp:194] Memory required for data: 35127808
I0818 10:45:00.832893  2034 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0818 10:45:00.832926  2034 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0818 10:45:00.832943  2034 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0818 10:45:00.832970  2034 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0818 10:45:00.833004  2034 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0818 10:45:00.833176  2034 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0818 10:45:00.833212  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.833243  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.833266  2034 net.cpp:194] Memory required for data: 51905024
I0818 10:45:00.833284  2034 layer_factory.hpp:77] Creating layer Convolution2
I0818 10:45:00.833333  2034 net.cpp:128] Creating Layer Convolution2
I0818 10:45:00.833353  2034 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0818 10:45:00.833380  2034 net.cpp:522] Convolution2 -> Convolution2
I0818 10:45:00.839653  2034 net.cpp:172] Setting up Convolution2
I0818 10:45:00.839682  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.839689  2034 net.cpp:194] Memory required for data: 60293632
I0818 10:45:00.839704  2034 layer_factory.hpp:77] Creating layer BatchNorm2
I0818 10:45:00.839715  2034 net.cpp:128] Creating Layer BatchNorm2
I0818 10:45:00.839725  2034 net.cpp:558] BatchNorm2 <- Convolution2
I0818 10:45:00.839733  2034 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0818 10:45:00.839994  2034 net.cpp:172] Setting up BatchNorm2
I0818 10:45:00.840008  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.840016  2034 net.cpp:194] Memory required for data: 68682240
I0818 10:45:00.840029  2034 layer_factory.hpp:77] Creating layer Scale2
I0818 10:45:00.840039  2034 net.cpp:128] Creating Layer Scale2
I0818 10:45:00.840044  2034 net.cpp:558] Scale2 <- Convolution2
I0818 10:45:00.840052  2034 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0818 10:45:00.840096  2034 layer_factory.hpp:77] Creating layer Scale2
I0818 10:45:00.840239  2034 net.cpp:172] Setting up Scale2
I0818 10:45:00.840252  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.840257  2034 net.cpp:194] Memory required for data: 77070848
I0818 10:45:00.840267  2034 layer_factory.hpp:77] Creating layer ReLU2
I0818 10:45:00.840298  2034 net.cpp:128] Creating Layer ReLU2
I0818 10:45:00.840304  2034 net.cpp:558] ReLU2 <- Convolution2
I0818 10:45:00.840312  2034 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0818 10:45:00.840566  2034 net.cpp:172] Setting up ReLU2
I0818 10:45:00.840584  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.840590  2034 net.cpp:194] Memory required for data: 85459456
I0818 10:45:00.840595  2034 layer_factory.hpp:77] Creating layer Convolution3
I0818 10:45:00.840608  2034 net.cpp:128] Creating Layer Convolution3
I0818 10:45:00.840615  2034 net.cpp:558] Convolution3 <- Convolution2
I0818 10:45:00.840623  2034 net.cpp:522] Convolution3 -> Convolution3
I0818 10:45:00.841974  2034 net.cpp:172] Setting up Convolution3
I0818 10:45:00.842000  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.842006  2034 net.cpp:194] Memory required for data: 93848064
I0818 10:45:00.842018  2034 layer_factory.hpp:77] Creating layer BatchNorm3
I0818 10:45:00.842032  2034 net.cpp:128] Creating Layer BatchNorm3
I0818 10:45:00.842038  2034 net.cpp:558] BatchNorm3 <- Convolution3
I0818 10:45:00.842047  2034 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0818 10:45:00.842310  2034 net.cpp:172] Setting up BatchNorm3
I0818 10:45:00.842325  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.842330  2034 net.cpp:194] Memory required for data: 102236672
I0818 10:45:00.842346  2034 layer_factory.hpp:77] Creating layer Scale3
I0818 10:45:00.842355  2034 net.cpp:128] Creating Layer Scale3
I0818 10:45:00.842360  2034 net.cpp:558] Scale3 <- Convolution3
I0818 10:45:00.842368  2034 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0818 10:45:00.842412  2034 layer_factory.hpp:77] Creating layer Scale3
I0818 10:45:00.842557  2034 net.cpp:172] Setting up Scale3
I0818 10:45:00.842571  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.842578  2034 net.cpp:194] Memory required for data: 110625280
I0818 10:45:00.842588  2034 layer_factory.hpp:77] Creating layer Eltwise1
I0818 10:45:00.842598  2034 net.cpp:128] Creating Layer Eltwise1
I0818 10:45:00.842603  2034 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0818 10:45:00.842609  2034 net.cpp:558] Eltwise1 <- Convolution3
I0818 10:45:00.842617  2034 net.cpp:522] Eltwise1 -> Eltwise1
I0818 10:45:00.842660  2034 net.cpp:172] Setting up Eltwise1
I0818 10:45:00.842671  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.842676  2034 net.cpp:194] Memory required for data: 119013888
I0818 10:45:00.842681  2034 layer_factory.hpp:77] Creating layer ReLU3
I0818 10:45:00.842689  2034 net.cpp:128] Creating Layer ReLU3
I0818 10:45:00.842694  2034 net.cpp:558] ReLU3 <- Eltwise1
I0818 10:45:00.842701  2034 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0818 10:45:00.843178  2034 net.cpp:172] Setting up ReLU3
I0818 10:45:00.843201  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.843207  2034 net.cpp:194] Memory required for data: 127402496
I0818 10:45:00.843214  2034 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0818 10:45:00.843224  2034 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0818 10:45:00.843228  2034 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0818 10:45:00.843238  2034 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0818 10:45:00.843250  2034 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0818 10:45:00.843303  2034 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0818 10:45:00.843313  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.843320  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.843325  2034 net.cpp:194] Memory required for data: 144179712
I0818 10:45:00.843331  2034 layer_factory.hpp:77] Creating layer Convolution4
I0818 10:45:00.843344  2034 net.cpp:128] Creating Layer Convolution4
I0818 10:45:00.843350  2034 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0818 10:45:00.843359  2034 net.cpp:522] Convolution4 -> Convolution4
I0818 10:45:00.844729  2034 net.cpp:172] Setting up Convolution4
I0818 10:45:00.844775  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.844781  2034 net.cpp:194] Memory required for data: 152568320
I0818 10:45:00.844794  2034 layer_factory.hpp:77] Creating layer BatchNorm4
I0818 10:45:00.844805  2034 net.cpp:128] Creating Layer BatchNorm4
I0818 10:45:00.844811  2034 net.cpp:558] BatchNorm4 <- Convolution4
I0818 10:45:00.844820  2034 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0818 10:45:00.845093  2034 net.cpp:172] Setting up BatchNorm4
I0818 10:45:00.845105  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.845111  2034 net.cpp:194] Memory required for data: 160956928
I0818 10:45:00.845124  2034 layer_factory.hpp:77] Creating layer Scale4
I0818 10:45:00.845132  2034 net.cpp:128] Creating Layer Scale4
I0818 10:45:00.845137  2034 net.cpp:558] Scale4 <- Convolution4
I0818 10:45:00.845144  2034 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0818 10:45:00.845190  2034 layer_factory.hpp:77] Creating layer Scale4
I0818 10:45:00.845338  2034 net.cpp:172] Setting up Scale4
I0818 10:45:00.845353  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.845360  2034 net.cpp:194] Memory required for data: 169345536
I0818 10:45:00.845369  2034 layer_factory.hpp:77] Creating layer ReLU4
I0818 10:45:00.845377  2034 net.cpp:128] Creating Layer ReLU4
I0818 10:45:00.845382  2034 net.cpp:558] ReLU4 <- Convolution4
I0818 10:45:00.845389  2034 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0818 10:45:00.845656  2034 net.cpp:172] Setting up ReLU4
I0818 10:45:00.845671  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.845677  2034 net.cpp:194] Memory required for data: 177734144
I0818 10:45:00.845683  2034 layer_factory.hpp:77] Creating layer Convolution5
I0818 10:45:00.845695  2034 net.cpp:128] Creating Layer Convolution5
I0818 10:45:00.845702  2034 net.cpp:558] Convolution5 <- Convolution4
I0818 10:45:00.845711  2034 net.cpp:522] Convolution5 -> Convolution5
I0818 10:45:00.847095  2034 net.cpp:172] Setting up Convolution5
I0818 10:45:00.847121  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.847126  2034 net.cpp:194] Memory required for data: 186122752
I0818 10:45:00.847141  2034 layer_factory.hpp:77] Creating layer BatchNorm5
I0818 10:45:00.847151  2034 net.cpp:128] Creating Layer BatchNorm5
I0818 10:45:00.847157  2034 net.cpp:558] BatchNorm5 <- Convolution5
I0818 10:45:00.847168  2034 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0818 10:45:00.847434  2034 net.cpp:172] Setting up BatchNorm5
I0818 10:45:00.847446  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.847451  2034 net.cpp:194] Memory required for data: 194511360
I0818 10:45:00.847467  2034 layer_factory.hpp:77] Creating layer Scale5
I0818 10:45:00.847476  2034 net.cpp:128] Creating Layer Scale5
I0818 10:45:00.847482  2034 net.cpp:558] Scale5 <- Convolution5
I0818 10:45:00.847489  2034 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0818 10:45:00.847535  2034 layer_factory.hpp:77] Creating layer Scale5
I0818 10:45:00.847687  2034 net.cpp:172] Setting up Scale5
I0818 10:45:00.847699  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.847704  2034 net.cpp:194] Memory required for data: 202899968
I0818 10:45:00.847714  2034 layer_factory.hpp:77] Creating layer Eltwise2
I0818 10:45:00.847723  2034 net.cpp:128] Creating Layer Eltwise2
I0818 10:45:00.847728  2034 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0818 10:45:00.847736  2034 net.cpp:558] Eltwise2 <- Convolution5
I0818 10:45:00.847743  2034 net.cpp:522] Eltwise2 -> Eltwise2
I0818 10:45:00.847774  2034 net.cpp:172] Setting up Eltwise2
I0818 10:45:00.847782  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.847787  2034 net.cpp:194] Memory required for data: 211288576
I0818 10:45:00.847792  2034 layer_factory.hpp:77] Creating layer ReLU5
I0818 10:45:00.847800  2034 net.cpp:128] Creating Layer ReLU5
I0818 10:45:00.847806  2034 net.cpp:558] ReLU5 <- Eltwise2
I0818 10:45:00.847813  2034 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0818 10:45:00.848093  2034 net.cpp:172] Setting up ReLU5
I0818 10:45:00.848109  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.848114  2034 net.cpp:194] Memory required for data: 219677184
I0818 10:45:00.848120  2034 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0818 10:45:00.848129  2034 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0818 10:45:00.848135  2034 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0818 10:45:00.848143  2034 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0818 10:45:00.848153  2034 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0818 10:45:00.848203  2034 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0818 10:45:00.848215  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.848223  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.848227  2034 net.cpp:194] Memory required for data: 236454400
I0818 10:45:00.848233  2034 layer_factory.hpp:77] Creating layer Convolution6
I0818 10:45:00.848246  2034 net.cpp:128] Creating Layer Convolution6
I0818 10:45:00.848251  2034 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0818 10:45:00.848260  2034 net.cpp:522] Convolution6 -> Convolution6
I0818 10:45:00.849633  2034 net.cpp:172] Setting up Convolution6
I0818 10:45:00.849660  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.849666  2034 net.cpp:194] Memory required for data: 244843008
I0818 10:45:00.849681  2034 layer_factory.hpp:77] Creating layer BatchNorm6
I0818 10:45:00.849696  2034 net.cpp:128] Creating Layer BatchNorm6
I0818 10:45:00.849702  2034 net.cpp:558] BatchNorm6 <- Convolution6
I0818 10:45:00.849711  2034 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0818 10:45:00.849990  2034 net.cpp:172] Setting up BatchNorm6
I0818 10:45:00.850003  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.850008  2034 net.cpp:194] Memory required for data: 253231616
I0818 10:45:00.850020  2034 layer_factory.hpp:77] Creating layer Scale6
I0818 10:45:00.850031  2034 net.cpp:128] Creating Layer Scale6
I0818 10:45:00.850039  2034 net.cpp:558] Scale6 <- Convolution6
I0818 10:45:00.850045  2034 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0818 10:45:00.850092  2034 layer_factory.hpp:77] Creating layer Scale6
I0818 10:45:00.850244  2034 net.cpp:172] Setting up Scale6
I0818 10:45:00.850257  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.850262  2034 net.cpp:194] Memory required for data: 261620224
I0818 10:45:00.850272  2034 layer_factory.hpp:77] Creating layer ReLU6
I0818 10:45:00.850281  2034 net.cpp:128] Creating Layer ReLU6
I0818 10:45:00.850286  2034 net.cpp:558] ReLU6 <- Convolution6
I0818 10:45:00.850292  2034 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0818 10:45:00.850777  2034 net.cpp:172] Setting up ReLU6
I0818 10:45:00.850800  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.850806  2034 net.cpp:194] Memory required for data: 270008832
I0818 10:45:00.850812  2034 layer_factory.hpp:77] Creating layer Convolution7
I0818 10:45:00.850826  2034 net.cpp:128] Creating Layer Convolution7
I0818 10:45:00.850834  2034 net.cpp:558] Convolution7 <- Convolution6
I0818 10:45:00.850844  2034 net.cpp:522] Convolution7 -> Convolution7
I0818 10:45:00.852021  2034 net.cpp:172] Setting up Convolution7
I0818 10:45:00.852047  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852052  2034 net.cpp:194] Memory required for data: 278397440
I0818 10:45:00.852066  2034 layer_factory.hpp:77] Creating layer BatchNorm7
I0818 10:45:00.852075  2034 net.cpp:128] Creating Layer BatchNorm7
I0818 10:45:00.852079  2034 net.cpp:558] BatchNorm7 <- Convolution7
I0818 10:45:00.852087  2034 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0818 10:45:00.852309  2034 net.cpp:172] Setting up BatchNorm7
I0818 10:45:00.852319  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852322  2034 net.cpp:194] Memory required for data: 286786048
I0818 10:45:00.852332  2034 layer_factory.hpp:77] Creating layer Scale7
I0818 10:45:00.852358  2034 net.cpp:128] Creating Layer Scale7
I0818 10:45:00.852365  2034 net.cpp:558] Scale7 <- Convolution7
I0818 10:45:00.852371  2034 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0818 10:45:00.852409  2034 layer_factory.hpp:77] Creating layer Scale7
I0818 10:45:00.852535  2034 net.cpp:172] Setting up Scale7
I0818 10:45:00.852545  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852548  2034 net.cpp:194] Memory required for data: 295174656
I0818 10:45:00.852556  2034 layer_factory.hpp:77] Creating layer Eltwise3
I0818 10:45:00.852566  2034 net.cpp:128] Creating Layer Eltwise3
I0818 10:45:00.852572  2034 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0818 10:45:00.852577  2034 net.cpp:558] Eltwise3 <- Convolution7
I0818 10:45:00.852584  2034 net.cpp:522] Eltwise3 -> Eltwise3
I0818 10:45:00.852609  2034 net.cpp:172] Setting up Eltwise3
I0818 10:45:00.852618  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852622  2034 net.cpp:194] Memory required for data: 303563264
I0818 10:45:00.852627  2034 layer_factory.hpp:77] Creating layer ReLU7
I0818 10:45:00.852633  2034 net.cpp:128] Creating Layer ReLU7
I0818 10:45:00.852638  2034 net.cpp:558] ReLU7 <- Eltwise3
I0818 10:45:00.852643  2034 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0818 10:45:00.852862  2034 net.cpp:172] Setting up ReLU7
I0818 10:45:00.852875  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852880  2034 net.cpp:194] Memory required for data: 311951872
I0818 10:45:00.852885  2034 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0818 10:45:00.852891  2034 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0818 10:45:00.852898  2034 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0818 10:45:00.852905  2034 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0818 10:45:00.852913  2034 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0818 10:45:00.852957  2034 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0818 10:45:00.852965  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852972  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.852980  2034 net.cpp:194] Memory required for data: 328729088
I0818 10:45:00.852984  2034 layer_factory.hpp:77] Creating layer Convolution8
I0818 10:45:00.852995  2034 net.cpp:128] Creating Layer Convolution8
I0818 10:45:00.852999  2034 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0818 10:45:00.853006  2034 net.cpp:522] Convolution8 -> Convolution8
I0818 10:45:00.854169  2034 net.cpp:172] Setting up Convolution8
I0818 10:45:00.854192  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.854197  2034 net.cpp:194] Memory required for data: 337117696
I0818 10:45:00.854207  2034 layer_factory.hpp:77] Creating layer BatchNorm8
I0818 10:45:00.854214  2034 net.cpp:128] Creating Layer BatchNorm8
I0818 10:45:00.854223  2034 net.cpp:558] BatchNorm8 <- Convolution8
I0818 10:45:00.854230  2034 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0818 10:45:00.854460  2034 net.cpp:172] Setting up BatchNorm8
I0818 10:45:00.854470  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.854475  2034 net.cpp:194] Memory required for data: 345506304
I0818 10:45:00.854483  2034 layer_factory.hpp:77] Creating layer Scale8
I0818 10:45:00.854491  2034 net.cpp:128] Creating Layer Scale8
I0818 10:45:00.854498  2034 net.cpp:558] Scale8 <- Convolution8
I0818 10:45:00.854506  2034 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0818 10:45:00.854542  2034 layer_factory.hpp:77] Creating layer Scale8
I0818 10:45:00.854683  2034 net.cpp:172] Setting up Scale8
I0818 10:45:00.854694  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.854698  2034 net.cpp:194] Memory required for data: 353894912
I0818 10:45:00.854707  2034 layer_factory.hpp:77] Creating layer ReLU8
I0818 10:45:00.854713  2034 net.cpp:128] Creating Layer ReLU8
I0818 10:45:00.854718  2034 net.cpp:558] ReLU8 <- Convolution8
I0818 10:45:00.854723  2034 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0818 10:45:00.854945  2034 net.cpp:172] Setting up ReLU8
I0818 10:45:00.854974  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.854977  2034 net.cpp:194] Memory required for data: 362283520
I0818 10:45:00.854982  2034 layer_factory.hpp:77] Creating layer Convolution9
I0818 10:45:00.854993  2034 net.cpp:128] Creating Layer Convolution9
I0818 10:45:00.855000  2034 net.cpp:558] Convolution9 <- Convolution8
I0818 10:45:00.855007  2034 net.cpp:522] Convolution9 -> Convolution9
I0818 10:45:00.856202  2034 net.cpp:172] Setting up Convolution9
I0818 10:45:00.856225  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.856230  2034 net.cpp:194] Memory required for data: 370672128
I0818 10:45:00.856240  2034 layer_factory.hpp:77] Creating layer BatchNorm9
I0818 10:45:00.856253  2034 net.cpp:128] Creating Layer BatchNorm9
I0818 10:45:00.856258  2034 net.cpp:558] BatchNorm9 <- Convolution9
I0818 10:45:00.856266  2034 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0818 10:45:00.856499  2034 net.cpp:172] Setting up BatchNorm9
I0818 10:45:00.856508  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.856513  2034 net.cpp:194] Memory required for data: 379060736
I0818 10:45:00.856523  2034 layer_factory.hpp:77] Creating layer Scale9
I0818 10:45:00.856530  2034 net.cpp:128] Creating Layer Scale9
I0818 10:45:00.856537  2034 net.cpp:558] Scale9 <- Convolution9
I0818 10:45:00.856544  2034 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0818 10:45:00.856580  2034 layer_factory.hpp:77] Creating layer Scale9
I0818 10:45:00.856709  2034 net.cpp:172] Setting up Scale9
I0818 10:45:00.856719  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.856724  2034 net.cpp:194] Memory required for data: 387449344
I0818 10:45:00.856731  2034 layer_factory.hpp:77] Creating layer Eltwise4
I0818 10:45:00.856739  2034 net.cpp:128] Creating Layer Eltwise4
I0818 10:45:00.856743  2034 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0818 10:45:00.856748  2034 net.cpp:558] Eltwise4 <- Convolution9
I0818 10:45:00.856755  2034 net.cpp:522] Eltwise4 -> Eltwise4
I0818 10:45:00.856781  2034 net.cpp:172] Setting up Eltwise4
I0818 10:45:00.856789  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.856793  2034 net.cpp:194] Memory required for data: 395837952
I0818 10:45:00.856797  2034 layer_factory.hpp:77] Creating layer ReLU9
I0818 10:45:00.856804  2034 net.cpp:128] Creating Layer ReLU9
I0818 10:45:00.856808  2034 net.cpp:558] ReLU9 <- Eltwise4
I0818 10:45:00.856814  2034 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0818 10:45:00.857035  2034 net.cpp:172] Setting up ReLU9
I0818 10:45:00.857048  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.857053  2034 net.cpp:194] Memory required for data: 404226560
I0818 10:45:00.857058  2034 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0818 10:45:00.857064  2034 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0818 10:45:00.857069  2034 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0818 10:45:00.857076  2034 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0818 10:45:00.857084  2034 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0818 10:45:00.857127  2034 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0818 10:45:00.857136  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.857142  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.857146  2034 net.cpp:194] Memory required for data: 421003776
I0818 10:45:00.857151  2034 layer_factory.hpp:77] Creating layer Convolution10
I0818 10:45:00.857161  2034 net.cpp:128] Creating Layer Convolution10
I0818 10:45:00.857165  2034 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0818 10:45:00.857173  2034 net.cpp:522] Convolution10 -> Convolution10
I0818 10:45:00.858405  2034 net.cpp:172] Setting up Convolution10
I0818 10:45:00.858428  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.858438  2034 net.cpp:194] Memory required for data: 429392384
I0818 10:45:00.858458  2034 layer_factory.hpp:77] Creating layer BatchNorm10
I0818 10:45:00.858484  2034 net.cpp:128] Creating Layer BatchNorm10
I0818 10:45:00.858490  2034 net.cpp:558] BatchNorm10 <- Convolution10
I0818 10:45:00.858500  2034 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0818 10:45:00.858767  2034 net.cpp:172] Setting up BatchNorm10
I0818 10:45:00.858778  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.858783  2034 net.cpp:194] Memory required for data: 437780992
I0818 10:45:00.858793  2034 layer_factory.hpp:77] Creating layer Scale10
I0818 10:45:00.858803  2034 net.cpp:128] Creating Layer Scale10
I0818 10:45:00.858808  2034 net.cpp:558] Scale10 <- Convolution10
I0818 10:45:00.858814  2034 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0818 10:45:00.858855  2034 layer_factory.hpp:77] Creating layer Scale10
I0818 10:45:00.858999  2034 net.cpp:172] Setting up Scale10
I0818 10:45:00.859009  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.859012  2034 net.cpp:194] Memory required for data: 446169600
I0818 10:45:00.859020  2034 layer_factory.hpp:77] Creating layer ReLU10
I0818 10:45:00.859026  2034 net.cpp:128] Creating Layer ReLU10
I0818 10:45:00.859031  2034 net.cpp:558] ReLU10 <- Convolution10
I0818 10:45:00.859038  2034 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0818 10:45:00.859457  2034 net.cpp:172] Setting up ReLU10
I0818 10:45:00.859477  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.859481  2034 net.cpp:194] Memory required for data: 454558208
I0818 10:45:00.859486  2034 layer_factory.hpp:77] Creating layer Convolution11
I0818 10:45:00.859500  2034 net.cpp:128] Creating Layer Convolution11
I0818 10:45:00.859505  2034 net.cpp:558] Convolution11 <- Convolution10
I0818 10:45:00.859515  2034 net.cpp:522] Convolution11 -> Convolution11
I0818 10:45:00.860777  2034 net.cpp:172] Setting up Convolution11
I0818 10:45:00.860802  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.860807  2034 net.cpp:194] Memory required for data: 462946816
I0818 10:45:00.860817  2034 layer_factory.hpp:77] Creating layer BatchNorm11
I0818 10:45:00.860828  2034 net.cpp:128] Creating Layer BatchNorm11
I0818 10:45:00.860833  2034 net.cpp:558] BatchNorm11 <- Convolution11
I0818 10:45:00.860844  2034 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0818 10:45:00.861091  2034 net.cpp:172] Setting up BatchNorm11
I0818 10:45:00.861101  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861106  2034 net.cpp:194] Memory required for data: 471335424
I0818 10:45:00.861115  2034 layer_factory.hpp:77] Creating layer Scale11
I0818 10:45:00.861124  2034 net.cpp:128] Creating Layer Scale11
I0818 10:45:00.861129  2034 net.cpp:558] Scale11 <- Convolution11
I0818 10:45:00.861135  2034 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0818 10:45:00.861174  2034 layer_factory.hpp:77] Creating layer Scale11
I0818 10:45:00.861325  2034 net.cpp:172] Setting up Scale11
I0818 10:45:00.861336  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861340  2034 net.cpp:194] Memory required for data: 479724032
I0818 10:45:00.861348  2034 layer_factory.hpp:77] Creating layer Eltwise5
I0818 10:45:00.861357  2034 net.cpp:128] Creating Layer Eltwise5
I0818 10:45:00.861362  2034 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0818 10:45:00.861367  2034 net.cpp:558] Eltwise5 <- Convolution11
I0818 10:45:00.861373  2034 net.cpp:522] Eltwise5 -> Eltwise5
I0818 10:45:00.861400  2034 net.cpp:172] Setting up Eltwise5
I0818 10:45:00.861408  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861413  2034 net.cpp:194] Memory required for data: 488112640
I0818 10:45:00.861416  2034 layer_factory.hpp:77] Creating layer ReLU11
I0818 10:45:00.861423  2034 net.cpp:128] Creating Layer ReLU11
I0818 10:45:00.861426  2034 net.cpp:558] ReLU11 <- Eltwise5
I0818 10:45:00.861433  2034 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0818 10:45:00.861661  2034 net.cpp:172] Setting up ReLU11
I0818 10:45:00.861675  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861678  2034 net.cpp:194] Memory required for data: 496501248
I0818 10:45:00.861698  2034 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0818 10:45:00.861706  2034 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0818 10:45:00.861711  2034 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0818 10:45:00.861719  2034 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0818 10:45:00.861728  2034 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0818 10:45:00.861780  2034 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0818 10:45:00.861790  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861796  2034 net.cpp:186] Top shape: 128 16 32 32 (2097152)
I0818 10:45:00.861800  2034 net.cpp:194] Memory required for data: 513278464
I0818 10:45:00.861804  2034 layer_factory.hpp:77] Creating layer Convolution12
I0818 10:45:00.861816  2034 net.cpp:128] Creating Layer Convolution12
I0818 10:45:00.861821  2034 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0818 10:45:00.861832  2034 net.cpp:522] Convolution12 -> Convolution12
I0818 10:45:00.863274  2034 net.cpp:172] Setting up Convolution12
I0818 10:45:00.863301  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.863304  2034 net.cpp:194] Memory required for data: 517472768
I0818 10:45:00.863315  2034 layer_factory.hpp:77] Creating layer BatchNorm12
I0818 10:45:00.863325  2034 net.cpp:128] Creating Layer BatchNorm12
I0818 10:45:00.863330  2034 net.cpp:558] BatchNorm12 <- Convolution12
I0818 10:45:00.863338  2034 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0818 10:45:00.863587  2034 net.cpp:172] Setting up BatchNorm12
I0818 10:45:00.863597  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.863602  2034 net.cpp:194] Memory required for data: 521667072
I0818 10:45:00.863611  2034 layer_factory.hpp:77] Creating layer Scale12
I0818 10:45:00.863618  2034 net.cpp:128] Creating Layer Scale12
I0818 10:45:00.863622  2034 net.cpp:558] Scale12 <- Convolution12
I0818 10:45:00.863631  2034 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0818 10:45:00.863669  2034 layer_factory.hpp:77] Creating layer Scale12
I0818 10:45:00.863813  2034 net.cpp:172] Setting up Scale12
I0818 10:45:00.863823  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.863828  2034 net.cpp:194] Memory required for data: 525861376
I0818 10:45:00.863837  2034 layer_factory.hpp:77] Creating layer Convolution13
I0818 10:45:00.863847  2034 net.cpp:128] Creating Layer Convolution13
I0818 10:45:00.863852  2034 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_1
I0818 10:45:00.863862  2034 net.cpp:522] Convolution13 -> Convolution13
I0818 10:45:00.866497  2034 net.cpp:172] Setting up Convolution13
I0818 10:45:00.866523  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.866528  2034 net.cpp:194] Memory required for data: 530055680
I0818 10:45:00.866539  2034 layer_factory.hpp:77] Creating layer BatchNorm13
I0818 10:45:00.866550  2034 net.cpp:128] Creating Layer BatchNorm13
I0818 10:45:00.866559  2034 net.cpp:558] BatchNorm13 <- Convolution13
I0818 10:45:00.866566  2034 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0818 10:45:00.866827  2034 net.cpp:172] Setting up BatchNorm13
I0818 10:45:00.866837  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.866842  2034 net.cpp:194] Memory required for data: 534249984
I0818 10:45:00.866852  2034 layer_factory.hpp:77] Creating layer Scale13
I0818 10:45:00.866861  2034 net.cpp:128] Creating Layer Scale13
I0818 10:45:00.866865  2034 net.cpp:558] Scale13 <- Convolution13
I0818 10:45:00.866871  2034 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0818 10:45:00.866912  2034 layer_factory.hpp:77] Creating layer Scale13
I0818 10:45:00.867053  2034 net.cpp:172] Setting up Scale13
I0818 10:45:00.867063  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.867067  2034 net.cpp:194] Memory required for data: 538444288
I0818 10:45:00.867074  2034 layer_factory.hpp:77] Creating layer ReLU12
I0818 10:45:00.867084  2034 net.cpp:128] Creating Layer ReLU12
I0818 10:45:00.867107  2034 net.cpp:558] ReLU12 <- Convolution13
I0818 10:45:00.867115  2034 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0818 10:45:00.867348  2034 net.cpp:172] Setting up ReLU12
I0818 10:45:00.867362  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.867367  2034 net.cpp:194] Memory required for data: 542638592
I0818 10:45:00.867372  2034 layer_factory.hpp:77] Creating layer Convolution14
I0818 10:45:00.867384  2034 net.cpp:128] Creating Layer Convolution14
I0818 10:45:00.867390  2034 net.cpp:558] Convolution14 <- Convolution13
I0818 10:45:00.867398  2034 net.cpp:522] Convolution14 -> Convolution14
I0818 10:45:00.868813  2034 net.cpp:172] Setting up Convolution14
I0818 10:45:00.868839  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.868844  2034 net.cpp:194] Memory required for data: 546832896
I0818 10:45:00.868854  2034 layer_factory.hpp:77] Creating layer BatchNorm14
I0818 10:45:00.868872  2034 net.cpp:128] Creating Layer BatchNorm14
I0818 10:45:00.868881  2034 net.cpp:558] BatchNorm14 <- Convolution14
I0818 10:45:00.868888  2034 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0818 10:45:00.869132  2034 net.cpp:172] Setting up BatchNorm14
I0818 10:45:00.869143  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.869148  2034 net.cpp:194] Memory required for data: 551027200
I0818 10:45:00.869156  2034 layer_factory.hpp:77] Creating layer Scale14
I0818 10:45:00.869165  2034 net.cpp:128] Creating Layer Scale14
I0818 10:45:00.869170  2034 net.cpp:558] Scale14 <- Convolution14
I0818 10:45:00.869175  2034 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0818 10:45:00.869216  2034 layer_factory.hpp:77] Creating layer Scale14
I0818 10:45:00.869354  2034 net.cpp:172] Setting up Scale14
I0818 10:45:00.869365  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.869369  2034 net.cpp:194] Memory required for data: 555221504
I0818 10:45:00.869377  2034 layer_factory.hpp:77] Creating layer Eltwise6
I0818 10:45:00.869385  2034 net.cpp:128] Creating Layer Eltwise6
I0818 10:45:00.869388  2034 net.cpp:558] Eltwise6 <- Convolution12
I0818 10:45:00.869393  2034 net.cpp:558] Eltwise6 <- Convolution14
I0818 10:45:00.869401  2034 net.cpp:522] Eltwise6 -> Eltwise6
I0818 10:45:00.869423  2034 net.cpp:172] Setting up Eltwise6
I0818 10:45:00.869432  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.869436  2034 net.cpp:194] Memory required for data: 559415808
I0818 10:45:00.869441  2034 layer_factory.hpp:77] Creating layer ReLU13
I0818 10:45:00.869447  2034 net.cpp:128] Creating Layer ReLU13
I0818 10:45:00.869451  2034 net.cpp:558] ReLU13 <- Eltwise6
I0818 10:45:00.869457  2034 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0818 10:45:00.869885  2034 net.cpp:172] Setting up ReLU13
I0818 10:45:00.869907  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.869911  2034 net.cpp:194] Memory required for data: 563610112
I0818 10:45:00.869917  2034 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0818 10:45:00.869927  2034 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0818 10:45:00.869932  2034 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0818 10:45:00.869942  2034 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0818 10:45:00.869952  2034 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0818 10:45:00.870003  2034 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0818 10:45:00.870013  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.870019  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.870023  2034 net.cpp:194] Memory required for data: 571998720
I0818 10:45:00.870028  2034 layer_factory.hpp:77] Creating layer Convolution15
I0818 10:45:00.870039  2034 net.cpp:128] Creating Layer Convolution15
I0818 10:45:00.870044  2034 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0818 10:45:00.870050  2034 net.cpp:522] Convolution15 -> Convolution15
I0818 10:45:00.871440  2034 net.cpp:172] Setting up Convolution15
I0818 10:45:00.871480  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.871484  2034 net.cpp:194] Memory required for data: 576193024
I0818 10:45:00.871495  2034 layer_factory.hpp:77] Creating layer BatchNorm15
I0818 10:45:00.871506  2034 net.cpp:128] Creating Layer BatchNorm15
I0818 10:45:00.871511  2034 net.cpp:558] BatchNorm15 <- Convolution15
I0818 10:45:00.871525  2034 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0818 10:45:00.871769  2034 net.cpp:172] Setting up BatchNorm15
I0818 10:45:00.871780  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.871784  2034 net.cpp:194] Memory required for data: 580387328
I0818 10:45:00.871794  2034 layer_factory.hpp:77] Creating layer Scale15
I0818 10:45:00.871801  2034 net.cpp:128] Creating Layer Scale15
I0818 10:45:00.871805  2034 net.cpp:558] Scale15 <- Convolution15
I0818 10:45:00.871811  2034 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0818 10:45:00.871853  2034 layer_factory.hpp:77] Creating layer Scale15
I0818 10:45:00.871994  2034 net.cpp:172] Setting up Scale15
I0818 10:45:00.872004  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.872007  2034 net.cpp:194] Memory required for data: 584581632
I0818 10:45:00.872015  2034 layer_factory.hpp:77] Creating layer ReLU14
I0818 10:45:00.872025  2034 net.cpp:128] Creating Layer ReLU14
I0818 10:45:00.872030  2034 net.cpp:558] ReLU14 <- Convolution15
I0818 10:45:00.872037  2034 net.cpp:509] ReLU14 -> Convolution15 (in-place)
I0818 10:45:00.872275  2034 net.cpp:172] Setting up ReLU14
I0818 10:45:00.872287  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.872292  2034 net.cpp:194] Memory required for data: 588775936
I0818 10:45:00.872297  2034 layer_factory.hpp:77] Creating layer Convolution16
I0818 10:45:00.872309  2034 net.cpp:128] Creating Layer Convolution16
I0818 10:45:00.872316  2034 net.cpp:558] Convolution16 <- Convolution15
I0818 10:45:00.872326  2034 net.cpp:522] Convolution16 -> Convolution16
I0818 10:45:00.873697  2034 net.cpp:172] Setting up Convolution16
I0818 10:45:00.873718  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.873723  2034 net.cpp:194] Memory required for data: 592970240
I0818 10:45:00.873734  2034 layer_factory.hpp:77] Creating layer BatchNorm16
I0818 10:45:00.873747  2034 net.cpp:128] Creating Layer BatchNorm16
I0818 10:45:00.873752  2034 net.cpp:558] BatchNorm16 <- Convolution16
I0818 10:45:00.873764  2034 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0818 10:45:00.874009  2034 net.cpp:172] Setting up BatchNorm16
I0818 10:45:00.874019  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874023  2034 net.cpp:194] Memory required for data: 597164544
I0818 10:45:00.874032  2034 layer_factory.hpp:77] Creating layer Scale16
I0818 10:45:00.874042  2034 net.cpp:128] Creating Layer Scale16
I0818 10:45:00.874047  2034 net.cpp:558] Scale16 <- Convolution16
I0818 10:45:00.874053  2034 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0818 10:45:00.874094  2034 layer_factory.hpp:77] Creating layer Scale16
I0818 10:45:00.874239  2034 net.cpp:172] Setting up Scale16
I0818 10:45:00.874248  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874251  2034 net.cpp:194] Memory required for data: 601358848
I0818 10:45:00.874259  2034 layer_factory.hpp:77] Creating layer Eltwise7
I0818 10:45:00.874266  2034 net.cpp:128] Creating Layer Eltwise7
I0818 10:45:00.874271  2034 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0818 10:45:00.874275  2034 net.cpp:558] Eltwise7 <- Convolution16
I0818 10:45:00.874284  2034 net.cpp:522] Eltwise7 -> Eltwise7
I0818 10:45:00.874305  2034 net.cpp:172] Setting up Eltwise7
I0818 10:45:00.874317  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874321  2034 net.cpp:194] Memory required for data: 605553152
I0818 10:45:00.874325  2034 layer_factory.hpp:77] Creating layer ReLU15
I0818 10:45:00.874331  2034 net.cpp:128] Creating Layer ReLU15
I0818 10:45:00.874336  2034 net.cpp:558] ReLU15 <- Eltwise7
I0818 10:45:00.874341  2034 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0818 10:45:00.874794  2034 net.cpp:172] Setting up ReLU15
I0818 10:45:00.874810  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874815  2034 net.cpp:194] Memory required for data: 609747456
I0818 10:45:00.874820  2034 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0818 10:45:00.874830  2034 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0818 10:45:00.874835  2034 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0818 10:45:00.874845  2034 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0818 10:45:00.874855  2034 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0818 10:45:00.874910  2034 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0818 10:45:00.874918  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874923  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.874927  2034 net.cpp:194] Memory required for data: 618136064
I0818 10:45:00.874931  2034 layer_factory.hpp:77] Creating layer Convolution17
I0818 10:45:00.874943  2034 net.cpp:128] Creating Layer Convolution17
I0818 10:45:00.874948  2034 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_0
I0818 10:45:00.874958  2034 net.cpp:522] Convolution17 -> Convolution17
I0818 10:45:00.876328  2034 net.cpp:172] Setting up Convolution17
I0818 10:45:00.876355  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.876360  2034 net.cpp:194] Memory required for data: 622330368
I0818 10:45:00.876370  2034 layer_factory.hpp:77] Creating layer BatchNorm17
I0818 10:45:00.876379  2034 net.cpp:128] Creating Layer BatchNorm17
I0818 10:45:00.876384  2034 net.cpp:558] BatchNorm17 <- Convolution17
I0818 10:45:00.876396  2034 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0818 10:45:00.876649  2034 net.cpp:172] Setting up BatchNorm17
I0818 10:45:00.876659  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.876664  2034 net.cpp:194] Memory required for data: 626524672
I0818 10:45:00.876674  2034 layer_factory.hpp:77] Creating layer Scale17
I0818 10:45:00.876680  2034 net.cpp:128] Creating Layer Scale17
I0818 10:45:00.876685  2034 net.cpp:558] Scale17 <- Convolution17
I0818 10:45:00.876690  2034 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0818 10:45:00.876732  2034 layer_factory.hpp:77] Creating layer Scale17
I0818 10:45:00.876878  2034 net.cpp:172] Setting up Scale17
I0818 10:45:00.876893  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.876896  2034 net.cpp:194] Memory required for data: 630718976
I0818 10:45:00.876904  2034 layer_factory.hpp:77] Creating layer ReLU16
I0818 10:45:00.876914  2034 net.cpp:128] Creating Layer ReLU16
I0818 10:45:00.876919  2034 net.cpp:558] ReLU16 <- Convolution17
I0818 10:45:00.876924  2034 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0818 10:45:00.877344  2034 net.cpp:172] Setting up ReLU16
I0818 10:45:00.877365  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.877369  2034 net.cpp:194] Memory required for data: 634913280
I0818 10:45:00.877374  2034 layer_factory.hpp:77] Creating layer Convolution18
I0818 10:45:00.877388  2034 net.cpp:128] Creating Layer Convolution18
I0818 10:45:00.877396  2034 net.cpp:558] Convolution18 <- Convolution17
I0818 10:45:00.877404  2034 net.cpp:522] Convolution18 -> Convolution18
I0818 10:45:00.878788  2034 net.cpp:172] Setting up Convolution18
I0818 10:45:00.878813  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.878818  2034 net.cpp:194] Memory required for data: 639107584
I0818 10:45:00.878831  2034 layer_factory.hpp:77] Creating layer BatchNorm18
I0818 10:45:00.878841  2034 net.cpp:128] Creating Layer BatchNorm18
I0818 10:45:00.878846  2034 net.cpp:558] BatchNorm18 <- Convolution18
I0818 10:45:00.878854  2034 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0818 10:45:00.879106  2034 net.cpp:172] Setting up BatchNorm18
I0818 10:45:00.879117  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879122  2034 net.cpp:194] Memory required for data: 643301888
I0818 10:45:00.879132  2034 layer_factory.hpp:77] Creating layer Scale18
I0818 10:45:00.879155  2034 net.cpp:128] Creating Layer Scale18
I0818 10:45:00.879160  2034 net.cpp:558] Scale18 <- Convolution18
I0818 10:45:00.879168  2034 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0818 10:45:00.879210  2034 layer_factory.hpp:77] Creating layer Scale18
I0818 10:45:00.879354  2034 net.cpp:172] Setting up Scale18
I0818 10:45:00.879366  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879370  2034 net.cpp:194] Memory required for data: 647496192
I0818 10:45:00.879379  2034 layer_factory.hpp:77] Creating layer Eltwise8
I0818 10:45:00.879385  2034 net.cpp:128] Creating Layer Eltwise8
I0818 10:45:00.879393  2034 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0818 10:45:00.879398  2034 net.cpp:558] Eltwise8 <- Convolution18
I0818 10:45:00.879405  2034 net.cpp:522] Eltwise8 -> Eltwise8
I0818 10:45:00.879427  2034 net.cpp:172] Setting up Eltwise8
I0818 10:45:00.879436  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879441  2034 net.cpp:194] Memory required for data: 651690496
I0818 10:45:00.879446  2034 layer_factory.hpp:77] Creating layer ReLU17
I0818 10:45:00.879451  2034 net.cpp:128] Creating Layer ReLU17
I0818 10:45:00.879456  2034 net.cpp:558] ReLU17 <- Eltwise8
I0818 10:45:00.879463  2034 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0818 10:45:00.879698  2034 net.cpp:172] Setting up ReLU17
I0818 10:45:00.879710  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879714  2034 net.cpp:194] Memory required for data: 655884800
I0818 10:45:00.879719  2034 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0818 10:45:00.879726  2034 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0818 10:45:00.879730  2034 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0818 10:45:00.879739  2034 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0818 10:45:00.879747  2034 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0818 10:45:00.879799  2034 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0818 10:45:00.879808  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879814  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.879818  2034 net.cpp:194] Memory required for data: 664273408
I0818 10:45:00.879822  2034 layer_factory.hpp:77] Creating layer Convolution19
I0818 10:45:00.879834  2034 net.cpp:128] Creating Layer Convolution19
I0818 10:45:00.879840  2034 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0818 10:45:00.879851  2034 net.cpp:522] Convolution19 -> Convolution19
I0818 10:45:00.881382  2034 net.cpp:172] Setting up Convolution19
I0818 10:45:00.881407  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.881412  2034 net.cpp:194] Memory required for data: 668467712
I0818 10:45:00.881425  2034 layer_factory.hpp:77] Creating layer BatchNorm19
I0818 10:45:00.881435  2034 net.cpp:128] Creating Layer BatchNorm19
I0818 10:45:00.881441  2034 net.cpp:558] BatchNorm19 <- Convolution19
I0818 10:45:00.881448  2034 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0818 10:45:00.881712  2034 net.cpp:172] Setting up BatchNorm19
I0818 10:45:00.881722  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.881726  2034 net.cpp:194] Memory required for data: 672662016
I0818 10:45:00.881747  2034 layer_factory.hpp:77] Creating layer Scale19
I0818 10:45:00.881760  2034 net.cpp:128] Creating Layer Scale19
I0818 10:45:00.881765  2034 net.cpp:558] Scale19 <- Convolution19
I0818 10:45:00.881772  2034 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0818 10:45:00.881813  2034 layer_factory.hpp:77] Creating layer Scale19
I0818 10:45:00.881953  2034 net.cpp:172] Setting up Scale19
I0818 10:45:00.881961  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.881966  2034 net.cpp:194] Memory required for data: 676856320
I0818 10:45:00.881973  2034 layer_factory.hpp:77] Creating layer ReLU18
I0818 10:45:00.881979  2034 net.cpp:128] Creating Layer ReLU18
I0818 10:45:00.881984  2034 net.cpp:558] ReLU18 <- Convolution19
I0818 10:45:00.882005  2034 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0818 10:45:00.882246  2034 net.cpp:172] Setting up ReLU18
I0818 10:45:00.882261  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.882264  2034 net.cpp:194] Memory required for data: 681050624
I0818 10:45:00.882269  2034 layer_factory.hpp:77] Creating layer Convolution20
I0818 10:45:00.882283  2034 net.cpp:128] Creating Layer Convolution20
I0818 10:45:00.882292  2034 net.cpp:558] Convolution20 <- Convolution19
I0818 10:45:00.882300  2034 net.cpp:522] Convolution20 -> Convolution20
I0818 10:45:00.883700  2034 net.cpp:172] Setting up Convolution20
I0818 10:45:00.883726  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.883731  2034 net.cpp:194] Memory required for data: 685244928
I0818 10:45:00.883744  2034 layer_factory.hpp:77] Creating layer BatchNorm20
I0818 10:45:00.883754  2034 net.cpp:128] Creating Layer BatchNorm20
I0818 10:45:00.883761  2034 net.cpp:558] BatchNorm20 <- Convolution20
I0818 10:45:00.883770  2034 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0818 10:45:00.884027  2034 net.cpp:172] Setting up BatchNorm20
I0818 10:45:00.884037  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884040  2034 net.cpp:194] Memory required for data: 689439232
I0818 10:45:00.884049  2034 layer_factory.hpp:77] Creating layer Scale20
I0818 10:45:00.884057  2034 net.cpp:128] Creating Layer Scale20
I0818 10:45:00.884060  2034 net.cpp:558] Scale20 <- Convolution20
I0818 10:45:00.884065  2034 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0818 10:45:00.884109  2034 layer_factory.hpp:77] Creating layer Scale20
I0818 10:45:00.884254  2034 net.cpp:172] Setting up Scale20
I0818 10:45:00.884264  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884268  2034 net.cpp:194] Memory required for data: 693633536
I0818 10:45:00.884276  2034 layer_factory.hpp:77] Creating layer Eltwise9
I0818 10:45:00.884284  2034 net.cpp:128] Creating Layer Eltwise9
I0818 10:45:00.884287  2034 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0818 10:45:00.884292  2034 net.cpp:558] Eltwise9 <- Convolution20
I0818 10:45:00.884301  2034 net.cpp:522] Eltwise9 -> Eltwise9
I0818 10:45:00.884322  2034 net.cpp:172] Setting up Eltwise9
I0818 10:45:00.884331  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884335  2034 net.cpp:194] Memory required for data: 697827840
I0818 10:45:00.884340  2034 layer_factory.hpp:77] Creating layer ReLU19
I0818 10:45:00.884349  2034 net.cpp:128] Creating Layer ReLU19
I0818 10:45:00.884356  2034 net.cpp:558] ReLU19 <- Eltwise9
I0818 10:45:00.884361  2034 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0818 10:45:00.884589  2034 net.cpp:172] Setting up ReLU19
I0818 10:45:00.884600  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884605  2034 net.cpp:194] Memory required for data: 702022144
I0818 10:45:00.884609  2034 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0818 10:45:00.884618  2034 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0818 10:45:00.884624  2034 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0818 10:45:00.884631  2034 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0818 10:45:00.884639  2034 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0818 10:45:00.884690  2034 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0818 10:45:00.884701  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884706  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.884711  2034 net.cpp:194] Memory required for data: 710410752
I0818 10:45:00.884714  2034 layer_factory.hpp:77] Creating layer Convolution21
I0818 10:45:00.884727  2034 net.cpp:128] Creating Layer Convolution21
I0818 10:45:00.884733  2034 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0818 10:45:00.884740  2034 net.cpp:522] Convolution21 -> Convolution21
I0818 10:45:00.886112  2034 net.cpp:172] Setting up Convolution21
I0818 10:45:00.886137  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.886157  2034 net.cpp:194] Memory required for data: 714605056
I0818 10:45:00.886168  2034 layer_factory.hpp:77] Creating layer BatchNorm21
I0818 10:45:00.886183  2034 net.cpp:128] Creating Layer BatchNorm21
I0818 10:45:00.886188  2034 net.cpp:558] BatchNorm21 <- Convolution21
I0818 10:45:00.886195  2034 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0818 10:45:00.886451  2034 net.cpp:172] Setting up BatchNorm21
I0818 10:45:00.886462  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.886467  2034 net.cpp:194] Memory required for data: 718799360
I0818 10:45:00.886476  2034 layer_factory.hpp:77] Creating layer Scale21
I0818 10:45:00.886483  2034 net.cpp:128] Creating Layer Scale21
I0818 10:45:00.886487  2034 net.cpp:558] Scale21 <- Convolution21
I0818 10:45:00.886493  2034 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0818 10:45:00.886534  2034 layer_factory.hpp:77] Creating layer Scale21
I0818 10:45:00.886704  2034 net.cpp:172] Setting up Scale21
I0818 10:45:00.886716  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.886723  2034 net.cpp:194] Memory required for data: 722993664
I0818 10:45:00.886730  2034 layer_factory.hpp:77] Creating layer ReLU20
I0818 10:45:00.886737  2034 net.cpp:128] Creating Layer ReLU20
I0818 10:45:00.886741  2034 net.cpp:558] ReLU20 <- Convolution21
I0818 10:45:00.886749  2034 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0818 10:45:00.887179  2034 net.cpp:172] Setting up ReLU20
I0818 10:45:00.887200  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.887205  2034 net.cpp:194] Memory required for data: 727187968
I0818 10:45:00.887210  2034 layer_factory.hpp:77] Creating layer Convolution22
I0818 10:45:00.887224  2034 net.cpp:128] Creating Layer Convolution22
I0818 10:45:00.887230  2034 net.cpp:558] Convolution22 <- Convolution21
I0818 10:45:00.887240  2034 net.cpp:522] Convolution22 -> Convolution22
I0818 10:45:00.888622  2034 net.cpp:172] Setting up Convolution22
I0818 10:45:00.888650  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.888658  2034 net.cpp:194] Memory required for data: 731382272
I0818 10:45:00.888667  2034 layer_factory.hpp:77] Creating layer BatchNorm22
I0818 10:45:00.888675  2034 net.cpp:128] Creating Layer BatchNorm22
I0818 10:45:00.888684  2034 net.cpp:558] BatchNorm22 <- Convolution22
I0818 10:45:00.888694  2034 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0818 10:45:00.888944  2034 net.cpp:172] Setting up BatchNorm22
I0818 10:45:00.888955  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.888959  2034 net.cpp:194] Memory required for data: 735576576
I0818 10:45:00.888969  2034 layer_factory.hpp:77] Creating layer Scale22
I0818 10:45:00.888976  2034 net.cpp:128] Creating Layer Scale22
I0818 10:45:00.888980  2034 net.cpp:558] Scale22 <- Convolution22
I0818 10:45:00.888985  2034 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0818 10:45:00.889027  2034 layer_factory.hpp:77] Creating layer Scale22
I0818 10:45:00.889173  2034 net.cpp:172] Setting up Scale22
I0818 10:45:00.889183  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.889186  2034 net.cpp:194] Memory required for data: 739770880
I0818 10:45:00.889194  2034 layer_factory.hpp:77] Creating layer Eltwise10
I0818 10:45:00.889204  2034 net.cpp:128] Creating Layer Eltwise10
I0818 10:45:00.889209  2034 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0818 10:45:00.889212  2034 net.cpp:558] Eltwise10 <- Convolution22
I0818 10:45:00.889219  2034 net.cpp:522] Eltwise10 -> Eltwise10
I0818 10:45:00.889243  2034 net.cpp:172] Setting up Eltwise10
I0818 10:45:00.889250  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.889255  2034 net.cpp:194] Memory required for data: 743965184
I0818 10:45:00.889258  2034 layer_factory.hpp:77] Creating layer ReLU21
I0818 10:45:00.889266  2034 net.cpp:128] Creating Layer ReLU21
I0818 10:45:00.889269  2034 net.cpp:558] ReLU21 <- Eltwise10
I0818 10:45:00.889277  2034 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0818 10:45:00.889535  2034 net.cpp:172] Setting up ReLU21
I0818 10:45:00.889554  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.889559  2034 net.cpp:194] Memory required for data: 748159488
I0818 10:45:00.889564  2034 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0818 10:45:00.889570  2034 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0818 10:45:00.889575  2034 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0818 10:45:00.889585  2034 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0818 10:45:00.889593  2034 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0818 10:45:00.889643  2034 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0818 10:45:00.889652  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.889658  2034 net.cpp:186] Top shape: 128 32 16 16 (1048576)
I0818 10:45:00.889662  2034 net.cpp:194] Memory required for data: 756548096
I0818 10:45:00.889667  2034 layer_factory.hpp:77] Creating layer Convolution23
I0818 10:45:00.889679  2034 net.cpp:128] Creating Layer Convolution23
I0818 10:45:00.889684  2034 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0818 10:45:00.889693  2034 net.cpp:522] Convolution23 -> Convolution23
I0818 10:45:00.890987  2034 net.cpp:172] Setting up Convolution23
I0818 10:45:00.891011  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.891016  2034 net.cpp:194] Memory required for data: 758645248
I0818 10:45:00.891026  2034 layer_factory.hpp:77] Creating layer BatchNorm23
I0818 10:45:00.891034  2034 net.cpp:128] Creating Layer BatchNorm23
I0818 10:45:00.891039  2034 net.cpp:558] BatchNorm23 <- Convolution23
I0818 10:45:00.891048  2034 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0818 10:45:00.891309  2034 net.cpp:172] Setting up BatchNorm23
I0818 10:45:00.891320  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.891324  2034 net.cpp:194] Memory required for data: 760742400
I0818 10:45:00.891333  2034 layer_factory.hpp:77] Creating layer Scale23
I0818 10:45:00.891343  2034 net.cpp:128] Creating Layer Scale23
I0818 10:45:00.891347  2034 net.cpp:558] Scale23 <- Convolution23
I0818 10:45:00.891353  2034 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0818 10:45:00.891393  2034 layer_factory.hpp:77] Creating layer Scale23
I0818 10:45:00.891541  2034 net.cpp:172] Setting up Scale23
I0818 10:45:00.891551  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.891556  2034 net.cpp:194] Memory required for data: 762839552
I0818 10:45:00.891563  2034 layer_factory.hpp:77] Creating layer Convolution24
I0818 10:45:00.891577  2034 net.cpp:128] Creating Layer Convolution24
I0818 10:45:00.891582  2034 net.cpp:558] Convolution24 <- Eltwise10_ReLU21_0_split_1
I0818 10:45:00.891589  2034 net.cpp:522] Convolution24 -> Convolution24
I0818 10:45:00.894644  2034 net.cpp:172] Setting up Convolution24
I0818 10:45:00.894687  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.894692  2034 net.cpp:194] Memory required for data: 764936704
I0818 10:45:00.894703  2034 layer_factory.hpp:77] Creating layer BatchNorm24
I0818 10:45:00.894713  2034 net.cpp:128] Creating Layer BatchNorm24
I0818 10:45:00.894719  2034 net.cpp:558] BatchNorm24 <- Convolution24
I0818 10:45:00.894728  2034 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0818 10:45:00.895002  2034 net.cpp:172] Setting up BatchNorm24
I0818 10:45:00.895012  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.895017  2034 net.cpp:194] Memory required for data: 767033856
I0818 10:45:00.895027  2034 layer_factory.hpp:77] Creating layer Scale24
I0818 10:45:00.895033  2034 net.cpp:128] Creating Layer Scale24
I0818 10:45:00.895038  2034 net.cpp:558] Scale24 <- Convolution24
I0818 10:45:00.895045  2034 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0818 10:45:00.895087  2034 layer_factory.hpp:77] Creating layer Scale24
I0818 10:45:00.895238  2034 net.cpp:172] Setting up Scale24
I0818 10:45:00.895248  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.895252  2034 net.cpp:194] Memory required for data: 769131008
I0818 10:45:00.895277  2034 layer_factory.hpp:77] Creating layer ReLU22
I0818 10:45:00.895285  2034 net.cpp:128] Creating Layer ReLU22
I0818 10:45:00.895295  2034 net.cpp:558] ReLU22 <- Convolution24
I0818 10:45:00.895303  2034 net.cpp:509] ReLU22 -> Convolution24 (in-place)
I0818 10:45:00.895541  2034 net.cpp:172] Setting up ReLU22
I0818 10:45:00.895555  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.895558  2034 net.cpp:194] Memory required for data: 771228160
I0818 10:45:00.895563  2034 layer_factory.hpp:77] Creating layer Convolution25
I0818 10:45:00.895576  2034 net.cpp:128] Creating Layer Convolution25
I0818 10:45:00.895581  2034 net.cpp:558] Convolution25 <- Convolution24
I0818 10:45:00.895592  2034 net.cpp:522] Convolution25 -> Convolution25
I0818 10:45:00.897809  2034 net.cpp:172] Setting up Convolution25
I0818 10:45:00.897833  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.897837  2034 net.cpp:194] Memory required for data: 773325312
I0818 10:45:00.897853  2034 layer_factory.hpp:77] Creating layer BatchNorm25
I0818 10:45:00.897861  2034 net.cpp:128] Creating Layer BatchNorm25
I0818 10:45:00.897871  2034 net.cpp:558] BatchNorm25 <- Convolution25
I0818 10:45:00.897884  2034 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0818 10:45:00.898159  2034 net.cpp:172] Setting up BatchNorm25
I0818 10:45:00.898169  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.898174  2034 net.cpp:194] Memory required for data: 775422464
I0818 10:45:00.898183  2034 layer_factory.hpp:77] Creating layer Scale25
I0818 10:45:00.898195  2034 net.cpp:128] Creating Layer Scale25
I0818 10:45:00.898202  2034 net.cpp:558] Scale25 <- Convolution25
I0818 10:45:00.898208  2034 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0818 10:45:00.898249  2034 layer_factory.hpp:77] Creating layer Scale25
I0818 10:45:00.898401  2034 net.cpp:172] Setting up Scale25
I0818 10:45:00.898412  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.898416  2034 net.cpp:194] Memory required for data: 777519616
I0818 10:45:00.898424  2034 layer_factory.hpp:77] Creating layer Eltwise11
I0818 10:45:00.898434  2034 net.cpp:128] Creating Layer Eltwise11
I0818 10:45:00.898439  2034 net.cpp:558] Eltwise11 <- Convolution23
I0818 10:45:00.898444  2034 net.cpp:558] Eltwise11 <- Convolution25
I0818 10:45:00.898450  2034 net.cpp:522] Eltwise11 -> Eltwise11
I0818 10:45:00.898474  2034 net.cpp:172] Setting up Eltwise11
I0818 10:45:00.898481  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.898485  2034 net.cpp:194] Memory required for data: 779616768
I0818 10:45:00.898489  2034 layer_factory.hpp:77] Creating layer ReLU23
I0818 10:45:00.898496  2034 net.cpp:128] Creating Layer ReLU23
I0818 10:45:00.898500  2034 net.cpp:558] ReLU23 <- Eltwise11
I0818 10:45:00.898506  2034 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0818 10:45:00.898952  2034 net.cpp:172] Setting up ReLU23
I0818 10:45:00.898974  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.898979  2034 net.cpp:194] Memory required for data: 781713920
I0818 10:45:00.898984  2034 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0818 10:45:00.898995  2034 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0818 10:45:00.899000  2034 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0818 10:45:00.899010  2034 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0818 10:45:00.899019  2034 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0818 10:45:00.899072  2034 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0818 10:45:00.899086  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.899092  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.899096  2034 net.cpp:194] Memory required for data: 785908224
I0818 10:45:00.899101  2034 layer_factory.hpp:77] Creating layer Convolution26
I0818 10:45:00.899112  2034 net.cpp:128] Creating Layer Convolution26
I0818 10:45:00.899116  2034 net.cpp:558] Convolution26 <- Eltwise11_ReLU23_0_split_0
I0818 10:45:00.899147  2034 net.cpp:522] Convolution26 -> Convolution26
I0818 10:45:00.900970  2034 net.cpp:172] Setting up Convolution26
I0818 10:45:00.901000  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.901003  2034 net.cpp:194] Memory required for data: 788005376
I0818 10:45:00.901013  2034 layer_factory.hpp:77] Creating layer BatchNorm26
I0818 10:45:00.901021  2034 net.cpp:128] Creating Layer BatchNorm26
I0818 10:45:00.901031  2034 net.cpp:558] BatchNorm26 <- Convolution26
I0818 10:45:00.901044  2034 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0818 10:45:00.901314  2034 net.cpp:172] Setting up BatchNorm26
I0818 10:45:00.901324  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.901329  2034 net.cpp:194] Memory required for data: 790102528
I0818 10:45:00.901340  2034 layer_factory.hpp:77] Creating layer Scale26
I0818 10:45:00.901346  2034 net.cpp:128] Creating Layer Scale26
I0818 10:45:00.901350  2034 net.cpp:558] Scale26 <- Convolution26
I0818 10:45:00.901358  2034 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0818 10:45:00.901398  2034 layer_factory.hpp:77] Creating layer Scale26
I0818 10:45:00.901548  2034 net.cpp:172] Setting up Scale26
I0818 10:45:00.901558  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.901562  2034 net.cpp:194] Memory required for data: 792199680
I0818 10:45:00.901571  2034 layer_factory.hpp:77] Creating layer ReLU24
I0818 10:45:00.901576  2034 net.cpp:128] Creating Layer ReLU24
I0818 10:45:00.901582  2034 net.cpp:558] ReLU24 <- Convolution26
I0818 10:45:00.901589  2034 net.cpp:509] ReLU24 -> Convolution26 (in-place)
I0818 10:45:00.901826  2034 net.cpp:172] Setting up ReLU24
I0818 10:45:00.901839  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.901844  2034 net.cpp:194] Memory required for data: 794296832
I0818 10:45:00.901849  2034 layer_factory.hpp:77] Creating layer Convolution27
I0818 10:45:00.901860  2034 net.cpp:128] Creating Layer Convolution27
I0818 10:45:00.901865  2034 net.cpp:558] Convolution27 <- Convolution26
I0818 10:45:00.901875  2034 net.cpp:522] Convolution27 -> Convolution27
I0818 10:45:00.903961  2034 net.cpp:172] Setting up Convolution27
I0818 10:45:00.903987  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.903992  2034 net.cpp:194] Memory required for data: 796393984
I0818 10:45:00.904006  2034 layer_factory.hpp:77] Creating layer BatchNorm27
I0818 10:45:00.904016  2034 net.cpp:128] Creating Layer BatchNorm27
I0818 10:45:00.904023  2034 net.cpp:558] BatchNorm27 <- Convolution27
I0818 10:45:00.904031  2034 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0818 10:45:00.904310  2034 net.cpp:172] Setting up BatchNorm27
I0818 10:45:00.904320  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.904325  2034 net.cpp:194] Memory required for data: 798491136
I0818 10:45:00.904335  2034 layer_factory.hpp:77] Creating layer Scale27
I0818 10:45:00.904357  2034 net.cpp:128] Creating Layer Scale27
I0818 10:45:00.904362  2034 net.cpp:558] Scale27 <- Convolution27
I0818 10:45:00.904368  2034 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0818 10:45:00.904409  2034 layer_factory.hpp:77] Creating layer Scale27
I0818 10:45:00.904557  2034 net.cpp:172] Setting up Scale27
I0818 10:45:00.904566  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.904568  2034 net.cpp:194] Memory required for data: 800588288
I0818 10:45:00.904577  2034 layer_factory.hpp:77] Creating layer Eltwise12
I0818 10:45:00.904587  2034 net.cpp:128] Creating Layer Eltwise12
I0818 10:45:00.904592  2034 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0818 10:45:00.904597  2034 net.cpp:558] Eltwise12 <- Convolution27
I0818 10:45:00.904603  2034 net.cpp:522] Eltwise12 -> Eltwise12
I0818 10:45:00.904630  2034 net.cpp:172] Setting up Eltwise12
I0818 10:45:00.904639  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.904642  2034 net.cpp:194] Memory required for data: 802685440
I0818 10:45:00.904647  2034 layer_factory.hpp:77] Creating layer ReLU25
I0818 10:45:00.904654  2034 net.cpp:128] Creating Layer ReLU25
I0818 10:45:00.904675  2034 net.cpp:558] ReLU25 <- Eltwise12
I0818 10:45:00.904680  2034 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0818 10:45:00.904925  2034 net.cpp:172] Setting up ReLU25
I0818 10:45:00.904940  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.904948  2034 net.cpp:194] Memory required for data: 804782592
I0818 10:45:00.904953  2034 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0818 10:45:00.904963  2034 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0818 10:45:00.904968  2034 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0818 10:45:00.904974  2034 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0818 10:45:00.904984  2034 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0818 10:45:00.905037  2034 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0818 10:45:00.905048  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.905055  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.905058  2034 net.cpp:194] Memory required for data: 808976896
I0818 10:45:00.905062  2034 layer_factory.hpp:77] Creating layer Convolution28
I0818 10:45:00.905077  2034 net.cpp:128] Creating Layer Convolution28
I0818 10:45:00.905082  2034 net.cpp:558] Convolution28 <- Eltwise12_ReLU25_0_split_0
I0818 10:45:00.905091  2034 net.cpp:522] Convolution28 -> Convolution28
I0818 10:45:00.906946  2034 net.cpp:172] Setting up Convolution28
I0818 10:45:00.906968  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.906972  2034 net.cpp:194] Memory required for data: 811074048
I0818 10:45:00.906982  2034 layer_factory.hpp:77] Creating layer BatchNorm28
I0818 10:45:00.906993  2034 net.cpp:128] Creating Layer BatchNorm28
I0818 10:45:00.906998  2034 net.cpp:558] BatchNorm28 <- Convolution28
I0818 10:45:00.907011  2034 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0818 10:45:00.907279  2034 net.cpp:172] Setting up BatchNorm28
I0818 10:45:00.907289  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.907294  2034 net.cpp:194] Memory required for data: 813171200
I0818 10:45:00.907302  2034 layer_factory.hpp:77] Creating layer Scale28
I0818 10:45:00.907311  2034 net.cpp:128] Creating Layer Scale28
I0818 10:45:00.907316  2034 net.cpp:558] Scale28 <- Convolution28
I0818 10:45:00.907321  2034 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0818 10:45:00.907364  2034 layer_factory.hpp:77] Creating layer Scale28
I0818 10:45:00.907521  2034 net.cpp:172] Setting up Scale28
I0818 10:45:00.907533  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.907537  2034 net.cpp:194] Memory required for data: 815268352
I0818 10:45:00.907546  2034 layer_factory.hpp:77] Creating layer ReLU26
I0818 10:45:00.907552  2034 net.cpp:128] Creating Layer ReLU26
I0818 10:45:00.907557  2034 net.cpp:558] ReLU26 <- Convolution28
I0818 10:45:00.907563  2034 net.cpp:509] ReLU26 -> Convolution28 (in-place)
I0818 10:45:00.907994  2034 net.cpp:172] Setting up ReLU26
I0818 10:45:00.908015  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.908020  2034 net.cpp:194] Memory required for data: 817365504
I0818 10:45:00.908025  2034 layer_factory.hpp:77] Creating layer Convolution29
I0818 10:45:00.908040  2034 net.cpp:128] Creating Layer Convolution29
I0818 10:45:00.908048  2034 net.cpp:558] Convolution29 <- Convolution28
I0818 10:45:00.908059  2034 net.cpp:522] Convolution29 -> Convolution29
I0818 10:45:00.910087  2034 net.cpp:172] Setting up Convolution29
I0818 10:45:00.910115  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.910120  2034 net.cpp:194] Memory required for data: 819462656
I0818 10:45:00.910132  2034 layer_factory.hpp:77] Creating layer BatchNorm29
I0818 10:45:00.910140  2034 net.cpp:128] Creating Layer BatchNorm29
I0818 10:45:00.910145  2034 net.cpp:558] BatchNorm29 <- Convolution29
I0818 10:45:00.910158  2034 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0818 10:45:00.910436  2034 net.cpp:172] Setting up BatchNorm29
I0818 10:45:00.910449  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.910467  2034 net.cpp:194] Memory required for data: 821559808
I0818 10:45:00.910478  2034 layer_factory.hpp:77] Creating layer Scale29
I0818 10:45:00.910485  2034 net.cpp:128] Creating Layer Scale29
I0818 10:45:00.910490  2034 net.cpp:558] Scale29 <- Convolution29
I0818 10:45:00.910495  2034 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0818 10:45:00.910539  2034 layer_factory.hpp:77] Creating layer Scale29
I0818 10:45:00.910712  2034 net.cpp:172] Setting up Scale29
I0818 10:45:00.910725  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.910729  2034 net.cpp:194] Memory required for data: 823656960
I0818 10:45:00.910738  2034 layer_factory.hpp:77] Creating layer Eltwise13
I0818 10:45:00.910748  2034 net.cpp:128] Creating Layer Eltwise13
I0818 10:45:00.910753  2034 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0818 10:45:00.910758  2034 net.cpp:558] Eltwise13 <- Convolution29
I0818 10:45:00.910764  2034 net.cpp:522] Eltwise13 -> Eltwise13
I0818 10:45:00.910787  2034 net.cpp:172] Setting up Eltwise13
I0818 10:45:00.910794  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.910797  2034 net.cpp:194] Memory required for data: 825754112
I0818 10:45:00.910802  2034 layer_factory.hpp:77] Creating layer ReLU27
I0818 10:45:00.910810  2034 net.cpp:128] Creating Layer ReLU27
I0818 10:45:00.910815  2034 net.cpp:558] ReLU27 <- Eltwise13
I0818 10:45:00.910820  2034 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0818 10:45:00.911069  2034 net.cpp:172] Setting up ReLU27
I0818 10:45:00.911083  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.911088  2034 net.cpp:194] Memory required for data: 827851264
I0818 10:45:00.911092  2034 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0818 10:45:00.911103  2034 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0818 10:45:00.911108  2034 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0818 10:45:00.911118  2034 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0818 10:45:00.911126  2034 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0818 10:45:00.911176  2034 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0818 10:45:00.911191  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.911197  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.911201  2034 net.cpp:194] Memory required for data: 832045568
I0818 10:45:00.911206  2034 layer_factory.hpp:77] Creating layer Convolution30
I0818 10:45:00.911217  2034 net.cpp:128] Creating Layer Convolution30
I0818 10:45:00.911222  2034 net.cpp:558] Convolution30 <- Eltwise13_ReLU27_0_split_0
I0818 10:45:00.911229  2034 net.cpp:522] Convolution30 -> Convolution30
I0818 10:45:00.913089  2034 net.cpp:172] Setting up Convolution30
I0818 10:45:00.913115  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.913120  2034 net.cpp:194] Memory required for data: 834142720
I0818 10:45:00.913130  2034 layer_factory.hpp:77] Creating layer BatchNorm30
I0818 10:45:00.913138  2034 net.cpp:128] Creating Layer BatchNorm30
I0818 10:45:00.913143  2034 net.cpp:558] BatchNorm30 <- Convolution30
I0818 10:45:00.913152  2034 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0818 10:45:00.913425  2034 net.cpp:172] Setting up BatchNorm30
I0818 10:45:00.913437  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.913441  2034 net.cpp:194] Memory required for data: 836239872
I0818 10:45:00.913451  2034 layer_factory.hpp:77] Creating layer Scale30
I0818 10:45:00.913458  2034 net.cpp:128] Creating Layer Scale30
I0818 10:45:00.913462  2034 net.cpp:558] Scale30 <- Convolution30
I0818 10:45:00.913471  2034 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0818 10:45:00.913512  2034 layer_factory.hpp:77] Creating layer Scale30
I0818 10:45:00.913671  2034 net.cpp:172] Setting up Scale30
I0818 10:45:00.913681  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.913686  2034 net.cpp:194] Memory required for data: 838337024
I0818 10:45:00.913693  2034 layer_factory.hpp:77] Creating layer ReLU28
I0818 10:45:00.913715  2034 net.cpp:128] Creating Layer ReLU28
I0818 10:45:00.913720  2034 net.cpp:558] ReLU28 <- Convolution30
I0818 10:45:00.913733  2034 net.cpp:509] ReLU28 -> Convolution30 (in-place)
I0818 10:45:00.913978  2034 net.cpp:172] Setting up ReLU28
I0818 10:45:00.913992  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.913996  2034 net.cpp:194] Memory required for data: 840434176
I0818 10:45:00.914001  2034 layer_factory.hpp:77] Creating layer Convolution31
I0818 10:45:00.914013  2034 net.cpp:128] Creating Layer Convolution31
I0818 10:45:00.914019  2034 net.cpp:558] Convolution31 <- Convolution30
I0818 10:45:00.914028  2034 net.cpp:522] Convolution31 -> Convolution31
I0818 10:45:00.916102  2034 net.cpp:172] Setting up Convolution31
I0818 10:45:00.916131  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.916136  2034 net.cpp:194] Memory required for data: 842531328
I0818 10:45:00.916146  2034 layer_factory.hpp:77] Creating layer BatchNorm31
I0818 10:45:00.916157  2034 net.cpp:128] Creating Layer BatchNorm31
I0818 10:45:00.916162  2034 net.cpp:558] BatchNorm31 <- Convolution31
I0818 10:45:00.916168  2034 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0818 10:45:00.916446  2034 net.cpp:172] Setting up BatchNorm31
I0818 10:45:00.916457  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.916462  2034 net.cpp:194] Memory required for data: 844628480
I0818 10:45:00.916472  2034 layer_factory.hpp:77] Creating layer Scale31
I0818 10:45:00.916479  2034 net.cpp:128] Creating Layer Scale31
I0818 10:45:00.916483  2034 net.cpp:558] Scale31 <- Convolution31
I0818 10:45:00.916489  2034 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0818 10:45:00.916533  2034 layer_factory.hpp:77] Creating layer Scale31
I0818 10:45:00.916690  2034 net.cpp:172] Setting up Scale31
I0818 10:45:00.916703  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.916707  2034 net.cpp:194] Memory required for data: 846725632
I0818 10:45:00.916714  2034 layer_factory.hpp:77] Creating layer Eltwise14
I0818 10:45:00.916723  2034 net.cpp:128] Creating Layer Eltwise14
I0818 10:45:00.916728  2034 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0818 10:45:00.916733  2034 net.cpp:558] Eltwise14 <- Convolution31
I0818 10:45:00.916739  2034 net.cpp:522] Eltwise14 -> Eltwise14
I0818 10:45:00.916764  2034 net.cpp:172] Setting up Eltwise14
I0818 10:45:00.916771  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.916775  2034 net.cpp:194] Memory required for data: 848822784
I0818 10:45:00.916779  2034 layer_factory.hpp:77] Creating layer ReLU29
I0818 10:45:00.916785  2034 net.cpp:128] Creating Layer ReLU29
I0818 10:45:00.916790  2034 net.cpp:558] ReLU29 <- Eltwise14
I0818 10:45:00.916797  2034 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0818 10:45:00.917045  2034 net.cpp:172] Setting up ReLU29
I0818 10:45:00.917060  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.917064  2034 net.cpp:194] Memory required for data: 850919936
I0818 10:45:00.917069  2034 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0818 10:45:00.917076  2034 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0818 10:45:00.917081  2034 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0818 10:45:00.917089  2034 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0818 10:45:00.917098  2034 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0818 10:45:00.917155  2034 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0818 10:45:00.917165  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.917173  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.917177  2034 net.cpp:194] Memory required for data: 855114240
I0818 10:45:00.917181  2034 layer_factory.hpp:77] Creating layer Convolution32
I0818 10:45:00.917193  2034 net.cpp:128] Creating Layer Convolution32
I0818 10:45:00.917197  2034 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_0
I0818 10:45:00.917207  2034 net.cpp:522] Convolution32 -> Convolution32
I0818 10:45:00.919113  2034 net.cpp:172] Setting up Convolution32
I0818 10:45:00.919139  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.919143  2034 net.cpp:194] Memory required for data: 857211392
I0818 10:45:00.919157  2034 layer_factory.hpp:77] Creating layer BatchNorm32
I0818 10:45:00.919164  2034 net.cpp:128] Creating Layer BatchNorm32
I0818 10:45:00.919169  2034 net.cpp:558] BatchNorm32 <- Convolution32
I0818 10:45:00.919183  2034 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0818 10:45:00.919456  2034 net.cpp:172] Setting up BatchNorm32
I0818 10:45:00.919466  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.919471  2034 net.cpp:194] Memory required for data: 859308544
I0818 10:45:00.919479  2034 layer_factory.hpp:77] Creating layer Scale32
I0818 10:45:00.919486  2034 net.cpp:128] Creating Layer Scale32
I0818 10:45:00.919490  2034 net.cpp:558] Scale32 <- Convolution32
I0818 10:45:00.919499  2034 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0818 10:45:00.919540  2034 layer_factory.hpp:77] Creating layer Scale32
I0818 10:45:00.919693  2034 net.cpp:172] Setting up Scale32
I0818 10:45:00.919704  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.919708  2034 net.cpp:194] Memory required for data: 861405696
I0818 10:45:00.919716  2034 layer_factory.hpp:77] Creating layer ReLU30
I0818 10:45:00.919723  2034 net.cpp:128] Creating Layer ReLU30
I0818 10:45:00.919726  2034 net.cpp:558] ReLU30 <- Convolution32
I0818 10:45:00.919734  2034 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0818 10:45:00.920168  2034 net.cpp:172] Setting up ReLU30
I0818 10:45:00.920192  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.920195  2034 net.cpp:194] Memory required for data: 863502848
I0818 10:45:00.920200  2034 layer_factory.hpp:77] Creating layer Convolution33
I0818 10:45:00.920215  2034 net.cpp:128] Creating Layer Convolution33
I0818 10:45:00.920222  2034 net.cpp:558] Convolution33 <- Convolution32
I0818 10:45:00.920231  2034 net.cpp:522] Convolution33 -> Convolution33
I0818 10:45:00.922466  2034 net.cpp:172] Setting up Convolution33
I0818 10:45:00.922493  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.922497  2034 net.cpp:194] Memory required for data: 865600000
I0818 10:45:00.922510  2034 layer_factory.hpp:77] Creating layer BatchNorm33
I0818 10:45:00.922520  2034 net.cpp:128] Creating Layer BatchNorm33
I0818 10:45:00.922525  2034 net.cpp:558] BatchNorm33 <- Convolution33
I0818 10:45:00.922533  2034 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0818 10:45:00.922832  2034 net.cpp:172] Setting up BatchNorm33
I0818 10:45:00.922843  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.922848  2034 net.cpp:194] Memory required for data: 867697152
I0818 10:45:00.922859  2034 layer_factory.hpp:77] Creating layer Scale33
I0818 10:45:00.922866  2034 net.cpp:128] Creating Layer Scale33
I0818 10:45:00.922870  2034 net.cpp:558] Scale33 <- Convolution33
I0818 10:45:00.922876  2034 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0818 10:45:00.922921  2034 layer_factory.hpp:77] Creating layer Scale33
I0818 10:45:00.923079  2034 net.cpp:172] Setting up Scale33
I0818 10:45:00.923090  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.923095  2034 net.cpp:194] Memory required for data: 869794304
I0818 10:45:00.923102  2034 layer_factory.hpp:77] Creating layer Eltwise15
I0818 10:45:00.923111  2034 net.cpp:128] Creating Layer Eltwise15
I0818 10:45:00.923116  2034 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0818 10:45:00.923121  2034 net.cpp:558] Eltwise15 <- Convolution33
I0818 10:45:00.923127  2034 net.cpp:522] Eltwise15 -> Eltwise15
I0818 10:45:00.923153  2034 net.cpp:172] Setting up Eltwise15
I0818 10:45:00.923161  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.923164  2034 net.cpp:194] Memory required for data: 871891456
I0818 10:45:00.923168  2034 layer_factory.hpp:77] Creating layer ReLU31
I0818 10:45:00.923174  2034 net.cpp:128] Creating Layer ReLU31
I0818 10:45:00.923178  2034 net.cpp:558] ReLU31 <- Eltwise15
I0818 10:45:00.923200  2034 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0818 10:45:00.923446  2034 net.cpp:172] Setting up ReLU31
I0818 10:45:00.923460  2034 net.cpp:186] Top shape: 128 64 8 8 (524288)
I0818 10:45:00.923465  2034 net.cpp:194] Memory required for data: 873988608
I0818 10:45:00.923470  2034 layer_factory.hpp:77] Creating layer Pooling1
I0818 10:45:00.923482  2034 net.cpp:128] Creating Layer Pooling1
I0818 10:45:00.923488  2034 net.cpp:558] Pooling1 <- Eltwise15
I0818 10:45:00.923499  2034 net.cpp:522] Pooling1 -> Pooling1
I0818 10:45:00.923794  2034 net.cpp:172] Setting up Pooling1
I0818 10:45:00.923810  2034 net.cpp:186] Top shape: 128 64 1 1 (8192)
I0818 10:45:00.923813  2034 net.cpp:194] Memory required for data: 874021376
I0818 10:45:00.923818  2034 layer_factory.hpp:77] Creating layer InnerProduct1
I0818 10:45:00.923828  2034 net.cpp:128] Creating Layer InnerProduct1
I0818 10:45:00.923832  2034 net.cpp:558] InnerProduct1 <- Pooling1
I0818 10:45:00.923841  2034 net.cpp:522] InnerProduct1 -> InnerProduct1
I0818 10:45:00.924019  2034 net.cpp:172] Setting up InnerProduct1
I0818 10:45:00.924033  2034 net.cpp:186] Top shape: 128 10 (1280)
I0818 10:45:00.924038  2034 net.cpp:194] Memory required for data: 874026496
I0818 10:45:00.924047  2034 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:45:00.924054  2034 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0818 10:45:00.924058  2034 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1
I0818 10:45:00.924063  2034 net.cpp:558] SoftmaxWithLoss1 <- Data2
I0818 10:45:00.924073  2034 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0818 10:45:00.924084  2034 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:45:00.924657  2034 net.cpp:172] Setting up SoftmaxWithLoss1
I0818 10:45:00.924681  2034 net.cpp:186] Top shape: (1)
I0818 10:45:00.924685  2034 net.cpp:189]     with loss weight 1
I0818 10:45:00.924715  2034 net.cpp:194] Memory required for data: 874026500
I0818 10:45:00.924721  2034 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0818 10:45:00.924728  2034 net.cpp:301] InnerProduct1 needs backward computation.
I0818 10:45:00.924732  2034 net.cpp:301] Pooling1 needs backward computation.
I0818 10:45:00.924736  2034 net.cpp:301] ReLU31 needs backward computation.
I0818 10:45:00.924741  2034 net.cpp:301] Eltwise15 needs backward computation.
I0818 10:45:00.924746  2034 net.cpp:301] Scale33 needs backward computation.
I0818 10:45:00.924749  2034 net.cpp:301] BatchNorm33 needs backward computation.
I0818 10:45:00.924753  2034 net.cpp:301] Convolution33 needs backward computation.
I0818 10:45:00.924757  2034 net.cpp:301] ReLU30 needs backward computation.
I0818 10:45:00.924762  2034 net.cpp:301] Scale32 needs backward computation.
I0818 10:45:00.924765  2034 net.cpp:301] BatchNorm32 needs backward computation.
I0818 10:45:00.924769  2034 net.cpp:301] Convolution32 needs backward computation.
I0818 10:45:00.924774  2034 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0818 10:45:00.924778  2034 net.cpp:301] ReLU29 needs backward computation.
I0818 10:45:00.924782  2034 net.cpp:301] Eltwise14 needs backward computation.
I0818 10:45:00.924787  2034 net.cpp:301] Scale31 needs backward computation.
I0818 10:45:00.924791  2034 net.cpp:301] BatchNorm31 needs backward computation.
I0818 10:45:00.924796  2034 net.cpp:301] Convolution31 needs backward computation.
I0818 10:45:00.924800  2034 net.cpp:301] ReLU28 needs backward computation.
I0818 10:45:00.924804  2034 net.cpp:301] Scale30 needs backward computation.
I0818 10:45:00.924808  2034 net.cpp:301] BatchNorm30 needs backward computation.
I0818 10:45:00.924813  2034 net.cpp:301] Convolution30 needs backward computation.
I0818 10:45:00.924816  2034 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0818 10:45:00.924821  2034 net.cpp:301] ReLU27 needs backward computation.
I0818 10:45:00.924825  2034 net.cpp:301] Eltwise13 needs backward computation.
I0818 10:45:00.924829  2034 net.cpp:301] Scale29 needs backward computation.
I0818 10:45:00.924847  2034 net.cpp:301] BatchNorm29 needs backward computation.
I0818 10:45:00.924851  2034 net.cpp:301] Convolution29 needs backward computation.
I0818 10:45:00.924855  2034 net.cpp:301] ReLU26 needs backward computation.
I0818 10:45:00.924860  2034 net.cpp:301] Scale28 needs backward computation.
I0818 10:45:00.924863  2034 net.cpp:301] BatchNorm28 needs backward computation.
I0818 10:45:00.924867  2034 net.cpp:301] Convolution28 needs backward computation.
I0818 10:45:00.924872  2034 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0818 10:45:00.924876  2034 net.cpp:301] ReLU25 needs backward computation.
I0818 10:45:00.924881  2034 net.cpp:301] Eltwise12 needs backward computation.
I0818 10:45:00.924885  2034 net.cpp:301] Scale27 needs backward computation.
I0818 10:45:00.924890  2034 net.cpp:301] BatchNorm27 needs backward computation.
I0818 10:45:00.924893  2034 net.cpp:301] Convolution27 needs backward computation.
I0818 10:45:00.924898  2034 net.cpp:301] ReLU24 needs backward computation.
I0818 10:45:00.924902  2034 net.cpp:301] Scale26 needs backward computation.
I0818 10:45:00.924906  2034 net.cpp:301] BatchNorm26 needs backward computation.
I0818 10:45:00.924911  2034 net.cpp:301] Convolution26 needs backward computation.
I0818 10:45:00.924914  2034 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0818 10:45:00.924919  2034 net.cpp:301] ReLU23 needs backward computation.
I0818 10:45:00.924923  2034 net.cpp:301] Eltwise11 needs backward computation.
I0818 10:45:00.924927  2034 net.cpp:301] Scale25 needs backward computation.
I0818 10:45:00.924932  2034 net.cpp:301] BatchNorm25 needs backward computation.
I0818 10:45:00.924937  2034 net.cpp:301] Convolution25 needs backward computation.
I0818 10:45:00.924940  2034 net.cpp:301] ReLU22 needs backward computation.
I0818 10:45:00.924944  2034 net.cpp:301] Scale24 needs backward computation.
I0818 10:45:00.924948  2034 net.cpp:301] BatchNorm24 needs backward computation.
I0818 10:45:00.924955  2034 net.cpp:301] Convolution24 needs backward computation.
I0818 10:45:00.924960  2034 net.cpp:301] Scale23 needs backward computation.
I0818 10:45:00.924964  2034 net.cpp:301] BatchNorm23 needs backward computation.
I0818 10:45:00.924968  2034 net.cpp:301] Convolution23 needs backward computation.
I0818 10:45:00.924976  2034 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0818 10:45:00.924981  2034 net.cpp:301] ReLU21 needs backward computation.
I0818 10:45:00.924985  2034 net.cpp:301] Eltwise10 needs backward computation.
I0818 10:45:00.924990  2034 net.cpp:301] Scale22 needs backward computation.
I0818 10:45:00.924995  2034 net.cpp:301] BatchNorm22 needs backward computation.
I0818 10:45:00.924999  2034 net.cpp:301] Convolution22 needs backward computation.
I0818 10:45:00.925004  2034 net.cpp:301] ReLU20 needs backward computation.
I0818 10:45:00.925007  2034 net.cpp:301] Scale21 needs backward computation.
I0818 10:45:00.925012  2034 net.cpp:301] BatchNorm21 needs backward computation.
I0818 10:45:00.925016  2034 net.cpp:301] Convolution21 needs backward computation.
I0818 10:45:00.925021  2034 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0818 10:45:00.925025  2034 net.cpp:301] ReLU19 needs backward computation.
I0818 10:45:00.925030  2034 net.cpp:301] Eltwise9 needs backward computation.
I0818 10:45:00.925035  2034 net.cpp:301] Scale20 needs backward computation.
I0818 10:45:00.925040  2034 net.cpp:301] BatchNorm20 needs backward computation.
I0818 10:45:00.925043  2034 net.cpp:301] Convolution20 needs backward computation.
I0818 10:45:00.925047  2034 net.cpp:301] ReLU18 needs backward computation.
I0818 10:45:00.925052  2034 net.cpp:301] Scale19 needs backward computation.
I0818 10:45:00.925056  2034 net.cpp:301] BatchNorm19 needs backward computation.
I0818 10:45:00.925060  2034 net.cpp:301] Convolution19 needs backward computation.
I0818 10:45:00.925065  2034 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0818 10:45:00.925070  2034 net.cpp:301] ReLU17 needs backward computation.
I0818 10:45:00.925081  2034 net.cpp:301] Eltwise8 needs backward computation.
I0818 10:45:00.925086  2034 net.cpp:301] Scale18 needs backward computation.
I0818 10:45:00.925091  2034 net.cpp:301] BatchNorm18 needs backward computation.
I0818 10:45:00.925096  2034 net.cpp:301] Convolution18 needs backward computation.
I0818 10:45:00.925099  2034 net.cpp:301] ReLU16 needs backward computation.
I0818 10:45:00.925104  2034 net.cpp:301] Scale17 needs backward computation.
I0818 10:45:00.925108  2034 net.cpp:301] BatchNorm17 needs backward computation.
I0818 10:45:00.925112  2034 net.cpp:301] Convolution17 needs backward computation.
I0818 10:45:00.925117  2034 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0818 10:45:00.925122  2034 net.cpp:301] ReLU15 needs backward computation.
I0818 10:45:00.925125  2034 net.cpp:301] Eltwise7 needs backward computation.
I0818 10:45:00.925133  2034 net.cpp:301] Scale16 needs backward computation.
I0818 10:45:00.925137  2034 net.cpp:301] BatchNorm16 needs backward computation.
I0818 10:45:00.925141  2034 net.cpp:301] Convolution16 needs backward computation.
I0818 10:45:00.925145  2034 net.cpp:301] ReLU14 needs backward computation.
I0818 10:45:00.925149  2034 net.cpp:301] Scale15 needs backward computation.
I0818 10:45:00.925154  2034 net.cpp:301] BatchNorm15 needs backward computation.
I0818 10:45:00.925158  2034 net.cpp:301] Convolution15 needs backward computation.
I0818 10:45:00.925163  2034 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0818 10:45:00.925168  2034 net.cpp:301] ReLU13 needs backward computation.
I0818 10:45:00.925173  2034 net.cpp:301] Eltwise6 needs backward computation.
I0818 10:45:00.925177  2034 net.cpp:301] Scale14 needs backward computation.
I0818 10:45:00.925181  2034 net.cpp:301] BatchNorm14 needs backward computation.
I0818 10:45:00.925185  2034 net.cpp:301] Convolution14 needs backward computation.
I0818 10:45:00.925190  2034 net.cpp:301] ReLU12 needs backward computation.
I0818 10:45:00.925194  2034 net.cpp:301] Scale13 needs backward computation.
I0818 10:45:00.925199  2034 net.cpp:301] BatchNorm13 needs backward computation.
I0818 10:45:00.925204  2034 net.cpp:301] Convolution13 needs backward computation.
I0818 10:45:00.925207  2034 net.cpp:301] Scale12 needs backward computation.
I0818 10:45:00.925212  2034 net.cpp:301] BatchNorm12 needs backward computation.
I0818 10:45:00.925216  2034 net.cpp:301] Convolution12 needs backward computation.
I0818 10:45:00.925226  2034 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0818 10:45:00.925231  2034 net.cpp:301] ReLU11 needs backward computation.
I0818 10:45:00.925235  2034 net.cpp:301] Eltwise5 needs backward computation.
I0818 10:45:00.925240  2034 net.cpp:301] Scale11 needs backward computation.
I0818 10:45:00.925245  2034 net.cpp:301] BatchNorm11 needs backward computation.
I0818 10:45:00.925248  2034 net.cpp:301] Convolution11 needs backward computation.
I0818 10:45:00.925253  2034 net.cpp:301] ReLU10 needs backward computation.
I0818 10:45:00.925257  2034 net.cpp:301] Scale10 needs backward computation.
I0818 10:45:00.925261  2034 net.cpp:301] BatchNorm10 needs backward computation.
I0818 10:45:00.925266  2034 net.cpp:301] Convolution10 needs backward computation.
I0818 10:45:00.925271  2034 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0818 10:45:00.925276  2034 net.cpp:301] ReLU9 needs backward computation.
I0818 10:45:00.925279  2034 net.cpp:301] Eltwise4 needs backward computation.
I0818 10:45:00.925284  2034 net.cpp:301] Scale9 needs backward computation.
I0818 10:45:00.925289  2034 net.cpp:301] BatchNorm9 needs backward computation.
I0818 10:45:00.925293  2034 net.cpp:301] Convolution9 needs backward computation.
I0818 10:45:00.925297  2034 net.cpp:301] ReLU8 needs backward computation.
I0818 10:45:00.925302  2034 net.cpp:301] Scale8 needs backward computation.
I0818 10:45:00.925307  2034 net.cpp:301] BatchNorm8 needs backward computation.
I0818 10:45:00.925310  2034 net.cpp:301] Convolution8 needs backward computation.
I0818 10:45:00.925323  2034 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0818 10:45:00.925328  2034 net.cpp:301] ReLU7 needs backward computation.
I0818 10:45:00.925331  2034 net.cpp:301] Eltwise3 needs backward computation.
I0818 10:45:00.925338  2034 net.cpp:301] Scale7 needs backward computation.
I0818 10:45:00.925341  2034 net.cpp:301] BatchNorm7 needs backward computation.
I0818 10:45:00.925345  2034 net.cpp:301] Convolution7 needs backward computation.
I0818 10:45:00.925349  2034 net.cpp:301] ReLU6 needs backward computation.
I0818 10:45:00.925354  2034 net.cpp:301] Scale6 needs backward computation.
I0818 10:45:00.925359  2034 net.cpp:301] BatchNorm6 needs backward computation.
I0818 10:45:00.925362  2034 net.cpp:301] Convolution6 needs backward computation.
I0818 10:45:00.925370  2034 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0818 10:45:00.925376  2034 net.cpp:301] ReLU5 needs backward computation.
I0818 10:45:00.925380  2034 net.cpp:301] Eltwise2 needs backward computation.
I0818 10:45:00.925385  2034 net.cpp:301] Scale5 needs backward computation.
I0818 10:45:00.925390  2034 net.cpp:301] BatchNorm5 needs backward computation.
I0818 10:45:00.925395  2034 net.cpp:301] Convolution5 needs backward computation.
I0818 10:45:00.925400  2034 net.cpp:301] ReLU4 needs backward computation.
I0818 10:45:00.925403  2034 net.cpp:301] Scale4 needs backward computation.
I0818 10:45:00.925407  2034 net.cpp:301] BatchNorm4 needs backward computation.
I0818 10:45:00.925411  2034 net.cpp:301] Convolution4 needs backward computation.
I0818 10:45:00.925416  2034 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0818 10:45:00.925420  2034 net.cpp:301] ReLU3 needs backward computation.
I0818 10:45:00.925426  2034 net.cpp:301] Eltwise1 needs backward computation.
I0818 10:45:00.925431  2034 net.cpp:301] Scale3 needs backward computation.
I0818 10:45:00.925434  2034 net.cpp:301] BatchNorm3 needs backward computation.
I0818 10:45:00.925438  2034 net.cpp:301] Convolution3 needs backward computation.
I0818 10:45:00.925443  2034 net.cpp:301] ReLU2 needs backward computation.
I0818 10:45:00.925447  2034 net.cpp:301] Scale2 needs backward computation.
I0818 10:45:00.925452  2034 net.cpp:301] BatchNorm2 needs backward computation.
I0818 10:45:00.925457  2034 net.cpp:301] Convolution2 needs backward computation.
I0818 10:45:00.925462  2034 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0818 10:45:00.925467  2034 net.cpp:301] ReLU1 needs backward computation.
I0818 10:45:00.925472  2034 net.cpp:301] Scale1 needs backward computation.
I0818 10:45:00.925475  2034 net.cpp:301] BatchNorm1 needs backward computation.
I0818 10:45:00.925479  2034 net.cpp:301] Convolution1 needs backward computation.
I0818 10:45:00.925484  2034 net.cpp:303] Data1 does not need backward computation.
I0818 10:45:00.925488  2034 net.cpp:348] This network produces output SoftmaxWithLoss1
I0818 10:45:00.925580  2034 net.cpp:363] Network initialization done.
I0818 10:45:00.947640  2034 upgrade_proto.cpp:79] Attempting to upgrade batch norm layers using deprecated params: ./test_ResNet_32.prototxt
I0818 10:45:00.947667  2034 upgrade_proto.cpp:82] Successfully upgraded batch norm layers using deprecated params.
I0818 10:45:00.947676  2034 solver.cpp:277] Creating test net (#0) specified by test_net file: ./test_ResNet_32.prototxt
I0818 10:45:00.947801  2034 net.cpp:390] layer_param.include_size():1
I0818 10:45:00.947810  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947816  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947820  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947825  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947829  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947834  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947836  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947840  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947844  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947865  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947868  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947872  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947876  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947880  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947885  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947888  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947892  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947896  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947899  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947903  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947907  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947911  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947916  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947919  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947923  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947927  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947930  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947934  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947938  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947942  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947947  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947950  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947953  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947957  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947962  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947965  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947968  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947973  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947976  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947980  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947984  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947988  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947993  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.947996  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.947999  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948004  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948007  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948011  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948015  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948019  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948022  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948026  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948030  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948035  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948038  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948042  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948045  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948050  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948053  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948057  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948061  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948065  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948068  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948072  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948076  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948081  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948084  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948088  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948098  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948102  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948107  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948110  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948113  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948118  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948122  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948127  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948129  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948133  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948137  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948141  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948144  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948148  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948153  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948156  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948160  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948164  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948168  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948171  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948175  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948179  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948184  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948187  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948190  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948194  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948199  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948202  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948205  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948210  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948213  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948217  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948221  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948225  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948228  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948232  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948236  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948240  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948245  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948248  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948251  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948256  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948259  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948263  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948267  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948271  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948274  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948278  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948282  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948287  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948290  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948293  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948297  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948302  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948305  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948309  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948312  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948316  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948328  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948333  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948335  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948343  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948345  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948349  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948354  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948357  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948361  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948365  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948369  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948372  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948376  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948380  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948384  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948387  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948391  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948395  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948400  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948403  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948406  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948410  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948415  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948418  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948421  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948426  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948429  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948433  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948437  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948441  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948444  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948448  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948452  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948457  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948460  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948464  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948468  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948472  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948475  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948479  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948483  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948487  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948490  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948495  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948498  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948503  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948506  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948510  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948514  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948518  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948521  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948525  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948529  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948534  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948536  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948540  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948544  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948549  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948552  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948562  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948566  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948570  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948575  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948578  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948582  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948586  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948590  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948593  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948597  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948601  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948604  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948608  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948612  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948616  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948621  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948624  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948627  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948632  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948635  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948639  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948643  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948647  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948650  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948654  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948658  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948662  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948667  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948670  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948674  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948678  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948683  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948686  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948689  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948693  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948698  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948701  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948705  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948709  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948712  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948716  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948727  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948731  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948734  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948740  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948742  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948746  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948750  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948755  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948758  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948761  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948765  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948770  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948773  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948777  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948781  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948786  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948788  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948799  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948803  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948807  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948810  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948814  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948818  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948822  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948827  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948829  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948833  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948837  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948842  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948845  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948848  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948853  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948856  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948860  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948864  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948868  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948873  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948875  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948879  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948884  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948887  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948891  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948895  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948899  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948902  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948906  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948910  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948915  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948917  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948922  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948925  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948930  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948933  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948937  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948940  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948945  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948948  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948952  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948956  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948961  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948964  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948968  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948971  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948976  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948979  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948983  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948987  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948990  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.948994  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.948998  2034 net.cpp:390] layer_param.include_size():0
I0818 10:45:00.949002  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.949007  2034 net.cpp:390] layer_param.include_size():1
I0818 10:45:00.949010  2034 net.cpp:391] layer_param.exclude_size():0
I0818 10:45:00.949937  2034 net.cpp:82] Initializing net from parameters: 
name: "resnet_cifar10"
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215684
    mean_value: 125.3
    mean_value: 122.9
    mean_value: 113.8
  }
  data_param {
    source: "/home/lijianfei/datasets/cifar10_40/test_lmdb"
    batch_size: 10
    backend: LMDB
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution1"
  top: "Convolution1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution1"
  bottom: "Convolution3"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Eltwise1"
  bottom: "Convolution5"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Eltwise2"
  bottom: "Convolution7"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Eltwise3"
  bottom: "Convolution9"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution10"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Convolution10"
  top: "Convolution10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Convolution10"
  top: "Convolution11"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 16
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Eltwise4"
  bottom: "Convolution11"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution12"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Convolution14"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Eltwise6"
  bottom: "Convolution16"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution17"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Convolution17"
  top: "Convolution17"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Convolution17"
  top: "Convolution18"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Eltwise7"
  bottom: "Convolution18"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution19"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Convolution19"
  top: "Convolution19"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Convolution19"
  top: "Convolution20"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Eltwise8"
  bottom: "Convolution20"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Eltwise9"
  top: "Convolution21"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Convolution21"
  top: "Convolution21"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Convolution21"
  top: "Convolution22"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Convolution22"
  top: "Convolution22"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Convolution22"
  top: "Convolution22"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise10"
  type: "Eltwise"
  bottom: "Eltwise9"
  bottom: "Convolution22"
  top: "Eltwise10"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Eltwise10"
  top: "Eltwise10"
}
layer {
  name: "Convolution23"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution23"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Convolution23"
  top: "Convolution23"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Convolution23"
  top: "Convolution23"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution24"
  type: "Convolution"
  bottom: "Eltwise10"
  top: "Convolution24"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Convolution24"
  top: "Convolution24"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Convolution24"
  top: "Convolution24"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution24"
  top: "Convolution24"
}
layer {
  name: "Convolution25"
  type: "Convolution"
  bottom: "Convolution24"
  top: "Convolution25"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm25"
  type: "BatchNorm"
  bottom: "Convolution25"
  top: "Convolution25"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale25"
  type: "Scale"
  bottom: "Convolution25"
  top: "Convolution25"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise11"
  type: "Eltwise"
  bottom: "Convolution23"
  bottom: "Convolution25"
  top: "Eltwise11"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU23"
  type: "ReLU"
  bottom: "Eltwise11"
  top: "Eltwise11"
}
layer {
  name: "Convolution26"
  type: "Convolution"
  bottom: "Eltwise11"
  top: "Convolution26"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm26"
  type: "BatchNorm"
  bottom: "Convolution26"
  top: "Convolution26"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale26"
  type: "Scale"
  bottom: "Convolution26"
  top: "Convolution26"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU24"
  type: "ReLU"
  bottom: "Convolution26"
  top: "Convolution26"
}
layer {
  name: "Convolution27"
  type: "Convolution"
  bottom: "Convolution26"
  top: "Convolution27"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm27"
  type: "BatchNorm"
  bottom: "Convolution27"
  top: "Convolution27"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale27"
  type: "Scale"
  bottom: "Convolution27"
  top: "Convolution27"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise12"
  type: "Eltwise"
  bottom: "Eltwise11"
  bottom: "Convolution27"
  top: "Eltwise12"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU25"
  type: "ReLU"
  bottom: "Eltwise12"
  top: "Eltwise12"
}
layer {
  name: "Convolution28"
  type: "Convolution"
  bottom: "Eltwise12"
  top: "Convolution28"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm28"
  type: "BatchNorm"
  bottom: "Convolution28"
  top: "Convolution28"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale28"
  type: "Scale"
  bottom: "Convolution28"
  top: "Convolution28"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU26"
  type: "ReLU"
  bottom: "Convolution28"
  top: "Convolution28"
}
layer {
  name: "Convolution29"
  type: "Convolution"
  bottom: "Convolution28"
  top: "Convolution29"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm29"
  type: "BatchNorm"
  bottom: "Convolution29"
  top: "Convolution29"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale29"
  type: "Scale"
  bottom: "Convolution29"
  top: "Convolution29"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise13"
  type: "Eltwise"
  bottom: "Eltwise12"
  bottom: "Convolution29"
  top: "Eltwise13"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU27"
  type: "ReLU"
  bottom: "Eltwise13"
  top: "Eltwise13"
}
layer {
  name: "Convolution30"
  type: "Convolution"
  bottom: "Eltwise13"
  top: "Convolution30"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm30"
  type: "BatchNorm"
  bottom: "Convolution30"
  top: "Convolution30"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale30"
  type: "Scale"
  bottom: "Convolution30"
  top: "Convolution30"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU28"
  type: "ReLU"
  bottom: "Convolution30"
  top: "Convolution30"
}
layer {
  name: "Convolution31"
  type: "Convolution"
  bottom: "Convolution30"
  top: "Convolution31"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm31"
  type: "BatchNorm"
  bottom: "Convolution31"
  top: "Convolution31"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale31"
  type: "Scale"
  bottom: "Convolution31"
  top: "Convolution31"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise14"
  type: "Eltwise"
  bottom: "Eltwise13"
  bottom: "Convolution31"
  top: "Eltwise14"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU29"
  type: "ReLU"
  bottom: "Eltwise14"
  top: "Eltwise14"
}
layer {
  name: "Convolution32"
  type: "Convolution"
  bottom: "Eltwise14"
  top: "Convolution32"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
layer {
  name: "BatchNorm32"
  type: "BatchNorm"
  bottom: "Convolution32"
  top: "Convolution32"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
}
layer {
  name: "Scale32"
  type: "Scale"
  bottom: "Convolution32"
  top: "Convolution32"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU30"
  type: "ReLU"
  bottom: "Convolution32"
  top: "Convolution32"
}
layer {
  name: "Convolution33"
  type: "Convolution"
  bottom: "Convolution32"
  top: "Convolution33"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "m
I0818 10:45:00.950481  2034 layer_factory.hpp:77] Creating layer Data1
I0818 10:45:00.987027  2034 db_lmdb.cpp:35] Opened lmdb /home/lijianfei/datasets/cifar10_40/test_lmdb
I0818 10:45:00.992475  2034 net.cpp:128] Creating Layer Data1
I0818 10:45:00.992504  2034 net.cpp:522] Data1 -> Data1
I0818 10:45:00.992525  2034 net.cpp:522] Data1 -> Data2
I0818 10:45:00.992764  2034 data_layer.cpp:45] output data size: 10,3,32,32
I0818 10:45:00.993659  2034 net.cpp:172] Setting up Data1
I0818 10:45:00.993688  2034 net.cpp:186] Top shape: 10 3 32 32 (30720)
I0818 10:45:00.993698  2034 net.cpp:186] Top shape: 10 (10)
I0818 10:45:00.993705  2034 net.cpp:194] Memory required for data: 122920
I0818 10:45:00.993711  2034 layer_factory.hpp:77] Creating layer Data2_Data1_1_split
I0818 10:45:00.993723  2034 net.cpp:128] Creating Layer Data2_Data1_1_split
I0818 10:45:00.993733  2034 net.cpp:558] Data2_Data1_1_split <- Data2
I0818 10:45:00.993743  2034 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_0
I0818 10:45:00.993757  2034 net.cpp:522] Data2_Data1_1_split -> Data2_Data1_1_split_1
I0818 10:45:00.994040  2034 net.cpp:172] Setting up Data2_Data1_1_split
I0818 10:45:00.994060  2034 net.cpp:186] Top shape: 10 (10)
I0818 10:45:00.994068  2034 net.cpp:186] Top shape: 10 (10)
I0818 10:45:00.994074  2034 net.cpp:194] Memory required for data: 123000
I0818 10:45:00.994082  2034 layer_factory.hpp:77] Creating layer Convolution1
I0818 10:45:00.994102  2034 net.cpp:128] Creating Layer Convolution1
I0818 10:45:00.994110  2034 net.cpp:558] Convolution1 <- Data1
I0818 10:45:00.994166  2034 net.cpp:522] Convolution1 -> Convolution1
I0818 10:45:01.026468  2034 net.cpp:172] Setting up Convolution1
I0818 10:45:01.026540  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.026558  2034 net.cpp:194] Memory required for data: 778360
I0818 10:45:01.026610  2034 layer_factory.hpp:77] Creating layer BatchNorm1
I0818 10:45:01.026687  2034 net.cpp:128] Creating Layer BatchNorm1
I0818 10:45:01.026716  2034 net.cpp:558] BatchNorm1 <- Convolution1
I0818 10:45:01.026757  2034 net.cpp:509] BatchNorm1 -> Convolution1 (in-place)
I0818 10:45:01.027951  2034 net.cpp:172] Setting up BatchNorm1
I0818 10:45:01.027994  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.028012  2034 net.cpp:194] Memory required for data: 1433720
I0818 10:45:01.028069  2034 layer_factory.hpp:77] Creating layer Scale1
I0818 10:45:01.028106  2034 net.cpp:128] Creating Layer Scale1
I0818 10:45:01.028129  2034 net.cpp:558] Scale1 <- Convolution1
I0818 10:45:01.028154  2034 net.cpp:509] Scale1 -> Convolution1 (in-place)
I0818 10:45:01.028384  2034 layer_factory.hpp:77] Creating layer Scale1
I0818 10:45:01.029033  2034 net.cpp:172] Setting up Scale1
I0818 10:45:01.029076  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.029093  2034 net.cpp:194] Memory required for data: 2089080
I0818 10:45:01.029125  2034 layer_factory.hpp:77] Creating layer ReLU1
I0818 10:45:01.029212  2034 net.cpp:128] Creating Layer ReLU1
I0818 10:45:01.029238  2034 net.cpp:558] ReLU1 <- Convolution1
I0818 10:45:01.029269  2034 net.cpp:509] ReLU1 -> Convolution1 (in-place)
I0818 10:45:01.030246  2034 net.cpp:172] Setting up ReLU1
I0818 10:45:01.030297  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.030315  2034 net.cpp:194] Memory required for data: 2744440
I0818 10:45:01.030334  2034 layer_factory.hpp:77] Creating layer Convolution1_ReLU1_0_split
I0818 10:45:01.030369  2034 net.cpp:128] Creating Layer Convolution1_ReLU1_0_split
I0818 10:45:01.030396  2034 net.cpp:558] Convolution1_ReLU1_0_split <- Convolution1
I0818 10:45:01.030431  2034 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_0
I0818 10:45:01.030472  2034 net.cpp:522] Convolution1_ReLU1_0_split -> Convolution1_ReLU1_0_split_1
I0818 10:45:01.030716  2034 net.cpp:172] Setting up Convolution1_ReLU1_0_split
I0818 10:45:01.030759  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.030786  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.030802  2034 net.cpp:194] Memory required for data: 4055160
I0818 10:45:01.030819  2034 layer_factory.hpp:77] Creating layer Convolution2
I0818 10:45:01.030875  2034 net.cpp:128] Creating Layer Convolution2
I0818 10:45:01.030895  2034 net.cpp:558] Convolution2 <- Convolution1_ReLU1_0_split_0
I0818 10:45:01.030936  2034 net.cpp:522] Convolution2 -> Convolution2
I0818 10:45:01.035275  2034 net.cpp:172] Setting up Convolution2
I0818 10:45:01.035301  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.035305  2034 net.cpp:194] Memory required for data: 4710520
I0818 10:45:01.035320  2034 layer_factory.hpp:77] Creating layer BatchNorm2
I0818 10:45:01.035331  2034 net.cpp:128] Creating Layer BatchNorm2
I0818 10:45:01.035336  2034 net.cpp:558] BatchNorm2 <- Convolution2
I0818 10:45:01.035346  2034 net.cpp:509] BatchNorm2 -> Convolution2 (in-place)
I0818 10:45:01.035634  2034 net.cpp:172] Setting up BatchNorm2
I0818 10:45:01.035645  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.035650  2034 net.cpp:194] Memory required for data: 5365880
I0818 10:45:01.035658  2034 layer_factory.hpp:77] Creating layer Scale2
I0818 10:45:01.035665  2034 net.cpp:128] Creating Layer Scale2
I0818 10:45:01.035670  2034 net.cpp:558] Scale2 <- Convolution2
I0818 10:45:01.035678  2034 net.cpp:509] Scale2 -> Convolution2 (in-place)
I0818 10:45:01.035727  2034 layer_factory.hpp:77] Creating layer Scale2
I0818 10:45:01.035888  2034 net.cpp:172] Setting up Scale2
I0818 10:45:01.035898  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.035903  2034 net.cpp:194] Memory required for data: 6021240
I0818 10:45:01.035910  2034 layer_factory.hpp:77] Creating layer ReLU2
I0818 10:45:01.035917  2034 net.cpp:128] Creating Layer ReLU2
I0818 10:45:01.035921  2034 net.cpp:558] ReLU2 <- Convolution2
I0818 10:45:01.035929  2034 net.cpp:509] ReLU2 -> Convolution2 (in-place)
I0818 10:45:01.036175  2034 net.cpp:172] Setting up ReLU2
I0818 10:45:01.036187  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.036192  2034 net.cpp:194] Memory required for data: 6676600
I0818 10:45:01.036196  2034 layer_factory.hpp:77] Creating layer Convolution3
I0818 10:45:01.036208  2034 net.cpp:128] Creating Layer Convolution3
I0818 10:45:01.036213  2034 net.cpp:558] Convolution3 <- Convolution2
I0818 10:45:01.036223  2034 net.cpp:522] Convolution3 -> Convolution3
I0818 10:45:01.037569  2034 net.cpp:172] Setting up Convolution3
I0818 10:45:01.037597  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.037602  2034 net.cpp:194] Memory required for data: 7331960
I0818 10:45:01.037614  2034 layer_factory.hpp:77] Creating layer BatchNorm3
I0818 10:45:01.037622  2034 net.cpp:128] Creating Layer BatchNorm3
I0818 10:45:01.037627  2034 net.cpp:558] BatchNorm3 <- Convolution3
I0818 10:45:01.037639  2034 net.cpp:509] BatchNorm3 -> Convolution3 (in-place)
I0818 10:45:01.037926  2034 net.cpp:172] Setting up BatchNorm3
I0818 10:45:01.037951  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.037956  2034 net.cpp:194] Memory required for data: 7987320
I0818 10:45:01.037968  2034 layer_factory.hpp:77] Creating layer Scale3
I0818 10:45:01.037979  2034 net.cpp:128] Creating Layer Scale3
I0818 10:45:01.037984  2034 net.cpp:558] Scale3 <- Convolution3
I0818 10:45:01.037991  2034 net.cpp:509] Scale3 -> Convolution3 (in-place)
I0818 10:45:01.038043  2034 layer_factory.hpp:77] Creating layer Scale3
I0818 10:45:01.038203  2034 net.cpp:172] Setting up Scale3
I0818 10:45:01.038214  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.038218  2034 net.cpp:194] Memory required for data: 8642680
I0818 10:45:01.038226  2034 layer_factory.hpp:77] Creating layer Eltwise1
I0818 10:45:01.038236  2034 net.cpp:128] Creating Layer Eltwise1
I0818 10:45:01.038240  2034 net.cpp:558] Eltwise1 <- Convolution1_ReLU1_0_split_1
I0818 10:45:01.038246  2034 net.cpp:558] Eltwise1 <- Convolution3
I0818 10:45:01.038252  2034 net.cpp:522] Eltwise1 -> Eltwise1
I0818 10:45:01.038282  2034 net.cpp:172] Setting up Eltwise1
I0818 10:45:01.038293  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.038297  2034 net.cpp:194] Memory required for data: 9298040
I0818 10:45:01.038301  2034 layer_factory.hpp:77] Creating layer ReLU3
I0818 10:45:01.038307  2034 net.cpp:128] Creating Layer ReLU3
I0818 10:45:01.038312  2034 net.cpp:558] ReLU3 <- Eltwise1
I0818 10:45:01.038319  2034 net.cpp:509] ReLU3 -> Eltwise1 (in-place)
I0818 10:45:01.038774  2034 net.cpp:172] Setting up ReLU3
I0818 10:45:01.038795  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.038800  2034 net.cpp:194] Memory required for data: 9953400
I0818 10:45:01.038805  2034 layer_factory.hpp:77] Creating layer Eltwise1_ReLU3_0_split
I0818 10:45:01.038816  2034 net.cpp:128] Creating Layer Eltwise1_ReLU3_0_split
I0818 10:45:01.038821  2034 net.cpp:558] Eltwise1_ReLU3_0_split <- Eltwise1
I0818 10:45:01.038828  2034 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_0
I0818 10:45:01.038837  2034 net.cpp:522] Eltwise1_ReLU3_0_split -> Eltwise1_ReLU3_0_split_1
I0818 10:45:01.038895  2034 net.cpp:172] Setting up Eltwise1_ReLU3_0_split
I0818 10:45:01.038905  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.038911  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.038915  2034 net.cpp:194] Memory required for data: 11264120
I0818 10:45:01.038919  2034 layer_factory.hpp:77] Creating layer Convolution4
I0818 10:45:01.038931  2034 net.cpp:128] Creating Layer Convolution4
I0818 10:45:01.038938  2034 net.cpp:558] Convolution4 <- Eltwise1_ReLU3_0_split_0
I0818 10:45:01.038944  2034 net.cpp:522] Convolution4 -> Convolution4
I0818 10:45:01.040293  2034 net.cpp:172] Setting up Convolution4
I0818 10:45:01.040318  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.040323  2034 net.cpp:194] Memory required for data: 11919480
I0818 10:45:01.040333  2034 layer_factory.hpp:77] Creating layer BatchNorm4
I0818 10:45:01.040341  2034 net.cpp:128] Creating Layer BatchNorm4
I0818 10:45:01.040346  2034 net.cpp:558] BatchNorm4 <- Convolution4
I0818 10:45:01.040355  2034 net.cpp:509] BatchNorm4 -> Convolution4 (in-place)
I0818 10:45:01.040640  2034 net.cpp:172] Setting up BatchNorm4
I0818 10:45:01.040652  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.040657  2034 net.cpp:194] Memory required for data: 12574840
I0818 10:45:01.040666  2034 layer_factory.hpp:77] Creating layer Scale4
I0818 10:45:01.040673  2034 net.cpp:128] Creating Layer Scale4
I0818 10:45:01.040678  2034 net.cpp:558] Scale4 <- Convolution4
I0818 10:45:01.040685  2034 net.cpp:509] Scale4 -> Convolution4 (in-place)
I0818 10:45:01.040733  2034 layer_factory.hpp:77] Creating layer Scale4
I0818 10:45:01.040889  2034 net.cpp:172] Setting up Scale4
I0818 10:45:01.040900  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.040904  2034 net.cpp:194] Memory required for data: 13230200
I0818 10:45:01.040912  2034 layer_factory.hpp:77] Creating layer ReLU4
I0818 10:45:01.040933  2034 net.cpp:128] Creating Layer ReLU4
I0818 10:45:01.040938  2034 net.cpp:558] ReLU4 <- Convolution4
I0818 10:45:01.040946  2034 net.cpp:509] ReLU4 -> Convolution4 (in-place)
I0818 10:45:01.041198  2034 net.cpp:172] Setting up ReLU4
I0818 10:45:01.041213  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.041216  2034 net.cpp:194] Memory required for data: 13885560
I0818 10:45:01.041221  2034 layer_factory.hpp:77] Creating layer Convolution5
I0818 10:45:01.041234  2034 net.cpp:128] Creating Layer Convolution5
I0818 10:45:01.041239  2034 net.cpp:558] Convolution5 <- Convolution4
I0818 10:45:01.041249  2034 net.cpp:522] Convolution5 -> Convolution5
I0818 10:45:01.042605  2034 net.cpp:172] Setting up Convolution5
I0818 10:45:01.042630  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.042634  2034 net.cpp:194] Memory required for data: 14540920
I0818 10:45:01.042644  2034 layer_factory.hpp:77] Creating layer BatchNorm5
I0818 10:45:01.042662  2034 net.cpp:128] Creating Layer BatchNorm5
I0818 10:45:01.042668  2034 net.cpp:558] BatchNorm5 <- Convolution5
I0818 10:45:01.042677  2034 net.cpp:509] BatchNorm5 -> Convolution5 (in-place)
I0818 10:45:01.042970  2034 net.cpp:172] Setting up BatchNorm5
I0818 10:45:01.042981  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.042985  2034 net.cpp:194] Memory required for data: 15196280
I0818 10:45:01.043000  2034 layer_factory.hpp:77] Creating layer Scale5
I0818 10:45:01.043007  2034 net.cpp:128] Creating Layer Scale5
I0818 10:45:01.043011  2034 net.cpp:558] Scale5 <- Convolution5
I0818 10:45:01.043017  2034 net.cpp:509] Scale5 -> Convolution5 (in-place)
I0818 10:45:01.043069  2034 layer_factory.hpp:77] Creating layer Scale5
I0818 10:45:01.043231  2034 net.cpp:172] Setting up Scale5
I0818 10:45:01.043242  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.043246  2034 net.cpp:194] Memory required for data: 15851640
I0818 10:45:01.043254  2034 layer_factory.hpp:77] Creating layer Eltwise2
I0818 10:45:01.043263  2034 net.cpp:128] Creating Layer Eltwise2
I0818 10:45:01.043268  2034 net.cpp:558] Eltwise2 <- Eltwise1_ReLU3_0_split_1
I0818 10:45:01.043273  2034 net.cpp:558] Eltwise2 <- Convolution5
I0818 10:45:01.043280  2034 net.cpp:522] Eltwise2 -> Eltwise2
I0818 10:45:01.043309  2034 net.cpp:172] Setting up Eltwise2
I0818 10:45:01.043316  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.043320  2034 net.cpp:194] Memory required for data: 16507000
I0818 10:45:01.043324  2034 layer_factory.hpp:77] Creating layer ReLU5
I0818 10:45:01.043334  2034 net.cpp:128] Creating Layer ReLU5
I0818 10:45:01.043339  2034 net.cpp:558] ReLU5 <- Eltwise2
I0818 10:45:01.043344  2034 net.cpp:509] ReLU5 -> Eltwise2 (in-place)
I0818 10:45:01.043589  2034 net.cpp:172] Setting up ReLU5
I0818 10:45:01.043603  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.043607  2034 net.cpp:194] Memory required for data: 17162360
I0818 10:45:01.043612  2034 layer_factory.hpp:77] Creating layer Eltwise2_ReLU5_0_split
I0818 10:45:01.043622  2034 net.cpp:128] Creating Layer Eltwise2_ReLU5_0_split
I0818 10:45:01.043627  2034 net.cpp:558] Eltwise2_ReLU5_0_split <- Eltwise2
I0818 10:45:01.043633  2034 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_0
I0818 10:45:01.043643  2034 net.cpp:522] Eltwise2_ReLU5_0_split -> Eltwise2_ReLU5_0_split_1
I0818 10:45:01.043699  2034 net.cpp:172] Setting up Eltwise2_ReLU5_0_split
I0818 10:45:01.043715  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.043720  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.043725  2034 net.cpp:194] Memory required for data: 18473080
I0818 10:45:01.043728  2034 layer_factory.hpp:77] Creating layer Convolution6
I0818 10:45:01.043740  2034 net.cpp:128] Creating Layer Convolution6
I0818 10:45:01.043745  2034 net.cpp:558] Convolution6 <- Eltwise2_ReLU5_0_split_0
I0818 10:45:01.043754  2034 net.cpp:522] Convolution6 -> Convolution6
I0818 10:45:01.045097  2034 net.cpp:172] Setting up Convolution6
I0818 10:45:01.045122  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.045140  2034 net.cpp:194] Memory required for data: 19128440
I0818 10:45:01.045151  2034 layer_factory.hpp:77] Creating layer BatchNorm6
I0818 10:45:01.045166  2034 net.cpp:128] Creating Layer BatchNorm6
I0818 10:45:01.045171  2034 net.cpp:558] BatchNorm6 <- Convolution6
I0818 10:45:01.045179  2034 net.cpp:509] BatchNorm6 -> Convolution6 (in-place)
I0818 10:45:01.045467  2034 net.cpp:172] Setting up BatchNorm6
I0818 10:45:01.045480  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.045483  2034 net.cpp:194] Memory required for data: 19783800
I0818 10:45:01.045493  2034 layer_factory.hpp:77] Creating layer Scale6
I0818 10:45:01.045500  2034 net.cpp:128] Creating Layer Scale6
I0818 10:45:01.045505  2034 net.cpp:558] Scale6 <- Convolution6
I0818 10:45:01.045511  2034 net.cpp:509] Scale6 -> Convolution6 (in-place)
I0818 10:45:01.045559  2034 layer_factory.hpp:77] Creating layer Scale6
I0818 10:45:01.045717  2034 net.cpp:172] Setting up Scale6
I0818 10:45:01.045728  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.045732  2034 net.cpp:194] Memory required for data: 20439160
I0818 10:45:01.045740  2034 layer_factory.hpp:77] Creating layer ReLU6
I0818 10:45:01.045748  2034 net.cpp:128] Creating Layer ReLU6
I0818 10:45:01.045753  2034 net.cpp:558] ReLU6 <- Convolution6
I0818 10:45:01.045759  2034 net.cpp:509] ReLU6 -> Convolution6 (in-place)
I0818 10:45:01.046010  2034 net.cpp:172] Setting up ReLU6
I0818 10:45:01.046025  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.046028  2034 net.cpp:194] Memory required for data: 21094520
I0818 10:45:01.046033  2034 layer_factory.hpp:77] Creating layer Convolution7
I0818 10:45:01.046046  2034 net.cpp:128] Creating Layer Convolution7
I0818 10:45:01.046051  2034 net.cpp:558] Convolution7 <- Convolution6
I0818 10:45:01.046064  2034 net.cpp:522] Convolution7 -> Convolution7
I0818 10:45:01.047426  2034 net.cpp:172] Setting up Convolution7
I0818 10:45:01.047452  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.047456  2034 net.cpp:194] Memory required for data: 21749880
I0818 10:45:01.047466  2034 layer_factory.hpp:77] Creating layer BatchNorm7
I0818 10:45:01.047480  2034 net.cpp:128] Creating Layer BatchNorm7
I0818 10:45:01.047485  2034 net.cpp:558] BatchNorm7 <- Convolution7
I0818 10:45:01.047492  2034 net.cpp:509] BatchNorm7 -> Convolution7 (in-place)
I0818 10:45:01.047782  2034 net.cpp:172] Setting up BatchNorm7
I0818 10:45:01.047793  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.047797  2034 net.cpp:194] Memory required for data: 22405240
I0818 10:45:01.047807  2034 layer_factory.hpp:77] Creating layer Scale7
I0818 10:45:01.047814  2034 net.cpp:128] Creating Layer Scale7
I0818 10:45:01.047818  2034 net.cpp:558] Scale7 <- Convolution7
I0818 10:45:01.047827  2034 net.cpp:509] Scale7 -> Convolution7 (in-place)
I0818 10:45:01.047876  2034 layer_factory.hpp:77] Creating layer Scale7
I0818 10:45:01.048038  2034 net.cpp:172] Setting up Scale7
I0818 10:45:01.048048  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.048054  2034 net.cpp:194] Memory required for data: 23060600
I0818 10:45:01.048061  2034 layer_factory.hpp:77] Creating layer Eltwise3
I0818 10:45:01.048069  2034 net.cpp:128] Creating Layer Eltwise3
I0818 10:45:01.048074  2034 net.cpp:558] Eltwise3 <- Eltwise2_ReLU5_0_split_1
I0818 10:45:01.048079  2034 net.cpp:558] Eltwise3 <- Convolution7
I0818 10:45:01.048086  2034 net.cpp:522] Eltwise3 -> Eltwise3
I0818 10:45:01.048115  2034 net.cpp:172] Setting up Eltwise3
I0818 10:45:01.048125  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.048128  2034 net.cpp:194] Memory required for data: 23715960
I0818 10:45:01.048132  2034 layer_factory.hpp:77] Creating layer ReLU7
I0818 10:45:01.048138  2034 net.cpp:128] Creating Layer ReLU7
I0818 10:45:01.048142  2034 net.cpp:558] ReLU7 <- Eltwise3
I0818 10:45:01.048148  2034 net.cpp:509] ReLU7 -> Eltwise3 (in-place)
I0818 10:45:01.048588  2034 net.cpp:172] Setting up ReLU7
I0818 10:45:01.048619  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.048624  2034 net.cpp:194] Memory required for data: 24371320
I0818 10:45:01.048629  2034 layer_factory.hpp:77] Creating layer Eltwise3_ReLU7_0_split
I0818 10:45:01.048638  2034 net.cpp:128] Creating Layer Eltwise3_ReLU7_0_split
I0818 10:45:01.048645  2034 net.cpp:558] Eltwise3_ReLU7_0_split <- Eltwise3
I0818 10:45:01.048651  2034 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_0
I0818 10:45:01.048661  2034 net.cpp:522] Eltwise3_ReLU7_0_split -> Eltwise3_ReLU7_0_split_1
I0818 10:45:01.048719  2034 net.cpp:172] Setting up Eltwise3_ReLU7_0_split
I0818 10:45:01.048727  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.048732  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.048737  2034 net.cpp:194] Memory required for data: 25682040
I0818 10:45:01.048740  2034 layer_factory.hpp:77] Creating layer Convolution8
I0818 10:45:01.048751  2034 net.cpp:128] Creating Layer Convolution8
I0818 10:45:01.048756  2034 net.cpp:558] Convolution8 <- Eltwise3_ReLU7_0_split_0
I0818 10:45:01.048766  2034 net.cpp:522] Convolution8 -> Convolution8
I0818 10:45:01.050134  2034 net.cpp:172] Setting up Convolution8
I0818 10:45:01.050156  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.050161  2034 net.cpp:194] Memory required for data: 26337400
I0818 10:45:01.050170  2034 layer_factory.hpp:77] Creating layer BatchNorm8
I0818 10:45:01.050181  2034 net.cpp:128] Creating Layer BatchNorm8
I0818 10:45:01.050187  2034 net.cpp:558] BatchNorm8 <- Convolution8
I0818 10:45:01.050197  2034 net.cpp:509] BatchNorm8 -> Convolution8 (in-place)
I0818 10:45:01.050490  2034 net.cpp:172] Setting up BatchNorm8
I0818 10:45:01.050500  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.050505  2034 net.cpp:194] Memory required for data: 26992760
I0818 10:45:01.050515  2034 layer_factory.hpp:77] Creating layer Scale8
I0818 10:45:01.050523  2034 net.cpp:128] Creating Layer Scale8
I0818 10:45:01.050528  2034 net.cpp:558] Scale8 <- Convolution8
I0818 10:45:01.050534  2034 net.cpp:509] Scale8 -> Convolution8 (in-place)
I0818 10:45:01.050586  2034 layer_factory.hpp:77] Creating layer Scale8
I0818 10:45:01.050756  2034 net.cpp:172] Setting up Scale8
I0818 10:45:01.050768  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.050773  2034 net.cpp:194] Memory required for data: 27648120
I0818 10:45:01.050781  2034 layer_factory.hpp:77] Creating layer ReLU8
I0818 10:45:01.050787  2034 net.cpp:128] Creating Layer ReLU8
I0818 10:45:01.050791  2034 net.cpp:558] ReLU8 <- Convolution8
I0818 10:45:01.050799  2034 net.cpp:509] ReLU8 -> Convolution8 (in-place)
I0818 10:45:01.051048  2034 net.cpp:172] Setting up ReLU8
I0818 10:45:01.051062  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.051067  2034 net.cpp:194] Memory required for data: 28303480
I0818 10:45:01.051071  2034 layer_factory.hpp:77] Creating layer Convolution9
I0818 10:45:01.051084  2034 net.cpp:128] Creating Layer Convolution9
I0818 10:45:01.051090  2034 net.cpp:558] Convolution9 <- Convolution8
I0818 10:45:01.051098  2034 net.cpp:522] Convolution9 -> Convolution9
I0818 10:45:01.052644  2034 net.cpp:172] Setting up Convolution9
I0818 10:45:01.052670  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.052675  2034 net.cpp:194] Memory required for data: 28958840
I0818 10:45:01.052686  2034 layer_factory.hpp:77] Creating layer BatchNorm9
I0818 10:45:01.052693  2034 net.cpp:128] Creating Layer BatchNorm9
I0818 10:45:01.052698  2034 net.cpp:558] BatchNorm9 <- Convolution9
I0818 10:45:01.052711  2034 net.cpp:509] BatchNorm9 -> Convolution9 (in-place)
I0818 10:45:01.053019  2034 net.cpp:172] Setting up BatchNorm9
I0818 10:45:01.053030  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053035  2034 net.cpp:194] Memory required for data: 29614200
I0818 10:45:01.053045  2034 layer_factory.hpp:77] Creating layer Scale9
I0818 10:45:01.053050  2034 net.cpp:128] Creating Layer Scale9
I0818 10:45:01.053056  2034 net.cpp:558] Scale9 <- Convolution9
I0818 10:45:01.053078  2034 net.cpp:509] Scale9 -> Convolution9 (in-place)
I0818 10:45:01.053130  2034 layer_factory.hpp:77] Creating layer Scale9
I0818 10:45:01.053297  2034 net.cpp:172] Setting up Scale9
I0818 10:45:01.053308  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053313  2034 net.cpp:194] Memory required for data: 30269560
I0818 10:45:01.053320  2034 layer_factory.hpp:77] Creating layer Eltwise4
I0818 10:45:01.053328  2034 net.cpp:128] Creating Layer Eltwise4
I0818 10:45:01.053333  2034 net.cpp:558] Eltwise4 <- Eltwise3_ReLU7_0_split_1
I0818 10:45:01.053337  2034 net.cpp:558] Eltwise4 <- Convolution9
I0818 10:45:01.053346  2034 net.cpp:522] Eltwise4 -> Eltwise4
I0818 10:45:01.053375  2034 net.cpp:172] Setting up Eltwise4
I0818 10:45:01.053385  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053388  2034 net.cpp:194] Memory required for data: 30924920
I0818 10:45:01.053392  2034 layer_factory.hpp:77] Creating layer ReLU9
I0818 10:45:01.053398  2034 net.cpp:128] Creating Layer ReLU9
I0818 10:45:01.053402  2034 net.cpp:558] ReLU9 <- Eltwise4
I0818 10:45:01.053408  2034 net.cpp:509] ReLU9 -> Eltwise4 (in-place)
I0818 10:45:01.053655  2034 net.cpp:172] Setting up ReLU9
I0818 10:45:01.053669  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053673  2034 net.cpp:194] Memory required for data: 31580280
I0818 10:45:01.053678  2034 layer_factory.hpp:77] Creating layer Eltwise4_ReLU9_0_split
I0818 10:45:01.053688  2034 net.cpp:128] Creating Layer Eltwise4_ReLU9_0_split
I0818 10:45:01.053692  2034 net.cpp:558] Eltwise4_ReLU9_0_split <- Eltwise4
I0818 10:45:01.053699  2034 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_0
I0818 10:45:01.053709  2034 net.cpp:522] Eltwise4_ReLU9_0_split -> Eltwise4_ReLU9_0_split_1
I0818 10:45:01.053766  2034 net.cpp:172] Setting up Eltwise4_ReLU9_0_split
I0818 10:45:01.053776  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053782  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.053786  2034 net.cpp:194] Memory required for data: 32891000
I0818 10:45:01.053791  2034 layer_factory.hpp:77] Creating layer Convolution10
I0818 10:45:01.053804  2034 net.cpp:128] Creating Layer Convolution10
I0818 10:45:01.053809  2034 net.cpp:558] Convolution10 <- Eltwise4_ReLU9_0_split_0
I0818 10:45:01.053817  2034 net.cpp:522] Convolution10 -> Convolution10
I0818 10:45:01.055196  2034 net.cpp:172] Setting up Convolution10
I0818 10:45:01.055222  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.055225  2034 net.cpp:194] Memory required for data: 33546360
I0818 10:45:01.055245  2034 layer_factory.hpp:77] Creating layer BatchNorm10
I0818 10:45:01.055261  2034 net.cpp:128] Creating Layer BatchNorm10
I0818 10:45:01.055266  2034 net.cpp:558] BatchNorm10 <- Convolution10
I0818 10:45:01.055274  2034 net.cpp:509] BatchNorm10 -> Convolution10 (in-place)
I0818 10:45:01.055570  2034 net.cpp:172] Setting up BatchNorm10
I0818 10:45:01.055582  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.055585  2034 net.cpp:194] Memory required for data: 34201720
I0818 10:45:01.055595  2034 layer_factory.hpp:77] Creating layer Scale10
I0818 10:45:01.055604  2034 net.cpp:128] Creating Layer Scale10
I0818 10:45:01.055609  2034 net.cpp:558] Scale10 <- Convolution10
I0818 10:45:01.055615  2034 net.cpp:509] Scale10 -> Convolution10 (in-place)
I0818 10:45:01.055668  2034 layer_factory.hpp:77] Creating layer Scale10
I0818 10:45:01.055831  2034 net.cpp:172] Setting up Scale10
I0818 10:45:01.055842  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.055846  2034 net.cpp:194] Memory required for data: 34857080
I0818 10:45:01.055855  2034 layer_factory.hpp:77] Creating layer ReLU10
I0818 10:45:01.055862  2034 net.cpp:128] Creating Layer ReLU10
I0818 10:45:01.055867  2034 net.cpp:558] ReLU10 <- Convolution10
I0818 10:45:01.055873  2034 net.cpp:509] ReLU10 -> Convolution10 (in-place)
I0818 10:45:01.056308  2034 net.cpp:172] Setting up ReLU10
I0818 10:45:01.056330  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.056357  2034 net.cpp:194] Memory required for data: 35512440
I0818 10:45:01.056365  2034 layer_factory.hpp:77] Creating layer Convolution11
I0818 10:45:01.056378  2034 net.cpp:128] Creating Layer Convolution11
I0818 10:45:01.056383  2034 net.cpp:558] Convolution11 <- Convolution10
I0818 10:45:01.056393  2034 net.cpp:522] Convolution11 -> Convolution11
I0818 10:45:01.057761  2034 net.cpp:172] Setting up Convolution11
I0818 10:45:01.057786  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.057791  2034 net.cpp:194] Memory required for data: 36167800
I0818 10:45:01.057801  2034 layer_factory.hpp:77] Creating layer BatchNorm11
I0818 10:45:01.057813  2034 net.cpp:128] Creating Layer BatchNorm11
I0818 10:45:01.057818  2034 net.cpp:558] BatchNorm11 <- Convolution11
I0818 10:45:01.057828  2034 net.cpp:509] BatchNorm11 -> Convolution11 (in-place)
I0818 10:45:01.058121  2034 net.cpp:172] Setting up BatchNorm11
I0818 10:45:01.058132  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058136  2034 net.cpp:194] Memory required for data: 36823160
I0818 10:45:01.058146  2034 layer_factory.hpp:77] Creating layer Scale11
I0818 10:45:01.058153  2034 net.cpp:128] Creating Layer Scale11
I0818 10:45:01.058157  2034 net.cpp:558] Scale11 <- Convolution11
I0818 10:45:01.058163  2034 net.cpp:509] Scale11 -> Convolution11 (in-place)
I0818 10:45:01.058217  2034 layer_factory.hpp:77] Creating layer Scale11
I0818 10:45:01.058384  2034 net.cpp:172] Setting up Scale11
I0818 10:45:01.058395  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058399  2034 net.cpp:194] Memory required for data: 37478520
I0818 10:45:01.058408  2034 layer_factory.hpp:77] Creating layer Eltwise5
I0818 10:45:01.058413  2034 net.cpp:128] Creating Layer Eltwise5
I0818 10:45:01.058418  2034 net.cpp:558] Eltwise5 <- Eltwise4_ReLU9_0_split_1
I0818 10:45:01.058423  2034 net.cpp:558] Eltwise5 <- Convolution11
I0818 10:45:01.058435  2034 net.cpp:522] Eltwise5 -> Eltwise5
I0818 10:45:01.058465  2034 net.cpp:172] Setting up Eltwise5
I0818 10:45:01.058475  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058480  2034 net.cpp:194] Memory required for data: 38133880
I0818 10:45:01.058483  2034 layer_factory.hpp:77] Creating layer ReLU11
I0818 10:45:01.058491  2034 net.cpp:128] Creating Layer ReLU11
I0818 10:45:01.058496  2034 net.cpp:558] ReLU11 <- Eltwise5
I0818 10:45:01.058503  2034 net.cpp:509] ReLU11 -> Eltwise5 (in-place)
I0818 10:45:01.058756  2034 net.cpp:172] Setting up ReLU11
I0818 10:45:01.058771  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058776  2034 net.cpp:194] Memory required for data: 38789240
I0818 10:45:01.058781  2034 layer_factory.hpp:77] Creating layer Eltwise5_ReLU11_0_split
I0818 10:45:01.058790  2034 net.cpp:128] Creating Layer Eltwise5_ReLU11_0_split
I0818 10:45:01.058795  2034 net.cpp:558] Eltwise5_ReLU11_0_split <- Eltwise5
I0818 10:45:01.058804  2034 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_0
I0818 10:45:01.058814  2034 net.cpp:522] Eltwise5_ReLU11_0_split -> Eltwise5_ReLU11_0_split_1
I0818 10:45:01.058868  2034 net.cpp:172] Setting up Eltwise5_ReLU11_0_split
I0818 10:45:01.058881  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058887  2034 net.cpp:186] Top shape: 10 16 32 32 (163840)
I0818 10:45:01.058892  2034 net.cpp:194] Memory required for data: 40099960
I0818 10:45:01.058895  2034 layer_factory.hpp:77] Creating layer Convolution12
I0818 10:45:01.058907  2034 net.cpp:128] Creating Layer Convolution12
I0818 10:45:01.058912  2034 net.cpp:558] Convolution12 <- Eltwise5_ReLU11_0_split_0
I0818 10:45:01.058921  2034 net.cpp:522] Convolution12 -> Convolution12
I0818 10:45:01.060263  2034 net.cpp:172] Setting up Convolution12
I0818 10:45:01.060288  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.060293  2034 net.cpp:194] Memory required for data: 40427640
I0818 10:45:01.060303  2034 layer_factory.hpp:77] Creating layer BatchNorm12
I0818 10:45:01.060313  2034 net.cpp:128] Creating Layer BatchNorm12
I0818 10:45:01.060333  2034 net.cpp:558] BatchNorm12 <- Convolution12
I0818 10:45:01.060343  2034 net.cpp:509] BatchNorm12 -> Convolution12 (in-place)
I0818 10:45:01.060633  2034 net.cpp:172] Setting up BatchNorm12
I0818 10:45:01.060645  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.060649  2034 net.cpp:194] Memory required for data: 40755320
I0818 10:45:01.060659  2034 layer_factory.hpp:77] Creating layer Scale12
I0818 10:45:01.060668  2034 net.cpp:128] Creating Layer Scale12
I0818 10:45:01.060673  2034 net.cpp:558] Scale12 <- Convolution12
I0818 10:45:01.060678  2034 net.cpp:509] Scale12 -> Convolution12 (in-place)
I0818 10:45:01.060730  2034 layer_factory.hpp:77] Creating layer Scale12
I0818 10:45:01.060894  2034 net.cpp:172] Setting up Scale12
I0818 10:45:01.060905  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.060909  2034 net.cpp:194] Memory required for data: 41083000
I0818 10:45:01.060917  2034 layer_factory.hpp:77] Creating layer Convolution13
I0818 10:45:01.060933  2034 net.cpp:128] Creating Layer Convolution13
I0818 10:45:01.060940  2034 net.cpp:558] Convolution13 <- Eltwise5_ReLU11_0_split_1
I0818 10:45:01.060947  2034 net.cpp:522] Convolution13 -> Convolution13
I0818 10:45:01.062361  2034 net.cpp:172] Setting up Convolution13
I0818 10:45:01.062386  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.062389  2034 net.cpp:194] Memory required for data: 41410680
I0818 10:45:01.062399  2034 layer_factory.hpp:77] Creating layer BatchNorm13
I0818 10:45:01.062410  2034 net.cpp:128] Creating Layer BatchNorm13
I0818 10:45:01.062415  2034 net.cpp:558] BatchNorm13 <- Convolution13
I0818 10:45:01.062424  2034 net.cpp:509] BatchNorm13 -> Convolution13 (in-place)
I0818 10:45:01.062719  2034 net.cpp:172] Setting up BatchNorm13
I0818 10:45:01.062732  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.062738  2034 net.cpp:194] Memory required for data: 41738360
I0818 10:45:01.062748  2034 layer_factory.hpp:77] Creating layer Scale13
I0818 10:45:01.062757  2034 net.cpp:128] Creating Layer Scale13
I0818 10:45:01.062762  2034 net.cpp:558] Scale13 <- Convolution13
I0818 10:45:01.062767  2034 net.cpp:509] Scale13 -> Convolution13 (in-place)
I0818 10:45:01.062820  2034 layer_factory.hpp:77] Creating layer Scale13
I0818 10:45:01.062986  2034 net.cpp:172] Setting up Scale13
I0818 10:45:01.062996  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.063000  2034 net.cpp:194] Memory required for data: 42066040
I0818 10:45:01.063009  2034 layer_factory.hpp:77] Creating layer ReLU12
I0818 10:45:01.063019  2034 net.cpp:128] Creating Layer ReLU12
I0818 10:45:01.063024  2034 net.cpp:558] ReLU12 <- Convolution13
I0818 10:45:01.063030  2034 net.cpp:509] ReLU12 -> Convolution13 (in-place)
I0818 10:45:01.063284  2034 net.cpp:172] Setting up ReLU12
I0818 10:45:01.063298  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.063303  2034 net.cpp:194] Memory required for data: 42393720
I0818 10:45:01.063308  2034 layer_factory.hpp:77] Creating layer Convolution14
I0818 10:45:01.063326  2034 net.cpp:128] Creating Layer Convolution14
I0818 10:45:01.063331  2034 net.cpp:558] Convolution14 <- Convolution13
I0818 10:45:01.063340  2034 net.cpp:522] Convolution14 -> Convolution14
I0818 10:45:01.064798  2034 net.cpp:172] Setting up Convolution14
I0818 10:45:01.064826  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.064831  2034 net.cpp:194] Memory required for data: 42721400
I0818 10:45:01.064844  2034 layer_factory.hpp:77] Creating layer BatchNorm14
I0818 10:45:01.064852  2034 net.cpp:128] Creating Layer BatchNorm14
I0818 10:45:01.064857  2034 net.cpp:558] BatchNorm14 <- Convolution14
I0818 10:45:01.064867  2034 net.cpp:509] BatchNorm14 -> Convolution14 (in-place)
I0818 10:45:01.065150  2034 net.cpp:172] Setting up BatchNorm14
I0818 10:45:01.065160  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.065165  2034 net.cpp:194] Memory required for data: 43049080
I0818 10:45:01.065174  2034 layer_factory.hpp:77] Creating layer Scale14
I0818 10:45:01.065199  2034 net.cpp:128] Creating Layer Scale14
I0818 10:45:01.065204  2034 net.cpp:558] Scale14 <- Convolution14
I0818 10:45:01.065210  2034 net.cpp:509] Scale14 -> Convolution14 (in-place)
I0818 10:45:01.065263  2034 layer_factory.hpp:77] Creating layer Scale14
I0818 10:45:01.065428  2034 net.cpp:172] Setting up Scale14
I0818 10:45:01.065439  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.065443  2034 net.cpp:194] Memory required for data: 43376760
I0818 10:45:01.065451  2034 layer_factory.hpp:77] Creating layer Eltwise6
I0818 10:45:01.065460  2034 net.cpp:128] Creating Layer Eltwise6
I0818 10:45:01.065464  2034 net.cpp:558] Eltwise6 <- Convolution12
I0818 10:45:01.065469  2034 net.cpp:558] Eltwise6 <- Convolution14
I0818 10:45:01.065476  2034 net.cpp:522] Eltwise6 -> Eltwise6
I0818 10:45:01.065502  2034 net.cpp:172] Setting up Eltwise6
I0818 10:45:01.065512  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.065516  2034 net.cpp:194] Memory required for data: 43704440
I0818 10:45:01.065521  2034 layer_factory.hpp:77] Creating layer ReLU13
I0818 10:45:01.065527  2034 net.cpp:128] Creating Layer ReLU13
I0818 10:45:01.065531  2034 net.cpp:558] ReLU13 <- Eltwise6
I0818 10:45:01.065539  2034 net.cpp:509] ReLU13 -> Eltwise6 (in-place)
I0818 10:45:01.065994  2034 net.cpp:172] Setting up ReLU13
I0818 10:45:01.066017  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.066023  2034 net.cpp:194] Memory required for data: 44032120
I0818 10:45:01.066028  2034 layer_factory.hpp:77] Creating layer Eltwise6_ReLU13_0_split
I0818 10:45:01.066036  2034 net.cpp:128] Creating Layer Eltwise6_ReLU13_0_split
I0818 10:45:01.066041  2034 net.cpp:558] Eltwise6_ReLU13_0_split <- Eltwise6
I0818 10:45:01.066051  2034 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_0
I0818 10:45:01.066059  2034 net.cpp:522] Eltwise6_ReLU13_0_split -> Eltwise6_ReLU13_0_split_1
I0818 10:45:01.066118  2034 net.cpp:172] Setting up Eltwise6_ReLU13_0_split
I0818 10:45:01.066128  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.066134  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.066138  2034 net.cpp:194] Memory required for data: 44687480
I0818 10:45:01.066143  2034 layer_factory.hpp:77] Creating layer Convolution15
I0818 10:45:01.066156  2034 net.cpp:128] Creating Layer Convolution15
I0818 10:45:01.066161  2034 net.cpp:558] Convolution15 <- Eltwise6_ReLU13_0_split_0
I0818 10:45:01.066170  2034 net.cpp:522] Convolution15 -> Convolution15
I0818 10:45:01.067839  2034 net.cpp:172] Setting up Convolution15
I0818 10:45:01.067864  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.067869  2034 net.cpp:194] Memory required for data: 45015160
I0818 10:45:01.067879  2034 layer_factory.hpp:77] Creating layer BatchNorm15
I0818 10:45:01.067888  2034 net.cpp:128] Creating Layer BatchNorm15
I0818 10:45:01.067893  2034 net.cpp:558] BatchNorm15 <- Convolution15
I0818 10:45:01.067903  2034 net.cpp:509] BatchNorm15 -> Convolution15 (in-place)
I0818 10:45:01.068192  2034 net.cpp:172] Setting up BatchNorm15
I0818 10:45:01.068203  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.068207  2034 net.cpp:194] Memory required for data: 45342840
I0818 10:45:01.068217  2034 layer_factory.hpp:77] Creating layer Scale15
I0818 10:45:01.068224  2034 net.cpp:128] Creating Layer Scale15
I0818 10:45:01.068229  2034 net.cpp:558] Scale15 <- Convolution15
I0818 10:45:01.068234  2034 net.cpp:509] Scale15 -> Convolution15 (in-place)
I0818 10:45:01.068286  2034 layer_factory.hpp:77] Creating layer Scale15
I0818 10:45:01.068451  2034 net.cpp:172] Setting up Scale15
I0818 10:45:01.068464  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.068469  2034 net.cpp:194] Memory required for data: 45670520
I0818 10:45:01.068476  2034 layer_factory.hpp:77] Creating layer ReLU14
I0818 10:45:01.068482  2034 net.cpp:128] Creating Layer ReLU14
I0818 10:45:01.068486  2034 net.cpp:558] ReLU14 <- Convolution15
I0818 10:45:01.068492  2034 net.cpp:509] ReLU14 -> Convolution15 (in-place)
I0818 10:45:01.068765  2034 net.cpp:172] Setting up ReLU14
I0818 10:45:01.068783  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.068787  2034 net.cpp:194] Memory required for data: 45998200
I0818 10:45:01.068791  2034 layer_factory.hpp:77] Creating layer Convolution16
I0818 10:45:01.068804  2034 net.cpp:128] Creating Layer Convolution16
I0818 10:45:01.068809  2034 net.cpp:558] Convolution16 <- Convolution15
I0818 10:45:01.068817  2034 net.cpp:522] Convolution16 -> Convolution16
I0818 10:45:01.070313  2034 net.cpp:172] Setting up Convolution16
I0818 10:45:01.070338  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.070343  2034 net.cpp:194] Memory required for data: 46325880
I0818 10:45:01.070353  2034 layer_factory.hpp:77] Creating layer BatchNorm16
I0818 10:45:01.070363  2034 net.cpp:128] Creating Layer BatchNorm16
I0818 10:45:01.070371  2034 net.cpp:558] BatchNorm16 <- Convolution16
I0818 10:45:01.070379  2034 net.cpp:509] BatchNorm16 -> Convolution16 (in-place)
I0818 10:45:01.070685  2034 net.cpp:172] Setting up BatchNorm16
I0818 10:45:01.070698  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.070703  2034 net.cpp:194] Memory required for data: 46653560
I0818 10:45:01.070713  2034 layer_factory.hpp:77] Creating layer Scale16
I0818 10:45:01.070719  2034 net.cpp:128] Creating Layer Scale16
I0818 10:45:01.070724  2034 net.cpp:558] Scale16 <- Convolution16
I0818 10:45:01.070730  2034 net.cpp:509] Scale16 -> Convolution16 (in-place)
I0818 10:45:01.070786  2034 layer_factory.hpp:77] Creating layer Scale16
I0818 10:45:01.070952  2034 net.cpp:172] Setting up Scale16
I0818 10:45:01.070963  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.070967  2034 net.cpp:194] Memory required for data: 46981240
I0818 10:45:01.070976  2034 layer_factory.hpp:77] Creating layer Eltwise7
I0818 10:45:01.070984  2034 net.cpp:128] Creating Layer Eltwise7
I0818 10:45:01.070989  2034 net.cpp:558] Eltwise7 <- Eltwise6_ReLU13_0_split_1
I0818 10:45:01.070996  2034 net.cpp:558] Eltwise7 <- Convolution16
I0818 10:45:01.071002  2034 net.cpp:522] Eltwise7 -> Eltwise7
I0818 10:45:01.071025  2034 net.cpp:172] Setting up Eltwise7
I0818 10:45:01.071033  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.071038  2034 net.cpp:194] Memory required for data: 47308920
I0818 10:45:01.071041  2034 layer_factory.hpp:77] Creating layer ReLU15
I0818 10:45:01.071049  2034 net.cpp:128] Creating Layer ReLU15
I0818 10:45:01.071054  2034 net.cpp:558] ReLU15 <- Eltwise7
I0818 10:45:01.071059  2034 net.cpp:509] ReLU15 -> Eltwise7 (in-place)
I0818 10:45:01.071305  2034 net.cpp:172] Setting up ReLU15
I0818 10:45:01.071319  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.071324  2034 net.cpp:194] Memory required for data: 47636600
I0818 10:45:01.071328  2034 layer_factory.hpp:77] Creating layer Eltwise7_ReLU15_0_split
I0818 10:45:01.071337  2034 net.cpp:128] Creating Layer Eltwise7_ReLU15_0_split
I0818 10:45:01.071342  2034 net.cpp:558] Eltwise7_ReLU15_0_split <- Eltwise7
I0818 10:45:01.071352  2034 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_0
I0818 10:45:01.071359  2034 net.cpp:522] Eltwise7_ReLU15_0_split -> Eltwise7_ReLU15_0_split_1
I0818 10:45:01.071414  2034 net.cpp:172] Setting up Eltwise7_ReLU15_0_split
I0818 10:45:01.071424  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.071429  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.071434  2034 net.cpp:194] Memory required for data: 48291960
I0818 10:45:01.071437  2034 layer_factory.hpp:77] Creating layer Convolution17
I0818 10:45:01.071449  2034 net.cpp:128] Creating Layer Convolution17
I0818 10:45:01.071455  2034 net.cpp:558] Convolution17 <- Eltwise7_ReLU15_0_split_0
I0818 10:45:01.071461  2034 net.cpp:522] Convolution17 -> Convolution17
I0818 10:45:01.072948  2034 net.cpp:172] Setting up Convolution17
I0818 10:45:01.072975  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.072980  2034 net.cpp:194] Memory required for data: 48619640
I0818 10:45:01.072993  2034 layer_factory.hpp:77] Creating layer BatchNorm17
I0818 10:45:01.073019  2034 net.cpp:128] Creating Layer BatchNorm17
I0818 10:45:01.073025  2034 net.cpp:558] BatchNorm17 <- Convolution17
I0818 10:45:01.073035  2034 net.cpp:509] BatchNorm17 -> Convolution17 (in-place)
I0818 10:45:01.073331  2034 net.cpp:172] Setting up BatchNorm17
I0818 10:45:01.073345  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.073349  2034 net.cpp:194] Memory required for data: 48947320
I0818 10:45:01.073359  2034 layer_factory.hpp:77] Creating layer Scale17
I0818 10:45:01.073366  2034 net.cpp:128] Creating Layer Scale17
I0818 10:45:01.073370  2034 net.cpp:558] Scale17 <- Convolution17
I0818 10:45:01.073376  2034 net.cpp:509] Scale17 -> Convolution17 (in-place)
I0818 10:45:01.073432  2034 layer_factory.hpp:77] Creating layer Scale17
I0818 10:45:01.073602  2034 net.cpp:172] Setting up Scale17
I0818 10:45:01.073612  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.073616  2034 net.cpp:194] Memory required for data: 49275000
I0818 10:45:01.073628  2034 layer_factory.hpp:77] Creating layer ReLU16
I0818 10:45:01.073637  2034 net.cpp:128] Creating Layer ReLU16
I0818 10:45:01.073642  2034 net.cpp:558] ReLU16 <- Convolution17
I0818 10:45:01.073648  2034 net.cpp:509] ReLU16 -> Convolution17 (in-place)
I0818 10:45:01.073896  2034 net.cpp:172] Setting up ReLU16
I0818 10:45:01.073909  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.073915  2034 net.cpp:194] Memory required for data: 49602680
I0818 10:45:01.073918  2034 layer_factory.hpp:77] Creating layer Convolution18
I0818 10:45:01.073930  2034 net.cpp:128] Creating Layer Convolution18
I0818 10:45:01.073935  2034 net.cpp:558] Convolution18 <- Convolution17
I0818 10:45:01.073945  2034 net.cpp:522] Convolution18 -> Convolution18
I0818 10:45:01.075433  2034 net.cpp:172] Setting up Convolution18
I0818 10:45:01.075459  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.075464  2034 net.cpp:194] Memory required for data: 49930360
I0818 10:45:01.075474  2034 layer_factory.hpp:77] Creating layer BatchNorm18
I0818 10:45:01.075484  2034 net.cpp:128] Creating Layer BatchNorm18
I0818 10:45:01.075489  2034 net.cpp:558] BatchNorm18 <- Convolution18
I0818 10:45:01.075496  2034 net.cpp:509] BatchNorm18 -> Convolution18 (in-place)
I0818 10:45:01.075793  2034 net.cpp:172] Setting up BatchNorm18
I0818 10:45:01.075804  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.075809  2034 net.cpp:194] Memory required for data: 50258040
I0818 10:45:01.075819  2034 layer_factory.hpp:77] Creating layer Scale18
I0818 10:45:01.075827  2034 net.cpp:128] Creating Layer Scale18
I0818 10:45:01.075832  2034 net.cpp:558] Scale18 <- Convolution18
I0818 10:45:01.075837  2034 net.cpp:509] Scale18 -> Convolution18 (in-place)
I0818 10:45:01.075891  2034 layer_factory.hpp:77] Creating layer Scale18
I0818 10:45:01.076064  2034 net.cpp:172] Setting up Scale18
I0818 10:45:01.076074  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.076078  2034 net.cpp:194] Memory required for data: 50585720
I0818 10:45:01.076086  2034 layer_factory.hpp:77] Creating layer Eltwise8
I0818 10:45:01.076092  2034 net.cpp:128] Creating Layer Eltwise8
I0818 10:45:01.076097  2034 net.cpp:558] Eltwise8 <- Eltwise7_ReLU15_0_split_1
I0818 10:45:01.076102  2034 net.cpp:558] Eltwise8 <- Convolution18
I0818 10:45:01.076112  2034 net.cpp:522] Eltwise8 -> Eltwise8
I0818 10:45:01.076138  2034 net.cpp:172] Setting up Eltwise8
I0818 10:45:01.076148  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.076151  2034 net.cpp:194] Memory required for data: 50913400
I0818 10:45:01.076156  2034 layer_factory.hpp:77] Creating layer ReLU17
I0818 10:45:01.076165  2034 net.cpp:128] Creating Layer ReLU17
I0818 10:45:01.076169  2034 net.cpp:558] ReLU17 <- Eltwise8
I0818 10:45:01.076175  2034 net.cpp:509] ReLU17 -> Eltwise8 (in-place)
I0818 10:45:01.076620  2034 net.cpp:172] Setting up ReLU17
I0818 10:45:01.076642  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.076647  2034 net.cpp:194] Memory required for data: 51241080
I0818 10:45:01.076668  2034 layer_factory.hpp:77] Creating layer Eltwise8_ReLU17_0_split
I0818 10:45:01.076680  2034 net.cpp:128] Creating Layer Eltwise8_ReLU17_0_split
I0818 10:45:01.076685  2034 net.cpp:558] Eltwise8_ReLU17_0_split <- Eltwise8
I0818 10:45:01.076694  2034 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_0
I0818 10:45:01.076706  2034 net.cpp:522] Eltwise8_ReLU17_0_split -> Eltwise8_ReLU17_0_split_1
I0818 10:45:01.076769  2034 net.cpp:172] Setting up Eltwise8_ReLU17_0_split
I0818 10:45:01.076776  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.076782  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.076786  2034 net.cpp:194] Memory required for data: 51896440
I0818 10:45:01.076791  2034 layer_factory.hpp:77] Creating layer Convolution19
I0818 10:45:01.076804  2034 net.cpp:128] Creating Layer Convolution19
I0818 10:45:01.076809  2034 net.cpp:558] Convolution19 <- Eltwise8_ReLU17_0_split_0
I0818 10:45:01.076817  2034 net.cpp:522] Convolution19 -> Convolution19
I0818 10:45:01.078313  2034 net.cpp:172] Setting up Convolution19
I0818 10:45:01.078339  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.078343  2034 net.cpp:194] Memory required for data: 52224120
I0818 10:45:01.078357  2034 layer_factory.hpp:77] Creating layer BatchNorm19
I0818 10:45:01.078367  2034 net.cpp:128] Creating Layer BatchNorm19
I0818 10:45:01.078372  2034 net.cpp:558] BatchNorm19 <- Convolution19
I0818 10:45:01.078382  2034 net.cpp:509] BatchNorm19 -> Convolution19 (in-place)
I0818 10:45:01.078691  2034 net.cpp:172] Setting up BatchNorm19
I0818 10:45:01.078702  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.078706  2034 net.cpp:194] Memory required for data: 52551800
I0818 10:45:01.078728  2034 layer_factory.hpp:77] Creating layer Scale19
I0818 10:45:01.078740  2034 net.cpp:128] Creating Layer Scale19
I0818 10:45:01.078748  2034 net.cpp:558] Scale19 <- Convolution19
I0818 10:45:01.078754  2034 net.cpp:509] Scale19 -> Convolution19 (in-place)
I0818 10:45:01.078812  2034 layer_factory.hpp:77] Creating layer Scale19
I0818 10:45:01.078984  2034 net.cpp:172] Setting up Scale19
I0818 10:45:01.078994  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.078997  2034 net.cpp:194] Memory required for data: 52879480
I0818 10:45:01.079005  2034 layer_factory.hpp:77] Creating layer ReLU18
I0818 10:45:01.079011  2034 net.cpp:128] Creating Layer ReLU18
I0818 10:45:01.079018  2034 net.cpp:558] ReLU18 <- Convolution19
I0818 10:45:01.079025  2034 net.cpp:509] ReLU18 -> Convolution19 (in-place)
I0818 10:45:01.079272  2034 net.cpp:172] Setting up ReLU18
I0818 10:45:01.079284  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.079288  2034 net.cpp:194] Memory required for data: 53207160
I0818 10:45:01.079293  2034 layer_factory.hpp:77] Creating layer Convolution20
I0818 10:45:01.079306  2034 net.cpp:128] Creating Layer Convolution20
I0818 10:45:01.079313  2034 net.cpp:558] Convolution20 <- Convolution19
I0818 10:45:01.079324  2034 net.cpp:522] Convolution20 -> Convolution20
I0818 10:45:01.080940  2034 net.cpp:172] Setting up Convolution20
I0818 10:45:01.080965  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.080970  2034 net.cpp:194] Memory required for data: 53534840
I0818 10:45:01.080981  2034 layer_factory.hpp:77] Creating layer BatchNorm20
I0818 10:45:01.080988  2034 net.cpp:128] Creating Layer BatchNorm20
I0818 10:45:01.080993  2034 net.cpp:558] BatchNorm20 <- Convolution20
I0818 10:45:01.081001  2034 net.cpp:509] BatchNorm20 -> Convolution20 (in-place)
I0818 10:45:01.081295  2034 net.cpp:172] Setting up BatchNorm20
I0818 10:45:01.081305  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.081308  2034 net.cpp:194] Memory required for data: 53862520
I0818 10:45:01.081318  2034 layer_factory.hpp:77] Creating layer Scale20
I0818 10:45:01.081327  2034 net.cpp:128] Creating Layer Scale20
I0818 10:45:01.081331  2034 net.cpp:558] Scale20 <- Convolution20
I0818 10:45:01.081339  2034 net.cpp:509] Scale20 -> Convolution20 (in-place)
I0818 10:45:01.081411  2034 layer_factory.hpp:77] Creating layer Scale20
I0818 10:45:01.081581  2034 net.cpp:172] Setting up Scale20
I0818 10:45:01.081591  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.081595  2034 net.cpp:194] Memory required for data: 54190200
I0818 10:45:01.081604  2034 layer_factory.hpp:77] Creating layer Eltwise9
I0818 10:45:01.081614  2034 net.cpp:128] Creating Layer Eltwise9
I0818 10:45:01.081621  2034 net.cpp:558] Eltwise9 <- Eltwise8_ReLU17_0_split_1
I0818 10:45:01.081626  2034 net.cpp:558] Eltwise9 <- Convolution20
I0818 10:45:01.081635  2034 net.cpp:522] Eltwise9 -> Eltwise9
I0818 10:45:01.081660  2034 net.cpp:172] Setting up Eltwise9
I0818 10:45:01.081668  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.081672  2034 net.cpp:194] Memory required for data: 54517880
I0818 10:45:01.081676  2034 layer_factory.hpp:77] Creating layer ReLU19
I0818 10:45:01.081683  2034 net.cpp:128] Creating Layer ReLU19
I0818 10:45:01.081687  2034 net.cpp:558] ReLU19 <- Eltwise9
I0818 10:45:01.081696  2034 net.cpp:509] ReLU19 -> Eltwise9 (in-place)
I0818 10:45:01.081939  2034 net.cpp:172] Setting up ReLU19
I0818 10:45:01.081951  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.081955  2034 net.cpp:194] Memory required for data: 54845560
I0818 10:45:01.081960  2034 layer_factory.hpp:77] Creating layer Eltwise9_ReLU19_0_split
I0818 10:45:01.081967  2034 net.cpp:128] Creating Layer Eltwise9_ReLU19_0_split
I0818 10:45:01.081971  2034 net.cpp:558] Eltwise9_ReLU19_0_split <- Eltwise9
I0818 10:45:01.081980  2034 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_0
I0818 10:45:01.081988  2034 net.cpp:522] Eltwise9_ReLU19_0_split -> Eltwise9_ReLU19_0_split_1
I0818 10:45:01.082049  2034 net.cpp:172] Setting up Eltwise9_ReLU19_0_split
I0818 10:45:01.082060  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.082067  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.082070  2034 net.cpp:194] Memory required for data: 55500920
I0818 10:45:01.082074  2034 layer_factory.hpp:77] Creating layer Convolution21
I0818 10:45:01.082087  2034 net.cpp:128] Creating Layer Convolution21
I0818 10:45:01.082095  2034 net.cpp:558] Convolution21 <- Eltwise9_ReLU19_0_split_0
I0818 10:45:01.082108  2034 net.cpp:522] Convolution21 -> Convolution21
I0818 10:45:01.083609  2034 net.cpp:172] Setting up Convolution21
I0818 10:45:01.083636  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.083640  2034 net.cpp:194] Memory required for data: 55828600
I0818 10:45:01.083650  2034 layer_factory.hpp:77] Creating layer BatchNorm21
I0818 10:45:01.083662  2034 net.cpp:128] Creating Layer BatchNorm21
I0818 10:45:01.083667  2034 net.cpp:558] BatchNorm21 <- Convolution21
I0818 10:45:01.083673  2034 net.cpp:509] BatchNorm21 -> Convolution21 (in-place)
I0818 10:45:01.083976  2034 net.cpp:172] Setting up BatchNorm21
I0818 10:45:01.083986  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.083992  2034 net.cpp:194] Memory required for data: 56156280
I0818 10:45:01.084002  2034 layer_factory.hpp:77] Creating layer Scale21
I0818 10:45:01.084008  2034 net.cpp:128] Creating Layer Scale21
I0818 10:45:01.084013  2034 net.cpp:558] Scale21 <- Convolution21
I0818 10:45:01.084020  2034 net.cpp:509] Scale21 -> Convolution21 (in-place)
I0818 10:45:01.084074  2034 layer_factory.hpp:77] Creating layer Scale21
I0818 10:45:01.084247  2034 net.cpp:172] Setting up Scale21
I0818 10:45:01.084257  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.084261  2034 net.cpp:194] Memory required for data: 56483960
I0818 10:45:01.084270  2034 layer_factory.hpp:77] Creating layer ReLU20
I0818 10:45:01.084275  2034 net.cpp:128] Creating Layer ReLU20
I0818 10:45:01.084280  2034 net.cpp:558] ReLU20 <- Convolution21
I0818 10:45:01.084287  2034 net.cpp:509] ReLU20 -> Convolution21 (in-place)
I0818 10:45:01.084733  2034 net.cpp:172] Setting up ReLU20
I0818 10:45:01.084755  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.084759  2034 net.cpp:194] Memory required for data: 56811640
I0818 10:45:01.084781  2034 layer_factory.hpp:77] Creating layer Convolution22
I0818 10:45:01.084795  2034 net.cpp:128] Creating Layer Convolution22
I0818 10:45:01.084801  2034 net.cpp:558] Convolution22 <- Convolution21
I0818 10:45:01.084811  2034 net.cpp:522] Convolution22 -> Convolution22
I0818 10:45:01.086313  2034 net.cpp:172] Setting up Convolution22
I0818 10:45:01.086338  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.086344  2034 net.cpp:194] Memory required for data: 57139320
I0818 10:45:01.086354  2034 layer_factory.hpp:77] Creating layer BatchNorm22
I0818 10:45:01.086364  2034 net.cpp:128] Creating Layer BatchNorm22
I0818 10:45:01.086371  2034 net.cpp:558] BatchNorm22 <- Convolution22
I0818 10:45:01.086381  2034 net.cpp:509] BatchNorm22 -> Convolution22 (in-place)
I0818 10:45:01.086694  2034 net.cpp:172] Setting up BatchNorm22
I0818 10:45:01.086705  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.086709  2034 net.cpp:194] Memory required for data: 57467000
I0818 10:45:01.086719  2034 layer_factory.hpp:77] Creating layer Scale22
I0818 10:45:01.086726  2034 net.cpp:128] Creating Layer Scale22
I0818 10:45:01.086733  2034 net.cpp:558] Scale22 <- Convolution22
I0818 10:45:01.086740  2034 net.cpp:509] Scale22 -> Convolution22 (in-place)
I0818 10:45:01.086794  2034 layer_factory.hpp:77] Creating layer Scale22
I0818 10:45:01.086963  2034 net.cpp:172] Setting up Scale22
I0818 10:45:01.086977  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.086980  2034 net.cpp:194] Memory required for data: 57794680
I0818 10:45:01.086988  2034 layer_factory.hpp:77] Creating layer Eltwise10
I0818 10:45:01.086995  2034 net.cpp:128] Creating Layer Eltwise10
I0818 10:45:01.087000  2034 net.cpp:558] Eltwise10 <- Eltwise9_ReLU19_0_split_1
I0818 10:45:01.087005  2034 net.cpp:558] Eltwise10 <- Convolution22
I0818 10:45:01.087011  2034 net.cpp:522] Eltwise10 -> Eltwise10
I0818 10:45:01.087038  2034 net.cpp:172] Setting up Eltwise10
I0818 10:45:01.087045  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.087049  2034 net.cpp:194] Memory required for data: 58122360
I0818 10:45:01.087054  2034 layer_factory.hpp:77] Creating layer ReLU21
I0818 10:45:01.087059  2034 net.cpp:128] Creating Layer ReLU21
I0818 10:45:01.087064  2034 net.cpp:558] ReLU21 <- Eltwise10
I0818 10:45:01.087072  2034 net.cpp:509] ReLU21 -> Eltwise10 (in-place)
I0818 10:45:01.087321  2034 net.cpp:172] Setting up ReLU21
I0818 10:45:01.087335  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.087340  2034 net.cpp:194] Memory required for data: 58450040
I0818 10:45:01.087344  2034 layer_factory.hpp:77] Creating layer Eltwise10_ReLU21_0_split
I0818 10:45:01.087352  2034 net.cpp:128] Creating Layer Eltwise10_ReLU21_0_split
I0818 10:45:01.087357  2034 net.cpp:558] Eltwise10_ReLU21_0_split <- Eltwise10
I0818 10:45:01.087365  2034 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_0
I0818 10:45:01.087374  2034 net.cpp:522] Eltwise10_ReLU21_0_split -> Eltwise10_ReLU21_0_split_1
I0818 10:45:01.087435  2034 net.cpp:172] Setting up Eltwise10_ReLU21_0_split
I0818 10:45:01.087441  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.087447  2034 net.cpp:186] Top shape: 10 32 16 16 (81920)
I0818 10:45:01.087451  2034 net.cpp:194] Memory required for data: 59105400
I0818 10:45:01.087455  2034 layer_factory.hpp:77] Creating layer Convolution23
I0818 10:45:01.087467  2034 net.cpp:128] Creating Layer Convolution23
I0818 10:45:01.087471  2034 net.cpp:558] Convolution23 <- Eltwise10_ReLU21_0_split_0
I0818 10:45:01.087481  2034 net.cpp:522] Convolution23 -> Convolution23
I0818 10:45:01.088907  2034 net.cpp:172] Setting up Convolution23
I0818 10:45:01.088929  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.088934  2034 net.cpp:194] Memory required for data: 59269240
I0818 10:45:01.088948  2034 layer_factory.hpp:77] Creating layer BatchNorm23
I0818 10:45:01.088959  2034 net.cpp:128] Creating Layer BatchNorm23
I0818 10:45:01.088965  2034 net.cpp:558] BatchNorm23 <- Convolution23
I0818 10:45:01.088994  2034 net.cpp:509] BatchNorm23 -> Convolution23 (in-place)
I0818 10:45:01.089306  2034 net.cpp:172] Setting up BatchNorm23
I0818 10:45:01.089319  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.089324  2034 net.cpp:194] Memory required for data: 59433080
I0818 10:45:01.089334  2034 layer_factory.hpp:77] Creating layer Scale23
I0818 10:45:01.089346  2034 net.cpp:128] Creating Layer Scale23
I0818 10:45:01.089351  2034 net.cpp:558] Scale23 <- Convolution23
I0818 10:45:01.089357  2034 net.cpp:509] Scale23 -> Convolution23 (in-place)
I0818 10:45:01.089413  2034 layer_factory.hpp:77] Creating layer Scale23
I0818 10:45:01.089591  2034 net.cpp:172] Setting up Scale23
I0818 10:45:01.089602  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.089607  2034 net.cpp:194] Memory required for data: 59596920
I0818 10:45:01.089614  2034 layer_factory.hpp:77] Creating layer Convolution24
I0818 10:45:01.089627  2034 net.cpp:128] Creating Layer Convolution24
I0818 10:45:01.089632  2034 net.cpp:558] Convolution24 <- Eltwise10_ReLU21_0_split_1
I0818 10:45:01.089642  2034 net.cpp:522] Convolution24 -> Convolution24
I0818 10:45:01.091315  2034 net.cpp:172] Setting up Convolution24
I0818 10:45:01.091336  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.091341  2034 net.cpp:194] Memory required for data: 59760760
I0818 10:45:01.091351  2034 layer_factory.hpp:77] Creating layer BatchNorm24
I0818 10:45:01.091361  2034 net.cpp:128] Creating Layer BatchNorm24
I0818 10:45:01.091367  2034 net.cpp:558] BatchNorm24 <- Convolution24
I0818 10:45:01.091377  2034 net.cpp:509] BatchNorm24 -> Convolution24 (in-place)
I0818 10:45:01.091687  2034 net.cpp:172] Setting up BatchNorm24
I0818 10:45:01.091703  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.091708  2034 net.cpp:194] Memory required for data: 59924600
I0818 10:45:01.091717  2034 layer_factory.hpp:77] Creating layer Scale24
I0818 10:45:01.091724  2034 net.cpp:128] Creating Layer Scale24
I0818 10:45:01.091730  2034 net.cpp:558] Scale24 <- Convolution24
I0818 10:45:01.091735  2034 net.cpp:509] Scale24 -> Convolution24 (in-place)
I0818 10:45:01.091794  2034 layer_factory.hpp:77] Creating layer Scale24
I0818 10:45:01.091970  2034 net.cpp:172] Setting up Scale24
I0818 10:45:01.091981  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.091985  2034 net.cpp:194] Memory required for data: 60088440
I0818 10:45:01.091994  2034 layer_factory.hpp:77] Creating layer ReLU22
I0818 10:45:01.092000  2034 net.cpp:128] Creating Layer ReLU22
I0818 10:45:01.092005  2034 net.cpp:558] ReLU22 <- Convolution24
I0818 10:45:01.092012  2034 net.cpp:509] ReLU22 -> Convolution24 (in-place)
I0818 10:45:01.092264  2034 net.cpp:172] Setting up ReLU22
I0818 10:45:01.092278  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.092283  2034 net.cpp:194] Memory required for data: 60252280
I0818 10:45:01.092288  2034 layer_factory.hpp:77] Creating layer Convolution25
I0818 10:45:01.092300  2034 net.cpp:128] Creating Layer Convolution25
I0818 10:45:01.092305  2034 net.cpp:558] Convolution25 <- Convolution24
I0818 10:45:01.092315  2034 net.cpp:522] Convolution25 -> Convolution25
I0818 10:45:01.095630  2034 net.cpp:172] Setting up Convolution25
I0818 10:45:01.095657  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.095662  2034 net.cpp:194] Memory required for data: 60416120
I0818 10:45:01.095674  2034 layer_factory.hpp:77] Creating layer BatchNorm25
I0818 10:45:01.095685  2034 net.cpp:128] Creating Layer BatchNorm25
I0818 10:45:01.095690  2034 net.cpp:558] BatchNorm25 <- Convolution25
I0818 10:45:01.095700  2034 net.cpp:509] BatchNorm25 -> Convolution25 (in-place)
I0818 10:45:01.096007  2034 net.cpp:172] Setting up BatchNorm25
I0818 10:45:01.096019  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.096024  2034 net.cpp:194] Memory required for data: 60579960
I0818 10:45:01.096032  2034 layer_factory.hpp:77] Creating layer Scale25
I0818 10:45:01.096040  2034 net.cpp:128] Creating Layer Scale25
I0818 10:45:01.096060  2034 net.cpp:558] Scale25 <- Convolution25
I0818 10:45:01.096068  2034 net.cpp:509] Scale25 -> Convolution25 (in-place)
I0818 10:45:01.096130  2034 layer_factory.hpp:77] Creating layer Scale25
I0818 10:45:01.096310  2034 net.cpp:172] Setting up Scale25
I0818 10:45:01.096321  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.096325  2034 net.cpp:194] Memory required for data: 60743800
I0818 10:45:01.096333  2034 layer_factory.hpp:77] Creating layer Eltwise11
I0818 10:45:01.096341  2034 net.cpp:128] Creating Layer Eltwise11
I0818 10:45:01.096345  2034 net.cpp:558] Eltwise11 <- Convolution23
I0818 10:45:01.096350  2034 net.cpp:558] Eltwise11 <- Convolution25
I0818 10:45:01.096359  2034 net.cpp:522] Eltwise11 -> Eltwise11
I0818 10:45:01.096390  2034 net.cpp:172] Setting up Eltwise11
I0818 10:45:01.096398  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.096402  2034 net.cpp:194] Memory required for data: 60907640
I0818 10:45:01.096406  2034 layer_factory.hpp:77] Creating layer ReLU23
I0818 10:45:01.096413  2034 net.cpp:128] Creating Layer ReLU23
I0818 10:45:01.096417  2034 net.cpp:558] ReLU23 <- Eltwise11
I0818 10:45:01.096423  2034 net.cpp:509] ReLU23 -> Eltwise11 (in-place)
I0818 10:45:01.096879  2034 net.cpp:172] Setting up ReLU23
I0818 10:45:01.096899  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.096904  2034 net.cpp:194] Memory required for data: 61071480
I0818 10:45:01.096909  2034 layer_factory.hpp:77] Creating layer Eltwise11_ReLU23_0_split
I0818 10:45:01.096922  2034 net.cpp:128] Creating Layer Eltwise11_ReLU23_0_split
I0818 10:45:01.096927  2034 net.cpp:558] Eltwise11_ReLU23_0_split <- Eltwise11
I0818 10:45:01.096935  2034 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_0
I0818 10:45:01.096946  2034 net.cpp:522] Eltwise11_ReLU23_0_split -> Eltwise11_ReLU23_0_split_1
I0818 10:45:01.097007  2034 net.cpp:172] Setting up Eltwise11_ReLU23_0_split
I0818 10:45:01.097014  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.097020  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.097024  2034 net.cpp:194] Memory required for data: 61399160
I0818 10:45:01.097028  2034 layer_factory.hpp:77] Creating layer Convolution26
I0818 10:45:01.097041  2034 net.cpp:128] Creating Layer Convolution26
I0818 10:45:01.097046  2034 net.cpp:558] Convolution26 <- Eltwise11_ReLU23_0_split_0
I0818 10:45:01.097054  2034 net.cpp:522] Convolution26 -> Convolution26
I0818 10:45:01.099191  2034 net.cpp:172] Setting up Convolution26
I0818 10:45:01.099218  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.099223  2034 net.cpp:194] Memory required for data: 61563000
I0818 10:45:01.099233  2034 layer_factory.hpp:77] Creating layer BatchNorm26
I0818 10:45:01.099244  2034 net.cpp:128] Creating Layer BatchNorm26
I0818 10:45:01.099249  2034 net.cpp:558] BatchNorm26 <- Convolution26
I0818 10:45:01.099258  2034 net.cpp:509] BatchNorm26 -> Convolution26 (in-place)
I0818 10:45:01.099578  2034 net.cpp:172] Setting up BatchNorm26
I0818 10:45:01.099588  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.099593  2034 net.cpp:194] Memory required for data: 61726840
I0818 10:45:01.099603  2034 layer_factory.hpp:77] Creating layer Scale26
I0818 10:45:01.099613  2034 net.cpp:128] Creating Layer Scale26
I0818 10:45:01.099617  2034 net.cpp:558] Scale26 <- Convolution26
I0818 10:45:01.099623  2034 net.cpp:509] Scale26 -> Convolution26 (in-place)
I0818 10:45:01.099678  2034 layer_factory.hpp:77] Creating layer Scale26
I0818 10:45:01.099858  2034 net.cpp:172] Setting up Scale26
I0818 10:45:01.099865  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.099869  2034 net.cpp:194] Memory required for data: 61890680
I0818 10:45:01.099877  2034 layer_factory.hpp:77] Creating layer ReLU24
I0818 10:45:01.099884  2034 net.cpp:128] Creating Layer ReLU24
I0818 10:45:01.099887  2034 net.cpp:558] ReLU24 <- Convolution26
I0818 10:45:01.099895  2034 net.cpp:509] ReLU24 -> Convolution26 (in-place)
I0818 10:45:01.100149  2034 net.cpp:172] Setting up ReLU24
I0818 10:45:01.100180  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.100185  2034 net.cpp:194] Memory required for data: 62054520
I0818 10:45:01.100190  2034 layer_factory.hpp:77] Creating layer Convolution27
I0818 10:45:01.100203  2034 net.cpp:128] Creating Layer Convolution27
I0818 10:45:01.100208  2034 net.cpp:558] Convolution27 <- Convolution26
I0818 10:45:01.100220  2034 net.cpp:522] Convolution27 -> Convolution27
I0818 10:45:01.102181  2034 net.cpp:172] Setting up Convolution27
I0818 10:45:01.102207  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.102211  2034 net.cpp:194] Memory required for data: 62218360
I0818 10:45:01.102221  2034 layer_factory.hpp:77] Creating layer BatchNorm27
I0818 10:45:01.102241  2034 net.cpp:128] Creating Layer BatchNorm27
I0818 10:45:01.102250  2034 net.cpp:558] BatchNorm27 <- Convolution27
I0818 10:45:01.102257  2034 net.cpp:509] BatchNorm27 -> Convolution27 (in-place)
I0818 10:45:01.102566  2034 net.cpp:172] Setting up BatchNorm27
I0818 10:45:01.102577  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.102582  2034 net.cpp:194] Memory required for data: 62382200
I0818 10:45:01.102592  2034 layer_factory.hpp:77] Creating layer Scale27
I0818 10:45:01.102602  2034 net.cpp:128] Creating Layer Scale27
I0818 10:45:01.102612  2034 net.cpp:558] Scale27 <- Convolution27
I0818 10:45:01.102618  2034 net.cpp:509] Scale27 -> Convolution27 (in-place)
I0818 10:45:01.102687  2034 layer_factory.hpp:77] Creating layer Scale27
I0818 10:45:01.102860  2034 net.cpp:172] Setting up Scale27
I0818 10:45:01.102872  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.102877  2034 net.cpp:194] Memory required for data: 62546040
I0818 10:45:01.102885  2034 layer_factory.hpp:77] Creating layer Eltwise12
I0818 10:45:01.102895  2034 net.cpp:128] Creating Layer Eltwise12
I0818 10:45:01.102900  2034 net.cpp:558] Eltwise12 <- Eltwise11_ReLU23_0_split_1
I0818 10:45:01.102905  2034 net.cpp:558] Eltwise12 <- Convolution27
I0818 10:45:01.102913  2034 net.cpp:522] Eltwise12 -> Eltwise12
I0818 10:45:01.102943  2034 net.cpp:172] Setting up Eltwise12
I0818 10:45:01.102952  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.102955  2034 net.cpp:194] Memory required for data: 62709880
I0818 10:45:01.102959  2034 layer_factory.hpp:77] Creating layer ReLU25
I0818 10:45:01.102967  2034 net.cpp:128] Creating Layer ReLU25
I0818 10:45:01.102972  2034 net.cpp:558] ReLU25 <- Eltwise12
I0818 10:45:01.102977  2034 net.cpp:509] ReLU25 -> Eltwise12 (in-place)
I0818 10:45:01.103224  2034 net.cpp:172] Setting up ReLU25
I0818 10:45:01.103238  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.103242  2034 net.cpp:194] Memory required for data: 62873720
I0818 10:45:01.103247  2034 layer_factory.hpp:77] Creating layer Eltwise12_ReLU25_0_split
I0818 10:45:01.103260  2034 net.cpp:128] Creating Layer Eltwise12_ReLU25_0_split
I0818 10:45:01.103266  2034 net.cpp:558] Eltwise12_ReLU25_0_split <- Eltwise12
I0818 10:45:01.103272  2034 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_0
I0818 10:45:01.103282  2034 net.cpp:522] Eltwise12_ReLU25_0_split -> Eltwise12_ReLU25_0_split_1
I0818 10:45:01.103341  2034 net.cpp:172] Setting up Eltwise12_ReLU25_0_split
I0818 10:45:01.103351  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.103358  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.103361  2034 net.cpp:194] Memory required for data: 63201400
I0818 10:45:01.103365  2034 layer_factory.hpp:77] Creating layer Convolution28
I0818 10:45:01.103377  2034 net.cpp:128] Creating Layer Convolution28
I0818 10:45:01.103382  2034 net.cpp:558] Convolution28 <- Eltwise12_ReLU25_0_split_0
I0818 10:45:01.103391  2034 net.cpp:522] Convolution28 -> Convolution28
I0818 10:45:01.105502  2034 net.cpp:172] Setting up Convolution28
I0818 10:45:01.105528  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.105533  2034 net.cpp:194] Memory required for data: 63365240
I0818 10:45:01.105547  2034 layer_factory.hpp:77] Creating layer BatchNorm28
I0818 10:45:01.105579  2034 net.cpp:128] Creating Layer BatchNorm28
I0818 10:45:01.105585  2034 net.cpp:558] BatchNorm28 <- Convolution28
I0818 10:45:01.105594  2034 net.cpp:509] BatchNorm28 -> Convolution28 (in-place)
I0818 10:45:01.105922  2034 net.cpp:172] Setting up BatchNorm28
I0818 10:45:01.105932  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.105937  2034 net.cpp:194] Memory required for data: 63529080
I0818 10:45:01.105947  2034 layer_factory.hpp:77] Creating layer Scale28
I0818 10:45:01.105957  2034 net.cpp:128] Creating Layer Scale28
I0818 10:45:01.105962  2034 net.cpp:558] Scale28 <- Convolution28
I0818 10:45:01.105968  2034 net.cpp:509] Scale28 -> Convolution28 (in-place)
I0818 10:45:01.106024  2034 layer_factory.hpp:77] Creating layer Scale28
I0818 10:45:01.106204  2034 net.cpp:172] Setting up Scale28
I0818 10:45:01.106214  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.106218  2034 net.cpp:194] Memory required for data: 63692920
I0818 10:45:01.106226  2034 layer_factory.hpp:77] Creating layer ReLU26
I0818 10:45:01.106232  2034 net.cpp:128] Creating Layer ReLU26
I0818 10:45:01.106240  2034 net.cpp:558] ReLU26 <- Convolution28
I0818 10:45:01.106246  2034 net.cpp:509] ReLU26 -> Convolution28 (in-place)
I0818 10:45:01.106500  2034 net.cpp:172] Setting up ReLU26
I0818 10:45:01.106513  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.106518  2034 net.cpp:194] Memory required for data: 63856760
I0818 10:45:01.106523  2034 layer_factory.hpp:77] Creating layer Convolution29
I0818 10:45:01.106535  2034 net.cpp:128] Creating Layer Convolution29
I0818 10:45:01.106542  2034 net.cpp:558] Convolution29 <- Convolution28
I0818 10:45:01.106555  2034 net.cpp:522] Convolution29 -> Convolution29
I0818 10:45:01.108497  2034 net.cpp:172] Setting up Convolution29
I0818 10:45:01.108523  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.108527  2034 net.cpp:194] Memory required for data: 64020600
I0818 10:45:01.108541  2034 layer_factory.hpp:77] Creating layer BatchNorm29
I0818 10:45:01.108552  2034 net.cpp:128] Creating Layer BatchNorm29
I0818 10:45:01.108561  2034 net.cpp:558] BatchNorm29 <- Convolution29
I0818 10:45:01.108568  2034 net.cpp:509] BatchNorm29 -> Convolution29 (in-place)
I0818 10:45:01.108886  2034 net.cpp:172] Setting up BatchNorm29
I0818 10:45:01.108896  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.108901  2034 net.cpp:194] Memory required for data: 64184440
I0818 10:45:01.108911  2034 layer_factory.hpp:77] Creating layer Scale29
I0818 10:45:01.108917  2034 net.cpp:128] Creating Layer Scale29
I0818 10:45:01.108922  2034 net.cpp:558] Scale29 <- Convolution29
I0818 10:45:01.108927  2034 net.cpp:509] Scale29 -> Convolution29 (in-place)
I0818 10:45:01.108986  2034 layer_factory.hpp:77] Creating layer Scale29
I0818 10:45:01.109163  2034 net.cpp:172] Setting up Scale29
I0818 10:45:01.109174  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.109177  2034 net.cpp:194] Memory required for data: 64348280
I0818 10:45:01.109185  2034 layer_factory.hpp:77] Creating layer Eltwise13
I0818 10:45:01.109192  2034 net.cpp:128] Creating Layer Eltwise13
I0818 10:45:01.109197  2034 net.cpp:558] Eltwise13 <- Eltwise12_ReLU25_0_split_1
I0818 10:45:01.109202  2034 net.cpp:558] Eltwise13 <- Convolution29
I0818 10:45:01.109210  2034 net.cpp:522] Eltwise13 -> Eltwise13
I0818 10:45:01.109242  2034 net.cpp:172] Setting up Eltwise13
I0818 10:45:01.109251  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.109256  2034 net.cpp:194] Memory required for data: 64512120
I0818 10:45:01.109259  2034 layer_factory.hpp:77] Creating layer ReLU27
I0818 10:45:01.109266  2034 net.cpp:128] Creating Layer ReLU27
I0818 10:45:01.109271  2034 net.cpp:558] ReLU27 <- Eltwise13
I0818 10:45:01.109275  2034 net.cpp:509] ReLU27 -> Eltwise13 (in-place)
I0818 10:45:01.109725  2034 net.cpp:172] Setting up ReLU27
I0818 10:45:01.109746  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.109750  2034 net.cpp:194] Memory required for data: 64675960
I0818 10:45:01.109755  2034 layer_factory.hpp:77] Creating layer Eltwise13_ReLU27_0_split
I0818 10:45:01.109781  2034 net.cpp:128] Creating Layer Eltwise13_ReLU27_0_split
I0818 10:45:01.109786  2034 net.cpp:558] Eltwise13_ReLU27_0_split <- Eltwise13
I0818 10:45:01.109797  2034 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_0
I0818 10:45:01.109807  2034 net.cpp:522] Eltwise13_ReLU27_0_split -> Eltwise13_ReLU27_0_split_1
I0818 10:45:01.109870  2034 net.cpp:172] Setting up Eltwise13_ReLU27_0_split
I0818 10:45:01.109882  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.109889  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.109892  2034 net.cpp:194] Memory required for data: 65003640
I0818 10:45:01.109896  2034 layer_factory.hpp:77] Creating layer Convolution30
I0818 10:45:01.109910  2034 net.cpp:128] Creating Layer Convolution30
I0818 10:45:01.109917  2034 net.cpp:558] Convolution30 <- Eltwise13_ReLU27_0_split_0
I0818 10:45:01.109925  2034 net.cpp:522] Convolution30 -> Convolution30
I0818 10:45:01.112082  2034 net.cpp:172] Setting up Convolution30
I0818 10:45:01.112109  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.112114  2034 net.cpp:194] Memory required for data: 65167480
I0818 10:45:01.112125  2034 layer_factory.hpp:77] Creating layer BatchNorm30
I0818 10:45:01.112134  2034 net.cpp:128] Creating Layer BatchNorm30
I0818 10:45:01.112144  2034 net.cpp:558] BatchNorm30 <- Convolution30
I0818 10:45:01.112154  2034 net.cpp:509] BatchNorm30 -> Convolution30 (in-place)
I0818 10:45:01.112481  2034 net.cpp:172] Setting up BatchNorm30
I0818 10:45:01.112491  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.112495  2034 net.cpp:194] Memory required for data: 65331320
I0818 10:45:01.112505  2034 layer_factory.hpp:77] Creating layer Scale30
I0818 10:45:01.112512  2034 net.cpp:128] Creating Layer Scale30
I0818 10:45:01.112516  2034 net.cpp:558] Scale30 <- Convolution30
I0818 10:45:01.112522  2034 net.cpp:509] Scale30 -> Convolution30 (in-place)
I0818 10:45:01.112579  2034 layer_factory.hpp:77] Creating layer Scale30
I0818 10:45:01.112758  2034 net.cpp:172] Setting up Scale30
I0818 10:45:01.112771  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.112774  2034 net.cpp:194] Memory required for data: 65495160
I0818 10:45:01.112782  2034 layer_factory.hpp:77] Creating layer ReLU28
I0818 10:45:01.112788  2034 net.cpp:128] Creating Layer ReLU28
I0818 10:45:01.112793  2034 net.cpp:558] ReLU28 <- Convolution30
I0818 10:45:01.112799  2034 net.cpp:509] ReLU28 -> Convolution30 (in-place)
I0818 10:45:01.113056  2034 net.cpp:172] Setting up ReLU28
I0818 10:45:01.113072  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.113080  2034 net.cpp:194] Memory required for data: 65659000
I0818 10:45:01.113085  2034 layer_factory.hpp:77] Creating layer Convolution31
I0818 10:45:01.113097  2034 net.cpp:128] Creating Layer Convolution31
I0818 10:45:01.113104  2034 net.cpp:558] Convolution31 <- Convolution30
I0818 10:45:01.113112  2034 net.cpp:522] Convolution31 -> Convolution31
I0818 10:45:01.115089  2034 net.cpp:172] Setting up Convolution31
I0818 10:45:01.115114  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.115119  2034 net.cpp:194] Memory required for data: 65822840
I0818 10:45:01.115134  2034 layer_factory.hpp:77] Creating layer BatchNorm31
I0818 10:45:01.115149  2034 net.cpp:128] Creating Layer BatchNorm31
I0818 10:45:01.115154  2034 net.cpp:558] BatchNorm31 <- Convolution31
I0818 10:45:01.115161  2034 net.cpp:509] BatchNorm31 -> Convolution31 (in-place)
I0818 10:45:01.115486  2034 net.cpp:172] Setting up BatchNorm31
I0818 10:45:01.115497  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.115501  2034 net.cpp:194] Memory required for data: 65986680
I0818 10:45:01.115511  2034 layer_factory.hpp:77] Creating layer Scale31
I0818 10:45:01.115519  2034 net.cpp:128] Creating Layer Scale31
I0818 10:45:01.115523  2034 net.cpp:558] Scale31 <- Convolution31
I0818 10:45:01.115530  2034 net.cpp:509] Scale31 -> Convolution31 (in-place)
I0818 10:45:01.115588  2034 layer_factory.hpp:77] Creating layer Scale31
I0818 10:45:01.115789  2034 net.cpp:172] Setting up Scale31
I0818 10:45:01.115799  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.115803  2034 net.cpp:194] Memory required for data: 66150520
I0818 10:45:01.115813  2034 layer_factory.hpp:77] Creating layer Eltwise14
I0818 10:45:01.115818  2034 net.cpp:128] Creating Layer Eltwise14
I0818 10:45:01.115826  2034 net.cpp:558] Eltwise14 <- Eltwise13_ReLU27_0_split_1
I0818 10:45:01.115833  2034 net.cpp:558] Eltwise14 <- Convolution31
I0818 10:45:01.115841  2034 net.cpp:522] Eltwise14 -> Eltwise14
I0818 10:45:01.115872  2034 net.cpp:172] Setting up Eltwise14
I0818 10:45:01.115883  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.115887  2034 net.cpp:194] Memory required for data: 66314360
I0818 10:45:01.115891  2034 layer_factory.hpp:77] Creating layer ReLU29
I0818 10:45:01.115898  2034 net.cpp:128] Creating Layer ReLU29
I0818 10:45:01.115902  2034 net.cpp:558] ReLU29 <- Eltwise14
I0818 10:45:01.115908  2034 net.cpp:509] ReLU29 -> Eltwise14 (in-place)
I0818 10:45:01.116348  2034 net.cpp:172] Setting up ReLU29
I0818 10:45:01.116367  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.116371  2034 net.cpp:194] Memory required for data: 66478200
I0818 10:45:01.116377  2034 layer_factory.hpp:77] Creating layer Eltwise14_ReLU29_0_split
I0818 10:45:01.116386  2034 net.cpp:128] Creating Layer Eltwise14_ReLU29_0_split
I0818 10:45:01.116392  2034 net.cpp:558] Eltwise14_ReLU29_0_split <- Eltwise14
I0818 10:45:01.116403  2034 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_0
I0818 10:45:01.116412  2034 net.cpp:522] Eltwise14_ReLU29_0_split -> Eltwise14_ReLU29_0_split_1
I0818 10:45:01.116482  2034 net.cpp:172] Setting up Eltwise14_ReLU29_0_split
I0818 10:45:01.116492  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.116497  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.116502  2034 net.cpp:194] Memory required for data: 66805880
I0818 10:45:01.116505  2034 layer_factory.hpp:77] Creating layer Convolution32
I0818 10:45:01.116518  2034 net.cpp:128] Creating Layer Convolution32
I0818 10:45:01.116523  2034 net.cpp:558] Convolution32 <- Eltwise14_ReLU29_0_split_0
I0818 10:45:01.116533  2034 net.cpp:522] Convolution32 -> Convolution32
I0818 10:45:01.118647  2034 net.cpp:172] Setting up Convolution32
I0818 10:45:01.118685  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.118690  2034 net.cpp:194] Memory required for data: 66969720
I0818 10:45:01.118703  2034 layer_factory.hpp:77] Creating layer BatchNorm32
I0818 10:45:01.118713  2034 net.cpp:128] Creating Layer BatchNorm32
I0818 10:45:01.118721  2034 net.cpp:558] BatchNorm32 <- Convolution32
I0818 10:45:01.118728  2034 net.cpp:509] BatchNorm32 -> Convolution32 (in-place)
I0818 10:45:01.119060  2034 net.cpp:172] Setting up BatchNorm32
I0818 10:45:01.119071  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.119074  2034 net.cpp:194] Memory required for data: 67133560
I0818 10:45:01.119086  2034 layer_factory.hpp:77] Creating layer Scale32
I0818 10:45:01.119092  2034 net.cpp:128] Creating Layer Scale32
I0818 10:45:01.119098  2034 net.cpp:558] Scale32 <- Convolution32
I0818 10:45:01.119104  2034 net.cpp:509] Scale32 -> Convolution32 (in-place)
I0818 10:45:01.119163  2034 layer_factory.hpp:77] Creating layer Scale32
I0818 10:45:01.119348  2034 net.cpp:172] Setting up Scale32
I0818 10:45:01.119359  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.119364  2034 net.cpp:194] Memory required for data: 67297400
I0818 10:45:01.119372  2034 layer_factory.hpp:77] Creating layer ReLU30
I0818 10:45:01.119379  2034 net.cpp:128] Creating Layer ReLU30
I0818 10:45:01.119382  2034 net.cpp:558] ReLU30 <- Convolution32
I0818 10:45:01.119388  2034 net.cpp:509] ReLU30 -> Convolution32 (in-place)
I0818 10:45:01.119846  2034 net.cpp:172] Setting up ReLU30
I0818 10:45:01.119869  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.119874  2034 net.cpp:194] Memory required for data: 67461240
I0818 10:45:01.119879  2034 layer_factory.hpp:77] Creating layer Convolution33
I0818 10:45:01.119910  2034 net.cpp:128] Creating Layer Convolution33
I0818 10:45:01.119915  2034 net.cpp:558] Convolution33 <- Convolution32
I0818 10:45:01.119923  2034 net.cpp:522] Convolution33 -> Convolution33
I0818 10:45:01.121901  2034 net.cpp:172] Setting up Convolution33
I0818 10:45:01.121927  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.121930  2034 net.cpp:194] Memory required for data: 67625080
I0818 10:45:01.121942  2034 layer_factory.hpp:77] Creating layer BatchNorm33
I0818 10:45:01.121951  2034 net.cpp:128] Creating Layer BatchNorm33
I0818 10:45:01.121958  2034 net.cpp:558] BatchNorm33 <- Convolution33
I0818 10:45:01.121968  2034 net.cpp:509] BatchNorm33 -> Convolution33 (in-place)
I0818 10:45:01.122288  2034 net.cpp:172] Setting up BatchNorm33
I0818 10:45:01.122300  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.122304  2034 net.cpp:194] Memory required for data: 67788920
I0818 10:45:01.122314  2034 layer_factory.hpp:77] Creating layer Scale33
I0818 10:45:01.122320  2034 net.cpp:128] Creating Layer Scale33
I0818 10:45:01.122325  2034 net.cpp:558] Scale33 <- Convolution33
I0818 10:45:01.122331  2034 net.cpp:509] Scale33 -> Convolution33 (in-place)
I0818 10:45:01.122390  2034 layer_factory.hpp:77] Creating layer Scale33
I0818 10:45:01.122575  2034 net.cpp:172] Setting up Scale33
I0818 10:45:01.122584  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.122589  2034 net.cpp:194] Memory required for data: 67952760
I0818 10:45:01.122596  2034 layer_factory.hpp:77] Creating layer Eltwise15
I0818 10:45:01.122606  2034 net.cpp:128] Creating Layer Eltwise15
I0818 10:45:01.122612  2034 net.cpp:558] Eltwise15 <- Eltwise14_ReLU29_0_split_1
I0818 10:45:01.122617  2034 net.cpp:558] Eltwise15 <- Convolution33
I0818 10:45:01.122623  2034 net.cpp:522] Eltwise15 -> Eltwise15
I0818 10:45:01.122673  2034 net.cpp:172] Setting up Eltwise15
I0818 10:45:01.122685  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.122690  2034 net.cpp:194] Memory required for data: 68116600
I0818 10:45:01.122695  2034 layer_factory.hpp:77] Creating layer ReLU31
I0818 10:45:01.122701  2034 net.cpp:128] Creating Layer ReLU31
I0818 10:45:01.122705  2034 net.cpp:558] ReLU31 <- Eltwise15
I0818 10:45:01.122711  2034 net.cpp:509] ReLU31 -> Eltwise15 (in-place)
I0818 10:45:01.122967  2034 net.cpp:172] Setting up ReLU31
I0818 10:45:01.122980  2034 net.cpp:186] Top shape: 10 64 8 8 (40960)
I0818 10:45:01.122984  2034 net.cpp:194] Memory required for data: 68280440
I0818 10:45:01.122989  2034 layer_factory.hpp:77] Creating layer Pooling1
I0818 10:45:01.122997  2034 net.cpp:128] Creating Layer Pooling1
I0818 10:45:01.123003  2034 net.cpp:558] Pooling1 <- Eltwise15
I0818 10:45:01.123011  2034 net.cpp:522] Pooling1 -> Pooling1
I0818 10:45:01.123284  2034 net.cpp:172] Setting up Pooling1
I0818 10:45:01.123298  2034 net.cpp:186] Top shape: 10 64 1 1 (640)
I0818 10:45:01.123303  2034 net.cpp:194] Memory required for data: 68283000
I0818 10:45:01.123307  2034 layer_factory.hpp:77] Creating layer InnerProduct1
I0818 10:45:01.123319  2034 net.cpp:128] Creating Layer InnerProduct1
I0818 10:45:01.123324  2034 net.cpp:558] InnerProduct1 <- Pooling1
I0818 10:45:01.123333  2034 net.cpp:522] InnerProduct1 -> InnerProduct1
I0818 10:45:01.123517  2034 net.cpp:172] Setting up InnerProduct1
I0818 10:45:01.123529  2034 net.cpp:186] Top shape: 10 10 (100)
I0818 10:45:01.123533  2034 net.cpp:194] Memory required for data: 68283400
I0818 10:45:01.123543  2034 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I0818 10:45:01.123553  2034 net.cpp:128] Creating Layer InnerProduct1_InnerProduct1_0_split
I0818 10:45:01.123558  2034 net.cpp:558] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I0818 10:45:01.123564  2034 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I0818 10:45:01.123571  2034 net.cpp:522] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I0818 10:45:01.123627  2034 net.cpp:172] Setting up InnerProduct1_InnerProduct1_0_split
I0818 10:45:01.123652  2034 net.cpp:186] Top shape: 10 10 (100)
I0818 10:45:01.123658  2034 net.cpp:186] Top shape: 10 10 (100)
I0818 10:45:01.123661  2034 net.cpp:194] Memory required for data: 68284200
I0818 10:45:01.123667  2034 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:45:01.123673  2034 net.cpp:128] Creating Layer SoftmaxWithLoss1
I0818 10:45:01.123678  2034 net.cpp:558] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I0818 10:45:01.123687  2034 net.cpp:558] SoftmaxWithLoss1 <- Data2_Data1_1_split_0
I0818 10:45:01.123694  2034 net.cpp:522] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I0818 10:45:01.123704  2034 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I0818 10:45:01.124321  2034 net.cpp:172] Setting up SoftmaxWithLoss1
I0818 10:45:01.124347  2034 net.cpp:186] Top shape: (1)
I0818 10:45:01.124352  2034 net.cpp:189]     with loss weight 1
I0818 10:45:01.124366  2034 net.cpp:194] Memory required for data: 68284204
I0818 10:45:01.124372  2034 layer_factory.hpp:77] Creating layer Accuracy1
I0818 10:45:01.124380  2034 net.cpp:128] Creating Layer Accuracy1
I0818 10:45:01.124385  2034 net.cpp:558] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I0818 10:45:01.124392  2034 net.cpp:558] Accuracy1 <- Data2_Data1_1_split_1
I0818 10:45:01.124398  2034 net.cpp:522] Accuracy1 -> Accuracy1
I0818 10:45:01.124413  2034 net.cpp:172] Setting up Accuracy1
I0818 10:45:01.124418  2034 net.cpp:186] Top shape: (1)
I0818 10:45:01.124421  2034 net.cpp:194] Memory required for data: 68284208
I0818 10:45:01.124426  2034 net.cpp:303] Accuracy1 does not need backward computation.
I0818 10:45:01.124431  2034 net.cpp:301] SoftmaxWithLoss1 needs backward computation.
I0818 10:45:01.124436  2034 net.cpp:301] InnerProduct1_InnerProduct1_0_split needs backward computation.
I0818 10:45:01.124441  2034 net.cpp:301] InnerProduct1 needs backward computation.
I0818 10:45:01.124445  2034 net.cpp:301] Pooling1 needs backward computation.
I0818 10:45:01.124450  2034 net.cpp:301] ReLU31 needs backward computation.
I0818 10:45:01.124454  2034 net.cpp:301] Eltwise15 needs backward computation.
I0818 10:45:01.124459  2034 net.cpp:301] Scale33 needs backward computation.
I0818 10:45:01.124464  2034 net.cpp:301] BatchNorm33 needs backward computation.
I0818 10:45:01.124467  2034 net.cpp:301] Convolution33 needs backward computation.
I0818 10:45:01.124471  2034 net.cpp:301] ReLU30 needs backward computation.
I0818 10:45:01.124475  2034 net.cpp:301] Scale32 needs backward computation.
I0818 10:45:01.124480  2034 net.cpp:301] BatchNorm32 needs backward computation.
I0818 10:45:01.124485  2034 net.cpp:301] Convolution32 needs backward computation.
I0818 10:45:01.124491  2034 net.cpp:301] Eltwise14_ReLU29_0_split needs backward computation.
I0818 10:45:01.124495  2034 net.cpp:301] ReLU29 needs backward computation.
I0818 10:45:01.124500  2034 net.cpp:301] Eltwise14 needs backward computation.
I0818 10:45:01.124505  2034 net.cpp:301] Scale31 needs backward computation.
I0818 10:45:01.124508  2034 net.cpp:301] BatchNorm31 needs backward computation.
I0818 10:45:01.124512  2034 net.cpp:301] Convolution31 needs backward computation.
I0818 10:45:01.124516  2034 net.cpp:301] ReLU28 needs backward computation.
I0818 10:45:01.124521  2034 net.cpp:301] Scale30 needs backward computation.
I0818 10:45:01.124524  2034 net.cpp:301] BatchNorm30 needs backward computation.
I0818 10:45:01.124528  2034 net.cpp:301] Convolution30 needs backward computation.
I0818 10:45:01.124533  2034 net.cpp:301] Eltwise13_ReLU27_0_split needs backward computation.
I0818 10:45:01.124537  2034 net.cpp:301] ReLU27 needs backward computation.
I0818 10:45:01.124542  2034 net.cpp:301] Eltwise13 needs backward computation.
I0818 10:45:01.124547  2034 net.cpp:301] Scale29 needs backward computation.
I0818 10:45:01.124550  2034 net.cpp:301] BatchNorm29 needs backward computation.
I0818 10:45:01.124554  2034 net.cpp:301] Convolution29 needs backward computation.
I0818 10:45:01.124559  2034 net.cpp:301] ReLU26 needs backward computation.
I0818 10:45:01.124577  2034 net.cpp:301] Scale28 needs backward computation.
I0818 10:45:01.124583  2034 net.cpp:301] BatchNorm28 needs backward computation.
I0818 10:45:01.124586  2034 net.cpp:301] Convolution28 needs backward computation.
I0818 10:45:01.124591  2034 net.cpp:301] Eltwise12_ReLU25_0_split needs backward computation.
I0818 10:45:01.124595  2034 net.cpp:301] ReLU25 needs backward computation.
I0818 10:45:01.124599  2034 net.cpp:301] Eltwise12 needs backward computation.
I0818 10:45:01.124604  2034 net.cpp:301] Scale27 needs backward computation.
I0818 10:45:01.124608  2034 net.cpp:301] BatchNorm27 needs backward computation.
I0818 10:45:01.124613  2034 net.cpp:301] Convolution27 needs backward computation.
I0818 10:45:01.124617  2034 net.cpp:301] ReLU24 needs backward computation.
I0818 10:45:01.124621  2034 net.cpp:301] Scale26 needs backward computation.
I0818 10:45:01.124625  2034 net.cpp:301] BatchNorm26 needs backward computation.
I0818 10:45:01.124629  2034 net.cpp:301] Convolution26 needs backward computation.
I0818 10:45:01.124634  2034 net.cpp:301] Eltwise11_ReLU23_0_split needs backward computation.
I0818 10:45:01.124639  2034 net.cpp:301] ReLU23 needs backward computation.
I0818 10:45:01.124644  2034 net.cpp:301] Eltwise11 needs backward computation.
I0818 10:45:01.124649  2034 net.cpp:301] Scale25 needs backward computation.
I0818 10:45:01.124655  2034 net.cpp:301] BatchNorm25 needs backward computation.
I0818 10:45:01.124660  2034 net.cpp:301] Convolution25 needs backward computation.
I0818 10:45:01.124663  2034 net.cpp:301] ReLU22 needs backward computation.
I0818 10:45:01.124668  2034 net.cpp:301] Scale24 needs backward computation.
I0818 10:45:01.124672  2034 net.cpp:301] BatchNorm24 needs backward computation.
I0818 10:45:01.124676  2034 net.cpp:301] Convolution24 needs backward computation.
I0818 10:45:01.124681  2034 net.cpp:301] Scale23 needs backward computation.
I0818 10:45:01.124686  2034 net.cpp:301] BatchNorm23 needs backward computation.
I0818 10:45:01.124689  2034 net.cpp:301] Convolution23 needs backward computation.
I0818 10:45:01.124694  2034 net.cpp:301] Eltwise10_ReLU21_0_split needs backward computation.
I0818 10:45:01.124699  2034 net.cpp:301] ReLU21 needs backward computation.
I0818 10:45:01.124703  2034 net.cpp:301] Eltwise10 needs backward computation.
I0818 10:45:01.124708  2034 net.cpp:301] Scale22 needs backward computation.
I0818 10:45:01.124712  2034 net.cpp:301] BatchNorm22 needs backward computation.
I0818 10:45:01.124717  2034 net.cpp:301] Convolution22 needs backward computation.
I0818 10:45:01.124722  2034 net.cpp:301] ReLU20 needs backward computation.
I0818 10:45:01.124725  2034 net.cpp:301] Scale21 needs backward computation.
I0818 10:45:01.124729  2034 net.cpp:301] BatchNorm21 needs backward computation.
I0818 10:45:01.124734  2034 net.cpp:301] Convolution21 needs backward computation.
I0818 10:45:01.124739  2034 net.cpp:301] Eltwise9_ReLU19_0_split needs backward computation.
I0818 10:45:01.124744  2034 net.cpp:301] ReLU19 needs backward computation.
I0818 10:45:01.124748  2034 net.cpp:301] Eltwise9 needs backward computation.
I0818 10:45:01.124753  2034 net.cpp:301] Scale20 needs backward computation.
I0818 10:45:01.124758  2034 net.cpp:301] BatchNorm20 needs backward computation.
I0818 10:45:01.124763  2034 net.cpp:301] Convolution20 needs backward computation.
I0818 10:45:01.124766  2034 net.cpp:301] ReLU18 needs backward computation.
I0818 10:45:01.124771  2034 net.cpp:301] Scale19 needs backward computation.
I0818 10:45:01.124775  2034 net.cpp:301] BatchNorm19 needs backward computation.
I0818 10:45:01.124779  2034 net.cpp:301] Convolution19 needs backward computation.
I0818 10:45:01.124784  2034 net.cpp:301] Eltwise8_ReLU17_0_split needs backward computation.
I0818 10:45:01.124789  2034 net.cpp:301] ReLU17 needs backward computation.
I0818 10:45:01.124794  2034 net.cpp:301] Eltwise8 needs backward computation.
I0818 10:45:01.124799  2034 net.cpp:301] Scale18 needs backward computation.
I0818 10:45:01.124804  2034 net.cpp:301] BatchNorm18 needs backward computation.
I0818 10:45:01.124814  2034 net.cpp:301] Convolution18 needs backward computation.
I0818 10:45:01.124819  2034 net.cpp:301] ReLU16 needs backward computation.
I0818 10:45:01.124825  2034 net.cpp:301] Scale17 needs backward computation.
I0818 10:45:01.124828  2034 net.cpp:301] BatchNorm17 needs backward computation.
I0818 10:45:01.124832  2034 net.cpp:301] Convolution17 needs backward computation.
I0818 10:45:01.124837  2034 net.cpp:301] Eltwise7_ReLU15_0_split needs backward computation.
I0818 10:45:01.124842  2034 net.cpp:301] ReLU15 needs backward computation.
I0818 10:45:01.124847  2034 net.cpp:301] Eltwise7 needs backward computation.
I0818 10:45:01.124852  2034 net.cpp:301] Scale16 needs backward computation.
I0818 10:45:01.124856  2034 net.cpp:301] BatchNorm16 needs backward computation.
I0818 10:45:01.124861  2034 net.cpp:301] Convolution16 needs backward computation.
I0818 10:45:01.124866  2034 net.cpp:301] ReLU14 needs backward computation.
I0818 10:45:01.124869  2034 net.cpp:301] Scale15 needs backward computation.
I0818 10:45:01.124873  2034 net.cpp:301] BatchNorm15 needs backward computation.
I0818 10:45:01.124878  2034 net.cpp:301] Convolution15 needs backward computation.
I0818 10:45:01.124886  2034 net.cpp:301] Eltwise6_ReLU13_0_split needs backward computation.
I0818 10:45:01.124891  2034 net.cpp:301] ReLU13 needs backward computation.
I0818 10:45:01.124894  2034 net.cpp:301] Eltwise6 needs backward computation.
I0818 10:45:01.124898  2034 net.cpp:301] Scale14 needs backward computation.
I0818 10:45:01.124903  2034 net.cpp:301] BatchNorm14 needs backward computation.
I0818 10:45:01.124907  2034 net.cpp:301] Convolution14 needs backward computation.
I0818 10:45:01.124912  2034 net.cpp:301] ReLU12 needs backward computation.
I0818 10:45:01.124917  2034 net.cpp:301] Scale13 needs backward computation.
I0818 10:45:01.124922  2034 net.cpp:301] BatchNorm13 needs backward computation.
I0818 10:45:01.124925  2034 net.cpp:301] Convolution13 needs backward computation.
I0818 10:45:01.124930  2034 net.cpp:301] Scale12 needs backward computation.
I0818 10:45:01.124934  2034 net.cpp:301] BatchNorm12 needs backward computation.
I0818 10:45:01.124938  2034 net.cpp:301] Convolution12 needs backward computation.
I0818 10:45:01.124943  2034 net.cpp:301] Eltwise5_ReLU11_0_split needs backward computation.
I0818 10:45:01.124948  2034 net.cpp:301] ReLU11 needs backward computation.
I0818 10:45:01.124953  2034 net.cpp:301] Eltwise5 needs backward computation.
I0818 10:45:01.124958  2034 net.cpp:301] Scale11 needs backward computation.
I0818 10:45:01.124963  2034 net.cpp:301] BatchNorm11 needs backward computation.
I0818 10:45:01.124966  2034 net.cpp:301] Convolution11 needs backward computation.
I0818 10:45:01.124970  2034 net.cpp:301] ReLU10 needs backward computation.
I0818 10:45:01.124975  2034 net.cpp:301] Scale10 needs backward computation.
I0818 10:45:01.124979  2034 net.cpp:301] BatchNorm10 needs backward computation.
I0818 10:45:01.124984  2034 net.cpp:301] Convolution10 needs backward computation.
I0818 10:45:01.124989  2034 net.cpp:301] Eltwise4_ReLU9_0_split needs backward computation.
I0818 10:45:01.124994  2034 net.cpp:301] ReLU9 needs backward computation.
I0818 10:45:01.124997  2034 net.cpp:301] Eltwise4 needs backward computation.
I0818 10:45:01.125003  2034 net.cpp:301] Scale9 needs backward computation.
I0818 10:45:01.125007  2034 net.cpp:301] BatchNorm9 needs backward computation.
I0818 10:45:01.125012  2034 net.cpp:301] Convolution9 needs backward computation.
I0818 10:45:01.125017  2034 net.cpp:301] ReLU8 needs backward computation.
I0818 10:45:01.125021  2034 net.cpp:301] Scale8 needs backward computation.
I0818 10:45:01.125026  2034 net.cpp:301] BatchNorm8 needs backward computation.
I0818 10:45:01.125030  2034 net.cpp:301] Convolution8 needs backward computation.
I0818 10:45:01.125036  2034 net.cpp:301] Eltwise3_ReLU7_0_split needs backward computation.
I0818 10:45:01.125041  2034 net.cpp:301] ReLU7 needs backward computation.
I0818 10:45:01.125044  2034 net.cpp:301] Eltwise3 needs backward computation.
I0818 10:45:01.125059  2034 net.cpp:301] Scale7 needs backward computation.
I0818 10:45:01.125066  2034 net.cpp:301] BatchNorm7 needs backward computation.
I0818 10:45:01.125069  2034 net.cpp:301] Convolution7 needs backward computation.
I0818 10:45:01.125074  2034 net.cpp:301] ReLU6 needs backward computation.
I0818 10:45:01.125078  2034 net.cpp:301] Scale6 needs backward computation.
I0818 10:45:01.125083  2034 net.cpp:301] BatchNorm6 needs backward computation.
I0818 10:45:01.125088  2034 net.cpp:301] Convolution6 needs backward computation.
I0818 10:45:01.125093  2034 net.cpp:301] Eltwise2_ReLU5_0_split needs backward computation.
I0818 10:45:01.125098  2034 net.cpp:301] ReLU5 needs backward computation.
I0818 10:45:01.125102  2034 net.cpp:301] Eltwise2 needs backward computation.
I0818 10:45:01.125108  2034 net.cpp:301] Scale5 needs backward computation.
I0818 10:45:01.125113  2034 net.cpp:301] BatchNorm5 needs backward computation.
I0818 10:45:01.125116  2034 net.cpp:301] Convolution5 needs backward computation.
I0818 10:45:01.125120  2034 net.cpp:301] ReLU4 needs backward computation.
I0818 10:45:01.125125  2034 net.cpp:301] Scale4 needs backward computation.
I0818 10:45:01.125129  2034 net.cpp:301] BatchNorm4 needs backward computation.
I0818 10:45:01.125134  2034 net.cpp:301] Convolution4 needs backward computation.
I0818 10:45:01.125138  2034 net.cpp:301] Eltwise1_ReLU3_0_split needs backward computation.
I0818 10:45:01.125144  2034 net.cpp:301] ReLU3 needs backward computation.
I0818 10:45:01.125147  2034 net.cpp:301] Eltwise1 needs backward computation.
I0818 10:45:01.125154  2034 net.cpp:301] Scale3 needs backward computation.
I0818 10:45:01.125157  2034 net.cpp:301] BatchNorm3 needs backward computation.
I0818 10:45:01.125161  2034 net.cpp:301] Convolution3 needs backward computation.
I0818 10:45:01.125166  2034 net.cpp:301] ReLU2 needs backward computation.
I0818 10:45:01.125171  2034 net.cpp:301] Scale2 needs backward computation.
I0818 10:45:01.125175  2034 net.cpp:301] BatchNorm2 needs backward computation.
I0818 10:45:01.125180  2034 net.cpp:301] Convolution2 needs backward computation.
I0818 10:45:01.125185  2034 net.cpp:301] Convolution1_ReLU1_0_split needs backward computation.
I0818 10:45:01.125190  2034 net.cpp:301] ReLU1 needs backward computation.
I0818 10:45:01.125195  2034 net.cpp:301] Scale1 needs backward computation.
I0818 10:45:01.125200  2034 net.cpp:301] BatchNorm1 needs backward computation.
I0818 10:45:01.125203  2034 net.cpp:301] Convolution1 needs backward computation.
I0818 10:45:01.125208  2034 net.cpp:303] Data2_Data1_1_split does not need backward computation.
I0818 10:45:01.125214  2034 net.cpp:303] Data1 does not need backward computation.
I0818 10:45:01.125218  2034 net.cpp:348] This network produces output Accuracy1
I0818 10:45:01.125222  2034 net.cpp:348] This network produces output SoftmaxWithLoss1
I0818 10:45:01.125309  2034 net.cpp:363] Network initialization done.
I0818 10:45:01.125843  2034 solver.cpp:110] Solver scaffolding done.
I0818 10:45:01.139701  2034 caffe.cpp:313] Starting Optimization
I0818 10:45:01.139722  2034 solver.cpp:425] Solving resnet_cifar10
I0818 10:45:01.139726  2034 solver.cpp:427] Learning Rate Policy: multistep
I0818 10:45:01.144841  2034 solver.cpp:514] Iteration 0, Testing net (#0)
I0818 10:45:05.975519  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:45:05.994382  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0
I0818 10:45:05.994423  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.30259 (* 1 = 2.30259 loss)
I0818 10:45:06.154834  2034 solver.cpp:357] Iteration 0 (-1.64901e+10 iter/s, 5.01455s/100 iters), loss = 4.65298
I0818 10:45:06.154893  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 4.65298 (* 1 = 4.65298 loss)
I0818 10:45:06.154925  2034 sgd_solver.cpp:165] Iteration 0, lr = 0.1
I0818 10:45:23.373916  2034 solver.cpp:357] Iteration 100 (5.8079 iter/s, 17.2179s/100 iters), loss = 1.79731
I0818 10:45:23.373976  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.79731 (* 1 = 1.79731 loss)
I0818 10:45:23.374022  2034 sgd_solver.cpp:165] Iteration 100, lr = 0.1
I0818 10:45:40.613245  2034 solver.cpp:357] Iteration 200 (5.80083 iter/s, 17.2389s/100 iters), loss = 1.6218
I0818 10:45:40.613466  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.6218 (* 1 = 1.6218 loss)
I0818 10:45:40.613476  2034 sgd_solver.cpp:165] Iteration 200, lr = 0.1
I0818 10:45:57.858494  2034 solver.cpp:357] Iteration 300 (5.79875 iter/s, 17.2451s/100 iters), loss = 1.5644
I0818 10:45:57.858556  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.5644 (* 1 = 1.5644 loss)
I0818 10:45:57.858566  2034 sgd_solver.cpp:165] Iteration 300, lr = 0.1
I0818 10:46:12.719022  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:46:15.122843  2034 solver.cpp:357] Iteration 400 (5.7922 iter/s, 17.2646s/100 iters), loss = 1.26407
I0818 10:46:15.122905  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.26407 (* 1 = 1.26407 loss)
I0818 10:46:15.122916  2034 sgd_solver.cpp:165] Iteration 400, lr = 0.1
I0818 10:46:32.211536  2034 solver.cpp:514] Iteration 500, Testing net (#0)
I0818 10:46:37.124505  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:46:37.143466  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.120399
I0818 10:46:37.143510  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 4.3225 (* 1 = 4.3225 loss)
I0818 10:46:37.314504  2034 solver.cpp:357] Iteration 500 (4.50608 iter/s, 22.1922s/100 iters), loss = 1.12847
I0818 10:46:37.314548  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.12847 (* 1 = 1.12847 loss)
I0818 10:46:37.314559  2034 sgd_solver.cpp:165] Iteration 500, lr = 0.1
I0818 10:46:54.576597  2034 solver.cpp:357] Iteration 600 (5.79262 iter/s, 17.2634s/100 iters), loss = 1.16407
I0818 10:46:54.576747  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.16407 (* 1 = 1.16407 loss)
I0818 10:46:54.576757  2034 sgd_solver.cpp:165] Iteration 600, lr = 0.1
I0818 10:47:11.853915  2034 solver.cpp:357] Iteration 700 (5.78758 iter/s, 17.2784s/100 iters), loss = 0.915313
I0818 10:47:11.853978  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.915313 (* 1 = 0.915313 loss)
I0818 10:47:11.853988  2034 sgd_solver.cpp:165] Iteration 700, lr = 0.1
I0818 10:47:25.167737  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:47:29.138620  2034 solver.cpp:357] Iteration 800 (5.78512 iter/s, 17.2857s/100 iters), loss = 0.939753
I0818 10:47:29.138687  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.939753 (* 1 = 0.939753 loss)
I0818 10:47:29.138700  2034 sgd_solver.cpp:165] Iteration 800, lr = 0.1
I0818 10:47:46.418462  2034 solver.cpp:357] Iteration 900 (5.78678 iter/s, 17.2808s/100 iters), loss = 1.01179
I0818 10:47:46.418526  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 1.01179 (* 1 = 1.01179 loss)
I0818 10:47:46.418536  2034 sgd_solver.cpp:165] Iteration 900, lr = 0.1
I0818 10:48:03.521809  2034 solver.cpp:514] Iteration 1000, Testing net (#0)
I0818 10:48:08.436144  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:48:08.454989  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1414
I0818 10:48:08.455031  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.61285 (* 1 = 2.61285 loss)
I0818 10:48:08.625543  2034 solver.cpp:357] Iteration 1000 (4.50284 iter/s, 22.2082s/100 iters), loss = 0.834196
I0818 10:48:08.625587  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.834196 (* 1 = 0.834196 loss)
I0818 10:48:08.625600  2034 sgd_solver.cpp:165] Iteration 1000, lr = 0.1
I0818 10:48:25.895103  2034 solver.cpp:357] Iteration 1100 (5.79026 iter/s, 17.2704s/100 iters), loss = 0.722277
I0818 10:48:25.895164  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.722277 (* 1 = 0.722277 loss)
I0818 10:48:25.895177  2034 sgd_solver.cpp:165] Iteration 1100, lr = 0.1
I0818 10:48:37.478606  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:48:43.170136  2034 solver.cpp:357] Iteration 1200 (5.78844 iter/s, 17.2758s/100 iters), loss = 0.704245
I0818 10:48:43.170197  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.704245 (* 1 = 0.704245 loss)
I0818 10:48:43.170207  2034 sgd_solver.cpp:165] Iteration 1200, lr = 0.1
I0818 10:49:00.440276  2034 solver.cpp:357] Iteration 1300 (5.79009 iter/s, 17.2709s/100 iters), loss = 0.73568
I0818 10:49:00.440341  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.73568 (* 1 = 0.73568 loss)
I0818 10:49:00.440353  2034 sgd_solver.cpp:165] Iteration 1300, lr = 0.1
I0818 10:49:18.161406  2034 solver.cpp:357] Iteration 1400 (5.64274 iter/s, 17.7219s/100 iters), loss = 0.679582
I0818 10:49:18.161571  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.679582 (* 1 = 0.679582 loss)
I0818 10:49:18.161583  2034 sgd_solver.cpp:165] Iteration 1400, lr = 0.1
I0818 10:49:47.613057  2034 solver.cpp:514] Iteration 1500, Testing net (#0)
I0818 10:50:14.433326  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:50:14.546838  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.117199
I0818 10:50:14.546876  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.73362 (* 1 = 2.73362 loss)
I0818 10:50:14.839578  2034 solver.cpp:357] Iteration 1500 (1.76428 iter/s, 56.6805s/100 iters), loss = 0.689396
I0818 10:50:14.839638  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.689396 (* 1 = 0.689396 loss)
I0818 10:50:14.839653  2034 sgd_solver.cpp:165] Iteration 1500, lr = 0.1
I0818 10:50:36.215922  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:50:51.569489  2034 solver.cpp:357] Iteration 1600 (2.72262 iter/s, 36.7294s/100 iters), loss = 0.654887
I0818 10:50:51.569623  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.654887 (* 1 = 0.654887 loss)
I0818 10:50:51.569639  2034 sgd_solver.cpp:165] Iteration 1600, lr = 0.1
I0818 10:51:28.325637  2034 solver.cpp:357] Iteration 1700 (2.72081 iter/s, 36.7537s/100 iters), loss = 0.835456
I0818 10:51:28.325800  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.835456 (* 1 = 0.835456 loss)
I0818 10:51:28.325812  2034 sgd_solver.cpp:165] Iteration 1700, lr = 0.1
I0818 10:52:05.171655  2034 solver.cpp:357] Iteration 1800 (2.71415 iter/s, 36.8439s/100 iters), loss = 0.796111
I0818 10:52:05.171831  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.796111 (* 1 = 0.796111 loss)
I0818 10:52:05.171842  2034 sgd_solver.cpp:165] Iteration 1800, lr = 0.1
I0818 10:52:41.951241  2034 solver.cpp:357] Iteration 1900 (2.71903 iter/s, 36.7778s/100 iters), loss = 0.665095
I0818 10:52:41.951417  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.665095 (* 1 = 0.665095 loss)
I0818 10:52:41.951428  2034 sgd_solver.cpp:165] Iteration 1900, lr = 0.1
I0818 10:53:00.072902  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:53:18.483094  2034 solver.cpp:514] Iteration 2000, Testing net (#0)
I0818 10:53:45.899775  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:53:45.970991  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1185
I0818 10:53:45.971040  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.6101 (* 1 = 2.6101 loss)
I0818 10:53:46.237545  2034 solver.cpp:357] Iteration 2000 (1.55556 iter/s, 64.2853s/100 iters), loss = 0.562502
I0818 10:53:46.237596  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.562502 (* 1 = 0.562502 loss)
I0818 10:53:46.237610  2034 sgd_solver.cpp:165] Iteration 2000, lr = 0.1
I0818 10:54:15.902218  2034 solver.cpp:357] Iteration 2100 (3.37094 iter/s, 29.6653s/100 iters), loss = 0.586026
I0818 10:54:15.902411  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.586026 (* 1 = 0.586026 loss)
I0818 10:54:15.902424  2034 sgd_solver.cpp:165] Iteration 2100, lr = 0.1
I0818 10:54:52.504803  2034 solver.cpp:357] Iteration 2200 (2.73199 iter/s, 36.6033s/100 iters), loss = 0.567546
I0818 10:54:52.505038  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.567546 (* 1 = 0.567546 loss)
I0818 10:54:52.505053  2034 sgd_solver.cpp:165] Iteration 2200, lr = 0.1
I0818 10:55:29.308006  2034 solver.cpp:357] Iteration 2300 (2.71723 iter/s, 36.8021s/100 iters), loss = 0.505428
I0818 10:55:29.308147  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.505428 (* 1 = 0.505428 loss)
I0818 10:55:29.308161  2034 sgd_solver.cpp:165] Iteration 2300, lr = 0.1
I0818 10:55:43.755770  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:56:06.093049  2034 solver.cpp:357] Iteration 2400 (2.71857 iter/s, 36.784s/100 iters), loss = 0.50244
I0818 10:56:06.093205  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.50244 (* 1 = 0.50244 loss)
I0818 10:56:06.093215  2034 sgd_solver.cpp:165] Iteration 2400, lr = 0.1
I0818 10:56:42.430737  2034 solver.cpp:514] Iteration 2500, Testing net (#0)
I0818 10:57:11.606743  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:57:11.683835  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1733
I0818 10:57:11.683897  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.77625 (* 1 = 2.77625 loss)
I0818 10:57:12.045976  2034 solver.cpp:357] Iteration 2500 (1.51619 iter/s, 65.9549s/100 iters), loss = 0.567525
I0818 10:57:12.046032  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.567525 (* 1 = 0.567525 loss)
I0818 10:57:12.046044  2034 sgd_solver.cpp:165] Iteration 2500, lr = 0.1
I0818 10:57:48.752975  2034 solver.cpp:357] Iteration 2600 (2.72435 iter/s, 36.7061s/100 iters), loss = 0.782414
I0818 10:57:48.753258  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.782414 (* 1 = 0.782414 loss)
I0818 10:57:48.753316  2034 sgd_solver.cpp:165] Iteration 2600, lr = 0.1
I0818 10:58:23.124997  2034 solver.cpp:357] Iteration 2700 (2.90926 iter/s, 34.373s/100 iters), loss = 0.764656
I0818 10:58:23.125161  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.764656 (* 1 = 0.764656 loss)
I0818 10:58:23.125174  2034 sgd_solver.cpp:165] Iteration 2700, lr = 0.1
I0818 10:58:32.091310  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 10:59:00.592818  2034 solver.cpp:357] Iteration 2800 (2.66902 iter/s, 37.467s/100 iters), loss = 0.390083
I0818 10:59:00.592958  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390083 (* 1 = 0.390083 loss)
I0818 10:59:00.592972  2034 sgd_solver.cpp:165] Iteration 2800, lr = 0.1
I0818 10:59:52.331821  2034 solver.cpp:357] Iteration 2900 (1.93282 iter/s, 51.7379s/100 iters), loss = 0.614783
I0818 10:59:52.332139  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.614783 (* 1 = 0.614783 loss)
I0818 10:59:52.332165  2034 sgd_solver.cpp:165] Iteration 2900, lr = 0.1
I0818 11:00:48.144071  2034 solver.cpp:514] Iteration 3000, Testing net (#0)
I0818 11:01:32.403630  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:01:32.542922  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.113799
I0818 11:01:32.542973  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.55941 (* 1 = 2.55941 loss)
I0818 11:01:32.993490  2034 solver.cpp:357] Iteration 3000 (0.993353 iter/s, 100.669s/100 iters), loss = 0.512514
I0818 11:01:32.993556  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.512514 (* 1 = 0.512514 loss)
I0818 11:01:32.993566  2034 sgd_solver.cpp:165] Iteration 3000, lr = 0.1
I0818 11:02:28.750694  2034 solver.cpp:357] Iteration 3100 (1.79343 iter/s, 55.7589s/100 iters), loss = 0.607246
I0818 11:02:28.750865  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.607246 (* 1 = 0.607246 loss)
I0818 11:02:28.750879  2034 sgd_solver.cpp:165] Iteration 3100, lr = 0.1
I0818 11:02:40.148771  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:03:24.352738  2034 solver.cpp:357] Iteration 3200 (1.79852 iter/s, 55.6013s/100 iters), loss = 0.591162
I0818 11:03:24.352959  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.591162 (* 1 = 0.591162 loss)
I0818 11:03:24.352972  2034 sgd_solver.cpp:165] Iteration 3200, lr = 0.1
I0818 11:04:20.234571  2034 solver.cpp:357] Iteration 3300 (1.78945 iter/s, 55.883s/100 iters), loss = 0.590343
I0818 11:04:20.234756  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.590343 (* 1 = 0.590343 loss)
I0818 11:04:20.234769  2034 sgd_solver.cpp:165] Iteration 3300, lr = 0.1
I0818 11:05:14.520901  2034 solver.cpp:357] Iteration 3400 (1.84213 iter/s, 54.2851s/100 iters), loss = 0.467341
I0818 11:05:14.521051  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467341 (* 1 = 0.467341 loss)
I0818 11:05:14.521064  2034 sgd_solver.cpp:165] Iteration 3400, lr = 0.1
I0818 11:06:05.651993  2034 solver.cpp:514] Iteration 3500, Testing net (#0)
I0818 11:06:51.027683  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:06:51.183099  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.1737
I0818 11:06:51.183145  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.53724 (* 1 = 2.53724 loss)
I0818 11:06:51.537117  2034 solver.cpp:357] Iteration 3500 (1.03075 iter/s, 97.0171s/100 iters), loss = 0.66573
I0818 11:06:51.537184  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.66573 (* 1 = 0.66573 loss)
I0818 11:06:51.537196  2034 sgd_solver.cpp:165] Iteration 3500, lr = 0.1
I0818 11:06:57.851438  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:07:47.359546  2034 solver.cpp:357] Iteration 3600 (1.79138 iter/s, 55.823s/100 iters), loss = 0.650357
I0818 11:07:47.359760  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.650357 (* 1 = 0.650357 loss)
I0818 11:07:47.359789  2034 sgd_solver.cpp:165] Iteration 3600, lr = 0.1
I0818 11:08:42.208890  2034 solver.cpp:357] Iteration 3700 (1.82316 iter/s, 54.8498s/100 iters), loss = 0.467633
I0818 11:08:42.209090  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467633 (* 1 = 0.467633 loss)
I0818 11:08:42.209118  2034 sgd_solver.cpp:165] Iteration 3700, lr = 0.1
I0818 11:09:34.477784  2034 solver.cpp:357] Iteration 3800 (1.91325 iter/s, 52.2671s/100 iters), loss = 0.343125
I0818 11:09:34.477900  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343125 (* 1 = 0.343125 loss)
I0818 11:09:34.477912  2034 sgd_solver.cpp:165] Iteration 3800, lr = 0.1
I0818 11:10:30.197013  2034 solver.cpp:357] Iteration 3900 (1.7947 iter/s, 55.7196s/100 iters), loss = 0.56693
I0818 11:10:30.197228  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.56693 (* 1 = 0.56693 loss)
I0818 11:10:30.197257  2034 sgd_solver.cpp:165] Iteration 3900, lr = 0.1
I0818 11:10:31.516705  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:11:25.507241  2034 solver.cpp:514] Iteration 4000, Testing net (#0)
I0818 11:12:09.009546  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:12:09.187902  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.2837
I0818 11:12:09.187966  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.1298 (* 1 = 2.1298 loss)
I0818 11:12:09.583751  2034 solver.cpp:357] Iteration 4000 (1.00615 iter/s, 99.389s/100 iters), loss = 0.551815
I0818 11:12:09.583815  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.551815 (* 1 = 0.551815 loss)
I0818 11:12:09.583827  2034 sgd_solver.cpp:165] Iteration 4000, lr = 0.1
I0818 11:13:01.041925  2034 solver.cpp:357] Iteration 4100 (1.94332 iter/s, 51.4583s/100 iters), loss = 0.576458
I0818 11:13:01.042235  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.576458 (* 1 = 0.576458 loss)
I0818 11:13:01.042299  2034 sgd_solver.cpp:165] Iteration 4100, lr = 0.1
I0818 11:13:56.977735  2034 solver.cpp:357] Iteration 4200 (1.78782 iter/s, 55.934s/100 iters), loss = 0.526146
I0818 11:13:56.977977  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.526146 (* 1 = 0.526146 loss)
I0818 11:13:56.977993  2034 sgd_solver.cpp:165] Iteration 4200, lr = 0.1
I0818 11:14:48.528185  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:14:52.853338  2034 solver.cpp:357] Iteration 4300 (1.78968 iter/s, 55.8758s/100 iters), loss = 0.379781
I0818 11:14:52.853420  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379781 (* 1 = 0.379781 loss)
I0818 11:14:52.853433  2034 sgd_solver.cpp:165] Iteration 4300, lr = 0.1
I0818 11:15:48.589692  2034 solver.cpp:357] Iteration 4400 (1.79422 iter/s, 55.7345s/100 iters), loss = 0.485458
I0818 11:15:48.589867  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.485458 (* 1 = 0.485458 loss)
I0818 11:15:48.589895  2034 sgd_solver.cpp:165] Iteration 4400, lr = 0.1
I0818 11:16:43.825078  2034 solver.cpp:514] Iteration 4500, Testing net (#0)
I0818 11:17:28.993389  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:17:29.132117  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.4588
I0818 11:17:29.132172  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.58256 (* 1 = 1.58256 loss)
I0818 11:17:29.637419  2034 solver.cpp:357] Iteration 4500 (0.98964 iter/s, 101.047s/100 iters), loss = 0.672875
I0818 11:17:29.637495  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.672875 (* 1 = 0.672875 loss)
I0818 11:17:29.637507  2034 sgd_solver.cpp:165] Iteration 4500, lr = 0.1
I0818 11:18:24.208187  2034 solver.cpp:357] Iteration 4600 (1.83258 iter/s, 54.568s/100 iters), loss = 0.410434
I0818 11:18:24.208341  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410434 (* 1 = 0.410434 loss)
I0818 11:18:24.208353  2034 sgd_solver.cpp:165] Iteration 4600, lr = 0.1
I0818 11:19:06.562391  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:19:14.468664  2034 solver.cpp:357] Iteration 4700 (1.98966 iter/s, 50.2598s/100 iters), loss = 0.36797
I0818 11:19:14.468744  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36797 (* 1 = 0.36797 loss)
I0818 11:19:14.468755  2034 sgd_solver.cpp:165] Iteration 4700, lr = 0.1
I0818 11:20:06.167670  2034 solver.cpp:357] Iteration 4800 (1.93437 iter/s, 51.6963s/100 iters), loss = 0.36816
I0818 11:20:06.167807  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36816 (* 1 = 0.36816 loss)
I0818 11:20:06.167819  2034 sgd_solver.cpp:165] Iteration 4800, lr = 0.1
I0818 11:21:02.057657  2034 solver.cpp:357] Iteration 4900 (1.78931 iter/s, 55.8874s/100 iters), loss = 0.524609
I0818 11:21:02.057734  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.524609 (* 1 = 0.524609 loss)
I0818 11:21:02.057745  2034 sgd_solver.cpp:165] Iteration 4900, lr = 0.1
I0818 11:21:57.515666  2034 solver.cpp:514] Iteration 5000, Testing net (#0)
I0818 11:22:42.952539  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:22:43.185336  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.3149
I0818 11:22:43.185392  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.8702 (* 1 = 1.8702 loss)
I0818 11:22:43.507370  2034 solver.cpp:357] Iteration 5000 (0.985678 iter/s, 101.453s/100 iters), loss = 0.425819
I0818 11:22:43.507439  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425819 (* 1 = 0.425819 loss)
I0818 11:22:43.507450  2034 sgd_solver.cpp:165] Iteration 5000, lr = 0.1
I0818 11:23:25.126588  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:23:39.262864  2034 solver.cpp:357] Iteration 5100 (1.79356 iter/s, 55.7552s/100 iters), loss = 0.475119
I0818 11:23:39.262945  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.475119 (* 1 = 0.475119 loss)
I0818 11:23:39.262956  2034 sgd_solver.cpp:165] Iteration 5100, lr = 0.1
I0818 11:24:35.073616  2034 solver.cpp:357] Iteration 5200 (1.79178 iter/s, 55.8105s/100 iters), loss = 0.627752
I0818 11:24:35.073957  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.627752 (* 1 = 0.627752 loss)
I0818 11:24:35.073972  2034 sgd_solver.cpp:165] Iteration 5200, lr = 0.1
I0818 11:25:30.867692  2034 solver.cpp:357] Iteration 5300 (1.79238 iter/s, 55.7917s/100 iters), loss = 0.52424
I0818 11:25:30.867844  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52424 (* 1 = 0.52424 loss)
I0818 11:25:30.867857  2034 sgd_solver.cpp:165] Iteration 5300, lr = 0.1
I0818 11:26:24.052500  2034 solver.cpp:357] Iteration 5400 (1.88032 iter/s, 53.1824s/100 iters), loss = 0.36293
I0818 11:26:24.052615  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36293 (* 1 = 0.36293 loss)
I0818 11:26:24.052628  2034 sgd_solver.cpp:165] Iteration 5400, lr = 0.1
I0818 11:26:56.487267  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:27:16.089771  2034 solver.cpp:514] Iteration 5500, Testing net (#0)
I0818 11:28:00.542893  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:28:00.686635  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7022
I0818 11:28:00.686744  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.861534 (* 1 = 0.861534 loss)
I0818 11:28:01.065374  2034 solver.cpp:357] Iteration 5500 (1.03078 iter/s, 97.0142s/100 iters), loss = 0.489241
I0818 11:28:01.065450  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.489241 (* 1 = 0.489241 loss)
I0818 11:28:01.065464  2034 sgd_solver.cpp:165] Iteration 5500, lr = 0.1
I0818 11:28:53.178038  2034 solver.cpp:357] Iteration 5600 (1.91901 iter/s, 52.1103s/100 iters), loss = 0.437386
I0818 11:28:53.178189  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.437386 (* 1 = 0.437386 loss)
I0818 11:28:53.178201  2034 sgd_solver.cpp:165] Iteration 5600, lr = 0.1
I0818 11:29:48.455862  2034 solver.cpp:357] Iteration 5700 (1.80912 iter/s, 55.2756s/100 iters), loss = 0.474682
I0818 11:29:48.456020  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474682 (* 1 = 0.474682 loss)
I0818 11:29:48.456032  2034 sgd_solver.cpp:165] Iteration 5700, lr = 0.1
I0818 11:30:44.125140  2034 solver.cpp:357] Iteration 5800 (1.79639 iter/s, 55.6671s/100 iters), loss = 0.295089
I0818 11:30:44.125299  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.295089 (* 1 = 0.295089 loss)
I0818 11:30:44.125311  2034 sgd_solver.cpp:165] Iteration 5800, lr = 0.1
I0818 11:31:15.009654  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:31:40.014434  2034 solver.cpp:357] Iteration 5900 (1.78925 iter/s, 55.8892s/100 iters), loss = 0.65088
I0818 11:31:40.014504  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.65088 (* 1 = 0.65088 loss)
I0818 11:31:40.014515  2034 sgd_solver.cpp:165] Iteration 5900, lr = 0.1
I0818 11:32:35.267554  2034 solver.cpp:514] Iteration 6000, Testing net (#0)
I0818 11:33:18.065510  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:33:18.251454  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6151
I0818 11:33:18.251518  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.09614 (* 1 = 1.09614 loss)
I0818 11:33:18.564676  2034 solver.cpp:357] Iteration 6000 (1.01469 iter/s, 98.5518s/100 iters), loss = 0.448767
I0818 11:33:18.564743  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.448767 (* 1 = 0.448767 loss)
I0818 11:33:18.564754  2034 sgd_solver.cpp:165] Iteration 6000, lr = 0.1
I0818 11:34:11.023538  2034 solver.cpp:357] Iteration 6100 (1.90626 iter/s, 52.4587s/100 iters), loss = 0.48694
I0818 11:34:11.023738  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48694 (* 1 = 0.48694 loss)
I0818 11:34:11.023768  2034 sgd_solver.cpp:165] Iteration 6100, lr = 0.1
I0818 11:35:06.931434  2034 solver.cpp:357] Iteration 6200 (1.78873 iter/s, 55.9057s/100 iters), loss = 0.391442
I0818 11:35:06.931793  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.391442 (* 1 = 0.391442 loss)
I0818 11:35:06.931823  2034 sgd_solver.cpp:165] Iteration 6200, lr = 0.1
I0818 11:35:32.165788  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:36:02.696926  2034 solver.cpp:357] Iteration 6300 (1.79322 iter/s, 55.7655s/100 iters), loss = 0.390846
I0818 11:36:02.697130  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390846 (* 1 = 0.390846 loss)
I0818 11:36:02.697141  2034 sgd_solver.cpp:165] Iteration 6300, lr = 0.1
I0818 11:36:58.237829  2034 solver.cpp:357] Iteration 6400 (1.80048 iter/s, 55.5409s/100 iters), loss = 0.56238
I0818 11:36:58.237957  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.56238 (* 1 = 0.56238 loss)
I0818 11:36:58.237968  2034 sgd_solver.cpp:165] Iteration 6400, lr = 0.1
I0818 11:37:52.913242  2034 solver.cpp:514] Iteration 6500, Testing net (#0)
I0818 11:38:34.239784  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:38:34.311877  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5329
I0818 11:38:34.311965  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.37999 (* 1 = 1.37999 loss)
I0818 11:38:34.746268  2034 solver.cpp:357] Iteration 6500 (1.03616 iter/s, 96.5101s/100 iters), loss = 0.43798
I0818 11:38:34.746345  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.43798 (* 1 = 0.43798 loss)
I0818 11:38:34.746357  2034 sgd_solver.cpp:165] Iteration 6500, lr = 0.1
I0818 11:39:29.982333  2034 solver.cpp:357] Iteration 6600 (1.81048 iter/s, 55.2339s/100 iters), loss = 0.438199
I0818 11:39:29.982494  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438199 (* 1 = 0.438199 loss)
I0818 11:39:29.982506  2034 sgd_solver.cpp:165] Iteration 6600, lr = 0.1
I0818 11:39:50.145552  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:40:22.994734  2034 solver.cpp:357] Iteration 6700 (1.88635 iter/s, 53.0123s/100 iters), loss = 0.538291
I0818 11:40:22.994894  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.538291 (* 1 = 0.538291 loss)
I0818 11:40:22.994906  2034 sgd_solver.cpp:165] Iteration 6700, lr = 0.1
I0818 11:41:15.734961  2034 solver.cpp:357] Iteration 6800 (1.89609 iter/s, 52.7401s/100 iters), loss = 0.331756
I0818 11:41:15.735095  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.331756 (* 1 = 0.331756 loss)
I0818 11:41:15.735106  2034 sgd_solver.cpp:165] Iteration 6800, lr = 0.1
I0818 11:42:11.344583  2034 solver.cpp:357] Iteration 6900 (1.79825 iter/s, 55.6096s/100 iters), loss = 0.460181
I0818 11:42:11.344707  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.460181 (* 1 = 0.460181 loss)
I0818 11:42:11.344719  2034 sgd_solver.cpp:165] Iteration 6900, lr = 0.1
I0818 11:43:06.751199  2034 solver.cpp:514] Iteration 7000, Testing net (#0)
I0818 11:43:52.081116  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:43:52.323627  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6776
I0818 11:43:52.323688  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07779 (* 1 = 1.07779 loss)
I0818 11:43:52.738888  2034 solver.cpp:357] Iteration 7000 (0.986233 iter/s, 101.396s/100 iters), loss = 0.394557
I0818 11:43:52.738958  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394557 (* 1 = 0.394557 loss)
I0818 11:43:52.738970  2034 sgd_solver.cpp:165] Iteration 7000, lr = 0.1
I0818 11:44:07.829675  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:44:48.530649  2034 solver.cpp:357] Iteration 7100 (1.79234 iter/s, 55.7929s/100 iters), loss = 0.457243
I0818 11:44:48.530783  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.457243 (* 1 = 0.457243 loss)
I0818 11:44:48.530795  2034 sgd_solver.cpp:165] Iteration 7100, lr = 0.1
I0818 11:45:44.405457  2034 solver.cpp:357] Iteration 7200 (1.78978 iter/s, 55.8727s/100 iters), loss = 0.625635
I0818 11:45:44.405575  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.625635 (* 1 = 0.625635 loss)
I0818 11:45:44.405588  2034 sgd_solver.cpp:165] Iteration 7200, lr = 0.1
I0818 11:46:40.174669  2034 solver.cpp:357] Iteration 7300 (1.79317 iter/s, 55.7671s/100 iters), loss = 0.485147
I0818 11:46:40.174909  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.485147 (* 1 = 0.485147 loss)
I0818 11:46:40.174923  2034 sgd_solver.cpp:165] Iteration 7300, lr = 0.1
I0818 11:47:32.275593  2034 solver.cpp:357] Iteration 7400 (1.91943 iter/s, 52.0987s/100 iters), loss = 0.488467
I0818 11:47:32.275796  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.488467 (* 1 = 0.488467 loss)
I0818 11:47:32.275822  2034 sgd_solver.cpp:165] Iteration 7400, lr = 0.1
I0818 11:47:40.343456  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:48:21.246243  2034 solver.cpp:514] Iteration 7500, Testing net (#0)
I0818 11:49:05.862268  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:49:06.080260  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5793
I0818 11:49:06.080310  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.3857 (* 1 = 1.3857 loss)
I0818 11:49:06.474782  2034 solver.cpp:357] Iteration 7500 (1.06157 iter/s, 94.1999s/100 iters), loss = 0.478471
I0818 11:49:06.474838  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478471 (* 1 = 0.478471 loss)
I0818 11:49:06.474848  2034 sgd_solver.cpp:165] Iteration 7500, lr = 0.1
I0818 11:50:02.296901  2034 solver.cpp:357] Iteration 7600 (1.79134 iter/s, 55.8242s/100 iters), loss = 0.398691
I0818 11:50:02.297086  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398691 (* 1 = 0.398691 loss)
I0818 11:50:02.297111  2034 sgd_solver.cpp:165] Iteration 7600, lr = 0.1
I0818 11:50:58.136168  2034 solver.cpp:357] Iteration 7700 (1.79079 iter/s, 55.8412s/100 iters), loss = 0.439393
I0818 11:50:58.136323  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439393 (* 1 = 0.439393 loss)
I0818 11:50:58.136334  2034 sgd_solver.cpp:165] Iteration 7700, lr = 0.1
I0818 11:51:53.694874  2034 solver.cpp:357] Iteration 7800 (1.79983 iter/s, 55.5607s/100 iters), loss = 0.322678
I0818 11:51:53.695014  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.322678 (* 1 = 0.322678 loss)
I0818 11:51:53.695027  2034 sgd_solver.cpp:165] Iteration 7800, lr = 0.1
I0818 11:51:58.317430  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:52:49.198581  2034 solver.cpp:357] Iteration 7900 (1.80175 iter/s, 55.5016s/100 iters), loss = 0.42113
I0818 11:52:49.198734  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42113 (* 1 = 0.42113 loss)
I0818 11:52:49.198745  2034 sgd_solver.cpp:165] Iteration 7900, lr = 0.1
I0818 11:53:44.721207  2034 solver.cpp:514] Iteration 8000, Testing net (#0)
I0818 11:54:26.669332  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:54:26.848486  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.787601
I0818 11:54:26.848542  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.612005 (* 1 = 0.612005 loss)
I0818 11:54:27.297571  2034 solver.cpp:357] Iteration 8000 (1.01936 iter/s, 98.1008s/100 iters), loss = 0.344681
I0818 11:54:27.297641  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344681 (* 1 = 0.344681 loss)
I0818 11:54:27.297654  2034 sgd_solver.cpp:165] Iteration 8000, lr = 0.1
I0818 11:55:20.390574  2034 solver.cpp:357] Iteration 8100 (1.88356 iter/s, 53.0908s/100 iters), loss = 0.389142
I0818 11:55:20.390751  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389142 (* 1 = 0.389142 loss)
I0818 11:55:20.390765  2034 sgd_solver.cpp:165] Iteration 8100, lr = 0.1
I0818 11:56:15.772778  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:56:16.224179  2034 solver.cpp:357] Iteration 8200 (1.7911 iter/s, 55.8316s/100 iters), loss = 0.425546
I0818 11:56:16.224252  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425546 (* 1 = 0.425546 loss)
I0818 11:56:16.224262  2034 sgd_solver.cpp:165] Iteration 8200, lr = 0.1
I0818 11:57:11.874390  2034 solver.cpp:357] Iteration 8300 (1.79694 iter/s, 55.6502s/100 iters), loss = 0.515478
I0818 11:57:11.874549  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.515478 (* 1 = 0.515478 loss)
I0818 11:57:11.874562  2034 sgd_solver.cpp:165] Iteration 8300, lr = 0.1
I0818 11:58:03.898422  2034 solver.cpp:357] Iteration 8400 (1.92212 iter/s, 52.0259s/100 iters), loss = 0.370342
I0818 11:58:03.898607  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.370342 (* 1 = 0.370342 loss)
I0818 11:58:03.898620  2034 sgd_solver.cpp:165] Iteration 8400, lr = 0.1
I0818 11:58:58.334545  2034 solver.cpp:514] Iteration 8500, Testing net (#0)
I0818 11:59:43.659346  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 11:59:43.827484  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6589
I0818 11:59:43.827565  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.05732 (* 1 = 1.05732 loss)
I0818 11:59:44.278826  2034 solver.cpp:357] Iteration 8500 (0.996213 iter/s, 100.38s/100 iters), loss = 0.431768
I0818 11:59:44.278905  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431768 (* 1 = 0.431768 loss)
I0818 11:59:44.278918  2034 sgd_solver.cpp:165] Iteration 8500, lr = 0.1
I0818 12:00:34.101020  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:00:40.094442  2034 solver.cpp:357] Iteration 8600 (1.79168 iter/s, 55.8135s/100 iters), loss = 0.432153
I0818 12:00:40.094521  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.432153 (* 1 = 0.432153 loss)
I0818 12:00:40.094532  2034 sgd_solver.cpp:165] Iteration 8600, lr = 0.1
I0818 12:01:32.342517  2034 solver.cpp:357] Iteration 8700 (1.91403 iter/s, 52.2458s/100 iters), loss = 0.401088
I0818 12:01:32.342731  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.401088 (* 1 = 0.401088 loss)
I0818 12:01:32.342762  2034 sgd_solver.cpp:165] Iteration 8700, lr = 0.1
I0818 12:02:25.542822  2034 solver.cpp:357] Iteration 8800 (1.87969 iter/s, 53.2002s/100 iters), loss = 0.485777
I0818 12:02:25.542949  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.485777 (* 1 = 0.485777 loss)
I0818 12:02:25.542959  2034 sgd_solver.cpp:165] Iteration 8800, lr = 0.1
I0818 12:03:21.433979  2034 solver.cpp:357] Iteration 8900 (1.78913 iter/s, 55.8932s/100 iters), loss = 0.422101
I0818 12:03:21.434154  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422101 (* 1 = 0.422101 loss)
I0818 12:03:21.434165  2034 sgd_solver.cpp:165] Iteration 8900, lr = 0.1
I0818 12:04:06.418114  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:04:16.927292  2034 solver.cpp:514] Iteration 9000, Testing net (#0)
I0818 12:05:02.081264  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:05:02.234082  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6796
I0818 12:05:02.234143  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.11081 (* 1 = 1.11081 loss)
I0818 12:05:02.731130  2034 solver.cpp:357] Iteration 9000 (0.987176 iter/s, 101.299s/100 iters), loss = 0.333944
I0818 12:05:02.731202  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.333944 (* 1 = 0.333944 loss)
I0818 12:05:02.731212  2034 sgd_solver.cpp:165] Iteration 9000, lr = 0.1
I0818 12:05:58.545541  2034 solver.cpp:357] Iteration 9100 (1.79165 iter/s, 55.8145s/100 iters), loss = 0.497111
I0818 12:05:58.545706  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497111 (* 1 = 0.497111 loss)
I0818 12:05:58.545717  2034 sgd_solver.cpp:165] Iteration 9100, lr = 0.1
I0818 12:06:54.039201  2034 solver.cpp:357] Iteration 9200 (1.80201 iter/s, 55.4937s/100 iters), loss = 0.385802
I0818 12:06:54.039438  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385802 (* 1 = 0.385802 loss)
I0818 12:06:54.039487  2034 sgd_solver.cpp:165] Iteration 9200, lr = 0.1
I0818 12:07:46.389295  2034 solver.cpp:357] Iteration 9300 (1.9103 iter/s, 52.3479s/100 iters), loss = 0.440122
I0818 12:07:46.389458  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.440122 (* 1 = 0.440122 loss)
I0818 12:07:46.389470  2034 sgd_solver.cpp:165] Iteration 9300, lr = 0.1
I0818 12:08:22.147969  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:08:37.264024  2034 solver.cpp:357] Iteration 9400 (1.96562 iter/s, 50.8746s/100 iters), loss = 0.508725
I0818 12:08:37.264096  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.508725 (* 1 = 0.508725 loss)
I0818 12:08:37.264106  2034 sgd_solver.cpp:165] Iteration 9400, lr = 0.1
I0818 12:09:30.554054  2034 solver.cpp:514] Iteration 9500, Testing net (#0)
I0818 12:10:15.786738  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:10:16.039429  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7089
I0818 12:10:16.039487  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.859318 (* 1 = 0.859318 loss)
I0818 12:10:16.456602  2034 solver.cpp:357] Iteration 9500 (1.00816 iter/s, 99.1904s/100 iters), loss = 0.428454
I0818 12:10:16.456678  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428454 (* 1 = 0.428454 loss)
I0818 12:10:16.456689  2034 sgd_solver.cpp:165] Iteration 9500, lr = 0.1
I0818 12:11:12.208211  2034 solver.cpp:357] Iteration 9600 (1.79379 iter/s, 55.748s/100 iters), loss = 0.496026
I0818 12:11:12.208410  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.496026 (* 1 = 0.496026 loss)
I0818 12:11:12.208439  2034 sgd_solver.cpp:165] Iteration 9600, lr = 0.1
I0818 12:12:07.906796  2034 solver.cpp:357] Iteration 9700 (1.79555 iter/s, 55.6932s/100 iters), loss = 0.431389
I0818 12:12:07.907104  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431389 (* 1 = 0.431389 loss)
I0818 12:12:07.907166  2034 sgd_solver.cpp:165] Iteration 9700, lr = 0.1
I0818 12:12:42.119827  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:13:03.824682  2034 solver.cpp:357] Iteration 9800 (1.7885 iter/s, 55.9128s/100 iters), loss = 0.402775
I0818 12:13:03.824750  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.402775 (* 1 = 0.402775 loss)
I0818 12:13:03.824760  2034 sgd_solver.cpp:165] Iteration 9800, lr = 0.1
I0818 12:13:59.406147  2034 solver.cpp:357] Iteration 9900 (1.79918 iter/s, 55.5808s/100 iters), loss = 0.342782
I0818 12:13:59.406267  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342782 (* 1 = 0.342782 loss)
I0818 12:13:59.406280  2034 sgd_solver.cpp:165] Iteration 9900, lr = 0.1
I0818 12:14:54.764348  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_10000.caffemodel
I0818 12:14:54.795759  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_10000.solverstate
I0818 12:14:54.800652  2034 solver.cpp:514] Iteration 10000, Testing net (#0)
I0818 12:15:36.018280  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:15:36.154505  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6699
I0818 12:15:36.154558  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.0852 (* 1 = 1.0852 loss)
I0818 12:15:36.489552  2034 solver.cpp:357] Iteration 10000 (1.03009 iter/s, 97.0787s/100 iters), loss = 0.377923
I0818 12:15:36.489663  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.377923 (* 1 = 0.377923 loss)
I0818 12:15:36.489688  2034 sgd_solver.cpp:165] Iteration 10000, lr = 0.1
I0818 12:16:30.416324  2034 solver.cpp:357] Iteration 10100 (1.85437 iter/s, 53.9266s/100 iters), loss = 0.497225
I0818 12:16:30.416460  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.497225 (* 1 = 0.497225 loss)
I0818 12:16:30.416473  2034 sgd_solver.cpp:165] Iteration 10100, lr = 0.1
I0818 12:16:57.919814  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:17:22.854102  2034 solver.cpp:357] Iteration 10200 (1.90717 iter/s, 52.4336s/100 iters), loss = 0.399343
I0818 12:17:22.854269  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.399343 (* 1 = 0.399343 loss)
I0818 12:17:22.854282  2034 sgd_solver.cpp:165] Iteration 10200, lr = 0.1
I0818 12:18:17.338574  2034 solver.cpp:357] Iteration 10300 (1.83545 iter/s, 54.4826s/100 iters), loss = 0.380355
I0818 12:18:17.338796  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380355 (* 1 = 0.380355 loss)
I0818 12:18:17.338809  2034 sgd_solver.cpp:165] Iteration 10300, lr = 0.1
I0818 12:19:12.970374  2034 solver.cpp:357] Iteration 10400 (1.79759 iter/s, 55.63s/100 iters), loss = 0.57067
I0818 12:19:12.970573  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.57067 (* 1 = 0.57067 loss)
I0818 12:19:12.970600  2034 sgd_solver.cpp:165] Iteration 10400, lr = 0.1
I0818 12:20:08.240763  2034 solver.cpp:514] Iteration 10500, Testing net (#0)
I0818 12:20:53.509390  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:20:53.624861  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6272
I0818 12:20:53.624920  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.34554 (* 1 = 1.34554 loss)
I0818 12:20:54.138468  2034 solver.cpp:357] Iteration 10500 (0.988445 iter/s, 101.169s/100 iters), loss = 0.445688
I0818 12:20:54.138550  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445688 (* 1 = 0.445688 loss)
I0818 12:20:54.138562  2034 sgd_solver.cpp:165] Iteration 10500, lr = 0.1
I0818 12:21:17.675370  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:21:49.849851  2034 solver.cpp:357] Iteration 10600 (1.79508 iter/s, 55.7079s/100 iters), loss = 0.525012
I0818 12:21:49.850013  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.525012 (* 1 = 0.525012 loss)
I0818 12:21:49.850024  2034 sgd_solver.cpp:165] Iteration 10600, lr = 0.1
I0818 12:22:41.242413  2034 solver.cpp:357] Iteration 10700 (1.94594 iter/s, 51.389s/100 iters), loss = 0.329851
I0818 12:22:41.242548  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.329851 (* 1 = 0.329851 loss)
I0818 12:22:41.242559  2034 sgd_solver.cpp:165] Iteration 10700, lr = 0.1
I0818 12:23:35.722265  2034 solver.cpp:357] Iteration 10800 (1.83552 iter/s, 54.4806s/100 iters), loss = 0.374804
I0818 12:23:35.722426  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.374804 (* 1 = 0.374804 loss)
I0818 12:23:35.722438  2034 sgd_solver.cpp:165] Iteration 10800, lr = 0.1
I0818 12:24:31.315543  2034 solver.cpp:357] Iteration 10900 (1.79882 iter/s, 55.5921s/100 iters), loss = 0.380331
I0818 12:24:31.315696  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.380331 (* 1 = 0.380331 loss)
I0818 12:24:31.315711  2034 sgd_solver.cpp:165] Iteration 10900, lr = 0.1
I0818 12:24:49.949750  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:25:26.687777  2034 solver.cpp:514] Iteration 11000, Testing net (#0)
I0818 12:26:11.833024  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:26:12.014546  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.492
I0818 12:26:12.014597  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.12166 (* 1 = 2.12166 loss)
I0818 12:26:12.495659  2034 solver.cpp:357] Iteration 11000 (0.988359 iter/s, 101.178s/100 iters), loss = 0.38937
I0818 12:26:12.495733  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38937 (* 1 = 0.38937 loss)
I0818 12:26:12.495745  2034 sgd_solver.cpp:165] Iteration 11000, lr = 0.1
I0818 12:27:05.148702  2034 solver.cpp:357] Iteration 11100 (1.89934 iter/s, 52.6498s/100 iters), loss = 0.515309
I0818 12:27:05.148847  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.515309 (* 1 = 0.515309 loss)
I0818 12:27:05.148860  2034 sgd_solver.cpp:165] Iteration 11100, lr = 0.1
I0818 12:27:59.430481  2034 solver.cpp:357] Iteration 11200 (1.84234 iter/s, 54.2787s/100 iters), loss = 0.27389
I0818 12:27:59.430610  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.27389 (* 1 = 0.27389 loss)
I0818 12:27:59.430622  2034 sgd_solver.cpp:165] Iteration 11200, lr = 0.1
I0818 12:28:55.310889  2034 solver.cpp:357] Iteration 11300 (1.78963 iter/s, 55.8774s/100 iters), loss = 0.523464
I0818 12:28:55.311126  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.523464 (* 1 = 0.523464 loss)
I0818 12:28:55.311139  2034 sgd_solver.cpp:165] Iteration 11300, lr = 0.1
I0818 12:29:08.013303  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:29:46.148538  2034 solver.cpp:357] Iteration 11400 (1.96717 iter/s, 50.8345s/100 iters), loss = 0.486298
I0818 12:29:46.148694  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.486298 (* 1 = 0.486298 loss)
I0818 12:29:46.148706  2034 sgd_solver.cpp:165] Iteration 11400, lr = 0.1
I0818 12:30:40.571404  2034 solver.cpp:514] Iteration 11500, Testing net (#0)
I0818 12:31:25.939013  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:31:26.086102  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.673
I0818 12:31:26.086153  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.961161 (* 1 = 0.961161 loss)
I0818 12:31:26.507210  2034 solver.cpp:357] Iteration 11500 (0.996445 iter/s, 100.357s/100 iters), loss = 0.52225
I0818 12:31:26.507284  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.52225 (* 1 = 0.52225 loss)
I0818 12:31:26.507297  2034 sgd_solver.cpp:165] Iteration 11500, lr = 0.1
I0818 12:32:22.201154  2034 solver.cpp:357] Iteration 11600 (1.79562 iter/s, 55.6911s/100 iters), loss = 0.452388
I0818 12:32:22.201262  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.452388 (* 1 = 0.452388 loss)
I0818 12:32:22.201274  2034 sgd_solver.cpp:165] Iteration 11600, lr = 0.1
I0818 12:33:18.016822  2034 solver.cpp:357] Iteration 11700 (1.7916 iter/s, 55.8159s/100 iters), loss = 0.456866
I0818 12:33:18.016981  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.456866 (* 1 = 0.456866 loss)
I0818 12:33:18.016993  2034 sgd_solver.cpp:165] Iteration 11700, lr = 0.1
I0818 12:33:26.149544  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:34:13.941725  2034 solver.cpp:357] Iteration 11800 (1.78807 iter/s, 55.9261s/100 iters), loss = 0.611338
I0818 12:34:13.941885  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.611338 (* 1 = 0.611338 loss)
I0818 12:34:13.941898  2034 sgd_solver.cpp:165] Iteration 11800, lr = 0.1
I0818 12:35:09.912334  2034 solver.cpp:357] Iteration 11900 (1.78674 iter/s, 55.9678s/100 iters), loss = 0.403111
I0818 12:35:09.912508  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403111 (* 1 = 0.403111 loss)
I0818 12:35:09.912520  2034 sgd_solver.cpp:165] Iteration 11900, lr = 0.1
I0818 12:36:05.315696  2034 solver.cpp:514] Iteration 12000, Testing net (#0)
I0818 12:36:41.984366  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:36:42.128798  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6221
I0818 12:36:42.128854  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.31403 (* 1 = 1.31403 loss)
I0818 12:36:42.502555  2034 solver.cpp:357] Iteration 12000 (1.08002 iter/s, 92.5905s/100 iters), loss = 0.421919
I0818 12:36:42.502631  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.421919 (* 1 = 0.421919 loss)
I0818 12:36:42.502645  2034 sgd_solver.cpp:165] Iteration 12000, lr = 0.1
I0818 12:37:35.915668  2034 solver.cpp:357] Iteration 12100 (1.8723 iter/s, 53.4103s/100 iters), loss = 0.368951
I0818 12:37:35.915845  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368951 (* 1 = 0.368951 loss)
I0818 12:37:35.915858  2034 sgd_solver.cpp:165] Iteration 12100, lr = 0.1
I0818 12:37:38.803711  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:38:31.578131  2034 solver.cpp:357] Iteration 12200 (1.79663 iter/s, 55.6597s/100 iters), loss = 0.342305
I0818 12:38:31.578238  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342305 (* 1 = 0.342305 loss)
I0818 12:38:31.578250  2034 sgd_solver.cpp:165] Iteration 12200, lr = 0.1
I0818 12:39:27.260213  2034 solver.cpp:357] Iteration 12300 (1.79593 iter/s, 55.6814s/100 iters), loss = 0.344328
I0818 12:39:27.260355  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.344328 (* 1 = 0.344328 loss)
I0818 12:39:27.260368  2034 sgd_solver.cpp:165] Iteration 12300, lr = 0.1
I0818 12:40:23.213546  2034 solver.cpp:357] Iteration 12400 (1.78729 iter/s, 55.9506s/100 iters), loss = 0.546146
I0818 12:40:23.213928  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.546146 (* 1 = 0.546146 loss)
I0818 12:40:23.213990  2034 sgd_solver.cpp:165] Iteration 12400, lr = 0.1
I0818 12:41:16.390235  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:41:18.560102  2034 solver.cpp:514] Iteration 12500, Testing net (#0)
I0818 12:42:03.638171  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:42:03.813295  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7075
I0818 12:42:03.813359  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.908898 (* 1 = 0.908898 loss)
I0818 12:42:04.234524  2034 solver.cpp:357] Iteration 12500 (0.989887 iter/s, 101.022s/100 iters), loss = 0.435504
I0818 12:42:04.234596  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.435504 (* 1 = 0.435504 loss)
I0818 12:42:04.234606  2034 sgd_solver.cpp:165] Iteration 12500, lr = 0.1
I0818 12:42:59.962460  2034 solver.cpp:357] Iteration 12600 (1.79445 iter/s, 55.7273s/100 iters), loss = 0.329894
I0818 12:42:59.962628  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.329894 (* 1 = 0.329894 loss)
I0818 12:42:59.962641  2034 sgd_solver.cpp:165] Iteration 12600, lr = 0.1
I0818 12:43:50.480073  2034 solver.cpp:357] Iteration 12700 (1.97963 iter/s, 50.5145s/100 iters), loss = 0.541469
I0818 12:43:50.480248  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.541469 (* 1 = 0.541469 loss)
I0818 12:43:50.480262  2034 sgd_solver.cpp:165] Iteration 12700, lr = 0.1
I0818 12:44:45.686808  2034 solver.cpp:357] Iteration 12800 (1.81157 iter/s, 55.2006s/100 iters), loss = 0.45692
I0818 12:44:45.686980  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.45692 (* 1 = 0.45692 loss)
I0818 12:44:45.686992  2034 sgd_solver.cpp:165] Iteration 12800, lr = 0.1
I0818 12:45:33.900161  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:45:41.544926  2034 solver.cpp:357] Iteration 12900 (1.79043 iter/s, 55.8524s/100 iters), loss = 0.389138
I0818 12:45:41.545011  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.389138 (* 1 = 0.389138 loss)
I0818 12:45:41.545023  2034 sgd_solver.cpp:165] Iteration 12900, lr = 0.1
I0818 12:46:33.762785  2034 solver.cpp:514] Iteration 13000, Testing net (#0)
I0818 12:47:17.193488  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:47:17.412456  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5698
I0818 12:47:17.412509  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.58782 (* 1 = 1.58782 loss)
I0818 12:47:17.731778  2034 solver.cpp:357] Iteration 13000 (1.03972 iter/s, 96.1797s/100 iters), loss = 0.330793
I0818 12:47:17.731845  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.330793 (* 1 = 0.330793 loss)
I0818 12:47:17.731858  2034 sgd_solver.cpp:165] Iteration 13000, lr = 0.1
I0818 12:48:13.920218  2034 solver.cpp:357] Iteration 13100 (1.77987 iter/s, 56.1839s/100 iters), loss = 0.400791
I0818 12:48:13.920357  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.400791 (* 1 = 0.400791 loss)
I0818 12:48:13.920370  2034 sgd_solver.cpp:165] Iteration 13100, lr = 0.1
I0818 12:49:09.674193  2034 solver.cpp:357] Iteration 13200 (1.7938 iter/s, 55.7477s/100 iters), loss = 0.379611
I0818 12:49:09.674433  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379611 (* 1 = 0.379611 loss)
I0818 12:49:09.674463  2034 sgd_solver.cpp:165] Iteration 13200, lr = 0.1
I0818 12:49:52.736903  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:50:05.120126  2034 solver.cpp:357] Iteration 13300 (1.80369 iter/s, 55.442s/100 iters), loss = 0.487062
I0818 12:50:05.120199  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487062 (* 1 = 0.487062 loss)
I0818 12:50:05.120210  2034 sgd_solver.cpp:165] Iteration 13300, lr = 0.1
I0818 12:50:55.544138  2034 solver.cpp:357] Iteration 13400 (1.98332 iter/s, 50.4205s/100 iters), loss = 0.543366
I0818 12:50:55.544337  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.543366 (* 1 = 0.543366 loss)
I0818 12:50:55.544350  2034 sgd_solver.cpp:165] Iteration 13400, lr = 0.1
I0818 12:51:50.655460  2034 solver.cpp:514] Iteration 13500, Testing net (#0)
I0818 12:52:36.028385  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:52:36.241058  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6307
I0818 12:52:36.241124  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.21207 (* 1 = 1.21207 loss)
I0818 12:52:36.635107  2034 solver.cpp:357] Iteration 13500 (0.989267 iter/s, 101.085s/100 iters), loss = 0.388443
I0818 12:52:36.635169  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.388443 (* 1 = 0.388443 loss)
I0818 12:52:36.635181  2034 sgd_solver.cpp:165] Iteration 13500, lr = 0.1
I0818 12:53:32.255787  2034 solver.cpp:357] Iteration 13600 (1.79792 iter/s, 55.6198s/100 iters), loss = 0.358642
I0818 12:53:32.255916  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358642 (* 1 = 0.358642 loss)
I0818 12:53:32.255928  2034 sgd_solver.cpp:165] Iteration 13600, lr = 0.1
I0818 12:54:09.956161  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:54:28.227073  2034 solver.cpp:357] Iteration 13700 (1.78672 iter/s, 55.9684s/100 iters), loss = 0.450801
I0818 12:54:28.227143  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450801 (* 1 = 0.450801 loss)
I0818 12:54:28.227154  2034 sgd_solver.cpp:165] Iteration 13700, lr = 0.1
I0818 12:55:23.966615  2034 solver.cpp:357] Iteration 13800 (1.79414 iter/s, 55.7369s/100 iters), loss = 0.424224
I0818 12:55:23.966766  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424224 (* 1 = 0.424224 loss)
I0818 12:55:23.966778  2034 sgd_solver.cpp:165] Iteration 13800, lr = 0.1
I0818 12:56:16.755210  2034 solver.cpp:357] Iteration 13900 (1.89444 iter/s, 52.7861s/100 iters), loss = 0.283956
I0818 12:56:16.755394  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.283956 (* 1 = 0.283956 loss)
I0818 12:56:16.755408  2034 sgd_solver.cpp:165] Iteration 13900, lr = 0.1
I0818 12:57:09.596953  2034 solver.cpp:514] Iteration 14000, Testing net (#0)
I0818 12:57:49.613988  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:57:49.798254  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6093
I0818 12:57:49.798316  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.24886 (* 1 = 1.24886 loss)
I0818 12:57:50.223881  2034 solver.cpp:357] Iteration 14000 (1.06993 iter/s, 93.4641s/100 iters), loss = 0.366757
I0818 12:57:50.223951  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366757 (* 1 = 0.366757 loss)
I0818 12:57:50.223963  2034 sgd_solver.cpp:165] Iteration 14000, lr = 0.1
I0818 12:58:22.369529  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 12:58:45.532397  2034 solver.cpp:357] Iteration 14100 (1.80811 iter/s, 55.3063s/100 iters), loss = 0.359264
I0818 12:58:45.532475  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359264 (* 1 = 0.359264 loss)
I0818 12:58:45.532488  2034 sgd_solver.cpp:165] Iteration 14100, lr = 0.1
I0818 12:59:41.281873  2034 solver.cpp:357] Iteration 14200 (1.79387 iter/s, 55.7453s/100 iters), loss = 0.575304
I0818 12:59:41.282177  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.575304 (* 1 = 0.575304 loss)
I0818 12:59:41.282240  2034 sgd_solver.cpp:165] Iteration 14200, lr = 0.1
I0818 13:00:37.079526  2034 solver.cpp:357] Iteration 14300 (1.79226 iter/s, 55.7956s/100 iters), loss = 0.439047
I0818 13:00:37.079650  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439047 (* 1 = 0.439047 loss)
I0818 13:00:37.079661  2034 sgd_solver.cpp:165] Iteration 14300, lr = 0.1
I0818 13:01:32.859652  2034 solver.cpp:357] Iteration 14400 (1.79275 iter/s, 55.7802s/100 iters), loss = 0.355253
I0818 13:01:32.859839  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355253 (* 1 = 0.355253 loss)
I0818 13:01:32.859850  2034 sgd_solver.cpp:165] Iteration 14400, lr = 0.1
I0818 13:02:00.235445  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:02:27.938700  2034 solver.cpp:514] Iteration 14500, Testing net (#0)
I0818 13:03:13.282361  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:03:13.511162  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.716
I0818 13:03:13.511231  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.935019 (* 1 = 0.935019 loss)
I0818 13:03:13.927942  2034 solver.cpp:357] Iteration 14500 (0.989445 iter/s, 101.067s/100 iters), loss = 0.42723
I0818 13:03:13.928020  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42723 (* 1 = 0.42723 loss)
I0818 13:03:13.928032  2034 sgd_solver.cpp:165] Iteration 14500, lr = 0.1
I0818 13:04:08.882341  2034 solver.cpp:357] Iteration 14600 (1.81982 iter/s, 54.9505s/100 iters), loss = 0.38876
I0818 13:04:08.882474  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38876 (* 1 = 0.38876 loss)
I0818 13:04:08.882488  2034 sgd_solver.cpp:165] Iteration 14600, lr = 0.1
I0818 13:04:59.786989  2034 solver.cpp:357] Iteration 14700 (1.96454 iter/s, 50.9026s/100 iters), loss = 0.385112
I0818 13:04:59.787101  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.385112 (* 1 = 0.385112 loss)
I0818 13:04:59.787112  2034 sgd_solver.cpp:165] Iteration 14700, lr = 0.1
I0818 13:05:53.009486  2034 solver.cpp:357] Iteration 14800 (1.87896 iter/s, 53.2208s/100 iters), loss = 0.358803
I0818 13:05:53.009651  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.358803 (* 1 = 0.358803 loss)
I0818 13:05:53.009682  2034 sgd_solver.cpp:165] Iteration 14800, lr = 0.1
I0818 13:06:13.606483  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:06:46.978307  2034 solver.cpp:357] Iteration 14900 (1.85305 iter/s, 53.9651s/100 iters), loss = 0.315705
I0818 13:06:46.978615  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.315705 (* 1 = 0.315705 loss)
I0818 13:06:46.978701  2034 sgd_solver.cpp:165] Iteration 14900, lr = 0.1
I0818 13:07:42.416100  2034 solver.cpp:514] Iteration 15000, Testing net (#0)
I0818 13:08:27.559785  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:08:27.766276  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6733
I0818 13:08:27.766324  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.961861 (* 1 = 0.961861 loss)
I0818 13:08:28.077431  2034 solver.cpp:357] Iteration 15000 (0.989138 iter/s, 101.098s/100 iters), loss = 0.474907
I0818 13:08:28.077509  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474907 (* 1 = 0.474907 loss)
I0818 13:08:28.077522  2034 sgd_solver.cpp:165] Iteration 15000, lr = 0.1
I0818 13:09:23.770289  2034 solver.cpp:357] Iteration 15100 (1.79568 iter/s, 55.6892s/100 iters), loss = 0.513249
I0818 13:09:23.770483  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.513249 (* 1 = 0.513249 loss)
I0818 13:09:23.770496  2034 sgd_solver.cpp:165] Iteration 15100, lr = 0.1
I0818 13:10:19.614130  2034 solver.cpp:357] Iteration 15200 (1.79076 iter/s, 55.8423s/100 iters), loss = 0.5221
I0818 13:10:19.614259  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.5221 (* 1 = 0.5221 loss)
I0818 13:10:19.614269  2034 sgd_solver.cpp:165] Iteration 15200, lr = 0.1
I0818 13:10:36.631203  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:11:14.237953  2034 solver.cpp:357] Iteration 15300 (1.83068 iter/s, 54.6244s/100 iters), loss = 0.308216
I0818 13:11:14.238070  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.308216 (* 1 = 0.308216 loss)
I0818 13:11:14.238082  2034 sgd_solver.cpp:165] Iteration 15300, lr = 0.1
I0818 13:12:05.234330  2034 solver.cpp:357] Iteration 15400 (1.96106 iter/s, 50.9928s/100 iters), loss = 0.490785
I0818 13:12:05.234613  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490785 (* 1 = 0.490785 loss)
I0818 13:12:05.234627  2034 sgd_solver.cpp:165] Iteration 15400, lr = 0.1
I0818 13:13:00.474815  2034 solver.cpp:514] Iteration 15500, Testing net (#0)
I0818 13:13:46.078348  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:13:46.217849  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6673
I0818 13:13:46.217932  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.07026 (* 1 = 1.07026 loss)
I0818 13:13:46.757833  2034 solver.cpp:357] Iteration 15500 (0.985021 iter/s, 101.521s/100 iters), loss = 0.442719
I0818 13:13:46.757905  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442719 (* 1 = 0.442719 loss)
I0818 13:13:46.757916  2034 sgd_solver.cpp:165] Iteration 15500, lr = 0.1
I0818 13:14:42.580034  2034 solver.cpp:357] Iteration 15600 (1.79145 iter/s, 55.8208s/100 iters), loss = 0.412282
I0818 13:14:42.580226  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412282 (* 1 = 0.412282 loss)
I0818 13:14:42.580255  2034 sgd_solver.cpp:165] Iteration 15600, lr = 0.1
I0818 13:14:53.873385  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:15:35.750115  2034 solver.cpp:357] Iteration 15700 (1.88081 iter/s, 53.1687s/100 iters), loss = 0.48305
I0818 13:15:35.750262  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.48305 (* 1 = 0.48305 loss)
I0818 13:15:35.750274  2034 sgd_solver.cpp:165] Iteration 15700, lr = 0.1
I0818 13:16:29.495687  2034 solver.cpp:357] Iteration 15800 (1.8606 iter/s, 53.7461s/100 iters), loss = 0.503792
I0818 13:16:29.495811  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.503792 (* 1 = 0.503792 loss)
I0818 13:16:29.495822  2034 sgd_solver.cpp:165] Iteration 15800, lr = 0.1
I0818 13:17:25.301262  2034 solver.cpp:357] Iteration 15900 (1.79198 iter/s, 55.8042s/100 iters), loss = 0.318124
I0818 13:17:25.301412  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.318124 (* 1 = 0.318124 loss)
I0818 13:17:25.301424  2034 sgd_solver.cpp:165] Iteration 15900, lr = 0.1
I0818 13:18:18.773042  2034 solver.cpp:514] Iteration 16000, Testing net (#0)
I0818 13:18:59.393936  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:18:59.556928  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.654701
I0818 13:18:59.556999  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.11409 (* 1 = 1.11409 loss)
I0818 13:19:00.062935  2034 solver.cpp:357] Iteration 16000 (1.05522 iter/s, 94.7672s/100 iters), loss = 0.453924
I0818 13:19:00.063000  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.453924 (* 1 = 0.453924 loss)
I0818 13:19:00.063011  2034 sgd_solver.cpp:165] Iteration 16000, lr = 0.1
I0818 13:19:06.328048  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:19:56.060338  2034 solver.cpp:357] Iteration 16100 (1.78574 iter/s, 55.9992s/100 iters), loss = 0.381927
I0818 13:19:56.060485  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381927 (* 1 = 0.381927 loss)
I0818 13:19:56.060497  2034 sgd_solver.cpp:165] Iteration 16100, lr = 0.1
I0818 13:20:51.987838  2034 solver.cpp:357] Iteration 16200 (1.78805 iter/s, 55.9269s/100 iters), loss = 0.300243
I0818 13:20:51.988055  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.300243 (* 1 = 0.300243 loss)
I0818 13:20:51.988085  2034 sgd_solver.cpp:165] Iteration 16200, lr = 0.1
I0818 13:21:47.640127  2034 solver.cpp:357] Iteration 16300 (1.7969 iter/s, 55.6514s/100 iters), loss = 0.31332
I0818 13:21:47.640260  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.31332 (* 1 = 0.31332 loss)
I0818 13:21:47.640272  2034 sgd_solver.cpp:165] Iteration 16300, lr = 0.1
I0818 13:22:43.343189  2034 solver.cpp:357] Iteration 16400 (1.79527 iter/s, 55.702s/100 iters), loss = 0.474173
I0818 13:22:43.343359  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.474173 (* 1 = 0.474173 loss)
I0818 13:22:43.343371  2034 sgd_solver.cpp:165] Iteration 16400, lr = 0.1
I0818 13:22:44.642139  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:23:38.455538  2034 solver.cpp:514] Iteration 16500, Testing net (#0)
I0818 13:24:23.698731  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:24:23.943220  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.620201
I0818 13:24:23.943279  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.23509 (* 1 = 1.23509 loss)
I0818 13:24:24.235400  2034 solver.cpp:357] Iteration 16500 (0.991113 iter/s, 100.897s/100 iters), loss = 0.407873
I0818 13:24:24.235467  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407873 (* 1 = 0.407873 loss)
I0818 13:24:24.235481  2034 sgd_solver.cpp:165] Iteration 16500, lr = 0.1
I0818 13:25:16.353085  2034 solver.cpp:357] Iteration 16600 (1.91872 iter/s, 52.1181s/100 iters), loss = 0.384106
I0818 13:25:16.353222  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384106 (* 1 = 0.384106 loss)
I0818 13:25:16.353235  2034 sgd_solver.cpp:165] Iteration 16600, lr = 0.1
I0818 13:26:05.507719  2034 solver.cpp:357] Iteration 16700 (2.03448 iter/s, 49.1527s/100 iters), loss = 0.428307
I0818 13:26:05.507861  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.428307 (* 1 = 0.428307 loss)
I0818 13:26:05.507874  2034 sgd_solver.cpp:165] Iteration 16700, lr = 0.1
I0818 13:26:57.204475  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:27:01.439407  2034 solver.cpp:357] Iteration 16800 (1.78795 iter/s, 55.93s/100 iters), loss = 0.405754
I0818 13:27:01.439482  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.405754 (* 1 = 0.405754 loss)
I0818 13:27:01.439496  2034 sgd_solver.cpp:165] Iteration 16800, lr = 0.1
I0818 13:27:57.040860  2034 solver.cpp:357] Iteration 16900 (1.79857 iter/s, 55.5996s/100 iters), loss = 0.340617
I0818 13:27:57.041023  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.340617 (* 1 = 0.340617 loss)
I0818 13:27:57.041034  2034 sgd_solver.cpp:165] Iteration 16900, lr = 0.1
I0818 13:28:52.342514  2034 solver.cpp:514] Iteration 17000, Testing net (#0)
I0818 13:29:37.584482  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:29:37.717698  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6977
I0818 13:29:37.717753  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.954922 (* 1 = 0.954922 loss)
I0818 13:29:38.238148  2034 solver.cpp:357] Iteration 17000 (0.988148 iter/s, 101.199s/100 iters), loss = 0.414756
I0818 13:29:38.238225  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414756 (* 1 = 0.414756 loss)
I0818 13:29:38.238238  2034 sgd_solver.cpp:165] Iteration 17000, lr = 0.1
I0818 13:30:33.924451  2034 solver.cpp:357] Iteration 17100 (1.79584 iter/s, 55.6842s/100 iters), loss = 0.368218
I0818 13:30:33.924616  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.368218 (* 1 = 0.368218 loss)
I0818 13:30:33.924628  2034 sgd_solver.cpp:165] Iteration 17100, lr = 0.1
I0818 13:31:20.366406  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:31:29.575850  2034 solver.cpp:357] Iteration 17200 (1.79697 iter/s, 55.6492s/100 iters), loss = 0.403906
I0818 13:31:29.575927  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403906 (* 1 = 0.403906 loss)
I0818 13:31:29.575938  2034 sgd_solver.cpp:165] Iteration 17200, lr = 0.1
I0818 13:32:23.345690  2034 solver.cpp:357] Iteration 17300 (1.85986 iter/s, 53.7676s/100 iters), loss = 0.392395
I0818 13:32:23.345840  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.392395 (* 1 = 0.392395 loss)
I0818 13:32:23.345851  2034 sgd_solver.cpp:165] Iteration 17300, lr = 0.1
I0818 13:33:15.284997  2034 solver.cpp:357] Iteration 17400 (1.92533 iter/s, 51.939s/100 iters), loss = 0.411079
I0818 13:33:15.285200  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41108 (* 1 = 0.41108 loss)
I0818 13:33:15.285243  2034 sgd_solver.cpp:165] Iteration 17400, lr = 0.1
I0818 13:34:10.583446  2034 solver.cpp:514] Iteration 17500, Testing net (#0)
I0818 13:34:53.454823  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:34:53.622965  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6169
I0818 13:34:53.623024  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.45368 (* 1 = 1.45368 loss)
I0818 13:34:54.075557  2034 solver.cpp:357] Iteration 17500 (1.01221 iter/s, 98.7938s/100 iters), loss = 0.296019
I0818 13:34:54.075621  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296019 (* 1 = 0.296019 loss)
I0818 13:34:54.075633  2034 sgd_solver.cpp:165] Iteration 17500, lr = 0.1
I0818 13:35:32.982740  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:35:47.191669  2034 solver.cpp:357] Iteration 17600 (1.88268 iter/s, 53.1157s/100 iters), loss = 0.355635
I0818 13:35:47.191742  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355635 (* 1 = 0.355635 loss)
I0818 13:35:47.191754  2034 sgd_solver.cpp:165] Iteration 17600, lr = 0.1
I0818 13:36:42.899868  2034 solver.cpp:357] Iteration 17700 (1.79508 iter/s, 55.7078s/100 iters), loss = 0.480915
I0818 13:36:42.900046  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.480915 (* 1 = 0.480915 loss)
I0818 13:36:42.900058  2034 sgd_solver.cpp:165] Iteration 17700, lr = 0.1
I0818 13:37:38.867738  2034 solver.cpp:357] Iteration 17800 (1.78675 iter/s, 55.9675s/100 iters), loss = 0.478752
I0818 13:37:38.867858  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.478752 (* 1 = 0.478752 loss)
I0818 13:37:38.867871  2034 sgd_solver.cpp:165] Iteration 17800, lr = 0.1
I0818 13:38:34.735096  2034 solver.cpp:357] Iteration 17900 (1.78997 iter/s, 55.867s/100 iters), loss = 0.281084
I0818 13:38:34.735265  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281084 (* 1 = 0.281084 loss)
I0818 13:38:34.735277  2034 sgd_solver.cpp:165] Iteration 17900, lr = 0.1
I0818 13:39:10.104411  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:39:27.701762  2034 solver.cpp:514] Iteration 18000, Testing net (#0)
I0818 13:40:09.210299  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:40:09.342099  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.409799
I0818 13:40:09.342147  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.6885 (* 1 = 3.6885 loss)
I0818 13:40:09.775997  2034 solver.cpp:357] Iteration 18000 (1.05219 iter/s, 95.0397s/100 iters), loss = 0.350895
I0818 13:40:09.776060  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.350896 (* 1 = 0.350896 loss)
I0818 13:40:09.776072  2034 sgd_solver.cpp:165] Iteration 18000, lr = 0.1
I0818 13:41:05.551687  2034 solver.cpp:357] Iteration 18100 (1.79291 iter/s, 55.7752s/100 iters), loss = 0.310728
I0818 13:41:05.551844  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.310728 (* 1 = 0.310728 loss)
I0818 13:41:05.551856  2034 sgd_solver.cpp:165] Iteration 18100, lr = 0.1
I0818 13:42:01.265617  2034 solver.cpp:357] Iteration 18200 (1.7949 iter/s, 55.7135s/100 iters), loss = 0.413774
I0818 13:42:01.265890  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.413774 (* 1 = 0.413774 loss)
I0818 13:42:01.265951  2034 sgd_solver.cpp:165] Iteration 18200, lr = 0.1
I0818 13:42:56.876132  2034 solver.cpp:357] Iteration 18300 (1.79824 iter/s, 55.61s/100 iters), loss = 0.283692
I0818 13:42:56.876257  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.283692 (* 1 = 0.283692 loss)
I0818 13:42:56.876269  2034 sgd_solver.cpp:165] Iteration 18300, lr = 0.1
I0818 13:43:27.875049  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:43:52.653689  2034 solver.cpp:357] Iteration 18400 (1.79292 iter/s, 55.7749s/100 iters), loss = 0.544412
I0818 13:43:52.653774  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.544412 (* 1 = 0.544412 loss)
I0818 13:43:52.653786  2034 sgd_solver.cpp:165] Iteration 18400, lr = 0.1
I0818 13:44:46.038358  2034 solver.cpp:514] Iteration 18500, Testing net (#0)
I0818 13:45:28.421378  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:45:28.560490  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7494
I0818 13:45:28.560534  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.770309 (* 1 = 0.770309 loss)
I0818 13:45:28.940985  2034 solver.cpp:357] Iteration 18500 (1.03855 iter/s, 96.288s/100 iters), loss = 0.375984
I0818 13:45:28.941057  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375984 (* 1 = 0.375984 loss)
I0818 13:45:28.941068  2034 sgd_solver.cpp:165] Iteration 18500, lr = 0.1
I0818 13:46:22.341979  2034 solver.cpp:357] Iteration 18600 (1.87272 iter/s, 53.3983s/100 iters), loss = 0.484022
I0818 13:46:22.342098  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.484022 (* 1 = 0.484022 loss)
I0818 13:46:22.342109  2034 sgd_solver.cpp:165] Iteration 18600, lr = 0.1
I0818 13:47:14.433248  2034 solver.cpp:357] Iteration 18700 (1.91966 iter/s, 52.0926s/100 iters), loss = 0.338241
I0818 13:47:14.433387  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338242 (* 1 = 0.338242 loss)
I0818 13:47:14.433400  2034 sgd_solver.cpp:165] Iteration 18700, lr = 0.1
I0818 13:47:39.647478  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:48:10.428309  2034 solver.cpp:357] Iteration 18800 (1.78596 iter/s, 55.9924s/100 iters), loss = 0.381666
I0818 13:48:10.428455  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381666 (* 1 = 0.381666 loss)
I0818 13:48:10.428467  2034 sgd_solver.cpp:165] Iteration 18800, lr = 0.1
I0818 13:49:06.472404  2034 solver.cpp:357] Iteration 18900 (1.78433 iter/s, 56.0435s/100 iters), loss = 0.384768
I0818 13:49:06.472594  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384768 (* 1 = 0.384768 loss)
I0818 13:49:06.472622  2034 sgd_solver.cpp:165] Iteration 18900, lr = 0.1
I0818 13:50:01.713495  2034 solver.cpp:514] Iteration 19000, Testing net (#0)
I0818 13:50:46.985584  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:50:47.220176  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.371499
I0818 13:50:47.220233  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 3.3467 (* 1 = 3.3467 loss)
I0818 13:50:47.668917  2034 solver.cpp:357] Iteration 19000 (0.988169 iter/s, 101.197s/100 iters), loss = 0.304096
I0818 13:50:47.669000  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.304096 (* 1 = 0.304096 loss)
I0818 13:50:47.669013  2034 sgd_solver.cpp:165] Iteration 19000, lr = 0.1
I0818 13:51:43.424695  2034 solver.cpp:357] Iteration 19100 (1.79362 iter/s, 55.7531s/100 iters), loss = 0.41351
I0818 13:51:43.424860  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41351 (* 1 = 0.41351 loss)
I0818 13:51:43.424871  2034 sgd_solver.cpp:165] Iteration 19100, lr = 0.1
I0818 13:52:03.684958  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:52:39.130352  2034 solver.cpp:357] Iteration 19200 (1.79523 iter/s, 55.7032s/100 iters), loss = 0.415519
I0818 13:52:39.130690  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.41552 (* 1 = 0.41552 loss)
I0818 13:52:39.130754  2034 sgd_solver.cpp:165] Iteration 19200, lr = 0.1
I0818 13:53:32.357239  2034 solver.cpp:357] Iteration 19300 (1.87886 iter/s, 53.2239s/100 iters), loss = 0.410813
I0818 13:53:32.357383  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.410813 (* 1 = 0.410813 loss)
I0818 13:53:32.357396  2034 sgd_solver.cpp:165] Iteration 19300, lr = 0.1
I0818 13:54:23.197247  2034 solver.cpp:357] Iteration 19400 (1.96712 iter/s, 50.8357s/100 iters), loss = 0.454103
I0818 13:54:23.197396  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.454103 (* 1 = 0.454103 loss)
I0818 13:54:23.197408  2034 sgd_solver.cpp:165] Iteration 19400, lr = 0.1
I0818 13:55:15.914253  2034 solver.cpp:514] Iteration 19500, Testing net (#0)
I0818 13:56:01.230329  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:56:01.338135  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5974
I0818 13:56:01.338196  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.30762 (* 1 = 1.30762 loss)
I0818 13:56:01.803704  2034 solver.cpp:357] Iteration 19500 (1.01415 iter/s, 98.6045s/100 iters), loss = 0.446118
I0818 13:56:01.803771  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446118 (* 1 = 0.446118 loss)
I0818 13:56:01.803782  2034 sgd_solver.cpp:165] Iteration 19500, lr = 0.1
I0818 13:56:16.943467  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 13:56:57.583077  2034 solver.cpp:357] Iteration 19600 (1.79291 iter/s, 55.7754s/100 iters), loss = 0.350722
I0818 13:56:57.583215  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.350722 (* 1 = 0.350722 loss)
I0818 13:56:57.583227  2034 sgd_solver.cpp:165] Iteration 19600, lr = 0.1
I0818 13:57:53.483197  2034 solver.cpp:357] Iteration 19700 (1.78896 iter/s, 55.8984s/100 iters), loss = 0.5263
I0818 13:57:53.483512  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.5263 (* 1 = 0.5263 loss)
I0818 13:57:53.483577  2034 sgd_solver.cpp:165] Iteration 19700, lr = 0.1
I0818 13:58:49.143767  2034 solver.cpp:357] Iteration 19800 (1.79673 iter/s, 55.6568s/100 iters), loss = 0.391319
I0818 13:58:49.143889  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.391319 (* 1 = 0.391319 loss)
I0818 13:58:49.143903  2034 sgd_solver.cpp:165] Iteration 19800, lr = 0.1
I0818 13:59:45.119709  2034 solver.cpp:357] Iteration 19900 (1.78653 iter/s, 55.9743s/100 iters), loss = 0.35133
I0818 13:59:45.119889  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.35133 (* 1 = 0.35133 loss)
I0818 13:59:45.119901  2034 sgd_solver.cpp:165] Iteration 19900, lr = 0.1
I0818 13:59:54.605199  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:00:37.122097  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_20000.caffemodel
I0818 14:00:37.141798  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_20000.solverstate
I0818 14:00:37.146631  2034 solver.cpp:514] Iteration 20000, Testing net (#0)
I0818 14:01:19.135212  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:01:19.307957  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7019
I0818 14:01:19.308019  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.00272 (* 1 = 1.00272 loss)
I0818 14:01:19.820855  2034 solver.cpp:357] Iteration 20000 (1.05599 iter/s, 94.6981s/100 iters), loss = 0.373712
I0818 14:01:19.820924  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373712 (* 1 = 0.373712 loss)
I0818 14:01:19.820935  2034 sgd_solver.cpp:165] Iteration 20000, lr = 0.1
I0818 14:02:15.819392  2034 solver.cpp:357] Iteration 20100 (1.78581 iter/s, 55.9971s/100 iters), loss = 0.37637
I0818 14:02:15.819506  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.37637 (* 1 = 0.37637 loss)
I0818 14:02:15.819519  2034 sgd_solver.cpp:165] Iteration 20100, lr = 0.1
I0818 14:03:11.718305  2034 solver.cpp:357] Iteration 20200 (1.78899 iter/s, 55.8976s/100 iters), loss = 0.407714
I0818 14:03:11.718444  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407714 (* 1 = 0.407714 loss)
I0818 14:03:11.718456  2034 sgd_solver.cpp:165] Iteration 20200, lr = 0.1
I0818 14:04:05.659268  2034 solver.cpp:357] Iteration 20300 (1.85392 iter/s, 53.9396s/100 iters), loss = 0.359075
I0818 14:04:05.659396  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.359075 (* 1 = 0.359075 loss)
I0818 14:04:05.659409  2034 sgd_solver.cpp:165] Iteration 20300, lr = 0.1
I0818 14:04:09.887145  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:04:58.466164  2034 solver.cpp:357] Iteration 20400 (1.89367 iter/s, 52.8076s/100 iters), loss = 0.469141
I0818 14:04:58.466542  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.469141 (* 1 = 0.469141 loss)
I0818 14:04:58.466605  2034 sgd_solver.cpp:165] Iteration 20400, lr = 0.1
I0818 14:05:53.631245  2034 solver.cpp:514] Iteration 20500, Testing net (#0)
I0818 14:06:39.005043  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:06:39.177129  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6083
I0818 14:06:39.177181  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.5283 (* 1 = 1.5283 loss)
I0818 14:06:39.514967  2034 solver.cpp:357] Iteration 20500 (0.989624 iter/s, 101.048s/100 iters), loss = 0.351776
I0818 14:06:39.515043  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.351777 (* 1 = 0.351777 loss)
I0818 14:06:39.515056  2034 sgd_solver.cpp:165] Iteration 20500, lr = 0.1
I0818 14:07:32.216380  2034 solver.cpp:357] Iteration 20600 (1.8976 iter/s, 52.6981s/100 iters), loss = 0.409522
I0818 14:07:32.216538  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409522 (* 1 = 0.409522 loss)
I0818 14:07:32.216550  2034 sgd_solver.cpp:165] Iteration 20600, lr = 0.1
I0818 14:08:24.593232  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:08:25.124693  2034 solver.cpp:357] Iteration 20700 (1.89018 iter/s, 52.905s/100 iters), loss = 0.382532
I0818 14:08:25.124770  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382532 (* 1 = 0.382532 loss)
I0818 14:08:25.124781  2034 sgd_solver.cpp:165] Iteration 20700, lr = 0.1
I0818 14:09:20.727926  2034 solver.cpp:357] Iteration 20800 (1.79856 iter/s, 55.6s/100 iters), loss = 0.454589
I0818 14:09:20.728047  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.454589 (* 1 = 0.454589 loss)
I0818 14:09:20.728060  2034 sgd_solver.cpp:165] Iteration 20800, lr = 0.1
I0818 14:10:16.603444  2034 solver.cpp:357] Iteration 20900 (1.78979 iter/s, 55.8723s/100 iters), loss = 0.333829
I0818 14:10:16.603582  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.333829 (* 1 = 0.333829 loss)
I0818 14:10:16.603595  2034 sgd_solver.cpp:165] Iteration 20900, lr = 0.1
I0818 14:11:11.944329  2034 solver.cpp:514] Iteration 21000, Testing net (#0)
I0818 14:11:57.218926  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:11:57.318341  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7888
I0818 14:11:57.318389  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.642879 (* 1 = 0.642879 loss)
I0818 14:11:57.861186  2034 solver.cpp:357] Iteration 21000 (0.9876 iter/s, 101.256s/100 iters), loss = 0.412072
I0818 14:11:57.861253  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.412072 (* 1 = 0.412072 loss)
I0818 14:11:57.861268  2034 sgd_solver.cpp:165] Iteration 21000, lr = 0.1
I0818 14:12:47.779073  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:12:53.780622  2034 solver.cpp:357] Iteration 21100 (1.78832 iter/s, 55.9184s/100 iters), loss = 0.439641
I0818 14:12:53.780695  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439641 (* 1 = 0.439641 loss)
I0818 14:12:53.780707  2034 sgd_solver.cpp:165] Iteration 21100, lr = 0.1
I0818 14:13:47.923782  2034 solver.cpp:357] Iteration 21200 (1.84706 iter/s, 54.14s/100 iters), loss = 0.332129
I0818 14:13:47.923976  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.332129 (* 1 = 0.332129 loss)
I0818 14:13:47.924005  2034 sgd_solver.cpp:165] Iteration 21200, lr = 0.1
I0818 14:14:37.203871  2034 solver.cpp:357] Iteration 21300 (2.02926 iter/s, 49.2789s/100 iters), loss = 0.467905
I0818 14:14:37.204012  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.467905 (* 1 = 0.467905 loss)
I0818 14:14:37.204025  2034 sgd_solver.cpp:165] Iteration 21300, lr = 0.1
I0818 14:15:30.561594  2034 solver.cpp:357] Iteration 21400 (1.87418 iter/s, 53.3567s/100 iters), loss = 0.487445
I0818 14:15:30.561902  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.487445 (* 1 = 0.487445 loss)
I0818 14:15:30.561965  2034 sgd_solver.cpp:165] Iteration 21400, lr = 0.1
I0818 14:16:15.303606  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:16:25.839314  2034 solver.cpp:514] Iteration 21500, Testing net (#0)
I0818 14:17:10.993981  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:17:11.031328  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7037
I0818 14:17:11.031370  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.899328 (* 1 = 0.899328 loss)
I0818 14:17:11.534304  2034 solver.cpp:357] Iteration 21500 (0.990384 iter/s, 100.971s/100 iters), loss = 0.205127
I0818 14:17:11.534379  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.205128 (* 1 = 0.205128 loss)
I0818 14:17:11.534392  2034 sgd_solver.cpp:165] Iteration 21500, lr = 0.1
I0818 14:18:07.381767  2034 solver.cpp:357] Iteration 21600 (1.79069 iter/s, 55.8444s/100 iters), loss = 0.445934
I0818 14:18:07.381970  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445934 (* 1 = 0.445934 loss)
I0818 14:18:07.381994  2034 sgd_solver.cpp:165] Iteration 21600, lr = 0.1
I0818 14:19:03.291558  2034 solver.cpp:357] Iteration 21700 (1.78863 iter/s, 55.9088s/100 iters), loss = 0.339005
I0818 14:19:03.291762  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339005 (* 1 = 0.339005 loss)
I0818 14:19:03.291795  2034 sgd_solver.cpp:165] Iteration 21700, lr = 0.1
I0818 14:19:59.358839  2034 solver.cpp:357] Iteration 21800 (1.7836 iter/s, 56.0663s/100 iters), loss = 0.438274
I0818 14:19:59.359050  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438275 (* 1 = 0.438275 loss)
I0818 14:19:59.359081  2034 sgd_solver.cpp:165] Iteration 21800, lr = 0.1
I0818 14:20:38.615205  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:20:55.594704  2034 solver.cpp:357] Iteration 21900 (1.77825 iter/s, 56.2349s/100 iters), loss = 0.450425
I0818 14:20:55.594918  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.450425 (* 1 = 0.450425 loss)
I0818 14:20:55.594964  2034 sgd_solver.cpp:165] Iteration 21900, lr = 0.1
I0818 14:21:44.657307  2034 solver.cpp:514] Iteration 22000, Testing net (#0)
I0818 14:22:26.337232  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:22:26.470657  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7549
I0818 14:22:26.470808  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.72139 (* 1 = 0.72139 loss)
I0818 14:22:27.008672  2034 solver.cpp:357] Iteration 22000 (1.09395 iter/s, 91.4118s/100 iters), loss = 0.376669
I0818 14:22:27.008879  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.376669 (* 1 = 0.376669 loss)
I0818 14:22:27.008927  2034 sgd_solver.cpp:165] Iteration 22000, lr = 0.1
I0818 14:23:21.635192  2034 solver.cpp:357] Iteration 22100 (1.83071 iter/s, 54.6235s/100 iters), loss = 0.490028
I0818 14:23:21.635306  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490028 (* 1 = 0.490028 loss)
I0818 14:23:21.635318  2034 sgd_solver.cpp:165] Iteration 22100, lr = 0.1
I0818 14:24:14.469308  2034 solver.cpp:357] Iteration 22200 (1.89268 iter/s, 52.8352s/100 iters), loss = 0.382495
I0818 14:24:14.469642  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.382495 (* 1 = 0.382495 loss)
I0818 14:24:14.469707  2034 sgd_solver.cpp:165] Iteration 22200, lr = 0.1
I0818 14:24:48.834018  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:25:10.503934  2034 solver.cpp:357] Iteration 22300 (1.78471 iter/s, 56.0316s/100 iters), loss = 0.394093
I0818 14:25:10.504010  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394093 (* 1 = 0.394093 loss)
I0818 14:25:10.504021  2034 sgd_solver.cpp:165] Iteration 22300, lr = 0.1
I0818 14:26:06.083546  2034 solver.cpp:357] Iteration 22400 (1.79936 iter/s, 55.5755s/100 iters), loss = 0.370221
I0818 14:26:06.083714  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.370221 (* 1 = 0.370221 loss)
I0818 14:26:06.083725  2034 sgd_solver.cpp:165] Iteration 22400, lr = 0.1
I0818 14:27:01.346792  2034 solver.cpp:514] Iteration 22500, Testing net (#0)
I0818 14:27:46.682859  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:27:46.838989  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7206
I0818 14:27:46.839035  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.86157 (* 1 = 0.86157 loss)
I0818 14:27:47.186730  2034 solver.cpp:357] Iteration 22500 (0.989202 iter/s, 101.092s/100 iters), loss = 0.342621
I0818 14:27:47.186805  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.342621 (* 1 = 0.342621 loss)
I0818 14:27:47.186817  2034 sgd_solver.cpp:165] Iteration 22500, lr = 0.1
I0818 14:28:38.957589  2034 solver.cpp:357] Iteration 22600 (1.93187 iter/s, 51.7633s/100 iters), loss = 0.355366
I0818 14:28:38.957768  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355366 (* 1 = 0.355366 loss)
I0818 14:28:38.957795  2034 sgd_solver.cpp:165] Iteration 22600, lr = 0.1
I0818 14:29:06.156788  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:29:32.669772  2034 solver.cpp:357] Iteration 22700 (1.86195 iter/s, 53.7071s/100 iters), loss = 0.424187
I0818 14:29:32.669903  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.424187 (* 1 = 0.424187 loss)
I0818 14:29:32.669914  2034 sgd_solver.cpp:165] Iteration 22700, lr = 0.1
I0818 14:30:28.521509  2034 solver.cpp:357] Iteration 22800 (1.79061 iter/s, 55.8469s/100 iters), loss = 0.317986
I0818 14:30:28.521687  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.317986 (* 1 = 0.317986 loss)
I0818 14:30:28.521713  2034 sgd_solver.cpp:165] Iteration 22800, lr = 0.1
I0818 14:31:24.387116  2034 solver.cpp:357] Iteration 22900 (1.79016 iter/s, 55.8611s/100 iters), loss = 0.407321
I0818 14:31:24.387292  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.407322 (* 1 = 0.407322 loss)
I0818 14:31:24.387305  2034 sgd_solver.cpp:165] Iteration 22900, lr = 0.1
I0818 14:32:20.146747  2034 solver.cpp:514] Iteration 23000, Testing net (#0)
I0818 14:33:03.114588  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:33:03.307265  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.5152
I0818 14:33:03.307423  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.86844 (* 1 = 1.86844 loss)
I0818 14:33:03.713937  2034 solver.cpp:357] Iteration 23000 (1.00686 iter/s, 99.319s/100 iters), loss = 0.381592
I0818 14:33:03.714133  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381592 (* 1 = 0.381592 loss)
I0818 14:33:03.714179  2034 sgd_solver.cpp:165] Iteration 23000, lr = 0.1
I0818 14:33:26.111997  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:33:56.870455  2034 solver.cpp:357] Iteration 23100 (1.88137 iter/s, 53.1527s/100 iters), loss = 0.489341
I0818 14:33:56.874876  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.489341 (* 1 = 0.489341 loss)
I0818 14:33:56.874938  2034 sgd_solver.cpp:165] Iteration 23100, lr = 0.1
I0818 14:34:52.994483  2034 solver.cpp:357] Iteration 23200 (1.78209 iter/s, 56.114s/100 iters), loss = 0.338628
I0818 14:34:52.996541  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338628 (* 1 = 0.338628 loss)
I0818 14:34:52.996644  2034 sgd_solver.cpp:165] Iteration 23200, lr = 0.1
I0818 14:35:42.459400  2034 solver.cpp:357] Iteration 23300 (2.02186 iter/s, 49.4595s/100 iters), loss = 0.361487
I0818 14:35:42.462795  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361487 (* 1 = 0.361487 loss)
I0818 14:35:42.462846  2034 sgd_solver.cpp:165] Iteration 23300, lr = 0.1
I0818 14:36:37.222718  2034 solver.cpp:357] Iteration 23400 (1.82631 iter/s, 54.7551s/100 iters), loss = 0.390605
I0818 14:36:37.222944  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.390605 (* 1 = 0.390605 loss)
I0818 14:36:37.222975  2034 sgd_solver.cpp:165] Iteration 23400, lr = 0.1
I0818 14:36:55.868248  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:37:32.832554  2034 solver.cpp:514] Iteration 23500, Testing net (#0)
I0818 14:38:16.716307  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:38:16.908579  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6348
I0818 14:38:16.908746  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.14304 (* 1 = 1.14304 loss)
I0818 14:38:17.454645  2034 solver.cpp:357] Iteration 23500 (0.997717 iter/s, 100.229s/100 iters), loss = 0.317956
I0818 14:38:17.454869  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.317956 (* 1 = 0.317956 loss)
I0818 14:38:17.454916  2034 sgd_solver.cpp:165] Iteration 23500, lr = 0.1
I0818 14:39:14.296516  2034 solver.cpp:357] Iteration 23600 (1.75942 iter/s, 56.8368s/100 iters), loss = 0.339372
I0818 14:39:14.298779  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.339372 (* 1 = 0.339372 loss)
I0818 14:39:14.298815  2034 sgd_solver.cpp:165] Iteration 23600, lr = 0.1
I0818 14:40:10.866048  2034 solver.cpp:357] Iteration 23700 (1.76789 iter/s, 56.5647s/100 iters), loss = 0.305825
I0818 14:40:10.866276  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.305825 (* 1 = 0.305825 loss)
I0818 14:40:10.866312  2034 sgd_solver.cpp:165] Iteration 23700, lr = 0.1
I0818 14:41:07.436830  2034 solver.cpp:357] Iteration 23800 (1.76785 iter/s, 56.5659s/100 iters), loss = 0.404592
I0818 14:41:07.437296  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.404592 (* 1 = 0.404592 loss)
I0818 14:41:07.437350  2034 sgd_solver.cpp:165] Iteration 23800, lr = 0.1
I0818 14:41:21.367167  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:42:01.270898  2034 solver.cpp:357] Iteration 23900 (1.85772 iter/s, 53.8293s/100 iters), loss = 0.38281
I0818 14:42:01.271445  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.38281 (* 1 = 0.38281 loss)
I0818 14:42:01.271620  2034 sgd_solver.cpp:165] Iteration 23900, lr = 0.1
I0818 14:42:47.705313  2034 solver.cpp:514] Iteration 24000, Testing net (#0)
I0818 14:43:31.979768  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:43:32.173425  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.542501
I0818 14:43:32.178872  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.72863 (* 1 = 1.72863 loss)
I0818 14:43:32.600107  2034 solver.cpp:357] Iteration 24000 (1.09497 iter/s, 91.3265s/100 iters), loss = 0.447864
I0818 14:43:32.600555  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.447864 (* 1 = 0.447864 loss)
I0818 14:43:32.600738  2034 sgd_solver.cpp:165] Iteration 24000, lr = 0.1
I0818 14:44:29.526978  2034 solver.cpp:357] Iteration 24100 (1.75678 iter/s, 56.9224s/100 iters), loss = 0.403612
I0818 14:44:29.527185  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.403612 (* 1 = 0.403612 loss)
I0818 14:44:29.527213  2034 sgd_solver.cpp:165] Iteration 24100, lr = 0.1
I0818 14:45:25.958730  2034 solver.cpp:357] Iteration 24200 (1.77213 iter/s, 56.4294s/100 iters), loss = 0.36203
I0818 14:45:25.966835  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36203 (* 1 = 0.36203 loss)
I0818 14:45:25.966881  2034 sgd_solver.cpp:165] Iteration 24200, lr = 0.1
I0818 14:45:34.104923  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:46:22.954823  2034 solver.cpp:357] Iteration 24300 (1.75477 iter/s, 56.9874s/100 iters), loss = 0.559363
I0818 14:46:22.962877  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.559363 (* 1 = 0.559363 loss)
I0818 14:46:22.962942  2034 sgd_solver.cpp:165] Iteration 24300, lr = 0.1
I0818 14:47:18.731637  2034 solver.cpp:357] Iteration 24400 (1.79314 iter/s, 55.7681s/100 iters), loss = 0.442809
I0818 14:47:18.734828  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442809 (* 1 = 0.442809 loss)
I0818 14:47:18.734859  2034 sgd_solver.cpp:165] Iteration 24400, lr = 0.1
I0818 14:48:14.555207  2034 solver.cpp:514] Iteration 24500, Testing net (#0)
I0818 14:48:59.247475  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:48:59.459697  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7289
I0818 14:48:59.459854  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.815424 (* 1 = 0.815424 loss)
I0818 14:48:59.797437  2034 solver.cpp:357] Iteration 24500 (0.989497 iter/s, 101.061s/100 iters), loss = 0.416854
I0818 14:48:59.797660  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416854 (* 1 = 0.416854 loss)
I0818 14:48:59.797709  2034 sgd_solver.cpp:165] Iteration 24500, lr = 0.1
I0818 14:49:51.874781  2034 solver.cpp:357] Iteration 24600 (1.92044 iter/s, 52.0714s/100 iters), loss = 0.36445
I0818 14:49:51.875211  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.36445 (* 1 = 0.36445 loss)
I0818 14:49:51.875298  2034 sgd_solver.cpp:165] Iteration 24600, lr = 0.1
I0818 14:49:54.781725  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:50:49.003454  2034 solver.cpp:357] Iteration 24700 (1.75046 iter/s, 57.128s/100 iters), loss = 0.307501
I0818 14:50:49.003679  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.307501 (* 1 = 0.307501 loss)
I0818 14:50:49.003712  2034 sgd_solver.cpp:165] Iteration 24700, lr = 0.1
I0818 14:51:45.682828  2034 solver.cpp:357] Iteration 24800 (1.76444 iter/s, 56.6752s/100 iters), loss = 0.320874
I0818 14:51:45.683081  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.320875 (* 1 = 0.320875 loss)
I0818 14:51:45.683109  2034 sgd_solver.cpp:165] Iteration 24800, lr = 0.1
I0818 14:52:39.406141  2034 solver.cpp:357] Iteration 24900 (1.86153 iter/s, 53.7192s/100 iters), loss = 0.490551
I0818 14:52:39.406713  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.490552 (* 1 = 0.490552 loss)
I0818 14:52:39.406893  2034 sgd_solver.cpp:165] Iteration 24900, lr = 0.1
I0818 14:53:30.869917  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:53:33.003527  2034 solver.cpp:514] Iteration 25000, Testing net (#0)
I0818 14:54:19.460470  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:54:19.614015  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.532601
I0818 14:54:19.618881  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.7193 (* 1 = 1.7193 loss)
I0818 14:54:20.123014  2034 solver.cpp:357] Iteration 25000 (0.992921 iter/s, 100.713s/100 iters), loss = 0.438697
I0818 14:54:20.126912  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.438697 (* 1 = 0.438697 loss)
I0818 14:54:20.127106  2034 sgd_solver.cpp:165] Iteration 25000, lr = 0.1
I0818 14:55:16.802731  2034 solver.cpp:357] Iteration 25100 (1.76443 iter/s, 56.6756s/100 iters), loss = 0.431223
I0818 14:55:16.803006  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431223 (* 1 = 0.431223 loss)
I0818 14:55:16.803055  2034 sgd_solver.cpp:165] Iteration 25100, lr = 0.1
I0818 14:56:03.492586  2034 solver.cpp:357] Iteration 25200 (2.14187 iter/s, 46.6882s/100 iters), loss = 0.554309
I0818 14:56:03.492822  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.55431 (* 1 = 0.55431 loss)
I0818 14:56:03.492854  2034 sgd_solver.cpp:165] Iteration 25200, lr = 0.1
I0818 14:56:58.056540  2034 solver.cpp:357] Iteration 25300 (1.83285 iter/s, 54.5599s/100 iters), loss = 0.387536
I0818 14:56:58.062793  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.387537 (* 1 = 0.387537 loss)
I0818 14:56:58.062834  2034 sgd_solver.cpp:165] Iteration 25300, lr = 0.1
I0818 14:57:46.499416  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:57:54.223762  2034 solver.cpp:357] Iteration 25400 (1.78066 iter/s, 56.1589s/100 iters), loss = 0.378723
I0818 14:57:54.223974  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.378723 (* 1 = 0.378723 loss)
I0818 14:57:54.224022  2034 sgd_solver.cpp:165] Iteration 25400, lr = 0.1
I0818 14:58:49.865969  2034 solver.cpp:514] Iteration 25500, Testing net (#0)
I0818 14:59:33.114084  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 14:59:33.250994  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.622501
I0818 14:59:33.251171  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20546 (* 1 = 1.20546 loss)
I0818 14:59:33.748718  2034 solver.cpp:357] Iteration 25500 (1.00479 iter/s, 99.5233s/100 iters), loss = 0.345338
I0818 14:59:33.748915  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.345338 (* 1 = 0.345338 loss)
I0818 14:59:33.748961  2034 sgd_solver.cpp:165] Iteration 25500, lr = 0.1
I0818 15:00:29.830657  2034 solver.cpp:357] Iteration 25600 (1.78302 iter/s, 56.0846s/100 iters), loss = 0.507089
I0818 15:00:29.830862  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.507089 (* 1 = 0.507089 loss)
I0818 15:00:29.830945  2034 sgd_solver.cpp:165] Iteration 25600, lr = 0.1
I0818 15:01:26.712965  2034 solver.cpp:357] Iteration 25700 (1.75779 iter/s, 56.8896s/100 iters), loss = 0.379584
I0818 15:01:26.714778  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379584 (* 1 = 0.379584 loss)
I0818 15:01:26.714812  2034 sgd_solver.cpp:165] Iteration 25700, lr = 0.1
I0818 15:02:08.152948  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:02:20.342836  2034 solver.cpp:357] Iteration 25800 (1.86449 iter/s, 53.6339s/100 iters), loss = 0.426606
I0818 15:02:20.343037  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.426606 (* 1 = 0.426606 loss)
I0818 15:02:20.343087  2034 sgd_solver.cpp:165] Iteration 25800, lr = 0.1
I0818 15:03:08.428952  2034 solver.cpp:357] Iteration 25900 (2.07949 iter/s, 48.0887s/100 iters), loss = 0.430745
I0818 15:03:08.434075  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.430745 (* 1 = 0.430745 loss)
I0818 15:03:08.434175  2034 sgd_solver.cpp:165] Iteration 25900, lr = 0.1
I0818 15:04:03.417296  2034 solver.cpp:514] Iteration 26000, Testing net (#0)
I0818 15:04:47.293449  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:04:47.481851  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7059
I0818 15:04:47.481986  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.0573 (* 1 = 1.0573 loss)
I0818 15:04:47.971258  2034 solver.cpp:357] Iteration 26000 (1.00452 iter/s, 99.55s/100 iters), loss = 0.416306
I0818 15:04:47.974776  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.416306 (* 1 = 0.416306 loss)
I0818 15:04:47.974835  2034 sgd_solver.cpp:165] Iteration 26000, lr = 0.1
I0818 15:05:44.986781  2034 solver.cpp:357] Iteration 26100 (1.75403 iter/s, 57.0115s/100 iters), loss = 0.327821
I0818 15:05:44.987159  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.327821 (* 1 = 0.327821 loss)
I0818 15:05:44.987249  2034 sgd_solver.cpp:165] Iteration 26100, lr = 0.1
I0818 15:06:22.264221  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:06:40.606412  2034 solver.cpp:357] Iteration 26200 (1.79775 iter/s, 55.6252s/100 iters), loss = 0.431042
I0818 15:06:40.606485  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.431042 (* 1 = 0.431042 loss)
I0818 15:06:40.606498  2034 sgd_solver.cpp:165] Iteration 26200, lr = 0.1
I0818 15:07:36.299618  2034 solver.cpp:357] Iteration 26300 (1.79551 iter/s, 55.6945s/100 iters), loss = 0.341711
I0818 15:07:36.299736  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.341711 (* 1 = 0.341711 loss)
I0818 15:07:36.299748  2034 sgd_solver.cpp:165] Iteration 26300, lr = 0.1
I0818 15:08:32.032402  2034 solver.cpp:357] Iteration 26400 (1.79425 iter/s, 55.7336s/100 iters), loss = 0.302079
I0818 15:08:32.032547  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302079 (* 1 = 0.302079 loss)
I0818 15:08:32.032559  2034 sgd_solver.cpp:165] Iteration 26400, lr = 0.1
I0818 15:09:26.516340  2034 solver.cpp:514] Iteration 26500, Testing net (#0)
I0818 15:10:06.817519  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:10:06.960590  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.575701
I0818 15:10:06.960956  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.68958 (* 1 = 1.68958 loss)
I0818 15:10:07.373757  2034 solver.cpp:357] Iteration 26500 (1.04882 iter/s, 95.3451s/100 iters), loss = 0.245736
I0818 15:10:07.374179  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.245736 (* 1 = 0.245736 loss)
I0818 15:10:07.374359  2034 sgd_solver.cpp:165] Iteration 26500, lr = 0.1
I0818 15:10:39.529321  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:11:02.745069  2034 solver.cpp:357] Iteration 26600 (1.80592 iter/s, 55.3735s/100 iters), loss = 0.379863
I0818 15:11:02.745278  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.379864 (* 1 = 0.379864 loss)
I0818 15:11:02.745326  2034 sgd_solver.cpp:165] Iteration 26600, lr = 0.1
I0818 15:11:56.381981  2034 solver.cpp:357] Iteration 26700 (1.8644 iter/s, 53.6365s/100 iters), loss = 0.458769
I0818 15:11:56.386781  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.458769 (* 1 = 0.458769 loss)
I0818 15:11:56.386816  2034 sgd_solver.cpp:165] Iteration 26700, lr = 0.1
I0818 15:12:52.985555  2034 solver.cpp:357] Iteration 26800 (1.76674 iter/s, 56.6013s/100 iters), loss = 0.375948
I0818 15:12:52.990938  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.375948 (* 1 = 0.375948 loss)
I0818 15:12:52.991044  2034 sgd_solver.cpp:165] Iteration 26800, lr = 0.1
I0818 15:13:49.118631  2034 solver.cpp:357] Iteration 26900 (1.78157 iter/s, 56.1302s/100 iters), loss = 0.409817
I0818 15:13:49.118841  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409817 (* 1 = 0.409817 loss)
I0818 15:13:49.118867  2034 sgd_solver.cpp:165] Iteration 26900, lr = 0.1
I0818 15:14:17.339897  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:14:45.250896  2034 solver.cpp:514] Iteration 27000, Testing net (#0)
I0818 15:15:28.643867  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:15:28.783251  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6343
I0818 15:15:28.783427  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.26278 (* 1 = 1.26278 loss)
I0818 15:15:29.167994  2034 solver.cpp:357] Iteration 27000 (0.999447 iter/s, 100.055s/100 iters), loss = 0.338376
I0818 15:15:29.168184  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.338376 (* 1 = 0.338376 loss)
I0818 15:15:29.168231  2034 sgd_solver.cpp:165] Iteration 27000, lr = 0.1
I0818 15:16:24.130823  2034 solver.cpp:357] Iteration 27100 (1.81945 iter/s, 54.9617s/100 iters), loss = 0.398991
I0818 15:16:24.131078  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.398991 (* 1 = 0.398991 loss)
I0818 15:16:24.131125  2034 sgd_solver.cpp:165] Iteration 27100, lr = 0.1
I0818 15:17:16.236670  2034 solver.cpp:357] Iteration 27200 (1.91914 iter/s, 52.1066s/100 iters), loss = 0.446104
I0818 15:17:16.236929  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.446105 (* 1 = 0.446105 loss)
I0818 15:17:16.236954  2034 sgd_solver.cpp:165] Iteration 27200, lr = 0.1
I0818 15:18:12.020618  2034 solver.cpp:357] Iteration 27300 (1.79261 iter/s, 55.7847s/100 iters), loss = 0.304314
I0818 15:18:12.020786  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.304314 (* 1 = 0.304314 loss)
I0818 15:18:12.020797  2034 sgd_solver.cpp:165] Iteration 27300, lr = 0.1
I0818 15:18:33.823796  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:19:07.598568  2034 solver.cpp:357] Iteration 27400 (1.79925 iter/s, 55.5786s/100 iters), loss = 0.275296
I0818 15:19:07.598772  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.275296 (* 1 = 0.275296 loss)
I0818 15:19:07.598799  2034 sgd_solver.cpp:165] Iteration 27400, lr = 0.1
I0818 15:20:02.790798  2034 solver.cpp:514] Iteration 27500, Testing net (#0)
I0818 15:20:46.984582  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:20:47.165823  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7214
I0818 15:20:47.165889  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.886997 (* 1 = 0.886997 loss)
I0818 15:20:47.580770  2034 solver.cpp:357] Iteration 27500 (1.00017 iter/s, 99.9828s/100 iters), loss = 0.394648
I0818 15:20:47.580832  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.394648 (* 1 = 0.394648 loss)
I0818 15:20:47.580843  2034 sgd_solver.cpp:165] Iteration 27500, lr = 0.1
I0818 15:21:39.815336  2034 solver.cpp:357] Iteration 27600 (1.91443 iter/s, 52.2348s/100 iters), loss = 0.439846
I0818 15:21:39.815621  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.439846 (* 1 = 0.439846 loss)
I0818 15:21:39.815652  2034 sgd_solver.cpp:165] Iteration 27600, lr = 0.1
I0818 15:22:35.203827  2034 solver.cpp:357] Iteration 27700 (1.80549 iter/s, 55.3867s/100 iters), loss = 0.49932
I0818 15:22:35.203992  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.49932 (* 1 = 0.49932 loss)
I0818 15:22:35.204004  2034 sgd_solver.cpp:165] Iteration 27700, lr = 0.1
I0818 15:22:52.247517  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:23:28.526916  2034 solver.cpp:357] Iteration 27800 (1.87535 iter/s, 53.3233s/100 iters), loss = 0.355486
I0818 15:23:28.527072  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.355486 (* 1 = 0.355486 loss)
I0818 15:23:28.527104  2034 sgd_solver.cpp:165] Iteration 27800, lr = 0.1
I0818 15:24:21.218454  2034 solver.cpp:357] Iteration 27900 (1.89783 iter/s, 52.6917s/100 iters), loss = 0.442048
I0818 15:24:21.218600  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.442049 (* 1 = 0.442049 loss)
I0818 15:24:21.218611  2034 sgd_solver.cpp:165] Iteration 27900, lr = 0.1
I0818 15:25:16.594494  2034 solver.cpp:514] Iteration 28000, Testing net (#0)
I0818 15:26:00.554966  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:26:00.691650  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6065
I0818 15:26:00.691812  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.20614 (* 1 = 1.20614 loss)
I0818 15:26:01.215708  2034 solver.cpp:357] Iteration 28000 (1.00003 iter/s, 99.9972s/100 iters), loss = 0.405329
I0818 15:26:01.215893  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.405329 (* 1 = 0.405329 loss)
I0818 15:26:01.215941  2034 sgd_solver.cpp:165] Iteration 28000, lr = 0.1
I0818 15:26:57.600143  2034 solver.cpp:357] Iteration 28100 (1.7736 iter/s, 56.3824s/100 iters), loss = 0.345796
I0818 15:26:57.602900  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.345796 (* 1 = 0.345796 loss)
I0818 15:26:57.603000  2034 sgd_solver.cpp:165] Iteration 28100, lr = 0.1
I0818 15:27:09.158674  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:27:54.315294  2034 solver.cpp:357] Iteration 28200 (1.76326 iter/s, 56.7131s/100 iters), loss = 0.384511
I0818 15:27:54.318835  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.384511 (* 1 = 0.384511 loss)
I0818 15:27:54.318869  2034 sgd_solver.cpp:165] Iteration 28200, lr = 0.1
I0818 15:28:50.629631  2034 solver.cpp:357] Iteration 28300 (1.77581 iter/s, 56.3122s/100 iters), loss = 0.396693
I0818 15:28:50.634865  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.396693 (* 1 = 0.396693 loss)
I0818 15:28:50.634912  2034 sgd_solver.cpp:165] Iteration 28300, lr = 0.1
I0818 15:29:46.735261  2034 solver.cpp:357] Iteration 28400 (1.78249 iter/s, 56.1014s/100 iters), loss = 0.276575
I0818 15:29:46.735401  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.276575 (* 1 = 0.276575 loss)
I0818 15:29:46.735411  2034 sgd_solver.cpp:165] Iteration 28400, lr = 0.1
I0818 15:30:37.094142  2034 solver.cpp:514] Iteration 28500, Testing net (#0)
I0818 15:31:15.903220  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:31:16.086560  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.529501
I0818 15:31:16.086614  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.58832 (* 1 = 1.58832 loss)
I0818 15:31:16.551906  2034 solver.cpp:357] Iteration 28500 (1.11334 iter/s, 89.82s/100 iters), loss = 0.498259
I0818 15:31:16.551964  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.498259 (* 1 = 0.498259 loss)
I0818 15:31:16.551975  2034 sgd_solver.cpp:165] Iteration 28500, lr = 0.1
I0818 15:31:22.750250  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:32:12.244242  2034 solver.cpp:357] Iteration 28600 (1.79558 iter/s, 55.6922s/100 iters), loss = 0.573449
I0818 15:32:12.244473  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.573449 (* 1 = 0.573449 loss)
I0818 15:32:12.244485  2034 sgd_solver.cpp:165] Iteration 28600, lr = 0.1
I0818 15:33:07.869586  2034 solver.cpp:357] Iteration 28700 (1.79774 iter/s, 55.6253s/100 iters), loss = 0.33651
I0818 15:33:07.869797  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.336511 (* 1 = 0.336511 loss)
I0818 15:33:07.869827  2034 sgd_solver.cpp:165] Iteration 28700, lr = 0.1
I0818 15:34:03.693565  2034 solver.cpp:357] Iteration 28800 (1.79135 iter/s, 55.8239s/100 iters), loss = 0.249516
I0818 15:34:03.693718  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.249516 (* 1 = 0.249516 loss)
I0818 15:34:03.693729  2034 sgd_solver.cpp:165] Iteration 28800, lr = 0.1
I0818 15:34:59.382217  2034 solver.cpp:357] Iteration 28900 (1.79578 iter/s, 55.686s/100 iters), loss = 0.281983
I0818 15:34:59.382371  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.281983 (* 1 = 0.281983 loss)
I0818 15:34:59.382383  2034 sgd_solver.cpp:165] Iteration 28900, lr = 0.1
I0818 15:35:00.763950  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:35:54.717847  2034 solver.cpp:514] Iteration 29000, Testing net (#0)
I0818 15:36:40.329659  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:36:40.580080  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.650001
I0818 15:36:40.580147  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.2246 (* 1 = 1.2246 loss)
I0818 15:36:40.999824  2034 solver.cpp:357] Iteration 29000 (0.984169 iter/s, 101.609s/100 iters), loss = 0.37143
I0818 15:36:40.999900  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.37143 (* 1 = 0.37143 loss)
I0818 15:36:40.999912  2034 sgd_solver.cpp:165] Iteration 29000, lr = 0.1
I0818 15:37:32.899624  2034 solver.cpp:357] Iteration 29100 (1.92694 iter/s, 51.8956s/100 iters), loss = 0.42151
I0818 15:37:32.899741  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.42151 (* 1 = 0.42151 loss)
I0818 15:37:32.899755  2034 sgd_solver.cpp:165] Iteration 29100, lr = 0.1
I0818 15:38:26.528067  2034 solver.cpp:357] Iteration 29200 (1.86482 iter/s, 53.6246s/100 iters), loss = 0.34748
I0818 15:38:26.528182  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.34748 (* 1 = 0.34748 loss)
I0818 15:38:26.528194  2034 sgd_solver.cpp:165] Iteration 29200, lr = 0.1
I0818 15:39:18.083535  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:39:22.438123  2034 solver.cpp:357] Iteration 29300 (1.78877 iter/s, 55.9043s/100 iters), loss = 0.361767
I0818 15:39:22.438202  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.361768 (* 1 = 0.361768 loss)
I0818 15:39:22.438215  2034 sgd_solver.cpp:165] Iteration 29300, lr = 0.1
I0818 15:40:17.002576  2034 solver.cpp:357] Iteration 29400 (1.83288 iter/s, 54.559s/100 iters), loss = 0.347389
I0818 15:40:17.002723  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.347389 (* 1 = 0.347389 loss)
I0818 15:40:17.002737  2034 sgd_solver.cpp:165] Iteration 29400, lr = 0.1
I0818 15:41:08.756393  2034 solver.cpp:514] Iteration 29500, Testing net (#0)
I0818 15:41:54.151616  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:41:54.257879  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6889
I0818 15:41:54.257936  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.18425 (* 1 = 1.18425 loss)
I0818 15:41:54.691951  2034 solver.cpp:357] Iteration 29500 (1.02371 iter/s, 97.6837s/100 iters), loss = 0.425346
I0818 15:41:54.692014  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.425346 (* 1 = 0.425346 loss)
I0818 15:41:54.692028  2034 sgd_solver.cpp:165] Iteration 29500, lr = 0.1
I0818 15:42:50.688532  2034 solver.cpp:357] Iteration 29600 (1.78591 iter/s, 55.9939s/100 iters), loss = 0.271056
I0818 15:42:50.688879  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.271056 (* 1 = 0.271056 loss)
I0818 15:42:50.688927  2034 sgd_solver.cpp:165] Iteration 29600, lr = 0.1
I0818 15:43:37.508240  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:43:46.713548  2034 solver.cpp:357] Iteration 29700 (1.78506 iter/s, 56.0204s/100 iters), loss = 0.185872
I0818 15:43:46.713629  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.185873 (* 1 = 0.185873 loss)
I0818 15:43:46.713640  2034 sgd_solver.cpp:165] Iteration 29700, lr = 0.1
I0818 15:44:38.899336  2034 solver.cpp:357] Iteration 29800 (1.9164 iter/s, 52.1813s/100 iters), loss = 0.328199
I0818 15:44:38.902793  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.328199 (* 1 = 0.328199 loss)
I0818 15:44:38.902827  2034 sgd_solver.cpp:165] Iteration 29800, lr = 0.1
I0818 15:45:32.312630  2034 solver.cpp:357] Iteration 29900 (1.87235 iter/s, 53.4089s/100 iters), loss = 0.366455
I0818 15:45:32.312928  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.366456 (* 1 = 0.366456 loss)
I0818 15:45:32.312974  2034 sgd_solver.cpp:165] Iteration 29900, lr = 0.1
I0818 15:46:28.242154  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_30000.caffemodel
I0818 15:46:28.299021  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_30000.solverstate
I0818 15:46:28.316970  2034 solver.cpp:514] Iteration 30000, Testing net (#0)
I0818 15:47:15.567641  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:47:15.814587  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7111
I0818 15:47:15.814824  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.849558 (* 1 = 0.849558 loss)
I0818 15:47:16.251883  2034 solver.cpp:357] Iteration 30000 (0.962137 iter/s, 103.935s/100 iters), loss = 0.352437
I0818 15:47:16.252065  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.352437 (* 1 = 0.352437 loss)
I0818 15:47:16.252110  2034 sgd_solver.cpp:165] Iteration 30000, lr = 0.1
I0818 15:47:58.409095  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:48:13.254101  2034 solver.cpp:357] Iteration 30100 (1.75432 iter/s, 57.0023s/100 iters), loss = 0.445621
I0818 15:48:13.254531  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445621 (* 1 = 0.445621 loss)
I0818 15:48:13.254724  2034 sgd_solver.cpp:165] Iteration 30100, lr = 0.1
I0818 15:49:09.923285  2034 solver.cpp:357] Iteration 30200 (1.76468 iter/s, 56.6675s/100 iters), loss = 0.409
I0818 15:49:09.926893  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.409 (* 1 = 0.409 loss)
I0818 15:49:09.926954  2034 sgd_solver.cpp:165] Iteration 30200, lr = 0.1
I0818 15:50:04.544267  2034 solver.cpp:357] Iteration 30300 (1.8309 iter/s, 54.6179s/100 iters), loss = 0.334604
I0818 15:50:04.544483  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.334605 (* 1 = 0.334605 loss)
I0818 15:50:04.544508  2034 sgd_solver.cpp:165] Iteration 30300, lr = 0.1
I0818 15:50:57.392870  2034 solver.cpp:357] Iteration 30400 (1.89219 iter/s, 52.8489s/100 iters), loss = 0.247701
I0818 15:50:57.394891  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.247701 (* 1 = 0.247701 loss)
I0818 15:50:57.394950  2034 sgd_solver.cpp:165] Iteration 30400, lr = 0.1
I0818 15:51:24.885627  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:51:42.578636  2034 solver.cpp:514] Iteration 30500, Testing net (#0)
I0818 15:52:28.334821  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:52:28.505144  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.6372
I0818 15:52:28.510778  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.40093 (* 1 = 1.40093 loss)
I0818 15:52:28.955444  2034 solver.cpp:357] Iteration 30500 (1.09219 iter/s, 91.5594s/100 iters), loss = 0.346159
I0818 15:52:28.958932  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.346159 (* 1 = 0.346159 loss)
I0818 15:52:28.959133  2034 sgd_solver.cpp:165] Iteration 30500, lr = 0.1
I0818 15:53:25.662474  2034 solver.cpp:357] Iteration 30600 (1.76356 iter/s, 56.7035s/100 iters), loss = 0.386965
I0818 15:53:25.671015  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.386965 (* 1 = 0.386965 loss)
I0818 15:53:25.671053  2034 sgd_solver.cpp:165] Iteration 30600, lr = 0.1
I0818 15:54:22.650810  2034 solver.cpp:357] Iteration 30700 (1.75498 iter/s, 56.9806s/100 iters), loss = 0.422653
I0818 15:54:22.654852  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.422654 (* 1 = 0.422654 loss)
I0818 15:54:22.654906  2034 sgd_solver.cpp:165] Iteration 30700, lr = 0.1
I0818 15:55:19.286764  2034 solver.cpp:357] Iteration 30800 (1.76587 iter/s, 56.6292s/100 iters), loss = 0.299144
I0818 15:55:19.287171  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.299144 (* 1 = 0.299144 loss)
I0818 15:55:19.287258  2034 sgd_solver.cpp:165] Iteration 30800, lr = 0.1
I0818 15:55:50.256093  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:56:16.108095  2034 solver.cpp:357] Iteration 30900 (1.7599 iter/s, 56.8213s/100 iters), loss = 0.552975
I0818 15:56:16.110842  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.552975 (* 1 = 0.552975 loss)
I0818 15:56:16.110915  2034 sgd_solver.cpp:165] Iteration 30900, lr = 0.1
I0818 15:57:12.092257  2034 solver.cpp:514] Iteration 31000, Testing net (#0)
I0818 15:57:56.255064  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 15:57:56.442162  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7321
I0818 15:57:56.446763  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.83824 (* 1 = 0.83824 loss)
I0818 15:57:56.920807  2034 solver.cpp:357] Iteration 31000 (0.991963 iter/s, 100.81s/100 iters), loss = 0.414567
I0818 15:57:56.921026  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.414567 (* 1 = 0.414567 loss)
I0818 15:57:56.921073  2034 sgd_solver.cpp:165] Iteration 31000, lr = 0.1
I0818 15:58:48.805814  2034 solver.cpp:357] Iteration 31100 (1.92746 iter/s, 51.8817s/100 iters), loss = 0.381903
I0818 15:58:48.806226  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.381903 (* 1 = 0.381903 loss)
I0818 15:58:48.806318  2034 sgd_solver.cpp:165] Iteration 31100, lr = 0.1
I0818 15:59:43.274430  2034 solver.cpp:357] Iteration 31200 (1.83603 iter/s, 54.4654s/100 iters), loss = 0.240562
I0818 15:59:43.282878  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.240562 (* 1 = 0.240562 loss)
I0818 15:59:43.282932  2034 sgd_solver.cpp:165] Iteration 31200, lr = 0.1
I0818 16:00:07.211529  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:00:38.044136  2034 solver.cpp:357] Iteration 31300 (1.8261 iter/s, 54.7616s/100 iters), loss = 0.308055
I0818 16:00:38.046838  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.308055 (* 1 = 0.308055 loss)
I0818 16:00:38.046890  2034 sgd_solver.cpp:165] Iteration 31300, lr = 0.1
I0818 16:01:35.424285  2034 solver.cpp:357] Iteration 31400 (1.74286 iter/s, 57.3769s/100 iters), loss = 0.445854
I0818 16:01:35.427062  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.445855 (* 1 = 0.445855 loss)
I0818 16:01:35.427248  2034 sgd_solver.cpp:165] Iteration 31400, lr = 0.1
I0818 16:02:31.651645  2034 solver.cpp:514] Iteration 31500, Testing net (#0)
I0818 16:03:17.862982  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:03:18.043576  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.401299
I0818 16:03:18.043932  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 2.90242 (* 1 = 2.90242 loss)
I0818 16:03:18.412719  2034 solver.cpp:357] Iteration 31500 (0.971003 iter/s, 102.986s/100 iters), loss = 0.296509
I0818 16:03:18.414891  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.296509 (* 1 = 0.296509 loss)
I0818 16:03:18.415087  2034 sgd_solver.cpp:165] Iteration 31500, lr = 0.1
I0818 16:04:15.481501  2034 solver.cpp:357] Iteration 31600 (1.75234 iter/s, 57.0667s/100 iters), loss = 0.302168
I0818 16:04:15.482846  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.302168 (* 1 = 0.302168 loss)
I0818 16:04:15.482899  2034 sgd_solver.cpp:165] Iteration 31600, lr = 0.1
I0818 16:04:33.078686  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:05:02.241665  2034 solver.cpp:357] Iteration 31700 (2.13863 iter/s, 46.759s/100 iters), loss = 0.373324
I0818 16:05:02.241951  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.373324 (* 1 = 0.373324 loss)
I0818 16:05:02.241999  2034 sgd_solver.cpp:165] Iteration 31700, lr = 0.1
I0818 16:05:56.348659  2034 solver.cpp:357] Iteration 31800 (1.8483 iter/s, 54.1038s/100 iters), loss = 0.343218
I0818 16:05:56.355067  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.343218 (* 1 = 0.343218 loss)
I0818 16:05:56.355259  2034 sgd_solver.cpp:165] Iteration 31800, lr = 0.1
I0818 16:06:52.877854  2034 solver.cpp:357] Iteration 31900 (1.76923 iter/s, 56.5218s/100 iters), loss = 0.521746
I0818 16:06:52.882779  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.521747 (* 1 = 0.521747 loss)
I0818 16:06:52.882812  2034 sgd_solver.cpp:165] Iteration 31900, lr = 0.1
I0818 16:07:48.868588  2034 solver.cpp:514] Iteration 32000, Testing net (#0)
I0818 16:08:32.145097  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:08:32.258064  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.635
I0818 16:08:32.258412  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 1.40931 (* 1 = 1.40931 loss)
I0818 16:08:32.620651  2034 solver.cpp:357] Iteration 32000 (1.00262 iter/s, 99.7383s/100 iters), loss = 0.345745
I0818 16:08:32.620921  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.345746 (* 1 = 0.345746 loss)
I0818 16:08:32.621145  2034 sgd_solver.cpp:64] MultiStep Status: Iteration 32000, step = 1
I0818 16:08:32.621292  2034 sgd_solver.cpp:165] Iteration 32000, lr = 0.01
I0818 16:08:47.121790  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:09:26.542984  2034 solver.cpp:357] Iteration 32100 (1.8545 iter/s, 53.9228s/100 iters), loss = 0.245172
I0818 16:09:26.546813  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.245172 (* 1 = 0.245172 loss)
I0818 16:09:26.546852  2034 sgd_solver.cpp:165] Iteration 32100, lr = 0.01
I0818 16:10:22.556988  2034 solver.cpp:357] Iteration 32200 (1.78538 iter/s, 56.0106s/100 iters), loss = 0.318275
I0818 16:10:22.558781  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.318276 (* 1 = 0.318276 loss)
I0818 16:10:22.558814  2034 sgd_solver.cpp:165] Iteration 32200, lr = 0.01
I0818 16:11:19.049679  2034 solver.cpp:357] Iteration 32300 (1.77025 iter/s, 56.4893s/100 iters), loss = 0.198736
I0818 16:11:19.054929  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.198736 (* 1 = 0.198736 loss)
I0818 16:11:19.055037  2034 sgd_solver.cpp:165] Iteration 32300, lr = 0.01
I0818 16:12:07.937366  2034 solver.cpp:357] Iteration 32400 (2.04573 iter/s, 48.8823s/100 iters), loss = 0.193263
I0818 16:12:07.942854  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.193263 (* 1 = 0.193263 loss)
I0818 16:12:07.942914  2034 sgd_solver.cpp:165] Iteration 32400, lr = 0.01
I0818 16:12:17.329910  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:13:03.822998  2034 solver.cpp:514] Iteration 32500, Testing net (#0)
I0818 16:13:49.818724  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:13:49.954037  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.867901
I0818 16:13:49.954170  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.393937 (* 1 = 0.393937 loss)
I0818 16:13:50.353708  2034 solver.cpp:357] Iteration 32500 (0.976441 iter/s, 102.413s/100 iters), loss = 0.194904
I0818 16:13:50.353915  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.194904 (* 1 = 0.194904 loss)
I0818 16:13:50.353962  2034 sgd_solver.cpp:165] Iteration 32500, lr = 0.01
I0818 16:14:46.702764  2034 solver.cpp:357] Iteration 32600 (1.7748 iter/s, 56.3444s/100 iters), loss = 0.162031
I0818 16:14:46.703143  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.162032 (* 1 = 0.162032 loss)
I0818 16:14:46.703172  2034 sgd_solver.cpp:165] Iteration 32600, lr = 0.01
I0818 16:15:43.635965  2034 solver.cpp:357] Iteration 32700 (1.75654 iter/s, 56.9302s/100 iters), loss = 0.232664
I0818 16:15:43.636384  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.232665 (* 1 = 0.232665 loss)
I0818 16:15:43.636471  2034 sgd_solver.cpp:165] Iteration 32700, lr = 0.01
I0818 16:16:40.299893  2034 solver.cpp:357] Iteration 32800 (1.76483 iter/s, 56.6628s/100 iters), loss = 0.155719
I0818 16:16:40.302999  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155719 (* 1 = 0.155719 loss)
I0818 16:16:40.303102  2034 sgd_solver.cpp:165] Iteration 32800, lr = 0.01
I0818 16:16:44.793161  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:17:37.073602  2034 solver.cpp:357] Iteration 32900 (1.76148 iter/s, 56.7704s/100 iters), loss = 0.126611
I0818 16:17:37.074849  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126612 (* 1 = 0.126612 loss)
I0818 16:17:37.074903  2034 sgd_solver.cpp:165] Iteration 32900, lr = 0.01
I0818 16:18:28.776664  2034 solver.cpp:514] Iteration 33000, Testing net (#0)
I0818 16:18:59.114241  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:18:59.294466  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.814001
I0818 16:18:59.294625  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.565607 (* 1 = 0.565607 loss)
I0818 16:18:59.663921  2034 solver.cpp:357] Iteration 33000 (1.21082 iter/s, 82.5888s/100 iters), loss = 0.138601
I0818 16:18:59.664106  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.138601 (* 1 = 0.138601 loss)
I0818 16:18:59.664155  2034 sgd_solver.cpp:165] Iteration 33000, lr = 0.01
I0818 16:19:55.915817  2034 solver.cpp:357] Iteration 33100 (1.77775 iter/s, 56.2509s/100 iters), loss = 0.202217
I0818 16:19:55.918820  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.202217 (* 1 = 0.202217 loss)
I0818 16:19:55.918870  2034 sgd_solver.cpp:165] Iteration 33100, lr = 0.01
I0818 16:20:52.407274  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:20:52.778724  2034 solver.cpp:357] Iteration 33200 (1.75876 iter/s, 56.8583s/100 iters), loss = 0.114128
I0818 16:20:52.778936  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114128 (* 1 = 0.114128 loss)
I0818 16:20:52.778982  2034 sgd_solver.cpp:165] Iteration 33200, lr = 0.01
I0818 16:21:48.745919  2034 solver.cpp:357] Iteration 33300 (1.78678 iter/s, 55.9667s/100 iters), loss = 0.204179
I0818 16:21:48.746325  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.204179 (* 1 = 0.204179 loss)
I0818 16:21:48.746420  2034 sgd_solver.cpp:165] Iteration 33300, lr = 0.01
I0818 16:22:44.875804  2034 solver.cpp:357] Iteration 33400 (1.78168 iter/s, 56.1267s/100 iters), loss = 0.150842
I0818 16:22:44.876072  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150842 (* 1 = 0.150842 loss)
I0818 16:22:44.876129  2034 sgd_solver.cpp:165] Iteration 33400, lr = 0.01
I0818 16:23:40.942252  2034 solver.cpp:514] Iteration 33500, Testing net (#0)
I0818 16:24:26.610051  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:24:26.830164  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.810201
I0818 16:24:26.830296  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.566913 (* 1 = 0.566913 loss)
I0818 16:24:27.141610  2034 solver.cpp:357] Iteration 33500 (0.977826 iter/s, 102.268s/100 iters), loss = 0.136189
I0818 16:24:27.142760  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136189 (* 1 = 0.136189 loss)
I0818 16:24:27.142815  2034 sgd_solver.cpp:165] Iteration 33500, lr = 0.01
I0818 16:25:11.381850  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:25:16.755864  2034 solver.cpp:357] Iteration 33600 (2.01556 iter/s, 49.6141s/100 iters), loss = 0.150959
I0818 16:25:16.756011  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150959 (* 1 = 0.150959 loss)
I0818 16:25:16.756098  2034 sgd_solver.cpp:165] Iteration 33600, lr = 0.01
I0818 16:26:09.605566  2034 solver.cpp:357] Iteration 33700 (1.8922 iter/s, 52.8486s/100 iters), loss = 0.174662
I0818 16:26:09.610860  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.174662 (* 1 = 0.174662 loss)
I0818 16:26:09.610918  2034 sgd_solver.cpp:165] Iteration 33700, lr = 0.01
I0818 16:27:05.995646  2034 solver.cpp:357] Iteration 33800 (1.77359 iter/s, 56.3827s/100 iters), loss = 0.121669
I0818 16:27:05.998814  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.121669 (* 1 = 0.121669 loss)
I0818 16:27:05.998908  2034 sgd_solver.cpp:165] Iteration 33800, lr = 0.01
I0818 16:28:00.731555  2034 solver.cpp:357] Iteration 33900 (1.82702 iter/s, 54.7339s/100 iters), loss = 0.150044
I0818 16:28:00.735074  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150044 (* 1 = 0.150044 loss)
I0818 16:28:00.735263  2034 sgd_solver.cpp:165] Iteration 33900, lr = 0.01
I0818 16:28:42.927639  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:28:53.285437  2034 solver.cpp:514] Iteration 34000, Testing net (#0)
I0818 16:29:39.407354  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:29:39.553843  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7865
I0818 16:29:39.554215  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.671855 (* 1 = 0.671855 loss)
I0818 16:29:40.009416  2034 solver.cpp:357] Iteration 34000 (1.0073 iter/s, 99.2757s/100 iters), loss = 0.11298
I0818 16:29:40.014925  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.112981 (* 1 = 0.112981 loss)
I0818 16:29:40.015121  2034 sgd_solver.cpp:165] Iteration 34000, lr = 0.01
I0818 16:30:36.728821  2034 solver.cpp:357] Iteration 34100 (1.76323 iter/s, 56.714s/100 iters), loss = 0.166585
I0818 16:30:36.730780  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.166585 (* 1 = 0.166585 loss)
I0818 16:30:36.730814  2034 sgd_solver.cpp:165] Iteration 34100, lr = 0.01
I0818 16:31:33.240494  2034 solver.cpp:357] Iteration 34200 (1.76964 iter/s, 56.5086s/100 iters), loss = 0.0709169
I0818 16:31:33.246804  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0709171 (* 1 = 0.0709171 loss)
I0818 16:31:33.246841  2034 sgd_solver.cpp:165] Iteration 34200, lr = 0.01
I0818 16:32:23.508273  2034 solver.cpp:357] Iteration 34300 (1.98956 iter/s, 50.2623s/100 iters), loss = 0.0998682
I0818 16:32:23.514928  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0998684 (* 1 = 0.0998684 loss)
I0818 16:32:23.515033  2034 sgd_solver.cpp:165] Iteration 34300, lr = 0.01
I0818 16:32:59.845605  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:33:16.553001  2034 solver.cpp:357] Iteration 34400 (1.88547 iter/s, 53.0371s/100 iters), loss = 0.196518
I0818 16:33:16.553207  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.196519 (* 1 = 0.196519 loss)
I0818 16:33:16.553273  2034 sgd_solver.cpp:165] Iteration 34400, lr = 0.01
I0818 16:34:12.310544  2034 solver.cpp:514] Iteration 34500, Testing net (#0)
I0818 16:34:55.644907  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:34:55.829918  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.7911
I0818 16:34:55.830107  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.65483 (* 1 = 0.65483 loss)
I0818 16:34:56.377463  2034 solver.cpp:357] Iteration 34500 (1.00175 iter/s, 99.8252s/100 iters), loss = 0.0763071
I0818 16:34:56.377646  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0763073 (* 1 = 0.0763073 loss)
I0818 16:34:56.377691  2034 sgd_solver.cpp:165] Iteration 34500, lr = 0.01
I0818 16:35:52.486955  2034 solver.cpp:357] Iteration 34600 (1.78233 iter/s, 56.1064s/100 iters), loss = 0.162203
I0818 16:35:52.487329  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.162203 (* 1 = 0.162203 loss)
I0818 16:35:52.487380  2034 sgd_solver.cpp:165] Iteration 34600, lr = 0.01
I0818 16:36:49.469736  2034 solver.cpp:357] Iteration 34700 (1.75491 iter/s, 56.9829s/100 iters), loss = 0.10421
I0818 16:36:49.470161  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.10421 (* 1 = 0.10421 loss)
I0818 16:36:49.470252  2034 sgd_solver.cpp:165] Iteration 34700, lr = 0.01
I0818 16:37:23.369240  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:37:44.228814  2034 solver.cpp:357] Iteration 34800 (1.82628 iter/s, 54.756s/100 iters), loss = 0.13424
I0818 16:37:44.229007  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13424 (* 1 = 0.13424 loss)
I0818 16:37:44.229053  2034 sgd_solver.cpp:165] Iteration 34800, lr = 0.01
I0818 16:38:38.188272  2034 solver.cpp:357] Iteration 34900 (1.85321 iter/s, 53.9604s/100 iters), loss = 0.187421
I0818 16:38:38.190781  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.187421 (* 1 = 0.187421 loss)
I0818 16:38:38.190814  2034 sgd_solver.cpp:165] Iteration 34900, lr = 0.01
I0818 16:39:29.882459  2034 solver.cpp:514] Iteration 35000, Testing net (#0)
I0818 16:40:13.367329  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:40:13.579000  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8036
I0818 16:40:13.579156  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.603754 (* 1 = 0.603754 loss)
I0818 16:40:14.090005  2034 solver.cpp:357] Iteration 35000 (1.04276 iter/s, 95.8995s/100 iters), loss = 0.13514
I0818 16:40:14.090181  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.13514 (* 1 = 0.13514 loss)
I0818 16:40:14.090227  2034 sgd_solver.cpp:165] Iteration 35000, lr = 0.01
I0818 16:41:10.124665  2034 solver.cpp:357] Iteration 35100 (1.78458 iter/s, 56.0356s/100 iters), loss = 0.147421
I0818 16:41:10.130842  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.147421 (* 1 = 0.147421 loss)
I0818 16:41:10.130887  2034 sgd_solver.cpp:165] Iteration 35100, lr = 0.01
I0818 16:41:40.004716  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:42:07.101833  2034 solver.cpp:357] Iteration 35200 (1.75525 iter/s, 56.9718s/100 iters), loss = 0.11791
I0818 16:42:07.106935  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11791 (* 1 = 0.11791 loss)
I0818 16:42:07.107043  2034 sgd_solver.cpp:165] Iteration 35200, lr = 0.01
I0818 16:43:03.322062  2034 solver.cpp:357] Iteration 35300 (1.77882 iter/s, 56.217s/100 iters), loss = 0.12547
I0818 16:43:03.322268  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12547 (* 1 = 0.12547 loss)
I0818 16:43:03.322297  2034 sgd_solver.cpp:165] Iteration 35300, lr = 0.01
I0818 16:43:59.213660  2034 solver.cpp:357] Iteration 35400 (1.78917 iter/s, 55.892s/100 iters), loss = 0.0860392
I0818 16:43:59.213959  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0860395 (* 1 = 0.0860395 loss)
I0818 16:43:59.214022  2034 sgd_solver.cpp:165] Iteration 35400, lr = 0.01
I0818 16:44:54.461197  2034 solver.cpp:514] Iteration 35500, Testing net (#0)
I0818 16:45:38.070482  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:45:38.162204  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.768301
I0818 16:45:38.162252  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.782208 (* 1 = 0.782208 loss)
I0818 16:45:38.572314  2034 solver.cpp:357] Iteration 35500 (1.00642 iter/s, 99.362s/100 iters), loss = 0.0899665
I0818 16:45:38.572381  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0899667 (* 1 = 0.0899667 loss)
I0818 16:45:38.572391  2034 sgd_solver.cpp:165] Iteration 35500, lr = 0.01
I0818 16:45:59.965692  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:46:30.616796  2034 solver.cpp:357] Iteration 35600 (1.92138 iter/s, 52.0459s/100 iters), loss = 0.206156
I0818 16:46:30.617211  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.206157 (* 1 = 0.206157 loss)
I0818 16:46:30.617262  2034 sgd_solver.cpp:165] Iteration 35600, lr = 0.01
I0818 16:47:24.556227  2034 solver.cpp:357] Iteration 35700 (1.85396 iter/s, 53.9386s/100 iters), loss = 0.144326
I0818 16:47:24.556444  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144327 (* 1 = 0.144327 loss)
I0818 16:47:24.556471  2034 sgd_solver.cpp:165] Iteration 35700, lr = 0.01
I0818 16:48:18.472565  2034 solver.cpp:357] Iteration 35800 (1.85468 iter/s, 53.9175s/100 iters), loss = 0.0741839
I0818 16:48:18.489189  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0741841 (* 1 = 0.0741841 loss)
I0818 16:48:18.489215  2034 sgd_solver.cpp:165] Iteration 35800, lr = 0.01
I0818 16:49:14.494629  2034 solver.cpp:357] Iteration 35900 (1.78547 iter/s, 56.0078s/100 iters), loss = 0.148407
I0818 16:49:15.550555  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.148407 (* 1 = 0.148407 loss)
I0818 16:49:15.550633  2034 sgd_solver.cpp:165] Iteration 35900, lr = 0.01
I0818 16:49:34.225409  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:50:10.951864  2034 solver.cpp:514] Iteration 36000, Testing net (#0)
I0818 16:50:56.085907  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:50:56.184084  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.817001
I0818 16:50:56.184147  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.5603 (* 1 = 0.5603 loss)
I0818 16:50:56.722010  2034 solver.cpp:357] Iteration 36000 (0.988384 iter/s, 101.175s/100 iters), loss = 0.143163
I0818 16:50:56.722080  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.143164 (* 1 = 0.143164 loss)
I0818 16:50:56.722093  2034 sgd_solver.cpp:165] Iteration 36000, lr = 0.01
I0818 16:51:52.256433  2034 solver.cpp:357] Iteration 36100 (1.80073 iter/s, 55.533s/100 iters), loss = 0.165981
I0818 16:51:52.256536  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.165981 (* 1 = 0.165981 loss)
I0818 16:51:52.256547  2034 sgd_solver.cpp:165] Iteration 36100, lr = 0.01
I0818 16:52:45.339283  2034 solver.cpp:357] Iteration 36200 (1.88384 iter/s, 53.0832s/100 iters), loss = 0.0514982
I0818 16:52:45.339433  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0514984 (* 1 = 0.0514984 loss)
I0818 16:52:45.339447  2034 sgd_solver.cpp:165] Iteration 36200, lr = 0.01
I0818 16:53:37.729960  2034 solver.cpp:357] Iteration 36300 (1.9088 iter/s, 52.3889s/100 iters), loss = 0.155274
I0818 16:53:37.730077  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155274 (* 1 = 0.155274 loss)
I0818 16:53:37.730087  2034 sgd_solver.cpp:165] Iteration 36300, lr = 0.01
I0818 16:53:51.220177  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:54:33.441936  2034 solver.cpp:357] Iteration 36400 (1.79487 iter/s, 55.7143s/100 iters), loss = 0.115037
I0818 16:54:33.442046  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.115037 (* 1 = 0.115037 loss)
I0818 16:54:33.442059  2034 sgd_solver.cpp:165] Iteration 36400, lr = 0.01
I0818 16:55:28.823060  2034 solver.cpp:514] Iteration 36500, Testing net (#0)
I0818 16:56:13.600713  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:56:13.737637  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.841001
I0818 16:56:13.737787  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.489137 (* 1 = 0.489137 loss)
I0818 16:56:14.103775  2034 solver.cpp:357] Iteration 36500 (0.993404 iter/s, 100.664s/100 iters), loss = 0.0982708
I0818 16:56:14.103966  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.098271 (* 1 = 0.098271 loss)
I0818 16:56:14.104013  2034 sgd_solver.cpp:165] Iteration 36500, lr = 0.01
I0818 16:57:08.710253  2034 solver.cpp:357] Iteration 36600 (1.83135 iter/s, 54.6044s/100 iters), loss = 0.0816146
I0818 16:57:08.710958  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0816148 (* 1 = 0.0816148 loss)
I0818 16:57:08.711141  2034 sgd_solver.cpp:165] Iteration 36600, lr = 0.01
I0818 16:58:02.780519  2034 solver.cpp:357] Iteration 36700 (1.84952 iter/s, 54.0682s/100 iters), loss = 0.0633853
I0818 16:58:02.780658  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0633855 (* 1 = 0.0633855 loss)
I0818 16:58:02.780669  2034 sgd_solver.cpp:165] Iteration 36700, lr = 0.01
I0818 16:58:10.867621  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 16:58:58.557523  2034 solver.cpp:357] Iteration 36800 (1.79285 iter/s, 55.777s/100 iters), loss = 0.104483
I0818 16:58:58.557824  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104483 (* 1 = 0.104483 loss)
I0818 16:58:58.557888  2034 sgd_solver.cpp:165] Iteration 36800, lr = 0.01
I0818 16:59:50.708413  2034 solver.cpp:357] Iteration 36900 (1.9176 iter/s, 52.1486s/100 iters), loss = 0.156071
I0818 16:59:50.708549  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156071 (* 1 = 0.156071 loss)
I0818 16:59:50.708562  2034 sgd_solver.cpp:165] Iteration 36900, lr = 0.01
I0818 17:00:44.470549  2034 solver.cpp:514] Iteration 37000, Testing net (#0)
I0818 17:01:27.815795  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:01:28.030443  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.841501
I0818 17:01:28.030505  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.494908 (* 1 = 0.494908 loss)
I0818 17:01:28.430932  2034 solver.cpp:357] Iteration 37000 (1.02329 iter/s, 97.724s/100 iters), loss = 0.10283
I0818 17:01:28.430986  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102831 (* 1 = 0.102831 loss)
I0818 17:01:28.430996  2034 sgd_solver.cpp:165] Iteration 37000, lr = 0.01
I0818 17:02:24.263581  2034 solver.cpp:357] Iteration 37100 (1.791 iter/s, 55.8346s/100 iters), loss = 0.0966503
I0818 17:02:24.263803  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0966505 (* 1 = 0.0966505 loss)
I0818 17:02:24.263847  2034 sgd_solver.cpp:165] Iteration 37100, lr = 0.01
I0818 17:02:27.319663  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:03:20.033109  2034 solver.cpp:357] Iteration 37200 (1.79304 iter/s, 55.7713s/100 iters), loss = 0.141647
I0818 17:03:20.033259  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.141647 (* 1 = 0.141647 loss)
I0818 17:03:20.033272  2034 sgd_solver.cpp:165] Iteration 37200, lr = 0.01
I0818 17:04:16.677744  2034 solver.cpp:357] Iteration 37300 (1.7654 iter/s, 56.6444s/100 iters), loss = 0.0754746
I0818 17:04:16.677880  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0754749 (* 1 = 0.0754749 loss)
I0818 17:04:16.677891  2034 sgd_solver.cpp:165] Iteration 37300, lr = 0.01
I0818 17:05:12.293056  2034 solver.cpp:357] Iteration 37400 (1.79808 iter/s, 55.615s/100 iters), loss = 0.164142
I0818 17:05:12.293220  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.164143 (* 1 = 0.164143 loss)
I0818 17:05:12.293233  2034 sgd_solver.cpp:165] Iteration 37400, lr = 0.01
I0818 17:06:05.261358  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:06:07.486042  2034 solver.cpp:514] Iteration 37500, Testing net (#0)
I0818 17:06:45.138141  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:06:45.289937  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.854401
I0818 17:06:45.290115  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.437149 (* 1 = 0.437149 loss)
I0818 17:06:45.702220  2034 solver.cpp:357] Iteration 37500 (1.07055 iter/s, 93.4101s/100 iters), loss = 0.0833123
I0818 17:06:45.702392  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0833126 (* 1 = 0.0833126 loss)
I0818 17:06:45.702437  2034 sgd_solver.cpp:165] Iteration 37500, lr = 0.01
I0818 17:07:38.542484  2034 solver.cpp:357] Iteration 37600 (1.89251 iter/s, 52.8398s/100 iters), loss = 0.0835611
I0818 17:07:38.546947  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0835613 (* 1 = 0.0835613 loss)
I0818 17:07:38.547052  2034 sgd_solver.cpp:165] Iteration 37600, lr = 0.01
I0818 17:08:35.238019  2034 solver.cpp:357] Iteration 37700 (1.76395 iter/s, 56.6909s/100 iters), loss = 0.125515
I0818 17:08:35.238306  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.125515 (* 1 = 0.125515 loss)
I0818 17:08:35.238353  2034 sgd_solver.cpp:165] Iteration 37700, lr = 0.01
I0818 17:09:31.719136  2034 solver.cpp:357] Iteration 37800 (1.77051 iter/s, 56.4808s/100 iters), loss = 0.153972
I0818 17:09:31.719424  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.153973 (* 1 = 0.153973 loss)
I0818 17:09:31.719472  2034 sgd_solver.cpp:165] Iteration 37800, lr = 0.01
I0818 17:10:19.507771  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:10:27.498808  2034 solver.cpp:357] Iteration 37900 (1.79285 iter/s, 55.7772s/100 iters), loss = 0.160608
I0818 17:10:27.499020  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.160608 (* 1 = 0.160608 loss)
I0818 17:10:27.499066  2034 sgd_solver.cpp:165] Iteration 37900, lr = 0.01
I0818 17:11:23.305097  2034 solver.cpp:514] Iteration 38000, Testing net (#0)
I0818 17:12:07.966166  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:12:08.014284  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8259
I0818 17:12:08.014334  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.54875 (* 1 = 0.54875 loss)
I0818 17:12:08.477241  2034 solver.cpp:357] Iteration 38000 (0.9903 iter/s, 100.98s/100 iters), loss = 0.0832982
I0818 17:12:08.477421  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0832984 (* 1 = 0.0832984 loss)
I0818 17:12:08.477468  2034 sgd_solver.cpp:165] Iteration 38000, lr = 0.01
I0818 17:13:04.362221  2034 solver.cpp:357] Iteration 38100 (1.78947 iter/s, 55.8825s/100 iters), loss = 0.0886553
I0818 17:13:04.366832  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0886556 (* 1 = 0.0886556 loss)
I0818 17:13:04.366869  2034 sgd_solver.cpp:165] Iteration 38100, lr = 0.01
I0818 17:13:51.810039  2034 solver.cpp:357] Iteration 38200 (2.10772 iter/s, 47.4447s/100 iters), loss = 0.0871986
I0818 17:13:51.814817  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0871989 (* 1 = 0.0871989 loss)
I0818 17:13:51.814859  2034 sgd_solver.cpp:165] Iteration 38200, lr = 0.01
I0818 17:14:34.946188  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:14:47.939997  2034 solver.cpp:357] Iteration 38300 (1.7818 iter/s, 56.1232s/100 iters), loss = 0.176974
I0818 17:14:47.940193  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.176974 (* 1 = 0.176974 loss)
I0818 17:14:47.940225  2034 sgd_solver.cpp:165] Iteration 38300, lr = 0.01
I0818 17:15:44.047536  2034 solver.cpp:357] Iteration 38400 (1.78231 iter/s, 56.1071s/100 iters), loss = 0.114696
I0818 17:15:44.050900  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.114697 (* 1 = 0.114697 loss)
I0818 17:15:44.050943  2034 sgd_solver.cpp:165] Iteration 38400, lr = 0.01
I0818 17:16:36.544232  2034 solver.cpp:514] Iteration 38500, Testing net (#0)
I0818 17:17:20.951373  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:17:21.141752  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.853802
I0818 17:17:21.141935  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.489355 (* 1 = 0.489355 loss)
I0818 17:17:21.697062  2034 solver.cpp:357] Iteration 38500 (1.02411 iter/s, 97.6459s/100 iters), loss = 0.100231
I0818 17:17:21.697217  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.100232 (* 1 = 0.100232 loss)
I0818 17:17:21.698724  2034 sgd_solver.cpp:165] Iteration 38500, lr = 0.01
I0818 17:18:18.414108  2034 solver.cpp:357] Iteration 38600 (1.76328 iter/s, 56.7126s/100 iters), loss = 0.0629151
I0818 17:18:18.414500  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0629153 (* 1 = 0.0629153 loss)
I0818 17:18:18.414531  2034 sgd_solver.cpp:165] Iteration 38600, lr = 0.01
I0818 17:18:56.608108  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:19:15.343885  2034 solver.cpp:357] Iteration 38700 (1.75668 iter/s, 56.9257s/100 iters), loss = 0.118389
I0818 17:19:15.344084  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.118389 (* 1 = 0.118389 loss)
I0818 17:19:15.344130  2034 sgd_solver.cpp:165] Iteration 38700, lr = 0.01
I0818 17:20:08.576058  2034 solver.cpp:357] Iteration 38800 (1.87877 iter/s, 53.2263s/100 iters), loss = 0.0863203
I0818 17:20:08.576278  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0863205 (* 1 = 0.0863205 loss)
I0818 17:20:08.576304  2034 sgd_solver.cpp:165] Iteration 38800, lr = 0.01
I0818 17:21:00.011807  2034 solver.cpp:357] Iteration 38900 (1.94438 iter/s, 51.4302s/100 iters), loss = 0.11918
I0818 17:21:00.012365  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119181 (* 1 = 0.119181 loss)
I0818 17:21:00.012545  2034 sgd_solver.cpp:165] Iteration 38900, lr = 0.01
I0818 17:21:55.619032  2034 solver.cpp:514] Iteration 39000, Testing net (#0)
I0818 17:22:41.953397  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:22:42.137468  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.846601
I0818 17:22:42.137656  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.492838 (* 1 = 0.492838 loss)
I0818 17:22:42.601120  2034 solver.cpp:357] Iteration 39000 (0.974799 iter/s, 102.585s/100 iters), loss = 0.0729036
I0818 17:22:42.601331  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0729039 (* 1 = 0.0729039 loss)
I0818 17:22:42.601378  2034 sgd_solver.cpp:165] Iteration 39000, lr = 0.01
I0818 17:23:16.132223  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:23:39.626634  2034 solver.cpp:357] Iteration 39100 (1.75369 iter/s, 57.0226s/100 iters), loss = 0.0918972
I0818 17:23:39.630821  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0918975 (* 1 = 0.0918975 loss)
I0818 17:23:39.630887  2034 sgd_solver.cpp:165] Iteration 39100, lr = 0.01
I0818 17:24:37.046172  2034 solver.cpp:357] Iteration 39200 (1.74171 iter/s, 57.4148s/100 iters), loss = 0.137365
I0818 17:24:37.046404  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137365 (* 1 = 0.137365 loss)
I0818 17:24:37.046450  2034 sgd_solver.cpp:165] Iteration 39200, lr = 0.01
I0818 17:25:31.756863  2034 solver.cpp:357] Iteration 39300 (1.82782 iter/s, 54.71s/100 iters), loss = 0.126443
I0818 17:25:31.757262  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.126443 (* 1 = 0.126443 loss)
I0818 17:25:31.757349  2034 sgd_solver.cpp:165] Iteration 39300, lr = 0.01
I0818 17:26:26.020068  2034 solver.cpp:357] Iteration 39400 (1.84295 iter/s, 54.2608s/100 iters), loss = 0.0526182
I0818 17:26:26.022895  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0526185 (* 1 = 0.0526185 loss)
I0818 17:26:26.022954  2034 sgd_solver.cpp:165] Iteration 39400, lr = 0.01
I0818 17:26:51.193699  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:27:12.321166  2034 solver.cpp:514] Iteration 39500, Testing net (#0)
I0818 17:27:56.619272  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:27:56.873843  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.838601
I0818 17:27:56.874024  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.540734 (* 1 = 0.540734 loss)
I0818 17:27:57.164503  2034 solver.cpp:357] Iteration 39500 (1.09719 iter/s, 91.1415s/100 iters), loss = 0.0826296
I0818 17:27:57.164700  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0826298 (* 1 = 0.0826298 loss)
I0818 17:27:57.164747  2034 sgd_solver.cpp:165] Iteration 39500, lr = 0.01
I0818 17:28:53.354835  2034 solver.cpp:357] Iteration 39600 (1.77973 iter/s, 56.1882s/100 iters), loss = 0.0685937
I0818 17:28:53.355134  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.068594 (* 1 = 0.068594 loss)
I0818 17:28:53.355159  2034 sgd_solver.cpp:165] Iteration 39600, lr = 0.01
I0818 17:29:49.998286  2034 solver.cpp:357] Iteration 39700 (1.76556 iter/s, 56.6392s/100 iters), loss = 0.155204
I0818 17:29:49.998877  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.155205 (* 1 = 0.155205 loss)
I0818 17:29:49.999055  2034 sgd_solver.cpp:165] Iteration 39700, lr = 0.01
I0818 17:30:45.702721  2034 solver.cpp:357] Iteration 39800 (1.79537 iter/s, 55.6988s/100 iters), loss = 0.0792645
I0818 17:30:45.703011  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0792647 (* 1 = 0.0792647 loss)
I0818 17:30:45.703060  2034 sgd_solver.cpp:165] Iteration 39800, lr = 0.01
I0818 17:31:07.871299  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:31:42.510159  2034 solver.cpp:357] Iteration 39900 (1.76042 iter/s, 56.8046s/100 iters), loss = 0.0822514
I0818 17:31:42.510377  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0822517 (* 1 = 0.0822517 loss)
I0818 17:31:42.510404  2034 sgd_solver.cpp:165] Iteration 39900, lr = 0.01
I0818 17:32:39.342782  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_40000.caffemodel
I0818 17:32:39.366238  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_40000.solverstate
I0818 17:32:39.371918  2034 solver.cpp:514] Iteration 40000, Testing net (#0)
I0818 17:33:25.739024  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:33:25.934751  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.873001
I0818 17:33:25.934952  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.399489 (* 1 = 0.399489 loss)
I0818 17:33:26.398192  2034 solver.cpp:357] Iteration 40000 (0.962588 iter/s, 103.887s/100 iters), loss = 0.129411
I0818 17:33:26.398418  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.129411 (* 1 = 0.129411 loss)
I0818 17:33:26.398464  2034 sgd_solver.cpp:165] Iteration 40000, lr = 0.01
I0818 17:34:17.174710  2034 solver.cpp:357] Iteration 40100 (1.96952 iter/s, 50.7739s/100 iters), loss = 0.0973128
I0818 17:34:17.174891  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.097313 (* 1 = 0.097313 loss)
I0818 17:34:17.174916  2034 sgd_solver.cpp:165] Iteration 40100, lr = 0.01
I0818 17:35:10.446784  2034 solver.cpp:357] Iteration 40200 (1.87715 iter/s, 53.2724s/100 iters), loss = 0.144447
I0818 17:35:10.446990  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144447 (* 1 = 0.144447 loss)
I0818 17:35:10.447016  2034 sgd_solver.cpp:165] Iteration 40200, lr = 0.01
I0818 17:35:27.029373  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:36:05.690683  2034 solver.cpp:357] Iteration 40300 (1.81028 iter/s, 55.2402s/100 iters), loss = 0.0927068
I0818 17:36:05.695032  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0927071 (* 1 = 0.0927071 loss)
I0818 17:36:05.695224  2034 sgd_solver.cpp:165] Iteration 40300, lr = 0.01
I0818 17:37:02.199551  2034 solver.cpp:357] Iteration 40400 (1.76975 iter/s, 56.5052s/100 iters), loss = 0.131249
I0818 17:37:02.199764  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.131249 (* 1 = 0.131249 loss)
I0818 17:37:02.199805  2034 sgd_solver.cpp:165] Iteration 40400, lr = 0.01
I0818 17:37:57.855674  2034 solver.cpp:514] Iteration 40500, Testing net (#0)
I0818 17:38:43.688159  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:38:43.826858  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.8359
I0818 17:38:43.826987  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.570979 (* 1 = 0.570979 loss)
I0818 17:38:44.291467  2034 solver.cpp:357] Iteration 40500 (0.979518 iter/s, 102.091s/100 iters), loss = 0.11024
I0818 17:38:44.291697  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11024 (* 1 = 0.11024 loss)
I0818 17:38:44.291746  2034 sgd_solver.cpp:165] Iteration 40500, lr = 0.01
I0818 17:39:40.809459  2034 solver.cpp:357] Iteration 40600 (1.76939 iter/s, 56.5165s/100 iters), loss = 0.0854626
I0818 17:39:40.814898  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0854628 (* 1 = 0.0854628 loss)
I0818 17:39:40.814961  2034 sgd_solver.cpp:165] Iteration 40600, lr = 0.01
I0818 17:39:52.252714  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:40:31.766628  2034 solver.cpp:357] Iteration 40700 (1.96274 iter/s, 50.9492s/100 iters), loss = 0.137003
I0818 17:40:31.771085  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.137004 (* 1 = 0.137004 loss)
I0818 17:40:31.771173  2034 sgd_solver.cpp:165] Iteration 40700, lr = 0.01
I0818 17:41:21.697222  2034 solver.cpp:357] Iteration 40800 (2.00293 iter/s, 49.9268s/100 iters), loss = 0.0497592
I0818 17:41:21.698850  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0497595 (* 1 = 0.0497595 loss)
I0818 17:41:21.698905  2034 sgd_solver.cpp:165] Iteration 40800, lr = 0.01
I0818 17:42:17.531908  2034 solver.cpp:357] Iteration 40900 (1.79105 iter/s, 55.8333s/100 iters), loss = 0.0447981
I0818 17:42:17.534787  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0447983 (* 1 = 0.0447983 loss)
I0818 17:42:17.534822  2034 sgd_solver.cpp:165] Iteration 40900, lr = 0.01
I0818 17:43:13.210733  2034 solver.cpp:514] Iteration 41000, Testing net (#0)
I0818 17:43:56.927405  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:43:57.036258  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.854101
I0818 17:43:57.036438  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.447645 (* 1 = 0.447645 loss)
I0818 17:43:57.472702  2034 solver.cpp:357] Iteration 41000 (1.00062 iter/s, 99.938s/100 iters), loss = 0.0625114
I0818 17:43:57.472905  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0625115 (* 1 = 0.0625115 loss)
I0818 17:43:57.472951  2034 sgd_solver.cpp:165] Iteration 41000, lr = 0.01
I0818 17:44:03.742883  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:44:52.137066  2034 solver.cpp:357] Iteration 41100 (1.82946 iter/s, 54.6609s/100 iters), loss = 0.0837602
I0818 17:44:52.142853  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0837604 (* 1 = 0.0837604 loss)
I0818 17:44:52.142897  2034 sgd_solver.cpp:165] Iteration 41100, lr = 0.01
I0818 17:45:48.107973  2034 solver.cpp:357] Iteration 41200 (1.78689 iter/s, 55.9632s/100 iters), loss = 0.0620264
I0818 17:45:48.110798  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0620265 (* 1 = 0.0620265 loss)
I0818 17:45:48.110838  2034 sgd_solver.cpp:165] Iteration 41200, lr = 0.01
I0818 17:46:44.478730  2034 solver.cpp:357] Iteration 41300 (1.7742 iter/s, 56.3635s/100 iters), loss = 0.0560385
I0818 17:46:44.478976  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0560386 (* 1 = 0.0560386 loss)
I0818 17:46:44.479004  2034 sgd_solver.cpp:165] Iteration 41300, lr = 0.01
I0818 17:47:37.101464  2034 solver.cpp:357] Iteration 41400 (1.90037 iter/s, 52.6213s/100 iters), loss = 0.0583042
I0818 17:47:37.101616  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0583044 (* 1 = 0.0583044 loss)
I0818 17:47:37.101630  2034 sgd_solver.cpp:165] Iteration 41400, lr = 0.01
I0818 17:47:38.251133  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:48:29.571727  2034 solver.cpp:514] Iteration 41500, Testing net (#0)
I0818 17:49:15.073987  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:49:15.269110  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.855101
I0818 17:49:15.269166  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.49263 (* 1 = 0.49263 loss)
I0818 17:49:15.695969  2034 solver.cpp:357] Iteration 41500 (1.01426 iter/s, 98.5939s/100 iters), loss = 0.116262
I0818 17:49:15.696039  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.116262 (* 1 = 0.116262 loss)
I0818 17:49:15.696050  2034 sgd_solver.cpp:165] Iteration 41500, lr = 0.01
I0818 17:50:11.551955  2034 solver.cpp:357] Iteration 41600 (1.79036 iter/s, 55.8547s/100 iters), loss = 0.0578734
I0818 17:50:11.552196  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0578735 (* 1 = 0.0578735 loss)
I0818 17:50:11.552207  2034 sgd_solver.cpp:165] Iteration 41600, lr = 0.01
I0818 17:51:07.355427  2034 solver.cpp:357] Iteration 41700 (1.79203 iter/s, 55.8025s/100 iters), loss = 0.120912
I0818 17:51:07.355584  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120912 (* 1 = 0.120912 loss)
I0818 17:51:07.355595  2034 sgd_solver.cpp:165] Iteration 41700, lr = 0.01
I0818 17:51:58.857933  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:52:03.168241  2034 solver.cpp:357] Iteration 41800 (1.79166 iter/s, 55.8143s/100 iters), loss = 0.0713996
I0818 17:52:03.168315  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0713997 (* 1 = 0.0713997 loss)
I0818 17:52:03.168326  2034 sgd_solver.cpp:165] Iteration 41800, lr = 0.01
I0818 17:52:58.614101  2034 solver.cpp:357] Iteration 41900 (1.80358 iter/s, 55.4452s/100 iters), loss = 0.0874414
I0818 17:52:58.614264  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0874416 (* 1 = 0.0874416 loss)
I0818 17:52:58.614275  2034 sgd_solver.cpp:165] Iteration 41900, lr = 0.01
I0818 17:53:53.907507  2034 solver.cpp:514] Iteration 42000, Testing net (#0)
I0818 17:54:32.192903  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:54:32.341936  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.802701
I0818 17:54:32.341995  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.716592 (* 1 = 0.716592 loss)
I0818 17:54:32.720315  2034 solver.cpp:357] Iteration 42000 (1.06262 iter/s, 94.1066s/100 iters), loss = 0.119741
I0818 17:54:32.720383  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.119741 (* 1 = 0.119741 loss)
I0818 17:54:32.720394  2034 sgd_solver.cpp:165] Iteration 42000, lr = 0.01
I0818 17:55:24.237610  2034 solver.cpp:357] Iteration 42100 (1.9411 iter/s, 51.5173s/100 iters), loss = 0.0763343
I0818 17:55:24.237751  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0763344 (* 1 = 0.0763344 loss)
I0818 17:55:24.237763  2034 sgd_solver.cpp:165] Iteration 42100, lr = 0.01
I0818 17:56:10.528156  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:56:19.721932  2034 solver.cpp:357] Iteration 42200 (1.80234 iter/s, 55.4835s/100 iters), loss = 0.059502
I0818 17:56:19.721999  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0595021 (* 1 = 0.0595021 loss)
I0818 17:56:19.722010  2034 sgd_solver.cpp:165] Iteration 42200, lr = 0.01
I0818 17:57:15.389782  2034 solver.cpp:357] Iteration 42300 (1.79641 iter/s, 55.6664s/100 iters), loss = 0.0779728
I0818 17:57:15.389955  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0779729 (* 1 = 0.0779729 loss)
I0818 17:57:15.389968  2034 sgd_solver.cpp:165] Iteration 42300, lr = 0.01
I0818 17:58:10.760515  2034 solver.cpp:357] Iteration 42400 (1.80611 iter/s, 55.3677s/100 iters), loss = 0.150257
I0818 17:58:10.760627  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.150257 (* 1 = 0.150257 loss)
I0818 17:58:10.760638  2034 sgd_solver.cpp:165] Iteration 42400, lr = 0.01
I0818 17:59:06.236991  2034 solver.cpp:514] Iteration 42500, Testing net (#0)
I0818 17:59:51.600270  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 17:59:51.795018  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.823801
I0818 17:59:51.795078  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.636827 (* 1 = 0.636827 loss)
I0818 17:59:52.298872  2034 solver.cpp:357] Iteration 42500 (0.984848 iter/s, 101.539s/100 iters), loss = 0.0425623
I0818 17:59:52.298933  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0425624 (* 1 = 0.0425624 loss)
I0818 17:59:52.298943  2034 sgd_solver.cpp:165] Iteration 42500, lr = 0.01
I0818 18:00:33.514298  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:00:47.930315  2034 solver.cpp:357] Iteration 42600 (1.79757 iter/s, 55.6305s/100 iters), loss = 0.113392
I0818 18:00:47.930395  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113392 (* 1 = 0.113392 loss)
I0818 18:00:47.930408  2034 sgd_solver.cpp:165] Iteration 42600, lr = 0.01
I0818 18:01:40.609917  2034 solver.cpp:357] Iteration 42700 (1.89838 iter/s, 52.6765s/100 iters), loss = 0.0345246
I0818 18:01:40.610080  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0345247 (* 1 = 0.0345247 loss)
I0818 18:01:40.610091  2034 sgd_solver.cpp:165] Iteration 42700, lr = 0.01
I0818 18:02:33.661387  2034 solver.cpp:357] Iteration 42800 (1.88507 iter/s, 53.0483s/100 iters), loss = 0.0722381
I0818 18:02:33.661542  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0722383 (* 1 = 0.0722383 loss)
I0818 18:02:33.661556  2034 sgd_solver.cpp:165] Iteration 42800, lr = 0.01
I0818 18:03:29.487341  2034 solver.cpp:357] Iteration 42900 (1.79138 iter/s, 55.8229s/100 iters), loss = 0.0620819
I0818 18:03:29.487504  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.062082 (* 1 = 0.062082 loss)
I0818 18:03:29.487516  2034 sgd_solver.cpp:165] Iteration 42900, lr = 0.01
I0818 18:04:03.140019  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:04:21.237332  2034 solver.cpp:514] Iteration 43000, Testing net (#0)
I0818 18:05:04.669209  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:05:04.920569  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.866402
I0818 18:05:04.920624  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.425096 (* 1 = 0.425096 loss)
I0818 18:05:05.182438  2034 solver.cpp:357] Iteration 43000 (1.04499 iter/s, 95.695s/100 iters), loss = 0.11658
I0818 18:05:05.182512  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.11658 (* 1 = 0.11658 loss)
I0818 18:05:05.182523  2034 sgd_solver.cpp:165] Iteration 43000, lr = 0.01
I0818 18:06:01.010113  2034 solver.cpp:357] Iteration 43100 (1.79133 iter/s, 55.8246s/100 iters), loss = 0.0689865
I0818 18:06:01.010237  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0689867 (* 1 = 0.0689867 loss)
I0818 18:06:01.010251  2034 sgd_solver.cpp:165] Iteration 43100, lr = 0.01
I0818 18:06:56.794602  2034 solver.cpp:357] Iteration 43200 (1.79265 iter/s, 55.7834s/100 iters), loss = 0.0686063
I0818 18:06:56.794734  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0686065 (* 1 = 0.0686065 loss)
I0818 18:06:56.794745  2034 sgd_solver.cpp:165] Iteration 43200, lr = 0.01
I0818 18:07:52.632436  2034 solver.cpp:357] Iteration 43300 (1.79087 iter/s, 55.8388s/100 iters), loss = 0.0630424
I0818 18:07:52.632599  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0630426 (* 1 = 0.0630426 loss)
I0818 18:07:52.632611  2034 sgd_solver.cpp:165] Iteration 43300, lr = 0.01
I0818 18:08:22.366461  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:08:44.970554  2034 solver.cpp:357] Iteration 43400 (1.91069 iter/s, 52.337s/100 iters), loss = 0.198529
I0818 18:08:44.970746  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.198529 (* 1 = 0.198529 loss)
I0818 18:08:44.970772  2034 sgd_solver.cpp:165] Iteration 43400, lr = 0.01
I0818 18:09:37.558755  2034 solver.cpp:514] Iteration 43500, Testing net (#0)
I0818 18:10:22.719369  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:10:22.909101  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.871602
I0818 18:10:22.909160  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.44631 (* 1 = 0.44631 loss)
I0818 18:10:23.405378  2034 solver.cpp:357] Iteration 43500 (1.01588 iter/s, 98.4366s/100 iters), loss = 0.156072
I0818 18:10:23.405445  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.156072 (* 1 = 0.156072 loss)
I0818 18:10:23.405457  2034 sgd_solver.cpp:165] Iteration 43500, lr = 0.01
I0818 18:11:19.355332  2034 solver.cpp:357] Iteration 43600 (1.78735 iter/s, 55.9489s/100 iters), loss = 0.180057
I0818 18:11:19.355576  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.180057 (* 1 = 0.180057 loss)
I0818 18:11:19.355587  2034 sgd_solver.cpp:165] Iteration 43600, lr = 0.01
I0818 18:12:15.098505  2034 solver.cpp:357] Iteration 43700 (1.79398 iter/s, 55.7421s/100 iters), loss = 0.0435421
I0818 18:12:15.098841  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0435423 (* 1 = 0.0435423 loss)
I0818 18:12:15.098908  2034 sgd_solver.cpp:165] Iteration 43700, lr = 0.01
I0818 18:12:40.257019  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:13:10.761744  2034 solver.cpp:357] Iteration 43800 (1.79662 iter/s, 55.66s/100 iters), loss = 0.0635572
I0818 18:13:10.761907  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0635573 (* 1 = 0.0635573 loss)
I0818 18:13:10.761919  2034 sgd_solver.cpp:165] Iteration 43800, lr = 0.01
I0818 18:14:03.279985  2034 solver.cpp:357] Iteration 43900 (1.90414 iter/s, 52.5171s/100 iters), loss = 0.136575
I0818 18:14:03.280118  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136576 (* 1 = 0.136576 loss)
I0818 18:14:03.280129  2034 sgd_solver.cpp:165] Iteration 43900, lr = 0.01
I0818 18:14:56.721181  2034 solver.cpp:514] Iteration 44000, Testing net (#0)
I0818 18:15:38.622750  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:15:38.802412  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.894602
I0818 18:15:38.802469  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.359413 (* 1 = 0.359413 loss)
I0818 18:15:39.165922  2034 solver.cpp:357] Iteration 44000 (1.04291 iter/s, 95.8857s/100 iters), loss = 0.101667
I0818 18:15:39.165983  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101667 (* 1 = 0.101667 loss)
I0818 18:15:39.165994  2034 sgd_solver.cpp:165] Iteration 44000, lr = 0.01
I0818 18:16:32.513782  2034 solver.cpp:357] Iteration 44100 (1.87446 iter/s, 53.3488s/100 iters), loss = 0.107338
I0818 18:16:32.513931  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107338 (* 1 = 0.107338 loss)
I0818 18:16:32.513942  2034 sgd_solver.cpp:165] Iteration 44100, lr = 0.01
I0818 18:16:52.751543  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:17:28.332712  2034 solver.cpp:357] Iteration 44200 (1.79148 iter/s, 55.8199s/100 iters), loss = 0.130605
I0818 18:17:28.332847  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.130605 (* 1 = 0.130605 loss)
I0818 18:17:28.332859  2034 sgd_solver.cpp:165] Iteration 44200, lr = 0.01
I0818 18:18:24.211635  2034 solver.cpp:357] Iteration 44300 (1.78962 iter/s, 55.8778s/100 iters), loss = 0.145076
I0818 18:18:24.211818  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145076 (* 1 = 0.145076 loss)
I0818 18:18:24.211830  2034 sgd_solver.cpp:165] Iteration 44300, lr = 0.01
I0818 18:19:20.157096  2034 solver.cpp:357] Iteration 44400 (1.78756 iter/s, 55.9423s/100 iters), loss = 0.120987
I0818 18:19:20.157251  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.120987 (* 1 = 0.120987 loss)
I0818 18:19:20.157279  2034 sgd_solver.cpp:165] Iteration 44400, lr = 0.01
I0818 18:20:15.359274  2034 solver.cpp:514] Iteration 44500, Testing net (#0)
I0818 18:20:58.781375  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:20:58.946434  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.817101
I0818 18:20:58.946588  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.729396 (* 1 = 0.729396 loss)
I0818 18:20:59.362061  2034 solver.cpp:357] Iteration 44500 (1.00802 iter/s, 99.2047s/100 iters), loss = 0.0819137
I0818 18:20:59.362239  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0819139 (* 1 = 0.0819139 loss)
I0818 18:20:59.362283  2034 sgd_solver.cpp:165] Iteration 44500, lr = 0.01
I0818 18:21:14.443627  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:21:55.250893  2034 solver.cpp:357] Iteration 44600 (1.7893 iter/s, 55.8877s/100 iters), loss = 0.0960126
I0818 18:21:55.254838  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0960128 (* 1 = 0.0960128 loss)
I0818 18:21:55.254891  2034 sgd_solver.cpp:165] Iteration 44600, lr = 0.01
I0818 18:22:44.912933  2034 solver.cpp:357] Iteration 44700 (2.01375 iter/s, 49.6586s/100 iters), loss = 0.0814233
I0818 18:22:44.913522  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0814234 (* 1 = 0.0814234 loss)
I0818 18:22:44.913703  2034 sgd_solver.cpp:165] Iteration 44700, lr = 0.01
I0818 18:23:36.149549  2034 solver.cpp:357] Iteration 44800 (1.95185 iter/s, 51.2333s/100 iters), loss = 0.088917
I0818 18:23:36.154798  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0889171 (* 1 = 0.0889171 loss)
I0818 18:23:36.154834  2034 sgd_solver.cpp:165] Iteration 44800, lr = 0.01
I0818 18:24:31.010010  2034 solver.cpp:357] Iteration 44900 (1.82299 iter/s, 54.855s/100 iters), loss = 0.0938839
I0818 18:24:31.010260  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.093884 (* 1 = 0.093884 loss)
I0818 18:24:31.010308  2034 sgd_solver.cpp:165] Iteration 44900, lr = 0.01
I0818 18:24:40.782014  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:25:27.409395  2034 solver.cpp:514] Iteration 45000, Testing net (#0)
I0818 18:26:11.005925  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:26:11.181243  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.852701
I0818 18:26:11.181430  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.520685 (* 1 = 0.520685 loss)
I0818 18:26:11.574889  2034 solver.cpp:357] Iteration 45000 (0.994332 iter/s, 100.57s/100 iters), loss = 0.104184
I0818 18:26:11.575028  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.104184 (* 1 = 0.104184 loss)
I0818 18:26:11.575057  2034 sgd_solver.cpp:165] Iteration 45000, lr = 0.01
I0818 18:27:08.634263  2034 solver.cpp:357] Iteration 45100 (1.75248 iter/s, 57.0618s/100 iters), loss = 0.099002
I0818 18:27:08.638800  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0990022 (* 1 = 0.0990022 loss)
I0818 18:27:08.638840  2034 sgd_solver.cpp:165] Iteration 45100, lr = 0.01
I0818 18:28:05.091619  2034 solver.cpp:357] Iteration 45200 (1.77138 iter/s, 56.4531s/100 iters), loss = 0.101823
I0818 18:28:05.091938  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.101823 (* 1 = 0.101823 loss)
I0818 18:28:05.092003  2034 sgd_solver.cpp:165] Iteration 45200, lr = 0.01
I0818 18:29:00.581774  2034 solver.cpp:357] Iteration 45300 (1.80213 iter/s, 55.4899s/100 iters), loss = 0.0620514
I0818 18:29:00.581917  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0620516 (* 1 = 0.0620516 loss)
I0818 18:29:00.581929  2034 sgd_solver.cpp:165] Iteration 45300, lr = 0.01
I0818 18:29:05.243343  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:29:51.976888  2034 solver.cpp:357] Iteration 45400 (1.94574 iter/s, 51.3943s/100 iters), loss = 0.0704019
I0818 18:29:51.977110  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.070402 (* 1 = 0.070402 loss)
I0818 18:29:51.977144  2034 sgd_solver.cpp:165] Iteration 45400, lr = 0.01
I0818 18:30:45.825623  2034 solver.cpp:514] Iteration 45500, Testing net (#0)
I0818 18:31:31.146075  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:31:31.289010  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.859801
I0818 18:31:31.289063  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.498143 (* 1 = 0.498143 loss)
I0818 18:31:31.672050  2034 solver.cpp:357] Iteration 45500 (1.00302 iter/s, 99.6993s/100 iters), loss = 0.144203
I0818 18:31:31.672117  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.144203 (* 1 = 0.144203 loss)
I0818 18:31:31.672127  2034 sgd_solver.cpp:165] Iteration 45500, lr = 0.01
I0818 18:32:27.388028  2034 solver.cpp:357] Iteration 45600 (1.79479 iter/s, 55.717s/100 iters), loss = 0.0815675
I0818 18:32:27.388200  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0815676 (* 1 = 0.0815676 loss)
I0818 18:32:27.388211  2034 sgd_solver.cpp:165] Iteration 45600, lr = 0.01
I0818 18:33:19.617022  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:33:19.955806  2034 solver.cpp:357] Iteration 45700 (1.90228 iter/s, 52.5685s/100 iters), loss = 0.0811111
I0818 18:33:19.955885  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0811112 (* 1 = 0.0811112 loss)
I0818 18:33:19.955898  2034 sgd_solver.cpp:165] Iteration 45700, lr = 0.01
I0818 18:34:14.250422  2034 solver.cpp:357] Iteration 45800 (1.84178 iter/s, 54.2953s/100 iters), loss = 0.0980643
I0818 18:34:14.250535  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0980644 (* 1 = 0.0980644 loss)
I0818 18:34:14.250545  2034 sgd_solver.cpp:165] Iteration 45800, lr = 0.01
I0818 18:35:10.372599  2034 solver.cpp:357] Iteration 45900 (1.78181 iter/s, 56.1228s/100 iters), loss = 0.0646901
I0818 18:35:10.372767  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0646902 (* 1 = 0.0646902 loss)
I0818 18:35:10.372781  2034 sgd_solver.cpp:165] Iteration 45900, lr = 0.01
I0818 18:36:05.520929  2034 solver.cpp:514] Iteration 46000, Testing net (#0)
I0818 18:36:46.304240  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:36:46.410565  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.858301
I0818 18:36:46.410648  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.500119 (* 1 = 0.500119 loss)
I0818 18:36:46.792227  2034 solver.cpp:357] Iteration 46000 (1.03713 iter/s, 96.42s/100 iters), loss = 0.113518
I0818 18:36:46.792296  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.113518 (* 1 = 0.113518 loss)
I0818 18:36:46.792309  2034 sgd_solver.cpp:165] Iteration 46000, lr = 0.01
I0818 18:37:35.287441  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:37:41.310887  2034 solver.cpp:357] Iteration 46100 (1.8343 iter/s, 54.5168s/100 iters), loss = 0.136077
I0818 18:37:41.310951  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.136077 (* 1 = 0.136077 loss)
I0818 18:37:41.310963  2034 sgd_solver.cpp:165] Iteration 46100, lr = 0.01
I0818 18:38:37.428130  2034 solver.cpp:357] Iteration 46200 (1.78198 iter/s, 56.1175s/100 iters), loss = 0.102462
I0818 18:38:37.430932  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.102462 (* 1 = 0.102462 loss)
I0818 18:38:37.431037  2034 sgd_solver.cpp:165] Iteration 46200, lr = 0.01
I0818 18:39:34.154161  2034 solver.cpp:357] Iteration 46300 (1.76292 iter/s, 56.7242s/100 iters), loss = 0.131693
I0818 18:39:34.154435  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.131694 (* 1 = 0.131694 loss)
I0818 18:39:34.154484  2034 sgd_solver.cpp:165] Iteration 46300, lr = 0.01
I0818 18:40:30.565927  2034 solver.cpp:357] Iteration 46400 (1.77274 iter/s, 56.4098s/100 iters), loss = 0.107806
I0818 18:40:30.566087  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.107806 (* 1 = 0.107806 loss)
I0818 18:40:30.566098  2034 sgd_solver.cpp:165] Iteration 46400, lr = 0.01
I0818 18:41:15.524792  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:41:26.152858  2034 solver.cpp:514] Iteration 46500, Testing net (#0)
I0818 18:42:11.058073  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:42:11.285418  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.860701
I0818 18:42:11.285583  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.472188 (* 1 = 0.472188 loss)
I0818 18:42:11.771735  2034 solver.cpp:357] Iteration 46500 (0.988068 iter/s, 101.208s/100 iters), loss = 0.046437
I0818 18:42:11.771909  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0464372 (* 1 = 0.0464372 loss)
I0818 18:42:11.771955  2034 sgd_solver.cpp:165] Iteration 46500, lr = 0.01
I0818 18:43:04.655447  2034 solver.cpp:357] Iteration 46600 (1.89095 iter/s, 52.8836s/100 iters), loss = 0.0758794
I0818 18:43:04.655858  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0758795 (* 1 = 0.0758795 loss)
I0818 18:43:04.655871  2034 sgd_solver.cpp:165] Iteration 46600, lr = 0.01
I0818 18:43:54.647320  2034 solver.cpp:357] Iteration 46700 (2.00027 iter/s, 49.9933s/100 iters), loss = 0.0800276
I0818 18:43:54.647511  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0800277 (* 1 = 0.0800277 loss)
I0818 18:43:54.647524  2034 sgd_solver.cpp:165] Iteration 46700, lr = 0.01
I0818 18:44:50.643966  2034 solver.cpp:357] Iteration 46800 (1.78589 iter/s, 55.9944s/100 iters), loss = 0.0711478
I0818 18:44:50.644098  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0711479 (* 1 = 0.0711479 loss)
I0818 18:44:50.644109  2034 sgd_solver.cpp:165] Iteration 46800, lr = 0.01
I0818 18:45:29.615445  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:45:46.323220  2034 solver.cpp:357] Iteration 46900 (1.79601 iter/s, 55.6791s/100 iters), loss = 0.109798
I0818 18:45:46.323297  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.109798 (* 1 = 0.109798 loss)
I0818 18:45:46.323308  2034 sgd_solver.cpp:165] Iteration 46900, lr = 0.01
I0818 18:46:41.470453  2034 solver.cpp:514] Iteration 47000, Testing net (#0)
I0818 18:47:26.536401  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:47:26.743796  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.862901
I0818 18:47:26.743845  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.490743 (* 1 = 0.490743 loss)
I0818 18:47:27.045752  2034 solver.cpp:357] Iteration 47000 (0.992833 iter/s, 100.722s/100 iters), loss = 0.0785754
I0818 18:47:27.045819  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0785755 (* 1 = 0.0785755 loss)
I0818 18:47:27.045831  2034 sgd_solver.cpp:165] Iteration 47000, lr = 0.01
I0818 18:48:22.780261  2034 solver.cpp:357] Iteration 47100 (1.79423 iter/s, 55.7343s/100 iters), loss = 0.14788
I0818 18:48:22.780441  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14788 (* 1 = 0.14788 loss)
I0818 18:48:22.780468  2034 sgd_solver.cpp:165] Iteration 47100, lr = 0.01
I0818 18:49:18.556813  2034 solver.cpp:357] Iteration 47200 (1.79281 iter/s, 55.7782s/100 iters), loss = 0.0660704
I0818 18:49:18.556990  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0660705 (* 1 = 0.0660705 loss)
I0818 18:49:18.557003  2034 sgd_solver.cpp:165] Iteration 47200, lr = 0.01
I0818 18:49:52.653769  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:50:13.177124  2034 solver.cpp:357] Iteration 47300 (1.8309 iter/s, 54.6178s/100 iters), loss = 0.0790029
I0818 18:50:13.177203  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.079003 (* 1 = 0.079003 loss)
I0818 18:50:13.177215  2034 sgd_solver.cpp:165] Iteration 47300, lr = 0.01
I0818 18:51:04.362891  2034 solver.cpp:357] Iteration 47400 (1.95377 iter/s, 51.1832s/100 iters), loss = 0.0952159
I0818 18:51:04.363034  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.095216 (* 1 = 0.095216 loss)
I0818 18:51:04.363046  2034 sgd_solver.cpp:165] Iteration 47400, lr = 0.01
I0818 18:51:59.596909  2034 solver.cpp:514] Iteration 47500, Testing net (#0)
I0818 18:52:40.748150  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:52:40.930608  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.852902
I0818 18:52:40.930691  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.537984 (* 1 = 0.537984 loss)
I0818 18:52:41.311033  2034 solver.cpp:357] Iteration 47500 (1.03149 iter/s, 96.9471s/100 iters), loss = 0.0736319
I0818 18:52:41.311100  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.073632 (* 1 = 0.073632 loss)
I0818 18:52:41.311110  2034 sgd_solver.cpp:165] Iteration 47500, lr = 0.01
I0818 18:53:36.064046  2034 solver.cpp:357] Iteration 47600 (1.82633 iter/s, 54.7547s/100 iters), loss = 0.145823
I0818 18:53:36.064235  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.145823 (* 1 = 0.145823 loss)
I0818 18:53:36.064246  2034 sgd_solver.cpp:165] Iteration 47600, lr = 0.01
I0818 18:54:05.465278  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:54:31.982777  2034 solver.cpp:357] Iteration 47700 (1.78843 iter/s, 55.9149s/100 iters), loss = 0.066864
I0818 18:54:31.983202  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0668641 (* 1 = 0.0668641 loss)
I0818 18:54:31.983291  2034 sgd_solver.cpp:165] Iteration 47700, lr = 0.01
I0818 18:55:28.958889  2034 solver.cpp:357] Iteration 47800 (1.75509 iter/s, 56.977s/100 iters), loss = 0.0636058
I0818 18:55:28.962787  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0636059 (* 1 = 0.0636059 loss)
I0818 18:55:28.962822  2034 sgd_solver.cpp:165] Iteration 47800, lr = 0.01
I0818 18:56:25.163698  2034 solver.cpp:357] Iteration 47900 (1.77929 iter/s, 56.2023s/100 iters), loss = 0.12798
I0818 18:56:25.166815  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.12798 (* 1 = 0.12798 loss)
I0818 18:56:25.166874  2034 sgd_solver.cpp:165] Iteration 47900, lr = 0.01
I0818 18:57:15.559475  2034 solver.cpp:514] Iteration 48000, Testing net (#0)
I0818 18:57:58.182035  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:57:58.370281  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.880602
I0818 18:57:58.374760  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.428342 (* 1 = 0.428342 loss)
I0818 18:57:58.922724  2034 solver.cpp:357] Iteration 48000 (1.06658 iter/s, 93.7573s/100 iters), loss = 0.14163
I0818 18:57:58.922938  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.14163 (* 1 = 0.14163 loss)
I0818 18:57:58.922981  2034 sgd_solver.cpp:64] MultiStep Status: Iteration 48000, step = 2
I0818 18:57:58.923020  2034 sgd_solver.cpp:165] Iteration 48000, lr = 0.001
I0818 18:58:22.411602  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 18:58:54.627732  2034 solver.cpp:357] Iteration 48100 (1.79514 iter/s, 55.7061s/100 iters), loss = 0.122518
I0818 18:58:54.630795  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.122518 (* 1 = 0.122518 loss)
I0818 18:58:54.630830  2034 sgd_solver.cpp:165] Iteration 48100, lr = 0.001
I0818 18:59:50.695022  2034 solver.cpp:357] Iteration 48200 (1.78375 iter/s, 56.0616s/100 iters), loss = 0.0599948
I0818 18:59:50.699079  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0599949 (* 1 = 0.0599949 loss)
I0818 18:59:50.699270  2034 sgd_solver.cpp:165] Iteration 48200, lr = 0.001
I0818 19:00:47.186828  2034 solver.cpp:357] Iteration 48300 (1.77037 iter/s, 56.4854s/100 iters), loss = 0.0823831
I0818 19:00:47.190805  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0823832 (* 1 = 0.0823832 loss)
I0818 19:00:47.190845  2034 sgd_solver.cpp:165] Iteration 48300, lr = 0.001
I0818 19:01:42.970147  2034 solver.cpp:357] Iteration 48400 (1.79284 iter/s, 55.7773s/100 iters), loss = 0.0361273
I0818 19:01:42.970392  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0361274 (* 1 = 0.0361274 loss)
I0818 19:01:42.970439  2034 sgd_solver.cpp:165] Iteration 48400, lr = 0.001
I0818 19:02:01.062292  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:02:35.654515  2034 solver.cpp:514] Iteration 48500, Testing net (#0)
I0818 19:03:19.865240  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:03:19.971664  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.913102
I0818 19:03:19.971823  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.295956 (* 1 = 0.295956 loss)
I0818 19:03:20.478440  2034 solver.cpp:357] Iteration 48500 (1.02562 iter/s, 97.5017s/100 iters), loss = 0.0477589
I0818 19:03:20.478643  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.047759 (* 1 = 0.047759 loss)
I0818 19:03:20.478693  2034 sgd_solver.cpp:165] Iteration 48500, lr = 0.001
I0818 19:04:10.149397  2034 solver.cpp:357] Iteration 48600 (2.01346 iter/s, 49.6658s/100 iters), loss = 0.0522863
I0818 19:04:10.154834  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0522864 (* 1 = 0.0522864 loss)
I0818 19:04:10.154878  2034 sgd_solver.cpp:165] Iteration 48600, lr = 0.001
I0818 19:05:04.141319  2034 solver.cpp:357] Iteration 48700 (1.85245 iter/s, 53.9825s/100 iters), loss = 0.0277017
I0818 19:05:04.146886  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0277018 (* 1 = 0.0277018 loss)
I0818 19:05:04.146953  2034 sgd_solver.cpp:165] Iteration 48700, lr = 0.001
I0818 19:06:00.386735  2034 solver.cpp:357] Iteration 48800 (1.77815 iter/s, 56.2383s/100 iters), loss = 0.0378046
I0818 19:06:00.386967  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0378047 (* 1 = 0.0378047 loss)
I0818 19:06:00.386996  2034 sgd_solver.cpp:165] Iteration 48800, lr = 0.001
I0818 19:06:14.351157  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:06:56.823256  2034 solver.cpp:357] Iteration 48900 (1.77196 iter/s, 56.4347s/100 iters), loss = 0.0479113
I0818 19:06:56.823663  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0479114 (* 1 = 0.0479114 loss)
I0818 19:06:56.823758  2034 sgd_solver.cpp:165] Iteration 48900, lr = 0.001
I0818 19:07:53.279125  2034 solver.cpp:514] Iteration 49000, Testing net (#0)
I0818 19:08:38.926846  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:08:39.166770  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923402
I0818 19:08:39.166960  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.262016 (* 1 = 0.262016 loss)
I0818 19:08:39.486743  2034 solver.cpp:357] Iteration 49000 (0.974103 iter/s, 102.659s/100 iters), loss = 0.0325025
I0818 19:08:39.487013  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0325027 (* 1 = 0.0325027 loss)
I0818 19:08:39.487063  2034 sgd_solver.cpp:165] Iteration 49000, lr = 0.001
I0818 19:09:36.136165  2034 solver.cpp:357] Iteration 49100 (1.7653 iter/s, 56.6478s/100 iters), loss = 0.0354289
I0818 19:09:36.139034  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.035429 (* 1 = 0.035429 loss)
I0818 19:09:36.139222  2034 sgd_solver.cpp:165] Iteration 49100, lr = 0.001
I0818 19:10:32.858741  2034 solver.cpp:357] Iteration 49200 (1.7631 iter/s, 56.7184s/100 iters), loss = 0.0220204
I0818 19:10:32.858901  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0220205 (* 1 = 0.0220205 loss)
I0818 19:10:32.858925  2034 sgd_solver.cpp:165] Iteration 49200, lr = 0.001
I0818 19:10:40.536955  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:11:15.740355  2034 solver.cpp:357] Iteration 49300 (2.33201 iter/s, 42.8815s/100 iters), loss = 0.0207387
I0818 19:11:15.747006  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0207388 (* 1 = 0.0207388 loss)
I0818 19:11:15.747118  2034 sgd_solver.cpp:165] Iteration 49300, lr = 0.001
I0818 19:12:02.050731  2034 solver.cpp:357] Iteration 49400 (2.15968 iter/s, 46.3033s/100 iters), loss = 0.0138499
I0818 19:12:02.050949  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138501 (* 1 = 0.0138501 loss)
I0818 19:12:02.050978  2034 sgd_solver.cpp:165] Iteration 49400, lr = 0.001
I0818 19:12:57.653659  2034 solver.cpp:514] Iteration 49500, Testing net (#0)
I0818 19:13:43.251108  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:13:43.373666  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922202
I0818 19:13:43.373848  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.273396 (* 1 = 0.273396 loss)
I0818 19:13:43.821681  2034 solver.cpp:357] Iteration 49500 (0.982624 iter/s, 101.768s/100 iters), loss = 0.0526664
I0818 19:13:43.821904  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0526666 (* 1 = 0.0526666 loss)
I0818 19:13:43.821954  2034 sgd_solver.cpp:165] Iteration 49500, lr = 0.001
I0818 19:14:39.968677  2034 solver.cpp:357] Iteration 49600 (1.78116 iter/s, 56.1433s/100 iters), loss = 0.0336527
I0818 19:14:39.968945  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0336529 (* 1 = 0.0336529 loss)
I0818 19:14:39.968974  2034 sgd_solver.cpp:165] Iteration 49600, lr = 0.001
I0818 19:14:42.971277  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:15:36.980998  2034 solver.cpp:357] Iteration 49700 (1.75405 iter/s, 57.0108s/100 iters), loss = 0.0271106
I0818 19:15:36.981397  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0271108 (* 1 = 0.0271108 loss)
I0818 19:15:36.981493  2034 sgd_solver.cpp:165] Iteration 49700, lr = 0.001
I0818 19:16:33.191979  2034 solver.cpp:357] Iteration 49800 (1.77912 iter/s, 56.2074s/100 iters), loss = 0.0559382
I0818 19:16:33.192287  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0559383 (* 1 = 0.0559383 loss)
I0818 19:16:33.192337  2034 sgd_solver.cpp:165] Iteration 49800, lr = 0.001
I0818 19:17:28.691100  2034 solver.cpp:357] Iteration 49900 (1.80188 iter/s, 55.4975s/100 iters), loss = 0.019066
I0818 19:17:28.691296  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0190661 (* 1 = 0.0190661 loss)
I0818 19:17:28.691323  2034 sgd_solver.cpp:165] Iteration 49900, lr = 0.001
I0818 19:18:17.293756  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:18:19.321239  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_50000.caffemodel
I0818 19:18:19.346530  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_50000.solverstate
I0818 19:18:19.360623  2034 solver.cpp:514] Iteration 50000, Testing net (#0)
I0818 19:19:05.429945  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:19:05.599467  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926602
I0818 19:19:05.599655  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.260163 (* 1 = 0.260163 loss)
I0818 19:19:06.142719  2034 solver.cpp:357] Iteration 50000 (1.02617 iter/s, 97.4495s/100 iters), loss = 0.0295337
I0818 19:19:06.142935  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0295338 (* 1 = 0.0295338 loss)
I0818 19:19:06.142982  2034 sgd_solver.cpp:165] Iteration 50000, lr = 0.001
I0818 19:20:03.033219  2034 solver.cpp:357] Iteration 50100 (1.75776 iter/s, 56.8905s/100 iters), loss = 0.034648
I0818 19:20:03.033428  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0346481 (* 1 = 0.0346481 loss)
I0818 19:20:03.033457  2034 sgd_solver.cpp:165] Iteration 50100, lr = 0.001
I0818 19:20:57.579064  2034 solver.cpp:357] Iteration 50200 (1.83336 iter/s, 54.5445s/100 iters), loss = 0.0339513
I0818 19:20:57.579303  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0339515 (* 1 = 0.0339515 loss)
I0818 19:20:57.579329  2034 sgd_solver.cpp:165] Iteration 50200, lr = 0.001
I0818 19:21:52.216940  2034 solver.cpp:357] Iteration 50300 (1.83027 iter/s, 54.6366s/100 iters), loss = 0.0461693
I0818 19:21:52.217182  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0461694 (* 1 = 0.0461694 loss)
I0818 19:21:52.217212  2034 sgd_solver.cpp:165] Iteration 50300, lr = 0.001
I0818 19:22:40.737893  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:22:48.570719  2034 solver.cpp:357] Iteration 50400 (1.77465 iter/s, 56.3491s/100 iters), loss = 0.0659017
I0818 19:22:48.570911  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0659018 (* 1 = 0.0659018 loss)
I0818 19:22:48.570947  2034 sgd_solver.cpp:165] Iteration 50400, lr = 0.001
I0818 19:23:44.518314  2034 solver.cpp:514] Iteration 50500, Testing net (#0)
I0818 19:24:27.448019  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:24:27.571061  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925402
I0818 19:24:27.571422  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.25992 (* 1 = 0.25992 loss)
I0818 19:24:27.954423  2034 solver.cpp:357] Iteration 50500 (1.00619 iter/s, 99.3848s/100 iters), loss = 0.0268117
I0818 19:24:27.958910  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0268118 (* 1 = 0.0268118 loss)
I0818 19:24:27.959110  2034 sgd_solver.cpp:165] Iteration 50500, lr = 0.001
I0818 19:25:16.139933  2034 solver.cpp:357] Iteration 50600 (2.07556 iter/s, 48.1798s/100 iters), loss = 0.0278963
I0818 19:25:16.140328  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0278964 (* 1 = 0.0278964 loss)
I0818 19:25:16.140379  2034 sgd_solver.cpp:165] Iteration 50600, lr = 0.001
I0818 19:26:12.358211  2034 solver.cpp:357] Iteration 50700 (1.77888 iter/s, 56.215s/100 iters), loss = 0.0186983
I0818 19:26:12.358630  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186985 (* 1 = 0.0186985 loss)
I0818 19:26:12.358894  2034 sgd_solver.cpp:165] Iteration 50700, lr = 0.001
I0818 19:26:56.634369  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:27:09.526334  2034 solver.cpp:357] Iteration 50800 (1.74932 iter/s, 57.1649s/100 iters), loss = 0.0468672
I0818 19:27:09.526485  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0468673 (* 1 = 0.0468673 loss)
I0818 19:27:09.526515  2034 sgd_solver.cpp:165] Iteration 50800, lr = 0.001
I0818 19:28:06.397529  2034 solver.cpp:357] Iteration 50900 (1.75846 iter/s, 56.8681s/100 iters), loss = 0.0599327
I0818 19:28:06.397737  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0599328 (* 1 = 0.0599328 loss)
I0818 19:28:06.397781  2034 sgd_solver.cpp:165] Iteration 50900, lr = 0.001
I0818 19:29:01.421370  2034 solver.cpp:514] Iteration 51000, Testing net (#0)
I0818 19:29:44.630383  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:29:44.833930  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923602
I0818 19:29:44.834173  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.26863 (* 1 = 0.26863 loss)
I0818 19:29:45.231822  2034 solver.cpp:357] Iteration 51000 (1.01178 iter/s, 98.8361s/100 iters), loss = 0.0366185
I0818 19:29:45.232214  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0366186 (* 1 = 0.0366186 loss)
I0818 19:29:45.232386  2034 sgd_solver.cpp:165] Iteration 51000, lr = 0.001
I0818 19:30:39.742209  2034 solver.cpp:357] Iteration 51100 (1.83449 iter/s, 54.5111s/100 iters), loss = 0.0181705
I0818 19:30:39.746879  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0181706 (* 1 = 0.0181706 loss)
I0818 19:30:39.746927  2034 sgd_solver.cpp:165] Iteration 51100, lr = 0.001
I0818 19:31:11.897614  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:31:27.316363  2034 solver.cpp:357] Iteration 51200 (2.10225 iter/s, 47.5681s/100 iters), loss = 0.0265313
I0818 19:31:27.316501  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0265314 (* 1 = 0.0265314 loss)
I0818 19:31:27.316526  2034 sgd_solver.cpp:165] Iteration 51200, lr = 0.001
I0818 19:32:21.913662  2034 solver.cpp:357] Iteration 51300 (1.83156 iter/s, 54.5982s/100 iters), loss = 0.0277427
I0818 19:32:21.919109  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0277428 (* 1 = 0.0277428 loss)
I0818 19:32:21.919308  2034 sgd_solver.cpp:165] Iteration 51300, lr = 0.001
I0818 19:33:18.322508  2034 solver.cpp:357] Iteration 51400 (1.77295 iter/s, 56.4032s/100 iters), loss = 0.0223974
I0818 19:33:18.322656  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0223975 (* 1 = 0.0223975 loss)
I0818 19:33:18.322681  2034 sgd_solver.cpp:165] Iteration 51400, lr = 0.001
I0818 19:34:13.770391  2034 solver.cpp:514] Iteration 51500, Testing net (#0)
I0818 19:34:59.006492  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:34:59.241631  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.923802
I0818 19:34:59.241685  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.279003 (* 1 = 0.279003 loss)
I0818 19:34:59.541018  2034 solver.cpp:357] Iteration 51500 (0.988008 iter/s, 101.214s/100 iters), loss = 0.00595751
I0818 19:34:59.541090  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00595763 (* 1 = 0.00595763 loss)
I0818 19:34:59.541100  2034 sgd_solver.cpp:165] Iteration 51500, lr = 0.001
I0818 19:35:32.011633  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:35:55.234441  2034 solver.cpp:357] Iteration 51600 (1.79562 iter/s, 55.6912s/100 iters), loss = 0.0116291
I0818 19:35:55.234513  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0116292 (* 1 = 0.0116292 loss)
I0818 19:35:55.234524  2034 sgd_solver.cpp:165] Iteration 51600, lr = 0.001
I0818 19:36:50.818459  2034 solver.cpp:357] Iteration 51700 (1.79915 iter/s, 55.5818s/100 iters), loss = 0.038297
I0818 19:36:50.818588  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0382972 (* 1 = 0.0382972 loss)
I0818 19:36:50.818601  2034 sgd_solver.cpp:165] Iteration 51700, lr = 0.001
I0818 19:37:46.829965  2034 solver.cpp:357] Iteration 51800 (1.78548 iter/s, 56.0073s/100 iters), loss = 0.0280311
I0818 19:37:46.830181  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0280312 (* 1 = 0.0280312 loss)
I0818 19:37:46.830210  2034 sgd_solver.cpp:165] Iteration 51800, lr = 0.001
I0818 19:38:38.881038  2034 solver.cpp:357] Iteration 51900 (1.92134 iter/s, 52.047s/100 iters), loss = 0.0185584
I0818 19:38:38.881184  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0185585 (* 1 = 0.0185585 loss)
I0818 19:38:38.881196  2034 sgd_solver.cpp:165] Iteration 51900, lr = 0.001
I0818 19:39:04.169404  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:39:32.060431  2034 solver.cpp:514] Iteration 52000, Testing net (#0)
I0818 19:40:14.034898  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:40:14.178683  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926302
I0818 19:40:14.178864  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.268382 (* 1 = 0.268382 loss)
I0818 19:40:14.642936  2034 solver.cpp:357] Iteration 52000 (1.04425 iter/s, 95.7623s/100 iters), loss = 0.0459338
I0818 19:40:14.642999  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0459339 (* 1 = 0.0459339 loss)
I0818 19:40:14.643012  2034 sgd_solver.cpp:165] Iteration 52000, lr = 0.001
I0818 19:41:08.748387  2034 solver.cpp:357] Iteration 52100 (1.8483 iter/s, 54.1037s/100 iters), loss = 0.0202371
I0818 19:41:08.748512  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0202372 (* 1 = 0.0202372 loss)
I0818 19:41:08.748524  2034 sgd_solver.cpp:165] Iteration 52100, lr = 0.001
I0818 19:42:04.547441  2034 solver.cpp:357] Iteration 52200 (1.79213 iter/s, 55.7994s/100 iters), loss = 0.0256529
I0818 19:42:04.547605  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.025653 (* 1 = 0.025653 loss)
I0818 19:42:04.547617  2034 sgd_solver.cpp:165] Iteration 52200, lr = 0.001
I0818 19:43:00.354132  2034 solver.cpp:357] Iteration 52300 (1.79195 iter/s, 55.8051s/100 iters), loss = 0.0301142
I0818 19:43:00.354303  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0301144 (* 1 = 0.0301144 loss)
I0818 19:43:00.354316  2034 sgd_solver.cpp:165] Iteration 52300, lr = 0.001
I0818 19:43:22.147174  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:43:55.902717  2034 solver.cpp:357] Iteration 52400 (1.80024 iter/s, 55.5481s/100 iters), loss = 0.0300035
I0818 19:43:55.902922  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0300036 (* 1 = 0.0300036 loss)
I0818 19:43:55.902937  2034 sgd_solver.cpp:165] Iteration 52400, lr = 0.001
I0818 19:44:51.269284  2034 solver.cpp:514] Iteration 52500, Testing net (#0)
I0818 19:45:32.584765  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:45:32.768373  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926502
I0818 19:45:32.768429  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.265726 (* 1 = 0.265726 loss)
I0818 19:45:33.110872  2034 solver.cpp:357] Iteration 52500 (1.02873 iter/s, 97.2071s/100 iters), loss = 0.0475207
I0818 19:45:33.110932  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0475208 (* 1 = 0.0475208 loss)
I0818 19:45:33.110944  2034 sgd_solver.cpp:165] Iteration 52500, lr = 0.001
I0818 19:46:26.950315  2034 solver.cpp:357] Iteration 52600 (1.85743 iter/s, 53.8379s/100 iters), loss = 0.0187137
I0818 19:46:26.950716  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0187139 (* 1 = 0.0187139 loss)
I0818 19:46:26.950780  2034 sgd_solver.cpp:165] Iteration 52600, lr = 0.001
I0818 19:47:22.918040  2034 solver.cpp:357] Iteration 52700 (1.78686 iter/s, 55.9642s/100 iters), loss = 0.0174403
I0818 19:47:22.918169  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0174404 (* 1 = 0.0174404 loss)
I0818 19:47:22.918182  2034 sgd_solver.cpp:165] Iteration 52700, lr = 0.001
I0818 19:47:39.799512  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:48:18.892078  2034 solver.cpp:357] Iteration 52800 (1.7866 iter/s, 55.9723s/100 iters), loss = 0.0127015
I0818 19:48:18.892225  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0127016 (* 1 = 0.0127016 loss)
I0818 19:48:18.892238  2034 sgd_solver.cpp:165] Iteration 52800, lr = 0.001
I0818 19:49:14.295382  2034 solver.cpp:357] Iteration 52900 (1.80506 iter/s, 55.3998s/100 iters), loss = 0.0250199
I0818 19:49:14.295549  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.02502 (* 1 = 0.02502 loss)
I0818 19:49:14.295562  2034 sgd_solver.cpp:165] Iteration 52900, lr = 0.001
I0818 19:50:06.584277  2034 solver.cpp:514] Iteration 53000, Testing net (#0)
I0818 19:50:49.703014  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:50:49.936796  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.922502
I0818 19:50:49.936846  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.281965 (* 1 = 0.281965 loss)
I0818 19:50:50.226158  2034 solver.cpp:357] Iteration 53000 (1.04245 iter/s, 95.9279s/100 iters), loss = 0.0138407
I0818 19:50:50.226224  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138408 (* 1 = 0.0138408 loss)
I0818 19:50:50.226238  2034 sgd_solver.cpp:165] Iteration 53000, lr = 0.001
I0818 19:51:46.042119  2034 solver.cpp:357] Iteration 53100 (1.79164 iter/s, 55.8146s/100 iters), loss = 0.0161462
I0818 19:51:46.042273  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0161463 (* 1 = 0.0161463 loss)
I0818 19:51:46.042284  2034 sgd_solver.cpp:165] Iteration 53100, lr = 0.001
I0818 19:51:57.336848  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:52:37.892725  2034 solver.cpp:357] Iteration 53200 (1.92859 iter/s, 51.8512s/100 iters), loss = 0.021019
I0818 19:52:37.892942  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0210191 (* 1 = 0.0210191 loss)
I0818 19:52:37.892967  2034 sgd_solver.cpp:165] Iteration 53200, lr = 0.001
I0818 19:53:31.792918  2034 solver.cpp:357] Iteration 53300 (1.85533 iter/s, 53.8989s/100 iters), loss = 0.0315178
I0818 19:53:31.793057  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0315179 (* 1 = 0.0315179 loss)
I0818 19:53:31.793068  2034 sgd_solver.cpp:165] Iteration 53300, lr = 0.001
I0818 19:54:27.503587  2034 solver.cpp:357] Iteration 53400 (1.79503 iter/s, 55.7094s/100 iters), loss = 0.0169417
I0818 19:54:27.503754  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0169418 (* 1 = 0.0169418 loss)
I0818 19:54:27.503765  2034 sgd_solver.cpp:165] Iteration 53400, lr = 0.001
I0818 19:55:22.854813  2034 solver.cpp:514] Iteration 53500, Testing net (#0)
I0818 19:56:08.221788  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:56:08.322346  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927102
I0818 19:56:08.322407  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.272296 (* 1 = 0.272296 loss)
I0818 19:56:08.858316  2034 solver.cpp:357] Iteration 53500 (0.986639 iter/s, 101.354s/100 iters), loss = 0.0186696
I0818 19:56:08.858393  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186697 (* 1 = 0.0186697 loss)
I0818 19:56:08.858407  2034 sgd_solver.cpp:165] Iteration 53500, lr = 0.001
I0818 19:56:15.097751  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 19:57:04.764135  2034 solver.cpp:357] Iteration 53600 (1.78883 iter/s, 55.9025s/100 iters), loss = 0.0188069
I0818 19:57:04.764371  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.018807 (* 1 = 0.018807 loss)
I0818 19:57:04.764385  2034 sgd_solver.cpp:165] Iteration 53600, lr = 0.001
I0818 19:58:00.474917  2034 solver.cpp:357] Iteration 53700 (1.79509 iter/s, 55.7074s/100 iters), loss = 0.00812705
I0818 19:58:00.475102  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00812713 (* 1 = 0.00812713 loss)
I0818 19:58:00.475116  2034 sgd_solver.cpp:165] Iteration 53700, lr = 0.001
I0818 19:58:56.314469  2034 solver.cpp:357] Iteration 53800 (1.79095 iter/s, 55.8363s/100 iters), loss = 0.0100814
I0818 19:58:56.314602  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0100815 (* 1 = 0.0100815 loss)
I0818 19:58:56.314615  2034 sgd_solver.cpp:165] Iteration 53800, lr = 0.001
I0818 19:59:44.987259  2034 solver.cpp:357] Iteration 53900 (2.05459 iter/s, 48.6714s/100 iters), loss = 0.0109436
I0818 19:59:44.987422  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0109437 (* 1 = 0.0109437 loss)
I0818 19:59:44.987435  2034 sgd_solver.cpp:165] Iteration 53900, lr = 0.001
I0818 19:59:46.003360  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:00:37.034994  2034 solver.cpp:514] Iteration 54000, Testing net (#0)
I0818 20:01:22.122375  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:01:22.343708  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.928602
I0818 20:01:22.343762  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.269901 (* 1 = 0.269901 loss)
I0818 20:01:22.647547  2034 solver.cpp:357] Iteration 54000 (1.02398 iter/s, 97.6578s/100 iters), loss = 0.010589
I0818 20:01:22.647611  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0105891 (* 1 = 0.0105891 loss)
I0818 20:01:22.647624  2034 sgd_solver.cpp:165] Iteration 54000, lr = 0.001
I0818 20:02:18.480880  2034 solver.cpp:357] Iteration 54100 (1.79108 iter/s, 55.8321s/100 iters), loss = 0.00714839
I0818 20:02:18.481082  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00714848 (* 1 = 0.00714848 loss)
I0818 20:02:18.481112  2034 sgd_solver.cpp:165] Iteration 54100, lr = 0.001
I0818 20:03:14.286909  2034 solver.cpp:357] Iteration 54200 (1.79196 iter/s, 55.8048s/100 iters), loss = 0.010459
I0818 20:03:14.287050  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0104591 (* 1 = 0.0104591 loss)
I0818 20:03:14.287062  2034 sgd_solver.cpp:165] Iteration 54200, lr = 0.001
I0818 20:04:05.799955  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:04:10.122604  2034 solver.cpp:357] Iteration 54300 (1.79108 iter/s, 55.8324s/100 iters), loss = 0.0468156
I0818 20:04:10.122686  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0468157 (* 1 = 0.0468157 loss)
I0818 20:04:10.122699  2034 sgd_solver.cpp:165] Iteration 54300, lr = 0.001
I0818 20:05:05.080539  2034 solver.cpp:357] Iteration 54400 (1.81961 iter/s, 54.9567s/100 iters), loss = 0.0288556
I0818 20:05:05.080677  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0288557 (* 1 = 0.0288557 loss)
I0818 20:05:05.080690  2034 sgd_solver.cpp:165] Iteration 54400, lr = 0.001
I0818 20:06:00.392040  2034 solver.cpp:514] Iteration 54500, Testing net (#0)
I0818 20:06:41.494637  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:06:41.572859  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927902
I0818 20:06:41.572908  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.27716 (* 1 = 0.27716 loss)
I0818 20:06:41.985728  2034 solver.cpp:357] Iteration 54500 (1.03196 iter/s, 96.9027s/100 iters), loss = 0.0179
I0818 20:06:41.985798  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0179001 (* 1 = 0.0179001 loss)
I0818 20:06:41.985810  2034 sgd_solver.cpp:165] Iteration 54500, lr = 0.001
I0818 20:07:36.335316  2034 solver.cpp:357] Iteration 54600 (1.8401 iter/s, 54.345s/100 iters), loss = 0.0111054
I0818 20:07:36.335535  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0111055 (* 1 = 0.0111055 loss)
I0818 20:07:36.335547  2034 sgd_solver.cpp:165] Iteration 54600, lr = 0.001
I0818 20:08:22.806861  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:08:32.181002  2034 solver.cpp:357] Iteration 54700 (1.79088 iter/s, 55.8384s/100 iters), loss = 0.0149484
I0818 20:08:32.181074  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0149485 (* 1 = 0.0149485 loss)
I0818 20:08:32.181085  2034 sgd_solver.cpp:165] Iteration 54700, lr = 0.001
I0818 20:09:24.977795  2034 solver.cpp:357] Iteration 54800 (1.89422 iter/s, 52.7921s/100 iters), loss = 0.00829251
I0818 20:09:24.977921  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00829262 (* 1 = 0.00829262 loss)
I0818 20:09:24.977933  2034 sgd_solver.cpp:165] Iteration 54800, lr = 0.001
I0818 20:10:19.069716  2034 solver.cpp:357] Iteration 54900 (1.84879 iter/s, 54.0894s/100 iters), loss = 0.0244957
I0818 20:10:19.069936  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0244958 (* 1 = 0.0244958 loss)
I0818 20:10:19.069964  2034 sgd_solver.cpp:165] Iteration 54900, lr = 0.001
I0818 20:11:14.404363  2034 solver.cpp:514] Iteration 55000, Testing net (#0)
I0818 20:11:59.615074  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:11:59.811924  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926802
I0818 20:11:59.811985  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.281047 (* 1 = 0.281047 loss)
I0818 20:12:00.260015  2034 solver.cpp:357] Iteration 55000 (0.988294 iter/s, 101.184s/100 iters), loss = 0.00435763
I0818 20:12:00.260074  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00435774 (* 1 = 0.00435774 loss)
I0818 20:12:00.260085  2034 sgd_solver.cpp:165] Iteration 55000, lr = 0.001
I0818 20:12:41.633546  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:12:56.067245  2034 solver.cpp:357] Iteration 55100 (1.79194 iter/s, 55.8055s/100 iters), loss = 0.0213022
I0818 20:12:56.067313  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0213023 (* 1 = 0.0213023 loss)
I0818 20:12:56.067325  2034 sgd_solver.cpp:165] Iteration 55100, lr = 0.001
I0818 20:13:47.411859  2034 solver.cpp:357] Iteration 55200 (1.94776 iter/s, 51.3411s/100 iters), loss = 0.00983422
I0818 20:13:47.412166  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00983431 (* 1 = 0.00983431 loss)
I0818 20:13:47.412231  2034 sgd_solver.cpp:165] Iteration 55200, lr = 0.001
I0818 20:14:41.924557  2034 solver.cpp:357] Iteration 55300 (1.83455 iter/s, 54.5092s/100 iters), loss = 0.0194157
I0818 20:14:41.924746  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0194158 (* 1 = 0.0194158 loss)
I0818 20:14:41.924759  2034 sgd_solver.cpp:165] Iteration 55300, lr = 0.001
I0818 20:15:37.588528  2034 solver.cpp:357] Iteration 55400 (1.7966 iter/s, 55.6606s/100 iters), loss = 0.032722
I0818 20:15:37.588946  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0327221 (* 1 = 0.0327221 loss)
I0818 20:15:37.589073  2034 sgd_solver.cpp:165] Iteration 55400, lr = 0.001
I0818 20:16:12.482988  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:16:32.030236  2034 solver.cpp:514] Iteration 55500, Testing net (#0)
I0818 20:17:17.324261  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:17:17.564761  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926602
I0818 20:17:17.564821  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.282609 (* 1 = 0.282609 loss)
I0818 20:17:17.867398  2034 solver.cpp:357] Iteration 55500 (0.997277 iter/s, 100.273s/100 iters), loss = 0.0160592
I0818 20:17:17.867461  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0160593 (* 1 = 0.0160593 loss)
I0818 20:17:17.867472  2034 sgd_solver.cpp:165] Iteration 55500, lr = 0.001
I0818 20:18:13.786298  2034 solver.cpp:357] Iteration 55600 (1.78833 iter/s, 55.9181s/100 iters), loss = 0.0156866
I0818 20:18:13.786509  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0156867 (* 1 = 0.0156867 loss)
I0818 20:18:13.786521  2034 sgd_solver.cpp:165] Iteration 55600, lr = 0.001
I0818 20:19:06.747697  2034 solver.cpp:357] Iteration 55700 (1.88827 iter/s, 52.9586s/100 iters), loss = 0.0236338
I0818 20:19:06.747929  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0236339 (* 1 = 0.0236339 loss)
I0818 20:19:06.747941  2034 sgd_solver.cpp:165] Iteration 55700, lr = 0.001
I0818 20:20:00.730087  2034 solver.cpp:357] Iteration 55800 (1.85255 iter/s, 53.9797s/100 iters), loss = 0.0173471
I0818 20:20:00.730233  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0173472 (* 1 = 0.0173472 loss)
I0818 20:20:00.730245  2034 sgd_solver.cpp:165] Iteration 55800, lr = 0.001
I0818 20:20:28.844439  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:20:51.600142  2034 solver.cpp:357] Iteration 55900 (1.96589 iter/s, 50.8675s/100 iters), loss = 0.0850734
I0818 20:20:51.600272  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0850735 (* 1 = 0.0850735 loss)
I0818 20:20:51.600286  2034 sgd_solver.cpp:165] Iteration 55900, lr = 0.001
I0818 20:21:46.042615  2034 solver.cpp:514] Iteration 56000, Testing net (#0)
I0818 20:22:31.504425  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:22:31.676661  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.929102
I0818 20:22:31.676728  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.271176 (* 1 = 0.271176 loss)
I0818 20:22:32.203547  2034 solver.cpp:357] Iteration 56000 (0.994029 iter/s, 100.601s/100 iters), loss = 0.0135733
I0818 20:22:32.203625  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0135734 (* 1 = 0.0135734 loss)
I0818 20:22:32.203636  2034 sgd_solver.cpp:165] Iteration 56000, lr = 0.001
I0818 20:23:28.063151  2034 solver.cpp:357] Iteration 56100 (1.79028 iter/s, 55.8571s/100 iters), loss = 0.00767734
I0818 20:23:28.063266  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00767743 (* 1 = 0.00767743 loss)
I0818 20:23:28.063275  2034 sgd_solver.cpp:165] Iteration 56100, lr = 0.001
I0818 20:24:23.893954  2034 solver.cpp:357] Iteration 56200 (1.79114 iter/s, 55.8304s/100 iters), loss = 0.0056541
I0818 20:24:23.894165  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00565419 (* 1 = 0.00565419 loss)
I0818 20:24:23.894194  2034 sgd_solver.cpp:165] Iteration 56200, lr = 0.001
I0818 20:24:48.497225  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:25:18.574147  2034 solver.cpp:357] Iteration 56300 (1.82896 iter/s, 54.6758s/100 iters), loss = 0.00798975
I0818 20:25:18.574446  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00798985 (* 1 = 0.00798985 loss)
I0818 20:25:18.574510  2034 sgd_solver.cpp:165] Iteration 56300, lr = 0.001
I0818 20:26:14.445742  2034 solver.cpp:357] Iteration 56400 (1.78996 iter/s, 55.8672s/100 iters), loss = 0.0152893
I0818 20:26:14.445853  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0152894 (* 1 = 0.0152894 loss)
I0818 20:26:14.445863  2034 sgd_solver.cpp:165] Iteration 56400, lr = 0.001
I0818 20:27:09.621933  2034 solver.cpp:514] Iteration 56500, Testing net (#0)
I0818 20:27:49.585860  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:27:49.770010  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.924602
I0818 20:27:49.770067  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.281091 (* 1 = 0.281091 loss)
I0818 20:27:50.222398  2034 solver.cpp:357] Iteration 56500 (1.0441 iter/s, 95.7765s/100 iters), loss = 0.0114673
I0818 20:27:50.222463  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0114674 (* 1 = 0.0114674 loss)
I0818 20:27:50.222476  2034 sgd_solver.cpp:165] Iteration 56500, lr = 0.001
I0818 20:28:42.512936  2034 solver.cpp:357] Iteration 56600 (1.91247 iter/s, 52.2883s/100 iters), loss = 0.0265572
I0818 20:28:42.513216  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0265573 (* 1 = 0.0265573 loss)
I0818 20:28:42.513250  2034 sgd_solver.cpp:165] Iteration 56600, lr = 0.001
I0818 20:29:01.550863  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:29:36.730036  2034 solver.cpp:357] Iteration 56700 (1.84458 iter/s, 54.2129s/100 iters), loss = 0.0125047
I0818 20:29:36.730180  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0125048 (* 1 = 0.0125048 loss)
I0818 20:29:36.730193  2034 sgd_solver.cpp:165] Iteration 56700, lr = 0.001
I0818 20:30:32.409680  2034 solver.cpp:357] Iteration 56800 (1.79612 iter/s, 55.6754s/100 iters), loss = 0.0143666
I0818 20:30:32.409893  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0143667 (* 1 = 0.0143667 loss)
I0818 20:30:32.409922  2034 sgd_solver.cpp:165] Iteration 56800, lr = 0.001
I0818 20:31:28.216707  2034 solver.cpp:357] Iteration 56900 (1.79202 iter/s, 55.8028s/100 iters), loss = 0.0276572
I0818 20:31:28.216866  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0276573 (* 1 = 0.0276573 loss)
I0818 20:31:28.216877  2034 sgd_solver.cpp:165] Iteration 56900, lr = 0.001
I0818 20:32:22.858724  2034 solver.cpp:514] Iteration 57000, Testing net (#0)
I0818 20:33:08.160917  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:33:08.263392  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925002
I0818 20:33:08.263437  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.287134 (* 1 = 0.287134 loss)
I0818 20:33:08.781404  2034 solver.cpp:357] Iteration 57000 (0.994403 iter/s, 100.563s/100 iters), loss = 0.0110663
I0818 20:33:08.781479  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0110664 (* 1 = 0.0110664 loss)
I0818 20:33:08.781491  2034 sgd_solver.cpp:165] Iteration 57000, lr = 0.001
I0818 20:33:23.961307  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:34:04.126750  2034 solver.cpp:357] Iteration 57100 (1.80697 iter/s, 55.3412s/100 iters), loss = 0.0160994
I0818 20:34:04.126982  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0160995 (* 1 = 0.0160995 loss)
I0818 20:34:04.127008  2034 sgd_solver.cpp:165] Iteration 57100, lr = 0.001
I0818 20:34:54.742319  2034 solver.cpp:357] Iteration 57200 (1.97576 iter/s, 50.6135s/100 iters), loss = 0.0235293
I0818 20:34:54.742442  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0235294 (* 1 = 0.0235294 loss)
I0818 20:34:54.742455  2034 sgd_solver.cpp:165] Iteration 57200, lr = 0.001
I0818 20:35:50.144884  2034 solver.cpp:357] Iteration 57300 (1.8051 iter/s, 55.3985s/100 iters), loss = 0.0231451
I0818 20:35:50.145164  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0231452 (* 1 = 0.0231452 loss)
I0818 20:35:50.145221  2034 sgd_solver.cpp:165] Iteration 57300, lr = 0.001
I0818 20:36:46.074676  2034 solver.cpp:357] Iteration 57400 (1.78796 iter/s, 55.9298s/100 iters), loss = 0.0208484
I0818 20:36:46.074811  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0208485 (* 1 = 0.0208485 loss)
I0818 20:36:46.074822  2034 sgd_solver.cpp:165] Iteration 57400, lr = 0.001
I0818 20:36:55.748972  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:37:41.396289  2034 solver.cpp:514] Iteration 57500, Testing net (#0)
I0818 20:38:23.369969  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:38:23.505795  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926802
I0818 20:38:23.505846  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.283589 (* 1 = 0.283589 loss)
I0818 20:38:23.896703  2034 solver.cpp:357] Iteration 57500 (1.02228 iter/s, 97.8203s/100 iters), loss = 0.00946628
I0818 20:38:23.896777  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00946637 (* 1 = 0.00946637 loss)
I0818 20:38:23.896790  2034 sgd_solver.cpp:165] Iteration 57500, lr = 0.001
I0818 20:39:17.873464  2034 solver.cpp:357] Iteration 57600 (1.85279 iter/s, 53.9727s/100 iters), loss = 0.0170183
I0818 20:39:17.873709  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0170184 (* 1 = 0.0170184 loss)
I0818 20:39:17.873723  2034 sgd_solver.cpp:165] Iteration 57600, lr = 0.001
I0818 20:40:13.736320  2034 solver.cpp:357] Iteration 57700 (1.79023 iter/s, 55.8588s/100 iters), loss = 0.0281203
I0818 20:40:13.736446  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0281204 (* 1 = 0.0281204 loss)
I0818 20:40:13.736459  2034 sgd_solver.cpp:165] Iteration 57700, lr = 0.001
I0818 20:41:07.415655  2034 solver.cpp:357] Iteration 57800 (1.86306 iter/s, 53.6753s/100 iters), loss = 0.00987999
I0818 20:41:07.415845  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00988008 (* 1 = 0.00988008 loss)
I0818 20:41:07.415871  2034 sgd_solver.cpp:165] Iteration 57800, lr = 0.001
I0818 20:41:11.547039  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:41:57.991736  2034 solver.cpp:357] Iteration 57900 (1.97718 iter/s, 50.5771s/100 iters), loss = 0.0136769
I0818 20:41:57.991824  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.013677 (* 1 = 0.013677 loss)
I0818 20:41:57.991835  2034 sgd_solver.cpp:165] Iteration 57900, lr = 0.001
I0818 20:42:53.277410  2034 solver.cpp:514] Iteration 58000, Testing net (#0)
I0818 20:43:38.367491  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:43:38.603783  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926102
I0818 20:43:38.603842  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.286769 (* 1 = 0.286769 loss)
I0818 20:43:39.072095  2034 solver.cpp:357] Iteration 58000 (0.989298 iter/s, 101.082s/100 iters), loss = 0.0115032
I0818 20:43:39.072157  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0115032 (* 1 = 0.0115032 loss)
I0818 20:43:39.072168  2034 sgd_solver.cpp:165] Iteration 58000, lr = 0.001
I0818 20:44:34.866595  2034 solver.cpp:357] Iteration 58100 (1.7923 iter/s, 55.7941s/100 iters), loss = 0.00696443
I0818 20:44:34.866785  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00696452 (* 1 = 0.00696452 loss)
I0818 20:44:34.866797  2034 sgd_solver.cpp:165] Iteration 58100, lr = 0.001
I0818 20:45:30.310442  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:45:30.659524  2034 solver.cpp:357] Iteration 58200 (1.79236 iter/s, 55.7922s/100 iters), loss = 0.00918956
I0818 20:45:30.659597  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00918965 (* 1 = 0.00918965 loss)
I0818 20:45:30.659608  2034 sgd_solver.cpp:165] Iteration 58200, lr = 0.001
I0818 20:46:26.563491  2034 solver.cpp:357] Iteration 58300 (1.7888 iter/s, 55.9033s/100 iters), loss = 0.0110705
I0818 20:46:26.563657  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0110706 (* 1 = 0.0110706 loss)
I0818 20:46:26.563668  2034 sgd_solver.cpp:165] Iteration 58300, lr = 0.001
I0818 20:47:22.343770  2034 solver.cpp:357] Iteration 58400 (1.79277 iter/s, 55.7795s/100 iters), loss = 0.0159668
I0818 20:47:22.343924  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0159669 (* 1 = 0.0159669 loss)
I0818 20:47:22.343935  2034 sgd_solver.cpp:165] Iteration 58400, lr = 0.001
I0818 20:48:13.859856  2034 solver.cpp:514] Iteration 58500, Testing net (#0)
I0818 20:48:42.711880  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:48:42.838846  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925202
I0818 20:48:42.839192  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.291816 (* 1 = 0.291816 loss)
I0818 20:48:43.284628  2034 solver.cpp:357] Iteration 58500 (1.23547 iter/s, 80.9406s/100 iters), loss = 0.0163038
I0818 20:48:43.285044  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0163039 (* 1 = 0.0163039 loss)
I0818 20:48:43.285223  2034 sgd_solver.cpp:165] Iteration 58500, lr = 0.001
I0818 20:49:32.728515  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:49:38.818485  2034 solver.cpp:357] Iteration 58600 (1.80079 iter/s, 55.5311s/100 iters), loss = 0.00854296
I0818 20:49:38.818560  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00854305 (* 1 = 0.00854305 loss)
I0818 20:49:38.818574  2034 sgd_solver.cpp:165] Iteration 58600, lr = 0.001
I0818 20:50:34.599704  2034 solver.cpp:357] Iteration 58700 (1.79282 iter/s, 55.7781s/100 iters), loss = 0.0152723
I0818 20:50:34.603034  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0152724 (* 1 = 0.0152724 loss)
I0818 20:50:34.603139  2034 sgd_solver.cpp:165] Iteration 58700, lr = 0.001
I0818 20:51:30.810420  2034 solver.cpp:357] Iteration 58800 (1.77912 iter/s, 56.2075s/100 iters), loss = 0.0186169
I0818 20:51:30.818876  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186169 (* 1 = 0.0186169 loss)
I0818 20:51:30.818919  2034 sgd_solver.cpp:165] Iteration 58800, lr = 0.001
I0818 20:52:26.530416  2034 solver.cpp:357] Iteration 58900 (1.79502 iter/s, 55.7096s/100 iters), loss = 0.010244
I0818 20:52:26.534831  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0102441 (* 1 = 0.0102441 loss)
I0818 20:52:26.534880  2034 sgd_solver.cpp:165] Iteration 58900, lr = 0.001
I0818 20:53:11.308725  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:53:21.803427  2034 solver.cpp:514] Iteration 59000, Testing net (#0)
I0818 20:54:04.954335  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:54:05.214220  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926402
I0818 20:54:05.226851  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.288351 (* 1 = 0.288351 loss)
I0818 20:54:05.588598  2034 solver.cpp:357] Iteration 59000 (1.00956 iter/s, 99.0534s/100 iters), loss = 0.00422087
I0818 20:54:05.588789  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00422096 (* 1 = 0.00422096 loss)
I0818 20:54:05.588836  2034 sgd_solver.cpp:165] Iteration 59000, lr = 0.001
I0818 20:55:00.972484  2034 solver.cpp:357] Iteration 59100 (1.80569 iter/s, 55.3805s/100 iters), loss = 0.0263402
I0818 20:55:00.972739  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0263403 (* 1 = 0.0263403 loss)
I0818 20:55:00.972786  2034 sgd_solver.cpp:165] Iteration 59100, lr = 0.001
I0818 20:55:48.820407  2034 solver.cpp:357] Iteration 59200 (2.09011 iter/s, 47.8444s/100 iters), loss = 0.0062916
I0818 20:55:48.820622  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00629169 (* 1 = 0.00629169 loss)
I0818 20:55:48.820667  2034 sgd_solver.cpp:165] Iteration 59200, lr = 0.001
I0818 20:56:44.867205  2034 solver.cpp:357] Iteration 59300 (1.7842 iter/s, 56.0474s/100 iters), loss = 0.00941597
I0818 20:56:44.867401  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00941607 (* 1 = 0.00941607 loss)
I0818 20:56:44.867507  2034 sgd_solver.cpp:165] Iteration 59300, lr = 0.001
I0818 20:57:23.412241  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:57:39.440253  2034 solver.cpp:357] Iteration 59400 (1.83252 iter/s, 54.5695s/100 iters), loss = 0.0395405
I0818 20:57:39.440541  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0395406 (* 1 = 0.0395406 loss)
I0818 20:57:39.440558  2034 sgd_solver.cpp:165] Iteration 59400, lr = 0.001
I0818 20:58:34.877960  2034 solver.cpp:514] Iteration 59500, Testing net (#0)
I0818 20:59:19.946805  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 20:59:20.133661  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926202
I0818 20:59:20.133708  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.288498 (* 1 = 0.288498 loss)
I0818 20:59:20.487357  2034 solver.cpp:357] Iteration 59500 (0.989644 iter/s, 101.046s/100 iters), loss = 0.00430291
I0818 20:59:20.487432  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00430301 (* 1 = 0.00430301 loss)
I0818 20:59:20.487443  2034 sgd_solver.cpp:165] Iteration 59500, lr = 0.001
I0818 21:00:16.293002  2034 solver.cpp:357] Iteration 59600 (1.79198 iter/s, 55.8042s/100 iters), loss = 0.0105777
I0818 21:00:16.293215  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0105778 (* 1 = 0.0105778 loss)
I0818 21:00:16.293228  2034 sgd_solver.cpp:165] Iteration 59600, lr = 0.001
I0818 21:01:12.086000  2034 solver.cpp:357] Iteration 59700 (1.79232 iter/s, 55.7935s/100 iters), loss = 0.0138383
I0818 21:01:12.086184  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138384 (* 1 = 0.0138384 loss)
I0818 21:01:12.086194  2034 sgd_solver.cpp:165] Iteration 59700, lr = 0.001
I0818 21:01:46.675174  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:02:03.786248  2034 solver.cpp:357] Iteration 59800 (1.93436 iter/s, 51.6966s/100 iters), loss = 0.00576665
I0818 21:02:03.786641  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00576675 (* 1 = 0.00576675 loss)
I0818 21:02:03.786844  2034 sgd_solver.cpp:165] Iteration 59800, lr = 0.001
I0818 21:02:55.212880  2034 solver.cpp:357] Iteration 59900 (1.94451 iter/s, 51.4269s/100 iters), loss = 0.0145959
I0818 21:02:55.218823  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.014596 (* 1 = 0.014596 loss)
I0818 21:02:55.218863  2034 sgd_solver.cpp:165] Iteration 59900, lr = 0.001
I0818 21:03:51.026541  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_60000.caffemodel
I0818 21:03:51.052500  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_60000.solverstate
I0818 21:03:51.057883  2034 solver.cpp:514] Iteration 60000, Testing net (#0)
I0818 21:04:35.060489  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:04:35.198678  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925802
I0818 21:04:35.198901  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.291347 (* 1 = 0.291347 loss)
I0818 21:04:35.640803  2034 solver.cpp:357] Iteration 60000 (0.995792 iter/s, 100.423s/100 iters), loss = 0.0133423
I0818 21:04:35.640990  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0133424 (* 1 = 0.0133424 loss)
I0818 21:04:35.641036  2034 sgd_solver.cpp:165] Iteration 60000, lr = 0.001
I0818 21:05:32.561228  2034 solver.cpp:357] Iteration 60100 (1.75689 iter/s, 56.9189s/100 iters), loss = 0.0249973
I0818 21:05:32.565749  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0249974 (* 1 = 0.0249974 loss)
I0818 21:05:32.565874  2034 sgd_solver.cpp:165] Iteration 60100, lr = 0.001
I0818 21:06:01.770376  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:06:28.541746  2034 solver.cpp:357] Iteration 60200 (1.78653 iter/s, 55.9746s/100 iters), loss = 0.0112896
I0818 21:06:28.546836  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0112897 (* 1 = 0.0112897 loss)
I0818 21:06:28.546891  2034 sgd_solver.cpp:165] Iteration 60200, lr = 0.001
I0818 21:07:22.399170  2034 solver.cpp:357] Iteration 60300 (1.85696 iter/s, 53.8515s/100 iters), loss = 0.00472717
I0818 21:07:22.402778  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00472725 (* 1 = 0.00472725 loss)
I0818 21:07:22.402828  2034 sgd_solver.cpp:165] Iteration 60300, lr = 0.001
I0818 21:08:19.038187  2034 solver.cpp:357] Iteration 60400 (1.76568 iter/s, 56.6354s/100 iters), loss = 0.00594544
I0818 21:08:19.043059  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00594553 (* 1 = 0.00594553 loss)
I0818 21:08:19.043252  2034 sgd_solver.cpp:165] Iteration 60400, lr = 0.001
I0818 21:09:10.980049  2034 solver.cpp:514] Iteration 60500, Testing net (#0)
I0818 21:09:54.153656  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:09:54.367396  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.925802
I0818 21:09:54.367512  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290599 (* 1 = 0.290599 loss)
I0818 21:09:54.766717  2034 solver.cpp:357] Iteration 60500 (1.04469 iter/s, 95.7224s/100 iters), loss = 0.00904551
I0818 21:09:54.766893  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0090456 (* 1 = 0.0090456 loss)
I0818 21:09:54.766922  2034 sgd_solver.cpp:165] Iteration 60500, lr = 0.001
I0818 21:10:18.976491  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:10:51.498090  2034 solver.cpp:357] Iteration 60600 (1.76279 iter/s, 56.7283s/100 iters), loss = 0.0100022
I0818 21:10:51.498689  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0100023 (* 1 = 0.0100023 loss)
I0818 21:10:51.498875  2034 sgd_solver.cpp:165] Iteration 60600, lr = 0.001
I0818 21:11:47.855851  2034 solver.cpp:357] Iteration 60700 (1.77449 iter/s, 56.3541s/100 iters), loss = 0.00700893
I0818 21:11:47.858793  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00700902 (* 1 = 0.00700902 loss)
I0818 21:11:47.858832  2034 sgd_solver.cpp:165] Iteration 60700, lr = 0.001
I0818 21:12:44.123203  2034 solver.cpp:357] Iteration 60800 (1.77735 iter/s, 56.2637s/100 iters), loss = 0.0059315
I0818 21:12:44.124274  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00593159 (* 1 = 0.00593159 loss)
I0818 21:12:44.124327  2034 sgd_solver.cpp:165] Iteration 60800, lr = 0.001
I0818 21:13:40.323309  2034 solver.cpp:357] Iteration 60900 (1.77937 iter/s, 56.1996s/100 iters), loss = 0.00958195
I0818 21:13:40.326802  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00958204 (* 1 = 0.00958204 loss)
I0818 21:13:40.326843  2034 sgd_solver.cpp:165] Iteration 60900, lr = 0.001
I0818 21:13:59.509541  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:14:36.529340  2034 solver.cpp:514] Iteration 61000, Testing net (#0)
I0818 21:15:19.681012  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:15:19.915591  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927202
I0818 21:15:19.915704  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.283227 (* 1 = 0.283227 loss)
I0818 21:15:20.334679  2034 solver.cpp:357] Iteration 61000 (0.999918 iter/s, 100.008s/100 iters), loss = 0.00929939
I0818 21:15:20.334856  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00929947 (* 1 = 0.00929947 loss)
I0818 21:15:20.334900  2034 sgd_solver.cpp:165] Iteration 61000, lr = 0.001
I0818 21:16:04.916218  2034 solver.cpp:357] Iteration 61100 (2.24309 iter/s, 44.5813s/100 iters), loss = 0.018658
I0818 21:16:04.921941  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0186581 (* 1 = 0.0186581 loss)
I0818 21:16:04.922044  2034 sgd_solver.cpp:165] Iteration 61100, lr = 0.001
I0818 21:16:57.048244  2034 solver.cpp:357] Iteration 61200 (1.91817 iter/s, 52.1331s/100 iters), loss = 0.00486797
I0818 21:16:57.048468  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00486804 (* 1 = 0.00486804 loss)
I0818 21:16:57.048563  2034 sgd_solver.cpp:165] Iteration 61200, lr = 0.001
I0818 21:17:53.905414  2034 solver.cpp:357] Iteration 61300 (1.75871 iter/s, 56.8597s/100 iters), loss = 0.00915947
I0818 21:17:53.905722  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00915954 (* 1 = 0.00915954 loss)
I0818 21:17:53.905772  2034 sgd_solver.cpp:165] Iteration 61300, lr = 0.001
I0818 21:18:07.960180  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:18:50.994184  2034 solver.cpp:357] Iteration 61400 (1.75153 iter/s, 57.0929s/100 iters), loss = 0.0141977
I0818 21:18:50.998873  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0141978 (* 1 = 0.0141978 loss)
I0818 21:18:50.998922  2034 sgd_solver.cpp:165] Iteration 61400, lr = 0.001
I0818 21:19:47.487439  2034 solver.cpp:514] Iteration 61500, Testing net (#0)
I0818 21:20:31.733873  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:20:31.878842  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927102
I0818 21:20:31.878978  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.287489 (* 1 = 0.287489 loss)
I0818 21:20:32.416008  2034 solver.cpp:357] Iteration 61500 (0.985967 iter/s, 101.423s/100 iters), loss = 0.00570628
I0818 21:20:32.416146  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00570635 (* 1 = 0.00570635 loss)
I0818 21:20:32.416175  2034 sgd_solver.cpp:165] Iteration 61500, lr = 0.001
I0818 21:21:28.600908  2034 solver.cpp:357] Iteration 61600 (1.77975 iter/s, 56.1877s/100 iters), loss = 0.0176324
I0818 21:21:28.601083  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0176325 (* 1 = 0.0176325 loss)
I0818 21:21:28.601166  2034 sgd_solver.cpp:165] Iteration 61600, lr = 0.001
I0818 21:22:24.156802  2034 solver.cpp:357] Iteration 61700 (1.79985 iter/s, 55.5603s/100 iters), loss = 0.00803429
I0818 21:22:24.166811  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00803436 (* 1 = 0.00803436 loss)
I0818 21:22:24.166851  2034 sgd_solver.cpp:165] Iteration 61700, lr = 0.001
I0818 21:22:30.772930  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:23:12.516039  2034 solver.cpp:357] Iteration 61800 (2.06822 iter/s, 48.3508s/100 iters), loss = 0.0122033
I0818 21:23:12.516279  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122034 (* 1 = 0.0122034 loss)
I0818 21:23:12.516314  2034 sgd_solver.cpp:165] Iteration 61800, lr = 0.001
I0818 21:24:08.312912  2034 solver.cpp:357] Iteration 61900 (1.79215 iter/s, 55.7988s/100 iters), loss = 0.0101473
I0818 21:24:08.313158  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0101473 (* 1 = 0.0101473 loss)
I0818 21:24:08.313205  2034 sgd_solver.cpp:165] Iteration 61900, lr = 0.001
I0818 21:25:03.788759  2034 solver.cpp:514] Iteration 62000, Testing net (#0)
I0818 21:25:47.320667  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:25:47.473199  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926402
I0818 21:25:47.473265  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.29034 (* 1 = 0.29034 loss)
I0818 21:25:47.944320  2034 solver.cpp:357] Iteration 62000 (1.00363 iter/s, 99.6379s/100 iters), loss = 0.0339137
I0818 21:25:47.944386  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0339138 (* 1 = 0.0339138 loss)
I0818 21:25:47.944398  2034 sgd_solver.cpp:165] Iteration 62000, lr = 0.001
I0818 21:26:40.711273  2034 solver.cpp:357] Iteration 62100 (1.89509 iter/s, 52.7681s/100 iters), loss = 0.00680903
I0818 21:26:40.711422  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00680911 (* 1 = 0.00680911 loss)
I0818 21:26:40.711434  2034 sgd_solver.cpp:165] Iteration 62100, lr = 0.001
I0818 21:26:43.656610  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:27:36.457597  2034 solver.cpp:357] Iteration 62200 (1.79381 iter/s, 55.7474s/100 iters), loss = 0.00724446
I0818 21:27:36.457713  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00724453 (* 1 = 0.00724453 loss)
I0818 21:27:36.457726  2034 sgd_solver.cpp:165] Iteration 62200, lr = 0.001
I0818 21:28:32.268899  2034 solver.cpp:357] Iteration 62300 (1.79172 iter/s, 55.8122s/100 iters), loss = 0.0215744
I0818 21:28:32.274958  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0215744 (* 1 = 0.0215744 loss)
I0818 21:28:32.275089  2034 sgd_solver.cpp:165] Iteration 62300, lr = 0.001
I0818 21:29:27.430955  2034 solver.cpp:357] Iteration 62400 (1.81303 iter/s, 55.1564s/100 iters), loss = 0.00982432
I0818 21:29:27.434795  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00982439 (* 1 = 0.00982439 loss)
I0818 21:29:27.434837  2034 sgd_solver.cpp:165] Iteration 62400, lr = 0.001
I0818 21:30:14.181213  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:30:16.197456  2034 solver.cpp:514] Iteration 62500, Testing net (#0)
I0818 21:31:00.895756  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:31:01.075507  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926502
I0818 21:31:01.075866  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.295303 (* 1 = 0.295303 loss)
I0818 21:31:01.450412  2034 solver.cpp:357] Iteration 62500 (1.06361 iter/s, 94.0198s/100 iters), loss = 0.0138254
I0818 21:31:01.450811  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0138255 (* 1 = 0.0138255 loss)
I0818 21:31:01.450987  2034 sgd_solver.cpp:165] Iteration 62500, lr = 0.001
I0818 21:31:58.054064  2034 solver.cpp:357] Iteration 62600 (1.7666 iter/s, 56.6058s/100 iters), loss = 0.0180874
I0818 21:31:58.054258  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0180875 (* 1 = 0.0180875 loss)
I0818 21:31:58.054286  2034 sgd_solver.cpp:165] Iteration 62600, lr = 0.001
I0818 21:32:54.697808  2034 solver.cpp:357] Iteration 62700 (1.76535 iter/s, 56.6461s/100 iters), loss = 0.0243684
I0818 21:32:54.698101  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0243685 (* 1 = 0.0243685 loss)
I0818 21:32:54.698148  2034 sgd_solver.cpp:165] Iteration 62700, lr = 0.001
I0818 21:33:50.550161  2034 solver.cpp:357] Iteration 62800 (1.79043 iter/s, 55.8526s/100 iters), loss = 0.00974275
I0818 21:33:50.550279  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00974282 (* 1 = 0.00974282 loss)
I0818 21:33:50.550292  2034 sgd_solver.cpp:165] Iteration 62800, lr = 0.001
I0818 21:34:38.499359  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:34:46.180824  2034 solver.cpp:357] Iteration 62900 (1.7975 iter/s, 55.6329s/100 iters), loss = 0.0231221
I0818 21:34:46.181006  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0231222 (* 1 = 0.0231222 loss)
I0818 21:34:46.181052  2034 sgd_solver.cpp:165] Iteration 62900, lr = 0.001
I0818 21:35:40.380182  2034 solver.cpp:514] Iteration 63000, Testing net (#0)
I0818 21:36:22.103046  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:36:22.291097  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.927802
I0818 21:36:22.291270  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.290036 (* 1 = 0.290036 loss)
I0818 21:36:22.733289  2034 solver.cpp:357] Iteration 63000 (1.03569 iter/s, 96.5543s/100 iters), loss = 0.0149184
I0818 21:36:22.733451  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0149184 (* 1 = 0.0149184 loss)
I0818 21:36:22.733496  2034 sgd_solver.cpp:165] Iteration 63000, lr = 0.001
I0818 21:37:12.100924  2034 solver.cpp:357] Iteration 63100 (2.02555 iter/s, 49.3694s/100 iters), loss = 0.00347229
I0818 21:37:12.106819  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00347234 (* 1 = 0.00347234 loss)
I0818 21:37:12.106859  2034 sgd_solver.cpp:165] Iteration 63100, lr = 0.001
I0818 21:38:08.413300  2034 solver.cpp:357] Iteration 63200 (1.77601 iter/s, 56.3059s/100 iters), loss = 0.0122549
I0818 21:38:08.418884  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.0122549 (* 1 = 0.0122549 loss)
I0818 21:38:08.418937  2034 sgd_solver.cpp:165] Iteration 63200, lr = 0.001
I0818 21:38:52.152034  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:39:04.584468  2034 solver.cpp:357] Iteration 63300 (1.78048 iter/s, 56.1647s/100 iters), loss = 0.00893236
I0818 21:39:04.584916  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00893242 (* 1 = 0.00893242 loss)
I0818 21:39:04.585131  2034 sgd_solver.cpp:165] Iteration 63300, lr = 0.001
I0818 21:39:59.816624  2034 solver.cpp:357] Iteration 63400 (1.81061 iter/s, 55.23s/100 iters), loss = 0.0066877
I0818 21:39:59.816785  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00668776 (* 1 = 0.00668776 loss)
I0818 21:39:59.816798  2034 sgd_solver.cpp:165] Iteration 63400, lr = 0.001
I0818 21:40:54.948546  2034 solver.cpp:514] Iteration 63500, Testing net (#0)
I0818 21:41:40.659381  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:41:40.852958  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926102
I0818 21:41:40.853018  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.295473 (* 1 = 0.295473 loss)
I0818 21:41:41.283238  2034 solver.cpp:357] Iteration 63500 (0.985531 iter/s, 101.468s/100 iters), loss = 0.00394066
I0818 21:41:41.283308  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00394072 (* 1 = 0.00394072 loss)
I0818 21:41:41.283321  2034 sgd_solver.cpp:165] Iteration 63500, lr = 0.001
I0818 21:42:37.103165  2034 solver.cpp:357] Iteration 63600 (1.79148 iter/s, 55.8197s/100 iters), loss = 0.00424248
I0818 21:42:37.103699  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00424254 (* 1 = 0.00424254 loss)
I0818 21:42:37.103726  2034 sgd_solver.cpp:165] Iteration 63600, lr = 0.001
I0818 21:43:12.995100  2092 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:43:29.566527  2034 solver.cpp:357] Iteration 63700 (1.90611 iter/s, 52.463s/100 iters), loss = 0.00682132
I0818 21:43:29.566649  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00682138 (* 1 = 0.00682138 loss)
I0818 21:43:29.566680  2034 sgd_solver.cpp:165] Iteration 63700, lr = 0.001
I0818 21:44:22.743806  2034 solver.cpp:357] Iteration 63800 (1.88044 iter/s, 53.1789s/100 iters), loss = 0.00549157
I0818 21:44:22.743993  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00549163 (* 1 = 0.00549163 loss)
I0818 21:44:22.744005  2034 sgd_solver.cpp:165] Iteration 63800, lr = 0.001
I0818 21:45:15.976227  2034 solver.cpp:357] Iteration 63900 (1.87857 iter/s, 53.232s/100 iters), loss = 0.00506804
I0818 21:45:15.976373  2034 solver.cpp:376]     Train net output #0: SoftmaxWithLoss1 = 0.00506809 (* 1 = 0.00506809 loss)
I0818 21:45:15.976384  2034 sgd_solver.cpp:165] Iteration 63900, lr = 0.001
I0818 21:46:09.209597  2034 solver.cpp:671] Snapshotting to binary proto file ./model_save/cifar10_ResNet_32_iter_64000.caffemodel
I0818 21:46:09.227190  2034 sgd_solver.cpp:372] Snapshotting solver state to binary proto file ./model_save/cifar10_ResNet_32_iter_64000.solverstate
I0818 21:46:09.314083  2034 solver.cpp:472] Iteration 64000, loss = 0.00712092
I0818 21:46:09.314131  2034 solver.cpp:514] Iteration 64000, Testing net (#0)
I0818 21:46:54.067343  2094 data_layer.cpp:73] Restarting data prefetching from start.
I0818 21:46:54.104872  2034 solver.cpp:580]     Test net output #0: Accuracy1 = 0.926302
I0818 21:46:54.105048  2034 solver.cpp:580]     Test net output #1: SoftmaxWithLoss1 = 0.291297 (* 1 = 0.291297 loss)
I0818 21:46:54.105089  2034 solver.cpp:479] Optimization Done.
I0818 21:46:54.117859  2034 caffe.cpp:326] Optimization Done.
